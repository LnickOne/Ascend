# 1 "/home/lnick/GitHub/Ascend/AscendC/lesson_01/0_introduction/0_helloworld/hello_world.cpp"
# 1 "<built-in>" 1
# 1 "<built-in>" 3
# 412 "<built-in>" 3
# 1 "<command line>" 1
# 1 "<built-in>" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 1 3
# 17 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 1 3
# 35 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long int ptrdiff_t;
# 46 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 3
typedef long unsigned int size_t;
# 102 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 3
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 1 3
# 19 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__stddef_max_align_t.h" 3
typedef struct {
  long long __clang_max_align_nonce1
      __attribute__((__aligned__(__alignof__(long long))));
  long double __clang_max_align_nonce2
      __attribute__((__aligned__(__alignof__(long double))));
} max_align_t;
# 103 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 2 3
# 18 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdint.h" 1 3
# 52 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdint.h" 3
# 1 "/usr/include/stdint.h" 1 3 4
# 26 "/usr/include/stdint.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 1 3 4
# 33 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 3 4
# 1 "/usr/include/features.h" 1 3 4
# 392 "/usr/include/features.h" 3 4
# 1 "/usr/include/features-time64.h" 1 3 4
# 20 "/usr/include/features-time64.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 21 "/usr/include/features-time64.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 1 3 4
# 19 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 20 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 2 3 4
# 22 "/usr/include/features-time64.h" 2 3 4
# 393 "/usr/include/features.h" 2 3 4
# 464 "/usr/include/features.h" 3 4
# 1 "/usr/include/stdc-predef.h" 1 3 4
# 465 "/usr/include/features.h" 2 3 4
# 486 "/usr/include/features.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 1 3 4
# 559 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 560 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/long-double.h" 1 3 4
# 561 "/usr/include/x86_64-linux-gnu/sys/cdefs.h" 2 3 4
# 487 "/usr/include/features.h" 2 3 4
# 510 "/usr/include/features.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 1 3 4
# 10 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/gnu/stubs-64.h" 1 3 4
# 11 "/usr/include/x86_64-linux-gnu/gnu/stubs.h" 2 3 4
# 511 "/usr/include/features.h" 2 3 4
# 34 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 2 3 4
# 27 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types.h" 1 3 4
# 27 "/usr/include/x86_64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 28 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 1 3 4
# 19 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 20 "/usr/include/x86_64-linux-gnu/bits/timesize.h" 2 3 4
# 29 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned char __u_char;
typedef unsigned short int __u_short;
typedef unsigned int __u_int;
typedef unsigned long int __u_long;


typedef signed char __int8_t;
typedef unsigned char __uint8_t;
typedef signed short int __int16_t;
typedef unsigned short int __uint16_t;
typedef signed int __int32_t;
typedef unsigned int __uint32_t;

typedef signed long int __int64_t;
typedef unsigned long int __uint64_t;






typedef __int8_t __int_least8_t;
typedef __uint8_t __uint_least8_t;
typedef __int16_t __int_least16_t;
typedef __uint16_t __uint_least16_t;
typedef __int32_t __int_least32_t;
typedef __uint32_t __uint_least32_t;
typedef __int64_t __int_least64_t;
typedef __uint64_t __uint_least64_t;



typedef long int __quad_t;
typedef unsigned long int __u_quad_t;







typedef long int __intmax_t;
typedef unsigned long int __uintmax_t;
# 141 "/usr/include/x86_64-linux-gnu/bits/types.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/typesizes.h" 1 3 4
# 142 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/time64.h" 1 3 4
# 143 "/usr/include/x86_64-linux-gnu/bits/types.h" 2 3 4


typedef unsigned long int __dev_t;
typedef unsigned int __uid_t;
typedef unsigned int __gid_t;
typedef unsigned long int __ino_t;
typedef unsigned long int __ino64_t;
typedef unsigned int __mode_t;
typedef unsigned long int __nlink_t;
typedef long int __off_t;
typedef long int __off64_t;
typedef int __pid_t;
typedef struct { int __val[2]; } __fsid_t;
typedef long int __clock_t;
typedef unsigned long int __rlim_t;
typedef unsigned long int __rlim64_t;
typedef unsigned int __id_t;
typedef long int __time_t;
typedef unsigned int __useconds_t;
typedef long int __suseconds_t;
typedef long int __suseconds64_t;

typedef int __daddr_t;
typedef int __key_t;


typedef int __clockid_t;


typedef void * __timer_t;


typedef long int __blksize_t;




typedef long int __blkcnt_t;
typedef long int __blkcnt64_t;


typedef unsigned long int __fsblkcnt_t;
typedef unsigned long int __fsblkcnt64_t;


typedef unsigned long int __fsfilcnt_t;
typedef unsigned long int __fsfilcnt64_t;


typedef long int __fsword_t;

typedef long int __ssize_t;


typedef long int __syscall_slong_t;

typedef unsigned long int __syscall_ulong_t;



typedef __off64_t __loff_t;
typedef char *__caddr_t;


typedef long int __intptr_t;


typedef unsigned int __socklen_t;




typedef int __sig_atomic_t;
# 28 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wchar.h" 1 3 4
# 29 "/usr/include/stdint.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/wordsize.h" 1 3 4
# 30 "/usr/include/stdint.h" 2 3 4




# 1 "/usr/include/x86_64-linux-gnu/bits/stdint-intn.h" 1 3 4
# 24 "/usr/include/x86_64-linux-gnu/bits/stdint-intn.h" 3 4
typedef __int8_t int8_t;
typedef __int16_t int16_t;
typedef __int32_t int32_t;
typedef __int64_t int64_t;
# 35 "/usr/include/stdint.h" 2 3 4


# 1 "/usr/include/x86_64-linux-gnu/bits/stdint-uintn.h" 1 3 4
# 24 "/usr/include/x86_64-linux-gnu/bits/stdint-uintn.h" 3 4
typedef __uint8_t uint8_t;
typedef __uint16_t uint16_t;
typedef __uint32_t uint32_t;
typedef __uint64_t uint64_t;
# 38 "/usr/include/stdint.h" 2 3 4





typedef __int_least8_t int_least8_t;
typedef __int_least16_t int_least16_t;
typedef __int_least32_t int_least32_t;
typedef __int_least64_t int_least64_t;


typedef __uint_least8_t uint_least8_t;
typedef __uint_least16_t uint_least16_t;
typedef __uint_least32_t uint_least32_t;
typedef __uint_least64_t uint_least64_t;





typedef signed char int_fast8_t;

typedef long int int_fast16_t;
typedef long int int_fast32_t;
typedef long int int_fast64_t;
# 71 "/usr/include/stdint.h" 3 4
typedef unsigned char uint_fast8_t;

typedef unsigned long int uint_fast16_t;
typedef unsigned long int uint_fast32_t;
typedef unsigned long int uint_fast64_t;
# 87 "/usr/include/stdint.h" 3 4
typedef long int intptr_t;


typedef unsigned long int uintptr_t;
# 101 "/usr/include/stdint.h" 3 4
typedef __intmax_t intmax_t;
typedef __uintmax_t uintmax_t;
# 53 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdint.h" 2 3
# 19 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3






# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 1 3
# 10 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_defines.h" 1 3
# 11 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3
# 33 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
typedef __bf16 bfloat16_t;
# 43 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
struct int4x2_t {
  char val;

  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t(){};


  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t(int4x2_t & x){
    val = x.val;
  };


  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t(__attribute__((cce_global)) int4x2_t & x){
    val = x.val;
  };


  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t(__attribute__((cce_unif_buff)) int4x2_t & x){
    val = x.val;
  };


  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t(const char lo, const char hi) {
    val = (lo & 0xf) & ((hi & 0xf) << 4);
  }


  [aicore] __inline__ __attribute__((cce_builtin_api, always_inline)) int4x2_t &operator=(const int4x2_t &x) {
    if (this == &x) {
      return *this;
    }
    val = x.val;
    return *this;
  }
};
# 106 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
namespace bisheng {
struct [[bisheng::mangling_hint(BFloat16)]] __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 { __BISHENG_BUILTIN_TYPE_PROXY_BFloat16() = delete; __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 (const __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_BFloat16 (__BISHENG_BUILTIN_TYPE_PROXY_BFloat16 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_BFloat16(int x) {} private: char Storage[2]; };
struct [[bisheng::mangling_hint(HFloat8)]] __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 { __BISHENG_BUILTIN_TYPE_PROXY_HFloat8() = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 (const __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat8 (__BISHENG_BUILTIN_TYPE_PROXY_HFloat8 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat8(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(HFloat4x2)]] __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 { __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 (const __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 (__BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_HFloat4x2(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float8_E4M3)]] __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 { __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 (const __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 (__BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E4M3(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float8_E5M2)]] __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 { __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 (const __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 (__BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E5M2(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float8_E8M0)]] __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0 { __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0 (const __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0 (__BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float8_E8M0(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float4_E2M1x2)]] __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2 { __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2 (const __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2 (__BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E2M1x2(int x) {} private: char Storage[1]; };
struct [[bisheng::mangling_hint(Float4_E1M2x2)]] __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2 { __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2() = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2 (const __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2 &) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2 (__BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2 &&) = delete; __BISHENG_BUILTIN_TYPE_PROXY_Float4_E1M2x2(int x) {} private: char Storage[1]; };
}
# 234 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 3
# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 1 3




typedef enum {

  CRFMODE_NONE = 0,
  CRFMODE_DEQSCALE_VDEQ8 = 8,
  CRFMODE_DEQSCALE_DEQ8 = 9,
  CRFMODE_DEQSCALE_VDEQ16 = 10,
  CRFMODE_DEQSCALE_DEQ16 = 11,
  CRFMODE_DEQSCALE_VDEQS16 = 12,
  CRFMODE_DEQSCALE_DEQS16 = 13,
  CRFMODE_DEQSCALE_VDEQ2S16 = 14,
  CRFMODE_DEQSCALE_DEQ2S16 = 15,

} ConvReluFix_t;

typedef enum {


  CRMODE_NONE = 0,
  CRMODE_F32toF16_NONE = 1,
  CRMODE_F32toF16_RELU = 2,
  CRMODE_S32toF16_NONE = 3,
  CRMODE_F16toF32_NONE = 4,
  CRMODE_NONE_RELU = 5,
  CRMODE_F16_MUL = 6,
  CRMODE_S32toF16_DEQSCALE_SPR = 7,
  CRMODE_DEQSCALE_VDEQ8 = 8,
  CRMODE_DEQSCALE_DEQ8 = 9,
  CRMODE_DEQSCALE_VDEQ16 = 10,
  CRMODE_DEQSCALE_DEQ16 = 11,
  CRMODE_DEQSCALE_VDEQS16 = 12,
  CRMODE_DEQSCALE_DEQS16 = 13,

} ConvRelu_t;

typedef enum {

  NoConversion = 0,
  CvtMode1 = 1,
  CvtMode2 = 2,

} CvtMode_t;

typedef enum {

  DI_featuremap = 0,
  DI_others = 1,

} DI_t;

typedef enum {


  DUAL_MODE0 = 0,
  DUAL_MODE1 = 1,
  DUAL_MODE2 = 2,
  DUAL_MODE3 = 3,
  DUAL_MODE4 = 4,

} DualMode_t;

typedef enum {

  MD_GM2UB = 0,
  MD_GM2L1,
  MD_GM2L0A,
  MD_GM2L0B,
  MD_UB2GM,
  MD_UB2UB,
  MD_L12GM,
  MD_L12L0A,
  MD_L12L0B,
  MD_L12L0C,
  MD_L12UB,
  MD_L12BT,
  MD_L12FB,
  MD_L0C2GM,
  MD_L0C2L1,

} MemoryDirection_t;

typedef enum {

    VALUE_INDEX = 0,
    INDEX_VALUE = 1,
    ONLY_VALUE = 2,
    ONLY_INDEX = 3,

} Order_t;

typedef enum {
# 108 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
} PadFuncMode_t;

typedef enum {


  NoPooling = 0,
  AVGPooling = 1,
  MAXPooling = 2,
  GAVGPooling = 3,

} Pool_t;

typedef enum {
# 139 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
} QMode_t;

typedef enum {

  NoConv= 0,
  VQS162B8_POST = 1,
  QS162B8_POST = 2,
  VQF162B8_POST = 3,
  QF162B8_POST = 4,
  VQS162S4_POST = 5,
  QS162S4_POST = 6,
  VQF162S4_POST = 7,
  QF162S4_POST = 8,
  VQS162S16_POST = 9,
  QS162S16_POST = 10,
  VQF162S16_POST = 11,
  QF162S16_POST = 12,
  VSHIFT2S4_POST = 13,
  SHIFT2S4_POST = 14,
  VSHIFT2S8_POST = 15,
  SHIFT2S8_POST = 16,
  VSHIFT2S16_POST = 17,
  SHIFT2S16_POST = 18,

} QuantMode_post;

typedef enum {

  NoQuant= 0,
# 176 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/cce_aicore_intrinsics.h" 3
  F322F16 = 1,
  VQF322HIF8_PRE = 2,
  QF322HIF8_PRE = 3,
  VQF322HIF8_PRE_HYBRID = 4,
  QF322HIF8_PRE_HYBRID = 5,

  AttachF16Mul = 6,




  VREQ8 = 8,
  REQ8 = 9,
  VDEQF16 = 10,
  DEQF16 = 11,






  VSHIFTS322S16 = 12,
  SHIFTS322S16 = 13,

  F322BF16 = 16,
  VQF162B8_PRE = 17,
  QF162B8_PRE = 18,
  VQF162S4_PRE = 19,
  QF162S4_PRE = 20,
  VREQ4 = 21,
  REQ4 = 22,
  VQF322B8_PRE = 23,
  QF322B8_PRE = 24,
  VQF322S4_PRE = 25,
  QF322S4_PRE = 26,
  VDEQS16 = 27,
  DEQS16 = 28,
  VQF162S16_PRE = 29,
  QF162S16_PRE = 30,
  VQF322F16_PRE = 31,
  QF322F16_PRE = 32,
  VQF322BF16_PRE = 33,
  QF322BF16_PRE = 34,
  VQS322BF16_PRE = 35,
  QS322BF16_PRE = 36,


} QuantMode_t;

typedef enum {

  NoRelu = 0,
  NormalRelu = 1,
  ScalarRelu = 2,
  VectorRelu = 3,
  LUTActivation = 4,

} ReluMode_t;

typedef enum {


  NoRELU = 0,
  NormalRELU = 1,
  LeakyRELU = 2,
  PRELU = 3,

} Relu_t;

typedef enum {




  DATA_EXP_0 = 48,
  DATA_EXP_1 = 49,
  DATA_EXP_2 = 50,
  DATA_EXP_3 = 51,

} VSPR_t;

typedef enum {

  ATOMIC_SUM = 0,


} atomic_op_t;

typedef enum {

  ATOMIC_NONE = 0,
  ATOMIC_F32 = 1,
  ATOMIC_F16 = 2,
  ATOMIC_S16 = 3,
  ATOMIC_S32 = 4,
  ATOMIC_S8 = 5,
  ATOMIC_BF16 = 6,

} atomic_type_t;

typedef enum {


  BM_DISABLE = 0,
  BM_ENABLE = 1,

} bm_t;

typedef enum {

  SINGLE_CACHE_LINE = 0,
  ENTIRE_DATA_CACHE,

} cache_line_t;

typedef enum {

  CSIZE0 = 0,
  CSIZE1 = 1,

} csize_t;

typedef enum {

  CACHELINE_ALL = 0,

  CACHELINE_UB,

  CACHELINE_OUT = 2,

  CACHELINE_ATOMIC,


} dcci_dst_t;

typedef enum {

  EVENT_ID0 = 0,
  EVENT_ID1,
  EVENT_ID2,
  EVENT_ID3,

  EVENT_ID4,
  EVENT_ID5,
  EVENT_ID6,
  EVENT_ID7,


} event_t;

typedef enum {

  DSB_ALL = 0,
  DSB_DDR,
  DSB_UB,
  DSB_SEQ,

} mem_dsb_t;

typedef enum {

  L1 = 0,
  L0A,
  L0B,
  L0C,
  UB,
  BT,

} mem_t;

typedef enum {


  PAD_NONE = 0,
  PAD_MODE1 = 1,
  PAD_MODE2 = 2,
  PAD_MODE3 = 3,
  PAD_MODE4 = 4,
  PAD_MODE5 = 5,
  PAD_MODE6 = 6,
  PAD_MODE7 = 7,
  PAD_MODE8 = 8,

} pad_t;

typedef enum {

  PIPE_S = 0,
  PIPE_V,
  PIPE_M,
  PIPE_MTE1,
  PIPE_MTE2,
  PIPE_MTE3,
  PIPE_ALL,

  PIPE_MTE4 = 7,
  PIPE_MTE5 = 8,

  PIPE_V2 = 9,



  PIPE_FIX = 10,



} pipe_t;

typedef enum {

  CROSS_CORE = 0,
  INTRA_BLOCK,
  BUFFER_ID,
  RESERVED,

} sync_mode_t;

typedef enum {







  VA0 = 0,
  VA1,
  VA2,
  VA3,
  VA4,
  VA5,
  VA6,
  VA7,


} ub_addr8_t;

typedef enum {

  UFMode0 = 0,
  Reserved,
  UFMode2,
  UFMode3,

} unit_flag_t;

typedef enum {

  L128 = 0,
  H128,

} vpart_t;

typedef enum {

  b8 = 0,
  b16 = 1,
  b32 = 2,
  s8 = 3,
  s32 = 4,
  f16 = 5,
  fmix = 6,
  f32 = 7,

} vtype_t;

namespace __cce_scalar {

typedef enum {


  NoREQ = 0,
  REQ = 1,
  VREQ = 2,

} Req_t;

typedef enum {

    MODE0 = 0,
    MODE1 = 1,
    MODE2 = 2,

} VSEL_mode_t;

typedef enum {

  inc = 0,
  dec = 1,

} addr_cal_mode_t;

typedef enum {

  W_3 = 0,
  W_5,

} w_size_t;

}

namespace bisheng {
namespace cce {

typedef enum {

    Comp = 0,
    Nocomp = 1,

} HscbReqType;

}
}







namespace __cce_scalar{

__attribute__((clang_builtin_alias(__builtin_cce___atom_add_hscb))) int32_t __atom_add_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___atom_cas_hscb))) int32_t __atom_cas_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___atom_exch_hscb))) int32_t __atom_exch_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___atom_max_hscb))) int32_t __atom_max_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___atom_min_hscb))) int32_t __atom_min_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___bt_alloc))) uint64_t __bt_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___bt_alloci))) uint64_t __bt_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___bt_free))) void __bt_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___bt_freei))) void __bt_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___ca_alloc))) __attribute__((cce_cube_a)) void * __ca_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___ca_alloci))) __attribute__((cce_cube_a)) void * __ca_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___ca_free))) void __ca_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___ca_freei))) void __ca_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___cb_alloc))) __attribute__((cce_cube_b)) void * __cb_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___cb_alloci))) __attribute__((cce_cube_b)) void * __cb_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___cb_free))) void __cb_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___cb_freei))) void __cb_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___cbuf_alloc))) __attribute__((cce_cube_buff)) void * __cbuf_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___cbuf_alloci))) __attribute__((cce_cube_buff)) void * __cbuf_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___cbuf_free))) void __cbuf_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___cbuf_freei))) void __cbuf_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___cc_alloc))) __attribute__((cce_cube_c)) void * __cc_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___cc_alloci))) __attribute__((cce_cube_c)) void * __cc_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___cc_free))) void __cc_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___cc_freei))) void __cc_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___cce_ldva))) void __cce_ldva(...);

__attribute__((clang_builtin_alias(__builtin_cce___cce_scatter_vnchwconv_b16))) void __cce_scatter_vnchwconv_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce___cce_scatter_vnchwconv_b8))) void __cce_scatter_vnchwconv_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce___dfx_region))) void __dfx_region(...);

__attribute__((clang_builtin_alias(__builtin_cce___fbuf_alloc))) __attribute__((cce_fixpipe_buff)) void * __fbuf_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___fbuf_alloci))) __attribute__((cce_fixpipe_buff)) void * __fbuf_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___fbuf_free))) void __fbuf_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___fbuf_freei))) void __fbuf_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___gqm))) uint64_t __gqm(...);

__attribute__((clang_builtin_alias(__builtin_cce___ib_set_stub))) void __ib_set_stub(...);

__attribute__((clang_builtin_alias(__builtin_cce___ib_wait_stub))) void __ib_wait_stub(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_f16))) __cce_half __ld_hscb_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_f32))) float __ld_hscb_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_s16))) int16_t __ld_hscb_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_s32))) int32_t __ld_hscb_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_s64))) int64_t __ld_hscb_s64(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_s8))) int8_t __ld_hscb_s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_u16))) uint64_t __ld_hscb_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_u32))) uint64_t __ld_hscb_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_u64))) uint64_t __ld_hscb_u64(...);

__attribute__((clang_builtin_alias(__builtin_cce___ld_hscb_u8))) uint64_t __ld_hscb_u8(...);

__attribute__((clang_builtin_alias(__builtin_cce___mad))) void __mad(...);

__attribute__((clang_builtin_alias(__builtin_cce___mad_s4))) void __mad_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce___mad_tf322f32))) void __mad_tf322f32(...);

__attribute__((clang_builtin_alias(__builtin_cce___memcpy))) void __memcpy(...);

__attribute__((clang_builtin_alias(__builtin_cce___mstx_dfx_report_stub))) void __mstx_dfx_report_stub(...);

__attribute__((clang_builtin_alias(__builtin_cce___prefetch_stop))) void __prefetch_stop(...);

__attribute__((clang_builtin_alias(__builtin_cce___pt_alloc))) uint64_t __pt_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___pt_alloci))) uint64_t __pt_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___pt_free))) void __pt_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___pt_freei))) void __pt_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___red_add_hscb))) void __red_add_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___red_max_hscb))) void __red_max_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___red_min_hscb))) void __red_min_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___sev))) void __sev(...);

__attribute__((clang_builtin_alias(__builtin_cce___sevl))) void __sevl(...);

__attribute__((clang_builtin_alias(__builtin_cce___st_hscb))) void __st_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___sync_all_stub))) void __sync_all_stub(...);

__attribute__((clang_builtin_alias(__builtin_cce___sync_hscb))) void __sync_hscb(...);

__attribute__((clang_builtin_alias(__builtin_cce___ubuf_alloc))) __attribute__((cce_unif_buff)) void * __ubuf_alloc(...);

__attribute__((clang_builtin_alias(__builtin_cce___ubuf_alloci))) __attribute__((cce_unif_buff)) void * __ubuf_alloci(...);

__attribute__((clang_builtin_alias(__builtin_cce___ubuf_free))) void __ubuf_free(...);

__attribute__((clang_builtin_alias(__builtin_cce___ubuf_freei))) void __ubuf_freei(...);

__attribute__((clang_builtin_alias(__builtin_cce___vabs))) void __vabs(...);

__attribute__((clang_builtin_alias(__builtin_cce___vadd))) void __vadd(...);

__attribute__((clang_builtin_alias(__builtin_cce___vaddrelu))) void __vaddrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce___vaddreluconv_f162s8))) void __vaddreluconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vaddreluconv_f322f16))) void __vaddreluconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce___vaddreluconv_s162s8))) void __vaddreluconv_s162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vadds))) void __vadds(...);

__attribute__((clang_builtin_alias(__builtin_cce___vand))) void __vand(...);

__attribute__((clang_builtin_alias(__builtin_cce___vaxpy))) void __vaxpy(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcgadd))) void __vcgadd(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcgmax))) void __vcgmax(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcgmin))) void __vcgmin(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcmax))) void __vcmax(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcmin))) void __vcmin(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_bf162s32a))) void __vconv_bf162s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_bf162s32c))) void __vconv_bf162s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_bf162s32f))) void __vconv_bf162s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_bf162s32r))) void __vconv_bf162s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_bf162s32z))) void __vconv_bf162s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_deqs162b8h))) void __vconv_deqs162b8h(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_deqs162b8l))) void __vconv_deqs162b8l(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s16a))) void __vconv_f162s16a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s16c))) void __vconv_f162s16c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s16f))) void __vconv_f162s16f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s16r))) void __vconv_f162s16r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s16z))) void __vconv_f162s16z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s32a))) void __vconv_f162s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s32c))) void __vconv_f162s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s32f))) void __vconv_f162s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s32r))) void __vconv_f162s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s32z))) void __vconv_f162s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4))) void __vconv_f162s4(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4a))) void __vconv_f162s4a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4c))) void __vconv_f162s4c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4f))) void __vconv_f162s4f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4r))) void __vconv_f162s4r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s4z))) void __vconv_f162s4z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8))) void __vconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8a))) void __vconv_f162s8a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8c))) void __vconv_f162s8c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8f))) void __vconv_f162s8f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8r))) void __vconv_f162s8r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162s8z))) void __vconv_f162s8z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8))) void __vconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8a))) void __vconv_f162u8a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8c))) void __vconv_f162u8c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8f))) void __vconv_f162u8f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8r))) void __vconv_f162u8r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f162u8z))) void __vconv_f162u8z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322bf16a))) void __vconv_f322bf16a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322bf16c))) void __vconv_f322bf16c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322bf16f))) void __vconv_f322bf16f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322bf16r))) void __vconv_f322bf16r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322bf16z))) void __vconv_f322bf16z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16))) void __vconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16a))) void __vconv_f322f16a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16c))) void __vconv_f322f16c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16f))) void __vconv_f322f16f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16o))) void __vconv_f322f16o(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16r))) void __vconv_f322f16r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f16z))) void __vconv_f322f16z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f32a))) void __vconv_f322f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f32c))) void __vconv_f322f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f32f))) void __vconv_f322f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f32r))) void __vconv_f322f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322f32z))) void __vconv_f322f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s16a))) void __vconv_f322s16a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s16c))) void __vconv_f322s16c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s16f))) void __vconv_f322s16f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s16r))) void __vconv_f322s16r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s16z))) void __vconv_f322s16z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s32a))) void __vconv_f322s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s32c))) void __vconv_f322s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s32f))) void __vconv_f322s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s32r))) void __vconv_f322s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s32z))) void __vconv_f322s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s64a))) void __vconv_f322s64a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s64c))) void __vconv_f322s64c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s64f))) void __vconv_f322s64f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s64r))) void __vconv_f322s64r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_f322s64z))) void __vconv_f322s64z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16))) void __vconv_s162f16(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16a))) void __vconv_s162f16a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16c))) void __vconv_s162f16c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16f))) void __vconv_s162f16f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16r))) void __vconv_s162f16r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s162f16z))) void __vconv_s162f16z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32))) void __vconv_s322f32(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32a))) void __vconv_s322f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32c))) void __vconv_s322f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32f))) void __vconv_s322f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32r))) void __vconv_s322f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s322f32z))) void __vconv_s322f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s642f32a))) void __vconv_s642f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s642f32c))) void __vconv_s642f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s642f32f))) void __vconv_s642f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s642f32r))) void __vconv_s642f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_s642f32z))) void __vconv_s642f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_vdeqs162b8h))) void __vconv_vdeqs162b8h(...);

__attribute__((clang_builtin_alias(__builtin_cce___vconv_vdeqs162b8l))) void __vconv_vdeqs162b8l(...);

__attribute__((clang_builtin_alias(__builtin_cce___vcpadd))) void __vcpadd(...);

__attribute__((clang_builtin_alias(__builtin_cce___vdiv))) void __vdiv(...);

__attribute__((clang_builtin_alias(__builtin_cce___vector_dup))) void __vector_dup(...);

__attribute__((clang_builtin_alias(__builtin_cce___vexp))) void __vexp(...);

__attribute__((clang_builtin_alias(__builtin_cce___vgather))) void __vgather(...);

__attribute__((clang_builtin_alias(__builtin_cce___vln))) void __vln(...);

__attribute__((clang_builtin_alias(__builtin_cce___vlrelu))) void __vlrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmadd))) void __vmadd(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmaddrelu))) void __vmaddrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmax))) void __vmax(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmaxs))) void __vmaxs(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmin))) void __vmin(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmins))) void __vmins(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmla))) void __vmla(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmul))) void __vmul(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmulconv_f162s8))) void __vmulconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmulconv_f162u8))) void __vmulconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vmuls))) void __vmuls(...);

__attribute__((clang_builtin_alias(__builtin_cce___vnot))) void __vnot(...);

__attribute__((clang_builtin_alias(__builtin_cce___vor))) void __vor(...);

__attribute__((clang_builtin_alias(__builtin_cce___vrec))) void __vrec(...);

__attribute__((clang_builtin_alias(__builtin_cce___vrelu))) void __vrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce___vrsqrt))) void __vrsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsel))) void __vsel(...);

__attribute__((clang_builtin_alias(__builtin_cce___vshl))) void __vshl(...);

__attribute__((clang_builtin_alias(__builtin_cce___vshr))) void __vshr(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsqrt))) void __vsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsub))) void __vsub(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsubrelu))) void __vsubrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsubreluconv_f162s8))) void __vsubreluconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsubreluconv_f322f16))) void __vsubreluconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce___vsubreluconv_s162s8))) void __vsubreluconv_s162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce___wait_ast_scb))) void __wait_ast_scb(...);

__attribute__((clang_builtin_alias(__builtin_cce___wait_prev_task))) void __wait_prev_task(...);

__attribute__((clang_builtin_alias(__builtin_cce___wfe))) void __wfe(...);

__attribute__((clang_builtin_alias(__builtin_cce_bcnt0))) int64_t bcnt0(...);

__attribute__((clang_builtin_alias(__builtin_cce_bcnt1))) int64_t bcnt1(...);

__attribute__((clang_builtin_alias(__builtin_cce_broadcast_ub_to_cc))) void broadcast_ub_to_cc(...);

__attribute__((clang_builtin_alias(__builtin_cce_clz))) int64_t clz(...);

__attribute__((clang_builtin_alias(__builtin_cce_col2img))) void col2img(...);

__attribute__((clang_builtin_alias(__builtin_cce_compress_ub_to_gm))) void compress_ub_to_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_f322f16o))) __cce_half conv_f322f16o(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_f322s32a))) int64_t conv_f322s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_f322s32c))) int64_t conv_f322s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_f322s32f))) int64_t conv_f322s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_f322s32r))) int64_t conv_f322s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_e4m3e4m3))) void conv_to_l1_e4m3e4m3(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_e4m3s4))) void conv_to_l1_e4m3s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_e4m3s8))) void conv_to_l1_e4m3s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_f16f16))) void conv_to_l1_f16f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_s16s8))) void conv_to_l1_s16s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_s4s4))) void conv_to_l1_s4s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_s8s4))) void conv_to_l1_s8s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_to_l1_s8s8))) void conv_to_l1_s8s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_ub_to_ub_s16s8))) void conv_ub_to_ub_s16s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_ub_to_ub_s8s4))) void conv_ub_to_ub_s8s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_conv_ub_to_ub_s8s8))) void conv_ub_to_ub_s8s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_bt))) void copy_cbuf_to_bt(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_cbuf_loopenhance))) void copy_cbuf_to_cbuf_loopenhance(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_fbuf))) void copy_cbuf_to_fbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_fbuf_v2))) void copy_cbuf_to_fbuf_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm))) void copy_cbuf_to_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm_align))) void copy_cbuf_to_gm_align(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm_align_no_padding))) void copy_cbuf_to_gm_align_no_padding(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm_align_v2))) void copy_cbuf_to_gm_align_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm_multi_nz2dn))) void copy_cbuf_to_gm_multi_nz2dn(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_gm_multi_nz2nd))) void copy_cbuf_to_gm_multi_nz2nd(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_pt))) void copy_cbuf_to_pt(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_cbuf_to_ubuf))) void copy_cbuf_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_depthwise_cc_to_ubuf))) void copy_depthwise_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf))) void copy_gm_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_align))) void copy_gm_to_cbuf_align(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_align_v2))) void copy_gm_to_cbuf_align_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_multi_dn2nz))) void copy_gm_to_cbuf_multi_dn2nz(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_multi_nd2nz))) void copy_gm_to_cbuf_multi_nd2nz(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_multi_nd2nz_b16))) void copy_gm_to_cbuf_multi_nd2nz_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_multi_nd2nz_b32s))) void copy_gm_to_cbuf_multi_nd2nz_b32s(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_multi_nd2nz_b8))) void copy_gm_to_cbuf_multi_nd2nz_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_cbuf_v2))) void copy_gm_to_cbuf_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf))) void copy_gm_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align))) void copy_gm_to_ubuf_align(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align_b16))) void copy_gm_to_ubuf_align_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align_b32))) void copy_gm_to_ubuf_align_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align_b8))) void copy_gm_to_ubuf_align_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align_no_padding))) void copy_gm_to_ubuf_align_no_padding(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_align_v2))) void copy_gm_to_ubuf_align_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_pad_b16))) void copy_gm_to_ubuf_pad_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_pad_b32))) void copy_gm_to_ubuf_pad_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_gm_to_ubuf_pad_b8))) void copy_gm_to_ubuf_pad_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cbuf_to_cc))) void copy_matrix_cbuf_to_cc(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_cbuf))) void copy_matrix_cc_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_cbuf_b4))) void copy_matrix_cc_to_cbuf_b4(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_cbuf_s4))) void copy_matrix_cc_to_cbuf_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_gm))) void copy_matrix_cc_to_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_gm_b4))) void copy_matrix_cc_to_gm_b4(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_gm_s4))) void copy_matrix_cc_to_gm_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_ub))) void copy_matrix_cc_to_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_ub_s4))) void copy_matrix_cc_to_ub_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_cc_to_ubuf))) void copy_matrix_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_matrix_ubuf_to_cc))) void copy_matrix_ubuf_to_cc(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_small_matrix_cc_to_ubuf))) void copy_small_matrix_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_small_matrix_ubuf_to_cc))) void copy_small_matrix_ubuf_to_cc(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_cbuf))) void copy_ubuf_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_fbuf))) void copy_ubuf_to_fbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm))) void copy_ubuf_to_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align))) void copy_ubuf_to_gm_align(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align_b16))) void copy_ubuf_to_gm_align_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align_b32))) void copy_ubuf_to_gm_align_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align_b8))) void copy_ubuf_to_gm_align_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align_no_padding))) void copy_ubuf_to_gm_align_no_padding(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_align_v2))) void copy_ubuf_to_gm_align_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_pad_b16))) void copy_ubuf_to_gm_pad_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_pad_b32))) void copy_ubuf_to_gm_pad_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_gm_pad_b8))) void copy_ubuf_to_gm_pad_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_ubuf_to_ubuf))) void copy_ubuf_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_vector_cc_to_ubuf))) void copy_vector_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_copy_vector_ubuf_to_cc))) void copy_vector_ubuf_to_cc(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_ca_matrix))) void create_ca_matrix(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_ca_matrix_bf16))) void create_ca_matrix_bf16(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_ca_matrix_h))) void create_ca_matrix_h(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_ca_matrix_ui))) void create_ca_matrix_ui(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cb_matrix))) void create_cb_matrix(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cb_matrix_bf16))) void create_cb_matrix_bf16(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cb_matrix_h))) void create_cb_matrix_h(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cb_matrix_ui))) void create_cb_matrix_ui(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cbuf_matrix))) void create_cbuf_matrix(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cbuf_matrix_bf16))) void create_cbuf_matrix_bf16(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cbuf_matrix_h))) void create_cbuf_matrix_h(...);

__attribute__((clang_builtin_alias(__builtin_cce_create_cbuf_matrix_ui))) void create_cbuf_matrix_ui(...);

__attribute__((clang_builtin_alias(__builtin_cce_dc_preload))) void dc_preload(...);

__attribute__((clang_builtin_alias(__builtin_cce_dcci))) void dcci(...);

__attribute__((clang_builtin_alias(__builtin_cce_dci))) void dci(...);

__attribute__((clang_builtin_alias(__builtin_cce_decompress_gm_to_cbuf))) void decompress_gm_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_decompress_gm_to_ub))) void decompress_gm_to_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_depthwise_conv))) void depthwise_conv(...);

__attribute__((clang_builtin_alias(__builtin_cce_depthwise_conv_v2))) void depthwise_conv_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_dsb))) void dsb(...);

__attribute__((clang_builtin_alias(__builtin_cce_fake_isa))) void fake_isa(...);

__attribute__((clang_builtin_alias(__builtin_cce_fake_isa_ret))) uint64_t fake_isa_ret(...);

__attribute__((clang_builtin_alias(__builtin_cce_fake_overflow_status_1))) uint64_t fake_overflow_status_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_fake_overflow_status_2))) uint64_t fake_overflow_status_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_ffts_cross_core_sync))) void ffts_cross_core_sync(...);

__attribute__((clang_builtin_alias(__builtin_cce_fifr1))) void fifr1(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_cbuf_to_cbuf_inner))) void fix_cbuf_to_cbuf_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_cbuf_to_gm_inner))) void fix_cbuf_to_gm_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_cbuf_to_ub_inner))) void fix_cbuf_to_ub_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_depthwisein_cc_to_cbuf))) void fix_depthwisein_cc_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_depthwisein_cc_to_ubuf))) void fix_depthwisein_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_depthwiseout_cc_to_cbuf))) void fix_depthwiseout_cc_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_matrix_cc_to_cbuf))) void fix_matrix_cc_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_matrix_cc_to_cbufubuf))) void fix_matrix_cc_to_cbufubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_matrix_cc_to_ubuf))) void fix_matrix_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_winograd_cc_to_cbuf))) void fix_winograd_cc_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_fix_winograd_cc_to_ubuf))) void fix_winograd_cc_to_ubuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_gather_gm_to_ubuf_u16))) void gather_gm_to_ubuf_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_gather_gm_to_ubuf_u32))) void gather_gm_to_ubuf_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_gather_gm_to_ubuf_u64))) void gather_gm_to_ubuf_u64(...);

__attribute__((clang_builtin_alias(__builtin_cce_gather_gm_to_ubuf_u8))) void gather_gm_to_ubuf_u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_acc_val))) int64_t get_acc_val(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_acsqid))) int64_t get_acsqid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ar))) int64_t get_ar(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_arch_ver))) int64_t get_arch_ver(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_reg_0))) int64_t get_ast_reg_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_reg_1))) int64_t get_ast_reg_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_reg_2))) int64_t get_ast_reg_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_reg_3))) int64_t get_ast_reg_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_scb_0))) int64_t get_ast_scb_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_scb_1))) int64_t get_ast_scb_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_scb_2))) int64_t get_ast_scb_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ast_scb_3))) int64_t get_ast_scb_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_blockdim))) int64_t get_blockdim(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_blockid))) int64_t get_blockid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_bt))) int64_t get_bmu_segm_bt(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_fb))) int64_t get_bmu_segm_fb(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_l0a))) int64_t get_bmu_segm_l0a(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_l0b))) int64_t get_bmu_segm_l0b(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_l0c))) int64_t get_bmu_segm_l0c(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_l1))) int64_t get_bmu_segm_l1(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_bmu_segm_ub))) int64_t get_bmu_segm_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_buf))) void get_buf(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_cmpmask))) void get_cmpmask(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_cond))) int64_t get_cond(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_cond_taskid))) int64_t get_cond_taskid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_condition_flag))) int64_t get_condition_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_coreid))) int64_t get_coreid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ctrl))) int64_t get_ctrl(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_data_main_base))) int64_t get_data_main_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_data_size))) int64_t get_data_size(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_data_ub_base))) int64_t get_data_ub_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ffts_base_addr))) int64_t get_ffts_base_addr(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_fpc))) int64_t get_fpc(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_groupblockdim))) int64_t get_groupblockdim(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_groupblockid))) int64_t get_groupblockid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_groupdim))) int64_t get_groupdim(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_groupid))) int64_t get_groupid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_icache_prl_st))) int64_t get_icache_prl_st(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_imm))) uint64_t get_imm(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ioa_base))) int64_t get_ioa_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_0))) int64_t get_ipc_reg_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_1))) int64_t get_ipc_reg_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_2))) int64_t get_ipc_reg_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_3))) int64_t get_ipc_reg_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_4))) int64_t get_ipc_reg_4(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_5))) int64_t get_ipc_reg_5(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_6))) int64_t get_ipc_reg_6(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_reg_7))) int64_t get_ipc_reg_7(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_0))) int64_t get_ipc_scb_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_1))) int64_t get_ipc_scb_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_10))) int64_t get_ipc_scb_10(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_11))) int64_t get_ipc_scb_11(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_12))) int64_t get_ipc_scb_12(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_13))) int64_t get_ipc_scb_13(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_14))) int64_t get_ipc_scb_14(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_15))) int64_t get_ipc_scb_15(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_2))) int64_t get_ipc_scb_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_3))) int64_t get_ipc_scb_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_4))) int64_t get_ipc_scb_4(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_5))) int64_t get_ipc_scb_5(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_6))) int64_t get_ipc_scb_6(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_7))) int64_t get_ipc_scb_7(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_8))) int64_t get_ipc_scb_8(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_ipc_scb_9))) int64_t get_ipc_scb_9(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_iqent))) int64_t get_iqent(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_k_num))) int64_t get_k_num(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_l2_in_main))) int64_t get_l2_in_main(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_l2_vaddr_base))) int64_t get_l2_vaddr_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_lpcnt))) int64_t get_lpcnt(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_max_min_cnt))) int64_t get_max_min_cnt(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_next_task))) void get_next_task(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_only_coreid))) int64_t get_only_coreid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_overflow_status))) uint64_t get_overflow_status(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_para_base))) int64_t get_para_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_pc))) int64_t get_pc(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_rpn_cor_ir))) int64_t get_rpn_cor_ir(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_rsvd_cnt))) int64_t get_rsvd_cnt(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_safety_crc_data))) int64_t get_safety_crc_data(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_safety_crc_en))) int64_t get_safety_crc_en(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_shmem_sz))) int64_t get_shmem_sz(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_smmu_tag_ver))) int64_t get_smmu_tag_ver(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_st_atomic_cfg))) int64_t get_st_atomic_cfg(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_stack_phy_base))) int64_t get_stack_phy_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_stackid))) int64_t get_stackid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_status))) int64_t get_status(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_subblockdim))) int64_t get_subblockdim(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_subblockid))) int64_t get_subblockid(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_sys_cnt))) int64_t get_sys_cnt(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_sys_va_base))) int64_t get_sys_va_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_thread_dim))) int64_t get_thread_dim(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_thread_id))) int64_t get_thread_id(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_tilingdata_base))) int64_t get_tilingdata_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_vl))) int64_t get_vl(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_vms4_sr))) int64_t get_vms4_sr(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_weight_base))) int64_t get_weight_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_get_workspace_base))) int64_t get_workspace_base(...);

__attribute__((clang_builtin_alias(__builtin_cce_group_conv))) void group_conv(...);

__attribute__((clang_builtin_alias(__builtin_cce_hebcd_out_to_ub))) void hebcd_out_to_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_hebce_l1_to_out))) void hebce_l1_to_out(...);

__attribute__((clang_builtin_alias(__builtin_cce_hebce_ub_to_out))) void hebce_ub_to_out(...);

__attribute__((clang_builtin_alias(__builtin_cce_hset_flag))) void hset_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_hwait_flag))) void hwait_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2col_cbuf_to_ca))) void img2col_cbuf_to_ca(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2col_cbuf_to_cb))) void img2col_cbuf_to_cb(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2col_cbuf_to_ub))) void img2col_cbuf_to_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_ca))) void img2colv2_cbuf_to_ca(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_ca_s4))) void img2colv2_cbuf_to_ca_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_cb))) void img2colv2_cbuf_to_cb(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_cb_s4))) void img2colv2_cbuf_to_cb_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_ub))) void img2colv2_cbuf_to_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_img2colv2_cbuf_to_ub_s4))) void img2colv2_cbuf_to_ub_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_insert_imm))) uint64_t insert_imm(...);

__attribute__((clang_builtin_alias(__builtin_cce_insert_reg))) uint64_t insert_reg(...);

__attribute__((clang_builtin_alias(__builtin_cce_insert_reg_f32))) __cce_half insert_reg_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_itm_cbuf_to_cbuf_inner))) void itm_cbuf_to_cbuf_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_ld_dev))) uint64_t ld_dev(...);

__attribute__((clang_builtin_alias(__builtin_cce_ldva))) void ldva(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca))) void load_cbuf_to_ca(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca_mx))) void load_cbuf_to_ca_mx(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca_s4))) void load_cbuf_to_ca_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca_transpose))) void load_cbuf_to_ca_transpose(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca_winograd))) void load_cbuf_to_ca_winograd(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_ca_winograd_v2))) void load_cbuf_to_ca_winograd_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb))) void load_cbuf_to_cb(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_b2))) void load_cbuf_to_cb_b2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_mx))) void load_cbuf_to_cb_mx(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_s4))) void load_cbuf_to_cb_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_sp))) void load_cbuf_to_cb_sp(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_transpose))) void load_cbuf_to_cb_transpose(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_transpose_s4))) void load_cbuf_to_cb_transpose_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_winograd))) void load_cbuf_to_cb_winograd(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_cbuf_to_cb_winograd_v2))) void load_cbuf_to_cb_winograd_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_decompress_header_from_gm))) void load_decompress_header_from_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_ca))) void load_gm_to_ca(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_ca_2dv2))) void load_gm_to_ca_2dv2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_ca_2dv2_s4))) void load_gm_to_ca_2dv2_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_ca_s4))) void load_gm_to_ca_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_ca_unzip))) void load_gm_to_ca_unzip(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cb))) void load_gm_to_cb(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cb_2dv2))) void load_gm_to_cb_2dv2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cb_2dv2_s4))) void load_gm_to_cb_2dv2_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cb_s4))) void load_gm_to_cb_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cb_unzip))) void load_gm_to_cb_unzip(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf))) void load_gm_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf_2dv2))) void load_gm_to_cbuf_2dv2(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf_2dv2_s4))) void load_gm_to_cbuf_2dv2_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf_s4))) void load_gm_to_cbuf_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf_unzip))) void load_gm_to_cbuf_unzip(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_gm_to_cbuf_xgamma))) void load_gm_to_cbuf_xgamma(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_image_to_cbuf))) void load_image_to_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_smask_table_from_cbuf))) void load_smask_table_from_cbuf(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_smask_table_from_gm))) void load_smask_table_from_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_smask_table_from_ub))) void load_smask_table_from_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_load_unzip_index_from_gm))) void load_unzip_index_from_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad))) void mad(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_b8u2))) void mad_b8u2(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_bf16s4))) void mad_bf16s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_e4m3s4))) void mad_e4m3s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_f16s4))) void mad_f16s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_f16u2))) void mad_f16u2(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_inner))) void mad_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_mx))) void mad_mx(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_s4))) void mad_s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_s4_inner))) void mad_s4_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_s8s4))) void mad_s8s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_sp))) void mad_sp(...);

__attribute__((clang_builtin_alias(__builtin_cce_mad_tf322f32))) void mad_tf322f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_e4m3e4m3))) void matmul_to_l1_e4m3e4m3(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_e4m3s4))) void matmul_to_l1_e4m3s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_e4m3s8))) void matmul_to_l1_e4m3s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_f16f16))) void matmul_to_l1_f16f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_s16s8))) void matmul_to_l1_s16s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_s4s4))) void matmul_to_l1_s4s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_s8s4))) void matmul_to_l1_s8s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_to_l1_s8s8))) void matmul_to_l1_s8s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_ub_to_ub_s16s8))) void matmul_ub_to_ub_s16s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_ub_to_ub_s8s4))) void matmul_ub_to_ub_s8s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_matmul_ub_to_ub_s8s8))) void matmul_ub_to_ub_s8s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_maxfilter))) void maxfilter(...);

__attribute__((clang_builtin_alias(__builtin_cce_minfilter))) void minfilter(...);

__attribute__((clang_builtin_alias(__builtin_cce_mov_vspr))) void mov_vspr(...);

__attribute__((clang_builtin_alias(__builtin_cce_mte4_mte5_off))) void mte4_mte5_off(...);

__attribute__((clang_builtin_alias(__builtin_cce_mte4_mte5_on))) void mte4_mte5_on(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_dci))) void mvf_dci(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b128_u16))) void mvf_gm_to_ub_b128_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b128_u32))) void mvf_gm_to_ub_b128_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b256_u16))) void mvf_gm_to_ub_b256_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b256_u32))) void mvf_gm_to_ub_b256_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b32_u16))) void mvf_gm_to_ub_b32_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b32_u32))) void mvf_gm_to_ub_b32_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b512_u16))) void mvf_gm_to_ub_b512_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b512_u32))) void mvf_gm_to_ub_b512_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b64_u16))) void mvf_gm_to_ub_b64_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_mvf_gm_to_ub_b64_u32))) void mvf_gm_to_ub_b64_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_nd_dma_dci))) void nd_dma_dci(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_out_to_ub_b16))) void nddma_out_to_ub_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_out_to_ub_b32))) void nddma_out_to_ub_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_out_to_ub_b8))) void nddma_out_to_ub_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_ub_to_ub_b16))) void nddma_ub_to_ub_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_ub_to_ub_b32))) void nddma_ub_to_ub_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_nddma_ub_to_ub_b8))) void nddma_ub_to_ub_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_pc_trace_off))) void pc_trace_off(...);

__attribute__((clang_builtin_alias(__builtin_cce_pc_trace_on))) void pc_trace_on(...);

__attribute__((clang_builtin_alias(__builtin_cce_pipe_barrier))) void pipe_barrier(...);

__attribute__((clang_builtin_alias(__builtin_cce_postproc_to_l1_inner))) void postproc_to_l1_inner(...);

__attribute__((clang_builtin_alias(__builtin_cce_preload))) void preload(...);

__attribute__((clang_builtin_alias(__builtin_cce_release_pbid))) void release_pbid(...);

__attribute__((clang_builtin_alias(__builtin_cce_rls_buf))) void rls_buf(...);

__attribute__((clang_builtin_alias(__builtin_cce_rpn_cor))) void rpn_cor(...);

__attribute__((clang_builtin_alias(__builtin_cce_rpn_cor_diag))) void rpn_cor_diag(...);

__attribute__((clang_builtin_alias(__builtin_cce_rpn_cor_diag2))) void rpn_cor_diag2(...);

__attribute__((clang_builtin_alias(__builtin_cce_sbitset0))) uint64_t sbitset0(...);

__attribute__((clang_builtin_alias(__builtin_cce_sbitset1))) uint64_t sbitset1(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_ubuf_to_gm_u16))) void scatter_ubuf_to_gm_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_ubuf_to_gm_u32))) void scatter_ubuf_to_gm_u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_ubuf_to_gm_u64))) void scatter_ubuf_to_gm_u64(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_ubuf_to_gm_u8))) void scatter_ubuf_to_gm_u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vabs))) void scatter_vabs(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vabs_f16))) void scatter_vabs_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vabs_f32))) void scatter_vabs_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vabs_s16))) void scatter_vabs_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadd))) void scatter_vadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadd_f16))) void scatter_vadd_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadd_f32))) void scatter_vadd_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadd_s16))) void scatter_vadd_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadd_s32))) void scatter_vadd_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadds))) void scatter_vadds(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadds_f16))) void scatter_vadds_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadds_f32))) void scatter_vadds_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadds_s16))) void scatter_vadds_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vadds_s32))) void scatter_vadds_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vaxpy))) void scatter_vaxpy(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vaxpy_f16))) void scatter_vaxpy_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vaxpy_f32))) void scatter_vaxpy_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vaxpy_fmix))) void scatter_vaxpy_fmix(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_eq))) void scatter_vcmp_eq(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_eq_f16))) void scatter_vcmp_eq_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_eq_f32))) void scatter_vcmp_eq_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_eq_s16))) void scatter_vcmp_eq_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ge))) void scatter_vcmp_ge(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ge_f16))) void scatter_vcmp_ge_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ge_f32))) void scatter_vcmp_ge_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ge_s16))) void scatter_vcmp_ge_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_gt))) void scatter_vcmp_gt(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_gt_f16))) void scatter_vcmp_gt_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_gt_f32))) void scatter_vcmp_gt_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_gt_s16))) void scatter_vcmp_gt_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_le))) void scatter_vcmp_le(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_le_f16))) void scatter_vcmp_le_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_le_f32))) void scatter_vcmp_le_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_le_s16))) void scatter_vcmp_le_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_lt))) void scatter_vcmp_lt(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_lt_f16))) void scatter_vcmp_lt_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_lt_f32))) void scatter_vcmp_lt_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_lt_s16))) void scatter_vcmp_lt_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ne))) void scatter_vcmp_ne(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ne_f16))) void scatter_vcmp_ne_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ne_f32))) void scatter_vcmp_ne_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcmp_ne_s16))) void scatter_vcmp_ne_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconcat_f16))) void scatter_vconcat_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconcat_f32))) void scatter_vconcat_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_deq))) void scatter_vconv_deq(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162f32))) void scatter_vconv_f162f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s32a))) void scatter_vconv_f162s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s32c))) void scatter_vconv_f162s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s32f))) void scatter_vconv_f162s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s32r))) void scatter_vconv_f162s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s32z))) void scatter_vconv_f162s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s8))) void scatter_vconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s8a))) void scatter_vconv_f162s8a(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s8c))) void scatter_vconv_f162s8c(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s8f))) void scatter_vconv_f162s8f(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162s8z))) void scatter_vconv_f162s8z(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162u8))) void scatter_vconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162u8a))) void scatter_vconv_f162u8a(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162u8c))) void scatter_vconv_f162u8c(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162u8f))) void scatter_vconv_f162u8f(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f162u8z))) void scatter_vconv_f162u8z(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322f16))) void scatter_vconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322f16o))) void scatter_vconv_f322f16o(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322s32a))) void scatter_vconv_f322s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322s32c))) void scatter_vconv_f322s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322s32f))) void scatter_vconv_f322s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322s32r))) void scatter_vconv_f322s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_f322s32z))) void scatter_vconv_f322s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_s322f32))) void scatter_vconv_s322f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_s82f16))) void scatter_vconv_s82f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vconv_u82f16))) void scatter_vconv_u82f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcshuffle_b16))) void scatter_vcshuffle_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcshuffle_b32))) void scatter_vcshuffle_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vcshuffle_b8))) void scatter_vcshuffle_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vdiv))) void scatter_vdiv(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vdiv_f16))) void scatter_vdiv_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vdiv_f32))) void scatter_vdiv_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vector_mov))) void scatter_vector_mov(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vector_mov_f16))) void scatter_vector_mov_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vector_mov_s16))) void scatter_vector_mov_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vector_mov_u16))) void scatter_vector_mov_u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vexp))) void scatter_vexp(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vexp_f16))) void scatter_vexp_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vexp_f32))) void scatter_vexp_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vextract_f16))) void scatter_vextract_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vextract_f32))) void scatter_vextract_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vln))) void scatter_vln(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vln_f16))) void scatter_vln_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vln_f32))) void scatter_vln_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmadd))) void scatter_vmadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmadd_f16))) void scatter_vmadd_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmadd_f32))) void scatter_vmadd_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaddrelu))) void scatter_vmaddrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaddrelu_f16))) void scatter_vmaddrelu_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaddrelu_f32))) void scatter_vmaddrelu_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmax))) void scatter_vmax(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmax_f16))) void scatter_vmax_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmax_f32))) void scatter_vmax_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmax_s16))) void scatter_vmax_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmax_s32))) void scatter_vmax_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaxs_f16))) void scatter_vmaxs_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaxs_f32))) void scatter_vmaxs_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaxs_s16))) void scatter_vmaxs_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmaxs_s32))) void scatter_vmaxs_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmin))) void scatter_vmin(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmin_f16))) void scatter_vmin_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmin_f32))) void scatter_vmin_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmin_s16))) void scatter_vmin_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmin_s32))) void scatter_vmin_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmins_f16))) void scatter_vmins_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmins_f32))) void scatter_vmins_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmins_s16))) void scatter_vmins_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmins_s32))) void scatter_vmins_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmla))) void scatter_vmla(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmla_f16))) void scatter_vmla_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmla_f32))) void scatter_vmla_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmla_fmix))) void scatter_vmla_fmix(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmul))) void scatter_vmul(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmul_f16))) void scatter_vmul_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmul_f32))) void scatter_vmul_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmul_s16))) void scatter_vmul_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmul_s32))) void scatter_vmul_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmulconv_f162s8))) void scatter_vmulconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmulconv_f162u8))) void scatter_vmulconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmuls))) void scatter_vmuls(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmuls_f16))) void scatter_vmuls_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmuls_f32))) void scatter_vmuls_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmuls_s16))) void scatter_vmuls_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vmuls_s32))) void scatter_vmuls_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vnchwconv_b16))) void scatter_vnchwconv_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vnchwconv_b32))) void scatter_vnchwconv_b32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vnchwconv_b8))) void scatter_vnchwconv_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrec))) void scatter_vrec(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrec_f16))) void scatter_vrec_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrec_f32))) void scatter_vrec_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrelu))) void scatter_vrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrelu_f16))) void scatter_vrelu_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrelu_f32))) void scatter_vrelu_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrelu_s32))) void scatter_vrelu_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrsqrt))) void scatter_vrsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrsqrt_f16))) void scatter_vrsqrt_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vrsqrt_f32))) void scatter_vrsqrt_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscmerge))) void scatter_vscmerge(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscmerge_b16))) void scatter_vscmerge_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscmerge_b8))) void scatter_vscmerge_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscmerge_f16))) void scatter_vscmerge_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscsplit))) void scatter_vscsplit(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscsplit_b16))) void scatter_vscsplit_b16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscsplit_b8))) void scatter_vscsplit_b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vscsplit_f16))) void scatter_vscsplit_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsel))) void scatter_vsel(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsel_f16))) void scatter_vsel_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsel_f32))) void scatter_vsel_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsqrt))) void scatter_vsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsqrt_f16))) void scatter_vsqrt_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsqrt_f32))) void scatter_vsqrt_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsub))) void scatter_vsub(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsub_f16))) void scatter_vsub_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsub_f32))) void scatter_vsub_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsub_s16))) void scatter_vsub_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_scatter_vsub_s32))) void scatter_vsub_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_0))) void set_aipp_spr_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_1))) void set_aipp_spr_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_10))) void set_aipp_spr_10(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_11))) void set_aipp_spr_11(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_12))) void set_aipp_spr_12(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_13))) void set_aipp_spr_13(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_14))) void set_aipp_spr_14(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_15))) void set_aipp_spr_15(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_16))) void set_aipp_spr_16(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_17))) void set_aipp_spr_17(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_18))) void set_aipp_spr_18(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_19))) void set_aipp_spr_19(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_2))) void set_aipp_spr_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_20))) void set_aipp_spr_20(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_21))) void set_aipp_spr_21(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_22))) void set_aipp_spr_22(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_23))) void set_aipp_spr_23(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_24))) void set_aipp_spr_24(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_25))) void set_aipp_spr_25(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_26))) void set_aipp_spr_26(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_27))) void set_aipp_spr_27(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_28))) void set_aipp_spr_28(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_29))) void set_aipp_spr_29(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_3))) void set_aipp_spr_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_30))) void set_aipp_spr_30(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_31))) void set_aipp_spr_31(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_4))) void set_aipp_spr_4(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_5))) void set_aipp_spr_5(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_6))) void set_aipp_spr_6(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_7))) void set_aipp_spr_7(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_8))) void set_aipp_spr_8(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_aipp_spr_9))) void set_aipp_spr_9(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_reg_0))) void set_ast_reg_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_reg_1))) void set_ast_reg_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_reg_2))) void set_ast_reg_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_reg_3))) void set_ast_reg_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_scb_0))) void set_ast_scb_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_scb_1))) void set_ast_scb_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_scb_2))) void set_ast_scb_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ast_scb_3))) void set_ast_scb_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atom_load_para))) void set_atom_load_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_add))) void set_atomic_add(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_bf16))) void set_atomic_bf16(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_f16))) void set_atomic_f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_f32))) void set_atomic_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_max))) void set_atomic_max(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_min))) void set_atomic_min(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_none))) void set_atomic_none(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_s16))) void set_atomic_s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_s32))) void set_atomic_s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_atomic_s8))) void set_atomic_s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_bt))) void set_bmu_segm_bt(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_fb))) void set_bmu_segm_fb(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_l0a))) void set_bmu_segm_l0a(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_l0b))) void set_bmu_segm_l0b(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_l0c))) void set_bmu_segm_l0c(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_l1))) void set_bmu_segm_l1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_bmu_segm_ub))) void set_bmu_segm_ub(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cache_normread))) void set_cache_normread(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cache_notwriteback))) void set_cache_notwriteback(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cache_readinv))) void set_cache_readinv(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cache_readlast))) void set_cache_readlast(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_channel_para))) void set_channel_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_channel_stride))) void set_channel_stride(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cmpmask))) void set_cmpmask(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cond))) void set_cond(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cond_taskid))) void set_cond_taskid(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_condition_flag))) void set_condition_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ctrl))) void set_ctrl(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_cube_stride_para))) void set_cube_stride_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_data_exp_0))) void set_data_exp_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_data_exp_1))) void set_data_exp_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_data_exp_2))) void set_data_exp_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_data_exp_3))) void set_data_exp_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_deqscale))) void set_deqscale(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_dtc_para))) void set_dtc_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_elt_antiq_para))) void set_elt_antiq_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_elt_src_para))) void set_elt_src_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fcol2img))) void set_fcol2img(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ffts_base_addr))) void set_ffts_base_addr(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fix_clip_relu))) void set_fix_clip_relu(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fixp_addr))) void set_fixp_addr(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fixp_nz_para))) void set_fixp_nz_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_flag))) void set_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fm_step_pos))) void set_fm_step_pos(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fmatrix))) void set_fmatrix(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fmatrix_b))) void set_fmatrix_b(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fmatrix_dual_0))) void set_fmatrix_dual_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fmatrix_dual_1))) void set_fmatrix_dual_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fp_para))) void set_fp_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fp_para_post))) void set_fp_para_post(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fp_post_cfg))) void set_fp_post_cfg(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fpc))) void set_fpc(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_fpdeq))) void set_fpdeq(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_intra_block))) void set_intra_block(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_0))) void set_ipc_reg_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_1))) void set_ipc_reg_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_2))) void set_ipc_reg_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_3))) void set_ipc_reg_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_4))) void set_ipc_reg_4(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_5))) void set_ipc_reg_5(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_6))) void set_ipc_reg_6(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_reg_7))) void set_ipc_reg_7(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_0))) void set_ipc_scb_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_1))) void set_ipc_scb_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_10))) void set_ipc_scb_10(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_11))) void set_ipc_scb_11(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_12))) void set_ipc_scb_12(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_13))) void set_ipc_scb_13(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_14))) void set_ipc_scb_14(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_15))) void set_ipc_scb_15(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_2))) void set_ipc_scb_2(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_3))) void set_ipc_scb_3(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_4))) void set_ipc_scb_4(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_5))) void set_ipc_scb_5(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_6))) void set_ipc_scb_6(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_7))) void set_ipc_scb_7(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_8))) void set_ipc_scb_8(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ipc_scb_9))) void set_ipc_scb_9(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_itm_anchor0))) void set_itm_anchor0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_itm_anchor1))) void set_itm_anchor1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_itm_image_para))) void set_itm_image_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_kernel))) void set_kernel(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l0_set_value_bf16))) void set_l0_set_value_bf16(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l0_set_value_h))) void set_l0_set_value_h(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l0_set_value_ui))) void set_l0_set_value_ui(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l0a_2d))) void set_l0a_2d(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l0b_2d))) void set_l0b_2d(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l1_2d))) void set_l1_2d(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l1_3d_size))) void set_l1_3d_size(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l3d_rpt))) void set_l3d_rpt(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_l3d_rpt_b))) void set_l3d_rpt_b(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop0_stride_nddma))) void set_loop0_stride_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop1_stride_l1toout))) void set_loop1_stride_l1toout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop1_stride_nddma))) void set_loop1_stride_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop1_stride_outtol1))) void set_loop1_stride_outtol1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop1_stride_outtoub))) void set_loop1_stride_outtoub(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop1_stride_ubtoout))) void set_loop1_stride_ubtoout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop2_stride_l1toout))) void set_loop2_stride_l1toout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop2_stride_nddma))) void set_loop2_stride_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop2_stride_outtol1))) void set_loop2_stride_outtol1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop2_stride_outtoub))) void set_loop2_stride_outtoub(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop2_stride_ubtoout))) void set_loop2_stride_ubtoout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop3_para))) void set_loop3_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop3_stride_nddma))) void set_loop3_stride_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop4_para))) void set_loop4_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop4_stride_nddma))) void set_loop4_stride_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop_size_l1toout))) void set_loop_size_l1toout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop_size_outtol1))) void set_loop_size_outtol1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop_size_outtoub))) void set_loop_size_outtoub(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loop_size_ubtoout))) void set_loop_size_ubtoout(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_loopenhance_para))) void set_loopenhance_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_low_pre_tbl))) void set_low_pre_tbl(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_lpcnt))) void set_lpcnt(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_lrelu_alpha))) void set_lrelu_alpha(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_elt_antiq_para))) void set_m_elt_antiq_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_elt_src_para))) void set_m_elt_src_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_fmatrix))) void set_m_fmatrix(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_fmatrix_dual_0))) void set_m_fmatrix_dual_0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_fmatrix_dual_1))) void set_m_fmatrix_dual_1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_fpc))) void set_m_fpc(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_padding))) void set_m_padding(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_quant_post))) void set_m_quant_post(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_quant_pre))) void set_m_quant_pre(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_m_relu_alpha))) void set_m_relu_alpha(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mask_count))) void set_mask_count(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mask_norm))) void set_mask_norm(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_matrix_para))) void set_matrix_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mov_pad_val))) void set_mov_pad_val(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte2_antiq_para))) void set_mte2_antiq_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte2_nz_para))) void set_mte2_nz_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte2_qtable0))) void set_mte2_qtable0(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte2_qtable1))) void set_mte2_qtable1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte2_src_para))) void set_mte2_src_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_mte3_nz_para))) void set_mte3_nz_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_nd_para))) void set_nd_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pad_cnt_nddma))) void set_pad_cnt_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pad_val_nddma))) void set_pad_val_nddma(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pad_val_outtol1))) void set_pad_val_outtol1(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pad_val_outtoub))) void set_pad_val_outtoub(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_padding))) void set_padding(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_padding_b))) void set_padding_b(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pcie_rd_ctrl))) void set_pcie_rd_ctrl(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pcie_wr_ctrl))) void set_pcie_wr_ctrl(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_pnt_coe))) void set_pnt_coe(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_quant_post))) void set_quant_post(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_quant_pre))) void set_quant_pre(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_rawheader_to_gm))) void set_rawheader_to_gm(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_relu_alpha))) void set_relu_alpha(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_reqscale))) void set_reqscale(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_rpn_cor_ir))) void set_rpn_cor_ir(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_rpn_offset))) void set_rpn_offset(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_rpn_offset_f32))) void set_rpn_offset_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_safety_crc_en))) void set_safety_crc_en(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_safety_crc_excp))) void set_safety_crc_excp(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_smask_index))) void set_smask_index(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_st_atomic_cfg))) void set_st_atomic_cfg(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_ub_2d))) void set_ub_2d(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_upscale_para))) void set_upscale_para(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_vector_mask))) void set_vector_mask(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_vector_mask_dup))) void set_vector_mask_dup(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_vpipe))) void set_vpipe(...);

__attribute__((clang_builtin_alias(__builtin_cce_set_vsp))) void set_vsp(...);

__attribute__((clang_builtin_alias(__builtin_cce_sff0))) int64_t sff0(...);

__attribute__((clang_builtin_alias(__builtin_cce_sff1))) int64_t sff1(...);

__attribute__((clang_builtin_alias(__builtin_cce_sflbits))) int64_t sflbits(...);

__attribute__((clang_builtin_alias(__builtin_cce_shl_imm_f32))) float shl_imm_f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_st_dev))) void st_dev(...);

__attribute__((clang_builtin_alias(__builtin_cce_store_l1_to_out_image))) void store_l1_to_out_image(...);

__attribute__((clang_builtin_alias(__builtin_cce_trap))) void trap(...);

__attribute__((clang_builtin_alias(__builtin_cce_try_wait))) int64_t try_wait(...);

__attribute__((clang_builtin_alias(__builtin_cce_use_pipe_v))) void use_pipe_v(...);

__attribute__((clang_builtin_alias(__builtin_cce_use_pipe_v2))) void use_pipe_v2(...);

__attribute__((clang_builtin_alias(__builtin_cce_v4dtrans))) void v4dtrans(...);

__attribute__((clang_builtin_alias(__builtin_cce_vabs))) void vabs(...);

__attribute__((clang_builtin_alias(__builtin_cce_vadd))) void vadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_vadd_masked))) void vadd_masked(...);

__attribute__((clang_builtin_alias(__builtin_cce_vadddeqrelu))) void vadddeqrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaddrelu))) void vaddrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaddreluconv_f162s8))) void vaddreluconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaddreluconv_f322f16))) void vaddreluconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaddreluconv_s162s8))) void vaddreluconv_s162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaddreluconv_vdeqs162b8))) void vaddreluconv_vdeqs162b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vadds))) void vadds(...);

__attribute__((clang_builtin_alias(__builtin_cce_vand))) void vand(...);

__attribute__((clang_builtin_alias(__builtin_cce_vaxpy))) void vaxpy(...);

__attribute__((clang_builtin_alias(__builtin_cce_vbi))) void vbi(...);

__attribute__((clang_builtin_alias(__builtin_cce_vbrcb))) void vbrcb(...);

__attribute__((clang_builtin_alias(__builtin_cce_vbs))) void vbs(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcadd))) void vcadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s162s32))) void vcbd_s162s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s162u32))) void vcbd_s162u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s162u8))) void vcbd_s162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s322s16))) void vcbd_s322s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s322u16))) void vcbd_s322u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_s322u8))) void vcbd_s322u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u162s32))) void vcbd_u162s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u162u32))) void vcbd_u162u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u162u8))) void vcbd_u162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u322s16))) void vcbd_u322s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u322u16))) void vcbd_u322u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u322u8))) void vcbd_u322u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u82s16))) void vcbd_u82s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u82s32))) void vcbd_u82s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u82u16))) void vcbd_u82u16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcbd_u82u32))) void vcbd_u82u32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcgadd))) void vcgadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcgmax))) void vcgmax(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcgmin))) void vcgmin(...);

__attribute__((clang_builtin_alias(__builtin_cce_vci))) void vci(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmax))) void vcmax(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmin))) void vcmin(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_eq))) void vcmp_eq(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_ge))) void vcmp_ge(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_gt))) void vcmp_gt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_le))) void vcmp_le(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_lt))) void vcmp_lt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmp_ne))) void vcmp_ne(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_eq))) void vcmpv_eq(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_ge))) void vcmpv_ge(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_gt))) void vcmpv_gt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_le))) void vcmpv_le(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_lt))) void vcmpv_lt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpv_ne))) void vcmpv_ne(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_eq))) void vcmpvs_eq(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_ge))) void vcmpvs_ge(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_gt))) void vcmpvs_gt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_le))) void vcmpvs_le(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_lt))) void vcmpvs_lt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcmpvs_ne))) void vcmpvs_ne(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconcat))) void vconcat(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162f32))) void vconv_bf162f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162s32a))) void vconv_bf162s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162s32c))) void vconv_bf162s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162s32f))) void vconv_bf162s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162s32r))) void vconv_bf162s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_bf162s32z))) void vconv_bf162s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_deq))) void vconv_deq(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_deqs162b8))) void vconv_deqs162b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_deqs162b8h))) void vconv_deqs162b8h(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_deqs162b8l))) void vconv_deqs162b8l(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_deqs322f16))) void vconv_deqs322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162f32))) void vconv_f162f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s16a))) void vconv_f162s16a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s16c))) void vconv_f162s16c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s16f))) void vconv_f162s16f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s16r))) void vconv_f162s16r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s16z))) void vconv_f162s16z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s32a))) void vconv_f162s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s32c))) void vconv_f162s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s32f))) void vconv_f162s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s32r))) void vconv_f162s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s32z))) void vconv_f162s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4))) void vconv_f162s4(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4a))) void vconv_f162s4a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4c))) void vconv_f162s4c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4f))) void vconv_f162s4f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4r))) void vconv_f162s4r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s4z))) void vconv_f162s4z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8))) void vconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8a))) void vconv_f162s8a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8c))) void vconv_f162s8c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8f))) void vconv_f162s8f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8r))) void vconv_f162s8r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162s8z))) void vconv_f162s8z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8))) void vconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8a))) void vconv_f162u8a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8c))) void vconv_f162u8c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8f))) void vconv_f162u8f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8r))) void vconv_f162u8r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f162u8z))) void vconv_f162u8z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322bf16a))) void vconv_f322bf16a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322bf16c))) void vconv_f322bf16c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322bf16f))) void vconv_f322bf16f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322bf16r))) void vconv_f322bf16r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322bf16z))) void vconv_f322bf16z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16))) void vconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16a))) void vconv_f322f16a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16c))) void vconv_f322f16c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16f))) void vconv_f322f16f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16o))) void vconv_f322f16o(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16r))) void vconv_f322f16r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f16z))) void vconv_f322f16z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f32a))) void vconv_f322f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f32c))) void vconv_f322f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f32f))) void vconv_f322f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f32r))) void vconv_f322f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322f32z))) void vconv_f322f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s16a))) void vconv_f322s16a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s16c))) void vconv_f322s16c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s16f))) void vconv_f322s16f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s16r))) void vconv_f322s16r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s16z))) void vconv_f322s16z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s32a))) void vconv_f322s32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s32c))) void vconv_f322s32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s32f))) void vconv_f322s32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s32r))) void vconv_f322s32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s32z))) void vconv_f322s32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s64a))) void vconv_f322s64a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s64c))) void vconv_f322s64c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s64f))) void vconv_f322s64f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s64r))) void vconv_f322s64r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_f322s64z))) void vconv_f322s64z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16))) void vconv_s162f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16a))) void vconv_s162f16a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16c))) void vconv_s162f16c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16f))) void vconv_s162f16f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16r))) void vconv_s162f16r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f16z))) void vconv_s162f16z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s162f32))) void vconv_s162f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32))) void vconv_s322f32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32a))) void vconv_s322f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32c))) void vconv_s322f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32f))) void vconv_s322f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32r))) void vconv_s322f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322f32z))) void vconv_s322f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322s16))) void vconv_s322s16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s322s64))) void vconv_s322s64(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s42f16))) void vconv_s42f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642f32a))) void vconv_s642f32a(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642f32c))) void vconv_s642f32c(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642f32f))) void vconv_s642f32f(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642f32r))) void vconv_s642f32r(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642f32z))) void vconv_s642f32z(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s642s32))) void vconv_s642s32(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_s82f16))) void vconv_s82f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_u82f16))) void vconv_u82f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_vdeqs162b8))) void vconv_vdeqs162b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_vdeqs162b8h))) void vconv_vdeqs162b8h(...);

__attribute__((clang_builtin_alias(__builtin_cce_vconv_vdeqs162b8l))) void vconv_vdeqs162b8l(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcopy))) void vcopy(...);

__attribute__((clang_builtin_alias(__builtin_cce_vcpadd))) void vcpadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_vdiv))) void vdiv(...);

__attribute__((clang_builtin_alias(__builtin_cce_vdp))) void vdp(...);

__attribute__((clang_builtin_alias(__builtin_cce_vector_dup))) void vector_dup(...);

__attribute__((clang_builtin_alias(__builtin_cce_velu))) void velu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vexp))) void vexp(...);

__attribute__((clang_builtin_alias(__builtin_cce_vextract))) void vextract(...);

__attribute__((clang_builtin_alias(__builtin_cce_vgather))) void vgather(...);

__attribute__((clang_builtin_alias(__builtin_cce_vgatherb))) void vgatherb(...);

__attribute__((clang_builtin_alias(__builtin_cce_vld_va_reg))) void vld_va_reg(...);

__attribute__((clang_builtin_alias(__builtin_cce_vln))) void vln(...);

__attribute__((clang_builtin_alias(__builtin_cce_vlrelu))) void vlrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmadd))) void vmadd(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmaddrelu))) void vmaddrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmax))) void vmax(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmaxs))) void vmaxs(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmin))) void vmin(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmins))) void vmins(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmla))) void vmla(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmrgsort4))) void vmrgsort4(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmul))) void vmul(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmulconv_f162s8))) void vmulconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmulconv_f162u8))) void vmulconv_f162u8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vmuls))) void vmuls(...);

__attribute__((clang_builtin_alias(__builtin_cce_vnot))) void vnot(...);

__attribute__((clang_builtin_alias(__builtin_cce_vor))) void vor(...);

__attribute__((clang_builtin_alias(__builtin_cce_vpadding))) void vpadding(...);

__attribute__((clang_builtin_alias(__builtin_cce_vrec))) void vrec(...);

__attribute__((clang_builtin_alias(__builtin_cce_vreduce))) void vreduce(...);

__attribute__((clang_builtin_alias(__builtin_cce_vreducev2))) void vreducev2(...);

__attribute__((clang_builtin_alias(__builtin_cce_vrelu))) void vrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vrsqrt))) void vrsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vscatter))) void vscatter(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsel))) void vsel(...);

__attribute__((clang_builtin_alias(__builtin_cce_vshl))) void vshl(...);

__attribute__((clang_builtin_alias(__builtin_cce_vshr))) void vshr(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsigmoid))) void vsigmoid(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsort))) void vsort(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsqrt))) void vsqrt(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsub))) void vsub(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsubrelu))) void vsubrelu(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsubreluconv_f162s8))) void vsubreluconv_f162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsubreluconv_f322f16))) void vsubreluconv_f322f16(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsubreluconv_s162s8))) void vsubreluconv_s162s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vsubreluconv_vdeqs162b8))) void vsubreluconv_vdeqs162b8(...);

__attribute__((clang_builtin_alias(__builtin_cce_vtanh))) void vtanh(...);

__attribute__((clang_builtin_alias(__builtin_cce_vtranspose))) void vtranspose(...);

__attribute__((clang_builtin_alias(__builtin_cce_wait_flag))) void wait_flag(...);

__attribute__((clang_builtin_alias(__builtin_cce_wait_flag_dev))) void wait_flag_dev(...);

__attribute__((clang_builtin_alias(__builtin_cce_wait_intra_block))) void wait_intra_block(...);

__attribute__((clang_builtin_alias(__builtin_cce_winograd_conv))) void winograd_conv(...);

__attribute__((clang_builtin_alias(__builtin_cce_winograd_to_l1_s16s8))) void winograd_to_l1_s16s8(...);

__attribute__((clang_builtin_alias(__builtin_cce_winograd_to_l1_s8s8))) void winograd_to_l1_s8s8(...);

}
# 235 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_types.h" 2 3
# 26 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 1 3
# 13 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct ulong_2;
# 68 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
namespace __cce_scalar {


struct __cce_builtin_block_t {
  __declspec( property(get = __fetch_builtin_idx)) unsigned long long idx; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_idx(void) { return get_block_idx(); };
  __declspec( property(get = __fetch_builtin_num)) unsigned long long num; static inline __attribute__((cce_builtin_api, always_inline))[aicore] unsigned long long __fetch_builtin_num(void) { return get_block_num(); };

private:
  [aicore] __cce_builtin_block_t() = delete; [aicore] __cce_builtin_block_t(const __cce_builtin_block_t &) = delete; [aicore] void operator=(const __cce_builtin_block_t &) const = delete; [aicore] __cce_builtin_block_t *operator&() const = delete;
};
extern const[aicore] __attribute__((weak)) __cce_builtin_block_t block;


struct __cce_builtin_block_idx_t { inline __attribute__((const, cce_builtin_api, always_inline))[aicore] operator int() const { return get_block_idx(); } private: [aicore] __cce_builtin_block_idx_t() = delete; [aicore] __cce_builtin_block_idx_t(const __cce_builtin_block_idx_t &) = delete; [aicore] void operator=(const __cce_builtin_block_idx_t &) const = delete; [aicore] __cce_builtin_block_idx_t *operator&() const = delete; }; extern const[aicore] __attribute__((weak)) __cce_builtin_block_idx_t block_idx;;
struct __cce_builtin_block_num_t { inline __attribute__((const, cce_builtin_api, always_inline))[aicore] operator int() const { return get_block_num(); } private: [aicore] __cce_builtin_block_num_t() = delete; [aicore] __cce_builtin_block_num_t(const __cce_builtin_block_num_t &) = delete; [aicore] void operator=(const __cce_builtin_block_num_t &) const = delete; [aicore] __cce_builtin_block_num_t *operator&() const = delete; }; extern const[aicore] __attribute__((weak)) __cce_builtin_block_num_t block_num;;

}

namespace __cce_simt {

struct __cce_builtin_block_idx_t { inline __attribute__((const, cce_builtin_api, always_inline))[aicore] operator int() const { return __cce_simt_get_BLOCKID(); } private: [aicore] __cce_builtin_block_idx_t() = delete; [aicore] __cce_builtin_block_idx_t(const __cce_builtin_block_idx_t &) = delete; [aicore] void operator=(const __cce_builtin_block_idx_t &) const = delete; [aicore] __cce_builtin_block_idx_t *operator&() const = delete; }; extern const[aicore] __attribute__((weak)) __cce_builtin_block_idx_t block_idx;;
struct __cce_builtin_block_num_t { inline __attribute__((const, cce_builtin_api, always_inline))[aicore] operator int() const { return __cce_simt_get_BLOCKID(); } private: [aicore] __cce_builtin_block_num_t() = delete; [aicore] __cce_builtin_block_num_t(const __cce_builtin_block_num_t &) = delete; [aicore] void operator=(const __cce_builtin_block_num_t &) const = delete; [aicore] __cce_builtin_block_num_t *operator&() const = delete; }; extern const[aicore] __attribute__((weak)) __cce_builtin_block_num_t block_num;;
}
# 102 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_builtin_vars.h" 3
struct ulong_2 {
  unsigned long long idx, num;
};

typedef struct ulong_2 ulong_2;
# 28 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 1 3
# 15 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime.h" 3
extern "C" {



unsigned int rtConfigureCall(uint32_t numBlocks, void *smDesc = nullptr, void *stream = nullptr);





}
# 30 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 1 3
# 22 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 3
typedef struct CceEventIdType {
  void *__opaque_token;
} CceEventIdType;
# 146 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_intrinsics.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void trap(uint64_t err_code) {
  __builtin_cce_trap_mov(err_code);
}
# 32 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3




# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 1 3
# 39 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
template <typename T> [aicore]
T convert_from_bytes(const unsigned char *buf);
# 1842 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
namespace mmad_t {
struct shape_t {
  uint16_t shape_M;
  uint16_t shape_K;
  uint16_t shape_N;
  [aicore] __attribute__((cce_builtin_api, always_inline)) shape_t(uint16_t shape_M = 16, uint16_t shape_K = 16,
                            uint16_t shape_N = 16)
      : shape_M(shape_M), shape_K(shape_K), shape_N(shape_N) {}
};

struct control_t {
  uint8_t unit_flag_ctrl;
  bool gemv_ctrl;
  bool BTbuf_ctrl;
  bool zero_Cmatrix_ctrl;
  [aicore] __attribute__((cce_builtin_api, always_inline)) control_t(uint8_t unit_flag_ctrl = 0, bool gemv_ctrl = 0,
                              bool BTbuf_ctrl = 0, bool zero_Cmatrix_ctrl = 0)
      : unit_flag_ctrl(unit_flag_ctrl), gemv_ctrl(gemv_ctrl),
        BTbuf_ctrl(BTbuf_ctrl), zero_Cmatrix_ctrl(zero_Cmatrix_ctrl) {}
};
}
# 2675 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void
copy_data_align64(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src, uint64_t size) {
  __builtin_cce_copy_from_ub_align64(dst, src, size);
}
# 2699 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_gm_align64(dst, src, size);
}
# 2717 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data_align64(uint8_t *dst, uint8_t *src,
                                             uint64_t size) {
  __builtin_cce_copy_from_stack_align64(dst, src, size);
}
# 2733 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_unif_buff)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_ub(dst, src, size);
}
# 2751 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, __attribute__((cce_global)) uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_gm(dst, src, size);
}
# 2770 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_data(uint8_t *dst, uint8_t *src,
                                     uint64_t size) {
  __builtin_cce_copy_from_stack(dst, src, size);
}
# 2796 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_set_flag(pipe_t p, pipe_t tp, event_t n) {
  __cce_scalar::set_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_wait_flag(pipe_t p, pipe_t tp, event_t n) {
  __cce_scalar::wait_flag(PIPE_M, PIPE_V, n);
  (void)p;
  (void)tp;
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __cce_pipe_barrier(pipe_t p) {
  __cce_scalar::pipe_barrier(PIPE_V);
  (void)p;
}






static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long abs(long long in) { return __builtin_cce_llabs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long abs(long in) { return __builtin_cce_labs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int abs(int in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short abs(short in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char abs(char in) { return __builtin_cce_abs(in); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float abs(float in) { return __builtin_cce_fabsf(in); };
# 2835 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long long in) {
  return __BUILTIN_CCE_SQRT_s64(in);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(long in) {
  return __BUILTIN_CCE_SQRT_s64(in);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(int in) {
  return __BUILTIN_CCE_SQRT_s64(in);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(short in) {
  return __BUILTIN_CCE_SQRT_s64(in);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int64_t sqrt(char in) {
  return __BUILTIN_CCE_SQRT_s64(in);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float sqrt(float in) { return __builtin_cce_sqrtf(in); }

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half sqrt(__cce_half in) { return __builtin_cce_sqrtf16(in); }


static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_S)))[aicore] void __dummy_pipe_s() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_M)))[aicore] void __dummy_pipe_m() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void __dummy_pipe_v() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE1)))[aicore] void __dummy_pipe_mte1() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE2)))[aicore] void __dummy_pipe_mte2() {}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_MTE3)))[aicore] void __dummy_pipe_mte3() {}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long max(long long in1, long long in2) {
  return __builtin_cce_llsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long max(long in1, long in2) {
  return __builtin_cce_lsmax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int max(int in1, int in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short max(short in1, short in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char max(char in1, char in2) {
  return __builtin_cce_smax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long max(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long max(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int max(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short max(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char max(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umax(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float max(float in1, float in2) {
  return __builtin_cce_fmaxf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half max(__cce_half in1, __cce_half in2) {
  return __builtin_cce_fmaxf16(in1, in2);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long long min(long long in1, long long in2) {
  return __builtin_cce_llsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] long min(long in1, long in2) {
  return __builtin_cce_lsmin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] int min(int in1, int in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] short min(short in1, short in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] char min(char in1, char in2) {
  return __builtin_cce_smin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long long min(unsigned long long in1,
                                             unsigned long long in2) {
  return __builtin_cce_llumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned long min(unsigned long in1, unsigned long in2) {
  return __builtin_cce_lumin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned int min(unsigned int in1, unsigned int in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned short min(unsigned short in1,
                                         unsigned short in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] unsigned char min(unsigned char in1, unsigned char in2) {
  return __builtin_cce_umin(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] float min(float in1, float in2) {
  return __builtin_cce_fminf(in1, in2);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] __cce_half min(__cce_half in1, __cce_half in2) {
  return __builtin_cce_fminf16(in1, in2);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_ubuf_to_sbuf(void *dst, __attribute__((cce_unif_buff)) void *src,
                                             uint64_t size, int64_t inc) {
  __BUILTIN_CCE_MOV_UB_TO_SB(dst, src, size, inc);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void copy_sbuf_to_ubuf(__attribute__((cce_unif_buff)) void *dst, void *src,
                                             uint64_t size, int64_t inc) {
  __BUILTIN_CCE_MOV_SB_TO_UB(dst, src, size, inc);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VBS16_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VBS16_f32(dst, src, config);
}
# 3018 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VBS32_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vbitsort(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                    __attribute__((cce_unif_buff)) unsigned int *src1,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VBS32_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                 __attribute__((cce_unif_buff)) __cce_half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VAADD_f16(dst, src0, src1, config);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vaadd(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                 __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VAADD_f32(dst, src0, src1, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) uint8_t *dst,
                                    __attribute__((cce_unif_buff)) uint8_t *src, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VMERGECH_b8(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vmergech(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                    uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VMERGECH_f16(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VRPAC_f16(dst, src, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void vrpac(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src,
                                 uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VRPAC_f32(dst, src, config);
}


static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src0,
                                __attribute__((cce_unif_buff)) __cce_half *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VIOU_f16(dst, src0, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void viou(__attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src0,
                                __attribute__((cce_unif_buff)) float *src1, uint8_t repeat) {
  uint64_t config = static_cast<uint64_t>(repeat) << 56;
  __BUILTIN_CCE_VIOU_f32(dst, src0, src1, config);
}



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] inline void icache_preload(int64_t n) {
  __cce_scalar::preload((const void *)__cce_scalar::get_pc(), n);
}
# 3207 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) uint64_t *src, unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) __cce_half *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((cce_builtin_api, always_inline))[aicore] uint64_t
get_vmrgsort_real_addr(__attribute__((cce_unif_buff)) float *src[4], unsigned shift,
                       uint64_t config = 0x0F00ULL) {
  uint64_t realAddr = 0;
  realAddr =
      (config & 0x0100ULL) ? (((uint64_t)src[0] >> shift) & 0xFFFFULL) : 0;
  realAddr |=
      ((config & 0x0200ULL) ? (((uint64_t)src[1] >> shift) & 0xFFFFULL) : 0)
      << 16;
  realAddr |=
      ((config & 0x0400ULL) ? (((uint64_t)src[2] >> shift) & 0xFFFFULL) : 0)
      << 32;
  realAddr |=
      ((config & 0x0800ULL) ? (((uint64_t)src[3] >> shift) & 0xFFFFULL) : 0)
      << 48;
  return realAddr;
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) __cce_half *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  __cce_scalar::vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask)
    __attribute__((cce_range_check("__BUILTIN_CCE_VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  __cce_scalar::vmrgsort4
  (dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, repeat, list0, list1, list2, list3,
   enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) __cce_half *dst,
                                                     __attribute__((cce_unif_buff)) __cce_half *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  __cce_scalar::vmrgsort4(dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) __cce_half *dst, __attribute__((cce_unif_buff)) __cce_half *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask)
    __attribute__((cce_range_check("__BUILTIN_CCE_VMRGSORT_f16_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  __cce_scalar::vmrgsort4
  (dst, (__attribute__((cce_unif_buff)) __cce_half *)realAddr, repeat, list0, list1, list2, list3,
   enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) uint64_t *src0,
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  __cce_scalar::vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) uint64_t *src, uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask)
    __attribute__((cce_range_check("__BUILTIN_CCE_VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  __cce_scalar::vmrgsort4
  (dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
   enable_exh_sus, mask);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(__attribute__((cce_unif_buff)) float *dst,
                                                     __attribute__((cce_unif_buff)) float *src0[4],
                                                     uint64_t src1,
                                                     uint64_t config) {
  uint64_t realAddr = get_vmrgsort_real_addr(src0, 3, config);
  __cce_scalar::vmrgsort4(dst, (__attribute__((cce_unif_buff)) float *)realAddr, src1, config);
}

static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void vmrgsort4(
    __attribute__((cce_unif_buff)) float *dst, __attribute__((cce_unif_buff)) float *src[4], uint8_t repeat, uint16_t list0,
    uint16_t list1, uint16_t list2, uint16_t list3, bool enable_exh_sus,
    uint8_t mask)
    __attribute__((cce_range_check("__BUILTIN_CCE_VMRGSORT_f32_V220_cfg"))) {
  uint64_t realAddr = get_vmrgsort_real_addr(src, 3);
  __cce_scalar::vmrgsort4
  (dst, (__attribute__((cce_unif_buff)) float *)realAddr, repeat, list0, list1, list2, list3,
   enable_exh_sus, mask);
}
# 3344 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(uint32_t value) { __cce_scalar::set_l0_set_value_ui(value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(__cce_half value) { __cce_scalar::set_l0_set_value_h(value); };



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_l0_set_value(bfloat16_t value) { __cce_scalar::set_l0_set_value_bf16(value); };
# 3363 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, __cce_half value) { __cce_scalar::create_ca_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, uint32_t value) { __cce_scalar::create_ca_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_ca_matrix( __attribute__((cce_cube_a)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { __cce_scalar::create_ca_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, __cce_half value) { __cce_scalar::create_cb_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, uint32_t value) { __cce_scalar::create_cb_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cb_matrix( __attribute__((cce_cube_b)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { __cce_scalar::create_cb_matrix_bf16(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, __cce_half value) { __cce_scalar::create_cbuf_matrix_h(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, uint32_t value) { __cce_scalar::create_cbuf_matrix_ui(dst, repeat, value); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void create_cbuf_matrix( __attribute__((cce_cube_buff)) bfloat16_t *dst, int64_t repeat, bfloat16_t value) { __cce_scalar::create_cbuf_matrix_bf16(dst, repeat, value); };
# 3633 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg_sb(ub_addr8_t addr, uint64_t *array) {
  switch (addr) {
  case VA0:
    __BUILTIN_CCE_MOVEVA((VA0), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA0), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA0), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    __BUILTIN_CCE_MOVEVA((VA1), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA1), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA1), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    __BUILTIN_CCE_MOVEVA((VA2), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA2), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA2), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    __BUILTIN_CCE_MOVEVA((VA3), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA3), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA3), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    __BUILTIN_CCE_MOVEVA((VA4), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA4), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA4), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    __BUILTIN_CCE_MOVEVA((VA5), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA5), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA5), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    __BUILTIN_CCE_MOVEVA((VA6), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA6), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA6), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    __BUILTIN_CCE_MOVEVA((VA7), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA7), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA7), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void set_va_reg(ub_addr8_t addr,
                                      __attribute__((cce_unif_buff)) uint64_t *array) {
  switch (addr) {
  case VA0:
    __BUILTIN_CCE_MOVEVA((VA0), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA0), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA0), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA0), 6, (array[6]), (array[7]));
    break;
  case VA1:
    __BUILTIN_CCE_MOVEVA((VA1), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA1), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA1), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA1), 6, (array[6]), (array[7]));
    break;
  case VA2:
    __BUILTIN_CCE_MOVEVA((VA2), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA2), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA2), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA2), 6, (array[6]), (array[7]));
    break;
  case VA3:
    __BUILTIN_CCE_MOVEVA((VA3), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA3), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA3), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA3), 6, (array[6]), (array[7]));
    break;
  case VA4:
    __BUILTIN_CCE_MOVEVA((VA4), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA4), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA4), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA4), 6, (array[6]), (array[7]));
    break;
  case VA5:
    __BUILTIN_CCE_MOVEVA((VA5), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA5), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA5), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA5), 6, (array[6]), (array[7]));
    break;
  case VA6:
    __BUILTIN_CCE_MOVEVA((VA6), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA6), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA6), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA6), 6, (array[6]), (array[7]));
    break;
  case VA7:
    __BUILTIN_CCE_MOVEVA((VA7), 0, (array[0]), (array[1]));
    __BUILTIN_CCE_MOVEVA((VA7), 2, (array[2]), (array[3]));
    __BUILTIN_CCE_MOVEVA((VA7), 4, (array[4]), (array[5]));
    __BUILTIN_CCE_MOVEVA((VA7), 6, (array[6]), (array[7]));
    break;
  default:
    break;
  }
}
# 4425 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void clear_overflow_status() {
  uint64_t a = __cce_scalar::fake_overflow_status_1();
  uint64_t b = __cce_scalar::fake_overflow_status_2();
  __BUILTIN_CCE_CLEAR_OVERFLOW_STATUS(a, b);
}
# 4869 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
# 4881 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_s4 (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_tf322f32 (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_s4 (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_tf322f32 (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint64_t config) { __cce_scalar::mad_sp ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_sp ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_sp (c, a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) bfloat16_t *a, __attribute__((cce_cube_b)) bfloat16_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::__mad (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::__mad (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::__mad (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::__mad (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_s4 (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void __mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, __attribute__((cce_bias_table_buff)) void * addr, uint16_t m, uint16_t k, uint16_t n, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::__mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_tf322f32 (c, a, b, m, k, n, cmatrixSource, cmatrixInitVal); } };





static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) __cce_half *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) __cce_half *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) __cce_half *a, __attribute__((cce_cube_b)) __cce_half *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) uint32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) uint32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) uint8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) uint8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint64_t config) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32(__attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint64_t config) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_s4 (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool kDirectionAlign, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_tf322f32 (c, a, b, m, k, n, unitFlag, kDirectionAlign, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_s4( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) void *a, __attribute__((cce_cube_b)) void *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_s4 ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_s4 (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_tf322f32( __attribute__((cce_cube_c)) float *c, __attribute__((cce_cube_a)) float *a, __attribute__((cce_cube_b)) float *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t featOffset, uint8_t smaskOffset, uint8_t unitFlag, bool kDirectionAlign, bool isWeightOffset, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_tf322f32 ((__attribute__((cce_cube_c)) float *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_tf322f32 (c, a, b, m, k, n, featOffset, smaskOffset, unitFlag, kDirectionAlign, isWeightOffset, cmatrixSource, cmatrixInitVal); } };

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint64_t config) { __cce_scalar::mad_sp ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, config); };
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void mad_sp( __attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, uint64_t addr, uint16_t m, uint16_t k, uint16_t n, uint8_t unitFlag, bool cmatrixSource, bool cmatrixInitVal) { if (cmatrixSource == 1) { __cce_scalar::mad_sp ((__attribute__((cce_cube_c)) int32_t *)(((uint64_t)c) & 0xffffffffULL | (((uint64_t)addr & 0xffffffffULL) << 32)), a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } else { __cce_scalar::mad_sp (c, a, b, m, k, n, unitFlag, cmatrixSource, cmatrixInitVal); } };
# 5039 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint8_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint8_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint16_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint16_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) __cce_half *
get_ub_virtual_address(__attribute__((cce_unif_buff)) __cce_half *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) int32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) int32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) uint32_t *
get_ub_virtual_address(__attribute__((cce_unif_buff)) uint32_t *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] __attribute__((cce_unif_buff)) float *
get_ub_virtual_address(__attribute__((cce_unif_buff)) float *addr) {
  return addr;
}
static __attribute__((overloadable, always_inline, cce_empty_func_body))[aicore] uint64_t get_ub_virtual_address(uint64_t addr) {
  return addr;
}
# 5123 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
template <typename T>
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(int64_t value, __attribute__((cce_global)) T *addr) {
  __atomic_store_n(addr, value, 0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(float value, __attribute__((cce_global)) float *addr) {
  typedef union {
    int32_t i;
    float f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int32_t *>(addr), tmp.i,
                   0);
}
static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void st_atomic(__cce_half value, __attribute__((cce_global)) __cce_half *addr) {
  typedef union {
    int16_t i;
    __cce_half f;
  } val;
  val tmp;
  tmp.f = value;
  __atomic_store_n(reinterpret_cast<__attribute__((cce_global)) int16_t *>(addr), tmp.i,
                   0);
}
# 5170 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void __vcadd(float& dst, __attribute__((cce_unif_buff)) float *src, uint32_t count) {
  __builtin_cce_set_vector_mask(0, count);
  __builtin_cce_vcadd(src, src, 0x800010001, 1);
  int result = __builtin_cce_get_acc_val();
  dst = (float &)(result);
}
static __attribute__((overloadable, cce_builtin_api, always_inline)) __attribute__((__pipe_type__(PIPE_V)))[aicore] void __vcadd(__cce_half& dst, __attribute__((cce_unif_buff)) __cce_half *src, uint32_t count) {
  __builtin_cce_set_vector_mask(0, count);
  __builtin_cce_vcadd(src, src, 0x800010001, 1);
  int result = __builtin_cce_get_acc_val();
  dst = (__cce_half &)(result);
}

namespace bisheng {
namespace cce {



static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_start() {
  __cce_scalar::pipe_barrier(PIPE_ALL);
  __cce_scalar::set_ctrl(__cce_scalar::sbitset1(__cce_scalar::get_ctrl(), 0));
  __cce_scalar::pipe_barrier(PIPE_ALL);
}

static __attribute__((overloadable, cce_builtin_api, always_inline))[aicore] void metrics_prof_stop() {
  __cce_scalar::pipe_barrier(PIPE_ALL);
  __cce_scalar::set_ctrl(__cce_scalar::sbitset0(__cce_scalar::get_ctrl(), 0));
  __cce_scalar::pipe_barrier(PIPE_ALL);
}
# 5242 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicore_functions.h" 3
using ::abs;
using ::sqrt;


}
}
# 37 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3

# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 1 3
# 71 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_aicpu_neon.h" 3
typedef float float32_t;
typedef __cce_half float16_t;
typedef double float64_t;
typedef int8_t poly8_t;
typedef int16_t poly16_t;
typedef uint64_t poly64_t;
typedef __uint128_t poly128_t;

typedef __attribute__((invalid_vector_type(8))) int8_t int8x8_t;
typedef __attribute__((invalid_vector_type(16))) int8_t int8x16_t;
typedef __attribute__((invalid_vector_type(4))) int16_t int16x4_t;
typedef __attribute__((invalid_vector_type(8))) int16_t int16x8_t;
typedef __attribute__((invalid_vector_type(2))) int32_t int32x2_t;
typedef __attribute__((invalid_vector_type(4))) int32_t int32x4_t;
typedef __attribute__((invalid_vector_type(1))) int64_t int64x1_t;
typedef __attribute__((invalid_vector_type(2))) int64_t int64x2_t;
typedef __attribute__((invalid_vector_type(8))) uint8_t uint8x8_t;
typedef __attribute__((invalid_vector_type(16))) uint8_t uint8x16_t;
typedef __attribute__((invalid_vector_type(4))) uint16_t uint16x4_t;
typedef __attribute__((invalid_vector_type(8))) uint16_t uint16x8_t;
typedef __attribute__((invalid_vector_type(2))) uint32_t uint32x2_t;
typedef __attribute__((invalid_vector_type(4))) uint32_t uint32x4_t;
typedef __attribute__((invalid_vector_type(1))) uint64_t uint64x1_t;
typedef __attribute__((invalid_vector_type(2))) uint64_t uint64x2_t;
typedef __attribute__((invalid_vector_type(4))) float16_t float16x4_t;
typedef __attribute__((invalid_vector_type(8))) float16_t float16x8_t;
typedef __attribute__((invalid_vector_type(2))) float32_t float32x2_t;
typedef __attribute__((invalid_vector_type(4))) float32_t float32x4_t;
typedef __attribute__((invalid_vector_type(1))) float64_t float64x1_t;
typedef __attribute__((invalid_vector_type(2))) float64_t float64x2_t;
typedef __attribute__((invalid_vector_type(8))) poly8_t poly8x8_t;
typedef __attribute__((invalid_vector_type(16))) poly8_t poly8x16_t;
typedef __attribute__((invalid_vector_type(4))) poly16_t poly16x4_t;
typedef __attribute__((invalid_vector_type(8))) poly16_t poly16x8_t;
typedef __attribute__((invalid_vector_type(1))) poly64_t poly64x1_t;
typedef __attribute__((invalid_vector_type(2))) poly64_t poly64x2_t;
# 39 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3



# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 1 3
# 65 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef long long vector_s64
    __attribute__((ext_vector_type(32), ));
typedef int vector_s32
    __attribute__((ext_vector_type(64), ));
typedef short vector_s16
    __attribute__((ext_vector_type(128), ));
typedef char vector_s8
    __attribute__((ext_vector_type(256), ));

typedef unsigned long long vector_u64
    __attribute__((ext_vector_type(32), ));
typedef unsigned int vector_u32
    __attribute__((ext_vector_type(64), ));
typedef unsigned short vector_u16
    __attribute__((ext_vector_type(128), ));
typedef unsigned char vector_u8
    __attribute__((ext_vector_type(256), ));
typedef float vector_f32
    __attribute__((ext_vector_type(64), ));
typedef __cce_half vector_f16
    __attribute__((ext_vector_type(128), ));
# 104 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef bfloat16_t vector_bf16 __attribute__((
    ext_vector_type(128), ));
# 130 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_vector_types.h" 3
typedef long long vector_bool
    __attribute__((ext_vector_type(4), ));


typedef char vector_align_data __attribute__((ext_vector_type(32)));





typedef struct vector_align {
  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align() {

    Data = __builtin_cce_init_vector_align_data();

  }

  [aicore] __attribute__((cce_builtin_api, always_inline))
  operator vector_align_data &() {
    return Data;
  }

  [aicore] __attribute__((cce_builtin_api, always_inline)) vector_align &
  operator=(vector_align_data alignData) {
    Data = alignData;
    return *this;
  }
  vector_align_data Data;
} vector_align;

typedef uint32_t vector_address __attribute__((ext_vector_type(1)));




typedef int wvector_s24
    __attribute__((ext_vector_type(256), ));
typedef long long wvector_s48
    __attribute__((ext_vector_type(128), ));
typedef signed __int128 wvector_s64
    __attribute__((ext_vector_type(64), ));






typedef struct vector_s64x2_t {
  vector_s64 val[2];
} vector_s64x2_t;

typedef struct vector_u64x2_t {
  vector_u64 val[2];
} vector_u64x2_t;

typedef struct vector_s32x2_t {
  vector_s32 val[2];
} vector_s32x2_t;

typedef struct vector_u32x2_t {
  vector_u32 val[2];
} vector_u32x2_t;

typedef struct vector_s16x2_t {
  vector_s16 val[2];
} vector_s16x2_t;

typedef struct vector_u16x2_t {
  vector_u16 val[2];
} vector_u16x2_t;

typedef struct vector_s8x2_t {
  vector_s8 val[2];
} vector_s8x2_t;

typedef struct vector_u8x2_t {
  vector_u8 val[2];
} vector_u8x2_t;

typedef struct vector_f32x2_t {
  vector_f32 val[2];
} vector_f32x2_t;

typedef struct vector_f16x2_t {
  vector_f16 val[2];
} vector_f16x2_t;

typedef struct vector_boolx2_t {
  vector_bool val[2];
} vector_boolx2_t;
# 43 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 2 3
# 103 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
extern "C" {

inline __attribute__((always_inline)) int __cce_getOrSetBlockNum(int value,
                                                                 int type) {
  static thread_local int local = 0;
  if (type == 0)
    local = value;
  return local;
}

}

inline __attribute__((always_inline)) unsigned int
__cce_rtConfigureCall(unsigned int numBlocks, void *smDesc = nullptr,
                      void *stream = nullptr) {
  __cce_getOrSetBlockNum(numBlocks, 0);
  return rtConfigureCall(numBlocks, smDesc, stream);
}
# 146 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/__clang_cce_runtime_wrapper.h" 3
typedef __cce_half half;
# 2 "<built-in>" 2
# 1 "/usr/include/stdio.h" 1 3 4
# 27 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/libc-header-start.h" 1 3 4
# 28 "/usr/include/stdio.h" 2 3 4

extern "C" {



# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 1 3 4
# 34 "/usr/include/stdio.h" 2 3 4


# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdarg.h" 1 3 4
# 14 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list va_list;
# 32 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stdarg.h" 3 4
typedef __builtin_va_list __gnuc_va_list;
# 37 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/x86_64-linux-gnu/bits/types/__fpos_t.h" 1 3 4




# 1 "/usr/include/x86_64-linux-gnu/bits/types/__mbstate_t.h" 1 3 4
# 13 "/usr/include/x86_64-linux-gnu/bits/types/__mbstate_t.h" 3 4
typedef struct
{
  int __count;
  union
  {
    int __wch;
    char __wchb[4];
  } __value;
} __mbstate_t;
# 6 "/usr/include/x86_64-linux-gnu/bits/types/__fpos_t.h" 2 3 4




typedef struct _G_fpos_t
{
  __off_t __pos;
  __mbstate_t __state;
} __fpos_t;
# 40 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/__fpos64_t.h" 1 3 4
# 10 "/usr/include/x86_64-linux-gnu/bits/types/__fpos64_t.h" 3 4
typedef struct _G_fpos64_t
{
  __off64_t __pos;
  __mbstate_t __state;
} __fpos64_t;
# 41 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/__FILE.h" 1 3 4



struct _IO_FILE;
typedef struct _IO_FILE __FILE;
# 42 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/FILE.h" 1 3 4



struct _IO_FILE;


typedef struct _IO_FILE FILE;
# 43 "/usr/include/stdio.h" 2 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/types/struct_FILE.h" 1 3 4
# 35 "/usr/include/x86_64-linux-gnu/bits/types/struct_FILE.h" 3 4
struct _IO_FILE;
struct _IO_marker;
struct _IO_codecvt;
struct _IO_wide_data;




typedef void _IO_lock_t;





struct _IO_FILE
{
  int _flags;


  char *_IO_read_ptr;
  char *_IO_read_end;
  char *_IO_read_base;
  char *_IO_write_base;
  char *_IO_write_ptr;
  char *_IO_write_end;
  char *_IO_buf_base;
  char *_IO_buf_end;


  char *_IO_save_base;
  char *_IO_backup_base;
  char *_IO_save_end;

  struct _IO_marker *_markers;

  struct _IO_FILE *_chain;

  int _fileno;
  int _flags2;
  __off_t _old_offset;


  unsigned short _cur_column;
  signed char _vtable_offset;
  char _shortbuf[1];

  _IO_lock_t *_lock;







  __off64_t _offset;

  struct _IO_codecvt *_codecvt;
  struct _IO_wide_data *_wide_data;
  struct _IO_FILE *_freeres_list;
  void *_freeres_buf;
  size_t __pad5;
  int _mode;

  char _unused2[15 * sizeof (int) - 4 * sizeof (void *) - sizeof (size_t)];
};
# 44 "/usr/include/stdio.h" 2 3 4


# 1 "/usr/include/x86_64-linux-gnu/bits/types/cookie_io_functions_t.h" 1 3 4
# 27 "/usr/include/x86_64-linux-gnu/bits/types/cookie_io_functions_t.h" 3 4
typedef __ssize_t cookie_read_function_t (void *__cookie, char *__buf,
                                          size_t __nbytes);







typedef __ssize_t cookie_write_function_t (void *__cookie, const char *__buf,
                                           size_t __nbytes);







typedef int cookie_seek_function_t (void *__cookie, __off64_t *__pos, int __w);


typedef int cookie_close_function_t (void *__cookie);






typedef struct _IO_cookie_io_functions_t
{
  cookie_read_function_t *read;
  cookie_write_function_t *write;
  cookie_seek_function_t *seek;
  cookie_close_function_t *close;
} cookie_io_functions_t;
# 47 "/usr/include/stdio.h" 2 3 4





typedef __gnuc_va_list va_list;
# 63 "/usr/include/stdio.h" 3 4
typedef __off_t off_t;






typedef __off64_t off64_t;






typedef __ssize_t ssize_t;






typedef __fpos_t fpos_t;




typedef __fpos64_t fpos64_t;
# 133 "/usr/include/stdio.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/stdio_lim.h" 1 3 4
# 134 "/usr/include/stdio.h" 2 3 4
# 143 "/usr/include/stdio.h" 3 4
extern FILE *stdin;
extern FILE *stdout;
extern FILE *stderr;






extern int remove (const char *__filename) noexcept (true);

extern int rename (const char *__old, const char *__new) noexcept (true);



extern int renameat (int __oldfd, const char *__old, int __newfd,
       const char *__new) noexcept (true);
# 170 "/usr/include/stdio.h" 3 4
extern int renameat2 (int __oldfd, const char *__old, int __newfd,
        const char *__new, unsigned int __flags) noexcept (true);






extern int fclose (FILE *__stream);
# 188 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile (void)
  __attribute__ ((__malloc__)) ;
# 200 "/usr/include/stdio.h" 3 4
extern FILE *tmpfile64 (void)
   __attribute__ ((__malloc__)) ;



extern char *tmpnam (char[20]) noexcept (true) ;




extern char *tmpnam_r (char __s[20]) noexcept (true) ;
# 222 "/usr/include/stdio.h" 3 4
extern char *tempnam (const char *__dir, const char *__pfx)
   noexcept (true) __attribute__ ((__malloc__)) ;






extern int fflush (FILE *__stream);
# 239 "/usr/include/stdio.h" 3 4
extern int fflush_unlocked (FILE *__stream);
# 249 "/usr/include/stdio.h" 3 4
extern int fcloseall (void);
# 258 "/usr/include/stdio.h" 3 4
extern FILE *fopen (const char *__restrict __filename,
      const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;




extern FILE *freopen (const char *__restrict __filename,
        const char *__restrict __modes,
        FILE *__restrict __stream) ;
# 283 "/usr/include/stdio.h" 3 4
extern FILE *fopen64 (const char *__restrict __filename,
        const char *__restrict __modes)
  __attribute__ ((__malloc__)) ;
extern FILE *freopen64 (const char *__restrict __filename,
   const char *__restrict __modes,
   FILE *__restrict __stream) ;




extern FILE *fdopen (int __fd, const char *__modes) noexcept (true)
  __attribute__ ((__malloc__)) ;





extern FILE *fopencookie (void *__restrict __magic_cookie,
     const char *__restrict __modes,
     cookie_io_functions_t __io_funcs) noexcept (true)
  __attribute__ ((__malloc__)) ;




extern FILE *fmemopen (void *__s, size_t __len, const char *__modes)
  noexcept (true) __attribute__ ((__malloc__)) ;




extern FILE *open_memstream (char **__bufloc, size_t *__sizeloc) noexcept (true)
  __attribute__ ((__malloc__)) ;
# 328 "/usr/include/stdio.h" 3 4
extern void setbuf (FILE *__restrict __stream, char *__restrict __buf) noexcept (true);



extern int setvbuf (FILE *__restrict __stream, char *__restrict __buf,
      int __modes, size_t __n) noexcept (true);




extern void setbuffer (FILE *__restrict __stream, char *__restrict __buf,
         size_t __size) noexcept (true);


extern void setlinebuf (FILE *__stream) noexcept (true);







extern int fprintf (FILE *__restrict __stream,
      const char *__restrict __format, ...);




extern int printf (const char *__restrict __format, ...);

extern int sprintf (char *__restrict __s,
      const char *__restrict __format, ...) noexcept (true);





extern int vfprintf (FILE *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg);




extern int vprintf (const char *__restrict __format, __gnuc_va_list __arg);

extern int vsprintf (char *__restrict __s, const char *__restrict __format,
       __gnuc_va_list __arg) noexcept (true);



extern int snprintf (char *__restrict __s, size_t __maxlen,
       const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 4)));

extern int vsnprintf (char *__restrict __s, size_t __maxlen,
        const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 3, 0)));





extern int vasprintf (char **__restrict __ptr, const char *__restrict __f,
        __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0))) ;
extern int __asprintf (char **__restrict __ptr,
         const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;
extern int asprintf (char **__restrict __ptr,
       const char *__restrict __fmt, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3))) ;




extern int vdprintf (int __fd, const char *__restrict __fmt,
       __gnuc_va_list __arg)
     __attribute__ ((__format__ (__printf__, 2, 0)));
extern int dprintf (int __fd, const char *__restrict __fmt, ...)
     __attribute__ ((__format__ (__printf__, 2, 3)));







extern int fscanf (FILE *__restrict __stream,
     const char *__restrict __format, ...) ;




extern int scanf (const char *__restrict __format, ...) ;

extern int sscanf (const char *__restrict __s,
     const char *__restrict __format, ...) noexcept (true);





# 1 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 1 3 4
# 119 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 1 3 4
# 24 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
# 1 "/usr/include/x86_64-linux-gnu/bits/long-double.h" 1 3 4
# 25 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 2 3 4
# 214 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
typedef float _Float32;
# 251 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
typedef double _Float64;
# 268 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
typedef double _Float32x;
# 285 "/usr/include/x86_64-linux-gnu/bits/floatn-common.h" 3 4
typedef long double _Float64x;
# 120 "/usr/include/x86_64-linux-gnu/bits/floatn.h" 2 3 4
# 431 "/usr/include/stdio.h" 2 3 4



extern int fscanf (FILE *__restrict __stream, const char *__restrict __format, ...) __asm__ ("" "__isoc99_fscanf") ;


extern int scanf (const char *__restrict __format, ...) __asm__ ("" "__isoc99_scanf") ;

extern int sscanf (const char *__restrict __s, const char *__restrict __format, ...) noexcept (true) __asm__ ("" "__isoc99_sscanf");
# 459 "/usr/include/stdio.h" 3 4
extern int vfscanf (FILE *__restrict __s, const char *__restrict __format,
      __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 2, 0))) ;





extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg)
     __attribute__ ((__format__ (__scanf__, 1, 0))) ;


extern int vsscanf (const char *__restrict __s,
      const char *__restrict __format, __gnuc_va_list __arg)
     noexcept (true) __attribute__ ((__format__ (__scanf__, 2, 0)));





extern int vfscanf (FILE *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vfscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0))) ;
extern int vscanf (const char *__restrict __format, __gnuc_va_list __arg) __asm__ ("" "__isoc99_vscanf")

     __attribute__ ((__format__ (__scanf__, 1, 0))) ;
extern int vsscanf (const char *__restrict __s, const char *__restrict __format, __gnuc_va_list __arg) noexcept (true) __asm__ ("" "__isoc99_vsscanf")



     __attribute__ ((__format__ (__scanf__, 2, 0)));
# 513 "/usr/include/stdio.h" 3 4
extern int fgetc (FILE *__stream);
extern int getc (FILE *__stream);





extern int getchar (void);






extern int getc_unlocked (FILE *__stream);
extern int getchar_unlocked (void);
# 538 "/usr/include/stdio.h" 3 4
extern int fgetc_unlocked (FILE *__stream);
# 549 "/usr/include/stdio.h" 3 4
extern int fputc (int __c, FILE *__stream);
extern int putc (int __c, FILE *__stream);





extern int putchar (int __c);
# 565 "/usr/include/stdio.h" 3 4
extern int fputc_unlocked (int __c, FILE *__stream);







extern int putc_unlocked (int __c, FILE *__stream);
extern int putchar_unlocked (int __c);






extern int getw (FILE *__stream);


extern int putw (int __w, FILE *__stream);







extern char *fgets (char *__restrict __s, int __n, FILE *__restrict __stream)
                                                         ;
# 615 "/usr/include/stdio.h" 3 4
extern char *fgets_unlocked (char *__restrict __s, int __n,
        FILE *__restrict __stream)
                                                  ;
# 632 "/usr/include/stdio.h" 3 4
extern __ssize_t __getdelim (char **__restrict __lineptr,
                             size_t *__restrict __n, int __delimiter,
                             FILE *__restrict __stream) ;
extern __ssize_t getdelim (char **__restrict __lineptr,
                           size_t *__restrict __n, int __delimiter,
                           FILE *__restrict __stream) ;







extern __ssize_t getline (char **__restrict __lineptr,
                          size_t *__restrict __n,
                          FILE *__restrict __stream) ;







extern int fputs (const char *__restrict __s, FILE *__restrict __stream);





extern int puts (const char *__s);






extern int ungetc (int __c, FILE *__stream);






extern size_t fread (void *__restrict __ptr, size_t __size,
       size_t __n, FILE *__restrict __stream) ;




extern size_t fwrite (const void *__restrict __ptr, size_t __size,
        size_t __n, FILE *__restrict __s);
# 691 "/usr/include/stdio.h" 3 4
extern int fputs_unlocked (const char *__restrict __s,
      FILE *__restrict __stream);
# 702 "/usr/include/stdio.h" 3 4
extern size_t fread_unlocked (void *__restrict __ptr, size_t __size,
         size_t __n, FILE *__restrict __stream) ;
extern size_t fwrite_unlocked (const void *__restrict __ptr, size_t __size,
          size_t __n, FILE *__restrict __stream);







extern int fseek (FILE *__stream, long int __off, int __whence);




extern long int ftell (FILE *__stream) ;




extern void rewind (FILE *__stream);
# 736 "/usr/include/stdio.h" 3 4
extern int fseeko (FILE *__stream, __off_t __off, int __whence);




extern __off_t ftello (FILE *__stream) ;
# 760 "/usr/include/stdio.h" 3 4
extern int fgetpos (FILE *__restrict __stream, fpos_t *__restrict __pos);




extern int fsetpos (FILE *__stream, const fpos_t *__pos);
# 779 "/usr/include/stdio.h" 3 4
extern int fseeko64 (FILE *__stream, __off64_t __off, int __whence);
extern __off64_t ftello64 (FILE *__stream) ;
extern int fgetpos64 (FILE *__restrict __stream, fpos64_t *__restrict __pos);
extern int fsetpos64 (FILE *__stream, const fpos64_t *__pos);



extern void clearerr (FILE *__stream) noexcept (true);

extern int feof (FILE *__stream) noexcept (true) ;

extern int ferror (FILE *__stream) noexcept (true) ;



extern void clearerr_unlocked (FILE *__stream) noexcept (true);
extern int feof_unlocked (FILE *__stream) noexcept (true) ;
extern int ferror_unlocked (FILE *__stream) noexcept (true) ;







extern void perror (const char *__s);




extern int fileno (FILE *__stream) noexcept (true) ;




extern int fileno_unlocked (FILE *__stream) noexcept (true) ;
# 823 "/usr/include/stdio.h" 3 4
extern int pclose (FILE *__stream);





extern FILE *popen (const char *__command, const char *__modes)
  __attribute__ ((__malloc__)) ;






extern char *ctermid (char *__s) noexcept (true)
                                     ;





extern char *cuserid (char *__s)
                                     ;




struct obstack;


extern int obstack_printf (struct obstack *__restrict __obstack,
      const char *__restrict __format, ...)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 3)));
extern int obstack_vprintf (struct obstack *__restrict __obstack,
       const char *__restrict __format,
       __gnuc_va_list __args)
     noexcept (true) __attribute__ ((__format__ (__printf__, 2, 0)));







extern void flockfile (FILE *__stream) noexcept (true);



extern int ftrylockfile (FILE *__stream) noexcept (true) ;


extern void funlockfile (FILE *__stream) noexcept (true);
# 885 "/usr/include/stdio.h" 3 4
extern int __uflow (FILE *);
extern int __overflow (FILE *, int);




# 1 "/usr/include/x86_64-linux-gnu/bits/stdio.h" 1 3 4
# 38 "/usr/include/x86_64-linux-gnu/bits/stdio.h" 3 4
extern __inline __attribute__ ((__gnu_inline__)) int
vprintf (const char *__restrict __fmt, __gnuc_va_list __arg)
{
  return vfprintf (stdout, __fmt, __arg);
}



extern __inline __attribute__ ((__gnu_inline__)) int
getchar (void)
{
  return getc (stdin);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fgetc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}





extern __inline __attribute__ ((__gnu_inline__)) int
getc_unlocked (FILE *__fp)
{
  return (__builtin_expect (((__fp)->_IO_read_ptr >= (__fp)->_IO_read_end), 0) ? __uflow (__fp) : *(unsigned char *) (__fp)->_IO_read_ptr++);
}


extern __inline __attribute__ ((__gnu_inline__)) int
getchar_unlocked (void)
{
  return (__builtin_expect (((stdin)->_IO_read_ptr >= (stdin)->_IO_read_end), 0) ? __uflow (stdin) : *(unsigned char *) (stdin)->_IO_read_ptr++);
}




extern __inline __attribute__ ((__gnu_inline__)) int
putchar (int __c)
{
  return putc (__c, stdout);
}




extern __inline __attribute__ ((__gnu_inline__)) int
fputc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) int
putc_unlocked (int __c, FILE *__stream)
{
  return (__builtin_expect (((__stream)->_IO_write_ptr >= (__stream)->_IO_write_end), 0) ? __overflow (__stream, (unsigned char) (__c)) : (unsigned char) (*(__stream)->_IO_write_ptr++ = (__c)));
}


extern __inline __attribute__ ((__gnu_inline__)) int
putchar_unlocked (int __c)
{
  return (__builtin_expect (((stdout)->_IO_write_ptr >= (stdout)->_IO_write_end), 0) ? __overflow (stdout, (unsigned char) (__c)) : (unsigned char) (*(stdout)->_IO_write_ptr++ = (__c)));
}





extern __inline __attribute__ ((__gnu_inline__)) __ssize_t
getline (char **__lineptr, size_t *__n, FILE *__stream)
{
  return __getdelim (__lineptr, __n, '\n', __stream);
}





extern __inline __attribute__ ((__gnu_inline__)) int
 feof_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0010) != 0);
}


extern __inline __attribute__ ((__gnu_inline__)) int
 ferror_unlocked (FILE *__stream) noexcept (true)
{
  return (((__stream)->_flags & 0x0020) != 0);
}
# 892 "/usr/include/stdio.h" 2 3 4
# 902 "/usr/include/stdio.h" 3 4
}
# 3 "<built-in>" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/../include/ascendc/asc_devkit_version.h" 1
# 4 "<built-in>" 2
# 1 "/home/lnick/GitHub/Ascend/AscendC/lesson_01/0_introduction/0_helloworld/hello_world.cpp" 2
# 10 "/home/lnick/GitHub/Ascend/AscendC/lesson_01/0_introduction/0_helloworld/hello_world.cpp"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h" 1
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_macros.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_macros.h"
# 1 "/usr/include/c++/11/cstdint" 1 3
# 33 "/usr/include/c++/11/cstdint" 3





# 1 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 1 3
# 278 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 3
namespace std
{
  typedef long unsigned int size_t;
  typedef long int ptrdiff_t;


  typedef decltype(nullptr) nullptr_t;

}
# 300 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 3
namespace std
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
namespace __gnu_cxx
{
  inline namespace __cxx11 __attribute__((__abi_tag__ ("cxx11"))) { }
}
# 586 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 3
# 1 "/usr/include/x86_64-linux-gnu/c++/11/bits/os_defines.h" 1 3
# 587 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 2 3


# 1 "/usr/include/x86_64-linux-gnu/c++/11/bits/cpu_defines.h" 1 3
# 590 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 2 3
# 777 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 3
# 1 "/usr/include/c++/11/pstl/pstl_config.h" 1 3
# 778 "/usr/include/x86_64-linux-gnu/c++/11/bits/c++config.h" 2 3
# 39 "/usr/include/c++/11/cstdint" 2 3





namespace std
{

  using ::int8_t;
  using ::int16_t;
  using ::int32_t;
  using ::int64_t;

  using ::int_fast8_t;
  using ::int_fast16_t;
  using ::int_fast32_t;
  using ::int_fast64_t;

  using ::int_least8_t;
  using ::int_least16_t;
  using ::int_least32_t;
  using ::int_least64_t;

  using ::intmax_t;
  using ::intptr_t;

  using ::uint8_t;
  using ::uint16_t;
  using ::uint32_t;
  using ::uint64_t;

  using ::uint_fast8_t;
  using ::uint_fast16_t;
  using ::uint_fast32_t;
  using ::uint_fast64_t;

  using ::uint_least8_t;
  using ::uint_least16_t;
  using ::uint_least32_t;
  using ::uint_least64_t;

  using ::uintmax_t;
  using ::uintptr_t;





}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_macros.h" 2
# 126 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_macros.h"
namespace AscendC {



constexpr int32_t QUE_MAX_EVENT = 8;



constexpr int32_t HF32_MODE_BIT = 46;
constexpr int32_t HF32_TRANS_MODE_BIT = 47;
constexpr int32_t MM_LAYOUT_MODE_BIT = 51;
constexpr int32_t LEAKY_RELU_MODE_BIT = 50;
constexpr int32_t CAST_MODE_BIT = 59;

constexpr int32_t MIX = 0;
constexpr int32_t AIC = 1;
constexpr int32_t AIV = 2;
}
# 156 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_macros.h"
constexpr int32_t g_coreType = AscendC::AIV;
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_log.h" 1
# 266 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_log.h"
namespace AscendC {
template <class... Args>
[aicore] inline void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args);

template <class... Args>
[aicore] static __attribute__ ((noinline)) void AssertPrint(__attribute__((cce_global)) const char* fmt, Args&&... args)
{
    AscendC::AssertImpl(fmt, args...);
}
# 423 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_log.h"
}

namespace AscendC{
# 436 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_log.h"
}
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/common_types.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/common_types.h"
namespace AscendC {
enum class TPosition : uint8_t {
    GM,
    A1,
    A2,
    B1,
    B2,
    C1,
    C2,
    CO1,
    CO2,
    VECIN,
    VECOUT,
    VECCALC,
    LCM = VECCALC,
    SPM,
    SHM = SPM,
    TSCM,
    C2PIPE2GM,
    C2PIPE2LOCAL,
    MAX,
};
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h" 2






namespace AscendC {
using QuePosition = TPosition;
enum class Hardware : uint8_t { GM, UB, L1, L0A, L0B, L0C, BIAS, FIXBUF, MAX };

enum class HardEvent : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    M_V,
    V_M,
    V_V,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE1_V,
    MTE2_M,
    M_MTE2,
    V_MTE1,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAic : uint8_t {

    MTE2_MTE1,
    MTE1_MTE2,
    MTE1_M,
    M_MTE1,
    MTE3_MTE1,
    MTE1_MTE3,
    MTE2_M,
    M_MTE2,
    M_FIX,
    FIX_M,
    MTE3_MTE2,
    MTE2_MTE3,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MTE2_FIX,
    FIX_MTE2,
    FIX_S,
    M_S,
    FIX_MTE3,
    MTE1_FIX,
    FIX_MTE1,
    FIX_FIX,
    MAX,
};

enum class HardEventAiv : uint8_t {

    MTE2_V,
    V_MTE2,
    MTE3_V,
    V_MTE3,
    V_V,
    MTE3_MTE2,
    MTE2_MTE3,
    S_V,
    V_S,
    S_MTE2,
    MTE2_S,
    S_MTE3,
    MTE3_S,
    MAX,
};

enum class MemoryT : uint8_t { L1 = 0, L0A, L0B, L0C, UB, BIAS };

enum class MemDsbT : uint8_t { ALL = 0, DDR, UB, SEQ };







constexpr uint8_t EVENT_NUM = static_cast<uint8_t>(HardEventAiv::MAX);



[aicore] constexpr uint8_t EventToIndexAic(HardEvent evt)
{


    if (evt == HardEvent::MTE2_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE1);
    } else if (evt == HardEvent::MTE1_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE2);
    } else if (evt == HardEvent::MTE1_M) {
        return static_cast<uint8_t>(HardEventAic::MTE1_M);
    } else if (evt == HardEvent::M_MTE1) {
        return static_cast<uint8_t>(HardEventAic::M_MTE1);
    } else if (evt == HardEvent::MTE3_MTE1) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE1);
    } else if (evt == HardEvent::MTE1_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE1_MTE3);
    } else if (evt == HardEvent::MTE2_M) {
        return static_cast<uint8_t>(HardEventAic::MTE2_M);
    } else if (evt == HardEvent::M_MTE2) {
        return static_cast<uint8_t>(HardEventAic::M_MTE2);
    } else if (evt == HardEvent::M_FIX) {
        return static_cast<uint8_t>(HardEventAic::M_FIX);
    } else if (evt == HardEvent::FIX_M) {
        return static_cast<uint8_t>(HardEventAic::FIX_M);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAic::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAic::MTE2_MTE3);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAic::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAic::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAic::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAic::MTE3_S);
    } else if (evt == HardEvent::MTE2_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE2_FIX);
    } else if (evt == HardEvent::FIX_MTE2) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE2);
    } else if (evt == HardEvent::FIX_S) {
        return static_cast<uint8_t>(HardEventAic::FIX_S);
    } else if (evt == HardEvent::M_S) {
        return static_cast<uint8_t>(HardEventAic::M_S);
    } else if (evt == HardEvent::FIX_MTE3) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE3);
    } else if (evt == HardEvent::FIX_MTE1) {
        return static_cast<uint8_t>(HardEventAic::FIX_MTE1);
    } else if (evt == HardEvent::MTE1_FIX) {
        return static_cast<uint8_t>(HardEventAic::MTE1_FIX);
    } else if (evt == HardEvent::FIX_FIX) {
        return static_cast<uint8_t>(HardEventAic::FIX_FIX);
    } else {
        return static_cast<uint8_t>(HardEventAic::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndexAiv(HardEvent evt)
{


    if (evt == HardEvent::MTE2_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_V);
    } else if (evt == HardEvent::V_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE2);
    } else if (evt == HardEvent::MTE3_V) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_V);
    } else if (evt == HardEvent::V_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::V_MTE3);
    } else if (evt == HardEvent::V_V) {
        return static_cast<uint8_t>(HardEventAiv::V_V);
    } else if (evt == HardEvent::MTE3_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_MTE2);
    } else if (evt == HardEvent::MTE2_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_MTE3);
    } else if (evt == HardEvent::S_V) {
        return static_cast<uint8_t>(HardEventAiv::S_V);
    } else if (evt == HardEvent::V_S) {
        return static_cast<uint8_t>(HardEventAiv::V_S);
    } else if (evt == HardEvent::S_MTE2) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE2);
    } else if (evt == HardEvent::MTE2_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE2_S);
    } else if (evt == HardEvent::S_MTE3) {
        return static_cast<uint8_t>(HardEventAiv::S_MTE3);
    } else if (evt == HardEvent::MTE3_S) {
        return static_cast<uint8_t>(HardEventAiv::MTE3_S);
    } else {
        return static_cast<uint8_t>(HardEventAiv::MAX);
    }
}

[aicore] constexpr uint8_t EventToIndex(HardEvent evt)
{






    return EventToIndexAiv(evt);

}



constexpr int32_t PIPE_NUM = 7;
constexpr pipe_t SUPPORTED_PIPE[PIPE_NUM] = { PIPE_S, PIPE_V, PIPE_M, PIPE_MTE1, PIPE_MTE2, PIPE_MTE3, PIPE_FIX };






template <pipe_t pipe>
[aicore] inline constexpr bool IsSplitVectorPipe()
{
    return pipe == PIPE_S || pipe == PIPE_V || pipe == PIPE_MTE2 || pipe == PIPE_MTE3;
}

template <pipe_t pipe>
[aicore] inline constexpr bool IsSplitCubePipe()
{





    return pipe == PIPE_S || pipe == PIPE_MTE1 || pipe == PIPE_MTE2 || pipe == PIPE_MTE3 || pipe == PIPE_FIX || pipe == PIPE_M;

}

template <pipe_t pipe>
[aicore] inline void PipeBarrierInternal()
{
# 279 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    pipe_barrier(pipe);

}

template <pipe_t srcPipe, pipe_t dstPipe>
[aicore] inline void SetFlagInternal(event_t evt)
{
# 298 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    set_flag(srcPipe, dstPipe, evt);

}

template <pipe_t srcPipe, pipe_t dstPipe>
[aicore] inline void WaitFlagInternal(event_t evt)
{
# 317 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    wait_flag(srcPipe, dstPipe, evt);

}


[aicore] constexpr bool IsSupportedPipe(pipe_t pipe)
{
    for (int i = 0; i < PIPE_NUM; i++) {
        if (pipe == SUPPORTED_PIPE[i]) {
            return true;
        }
    }
    return false;
}

[aicore] constexpr TPosition GetDefaultPosition(Hardware hard)
{
    if (hard == Hardware::UB) {
        return TPosition::VECCALC;
    } else if (hard == Hardware::L1) {
        return TPosition::A1;
    } else if (hard == Hardware::L0A) {
        return TPosition::A2;
    } else if (hard == Hardware::L0B) {
        return TPosition::B2;
    } else if (hard == Hardware::L0C) {
        return TPosition::CO1;
    } else if (hard == Hardware::BIAS) {
        return TPosition::C2;
    } else if (hard == Hardware::FIXBUF) {
        return TPosition::C2PIPE2GM;
    }
    return TPosition::MAX;
}

[aicore] constexpr Hardware GetPhyType(TPosition pos)
{
                                 ;
    Hardware hard = Hardware::UB;
    if (pos == TPosition::GM) {
        hard = Hardware::GM;
    } else if (pos == TPosition::A1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::A2) {
hard = Hardware::L0A;
    } else if (pos == TPosition::B1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::B2) {
        hard = Hardware::L0B;
# 374 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    } else if (pos == TPosition::C1) {
        hard = Hardware::L1;
    } else if (pos == TPosition::C2) {
        hard = Hardware::BIAS;
    } else if (pos == TPosition::CO2) {
        hard = Hardware::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        hard = Hardware::FIXBUF;
# 411 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    } else if (pos == TPosition::CO1) {
hard = Hardware::L0C;
    } else if (pos == TPosition::SHM) {
        hard = Hardware::L1;
    } else if (pos == TPosition::TSCM) {
        hard = Hardware::L1;
    }
    return hard;
}

[aicore] constexpr TPosition GetPosition(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;






    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return srcPos;
    }

    return dstPos;
}

[aicore] constexpr Hardware GetBufferPos(TPosition srcPos, TPosition dstPos)
{

                                                                       ;
                                                                          ;






    if ((dstPos == TPosition::GM) || (dstPos == TPosition::CO2)) {
        return GetPhyType(srcPos);
    }

    return GetPhyType(dstPos);
}

[aicore] constexpr TPosition GetBufferLogicPos(TPosition pos, bool isSrc)
{
                                ;
                                     ;
                                 ;
    if (pos == TPosition::A1) {
        return isSrc ? TPosition::GM : TPosition::A1;
    } else if (pos == TPosition::B1) {
        return isSrc ? TPosition::GM : TPosition::B1;
    } else if (pos == TPosition::C1) {
        return isSrc ? TPosition::GM : TPosition::C1;
    } else if (pos == TPosition::A2) {
        return isSrc ? TPosition::A1 : TPosition::A2;
    } else if (pos == TPosition::B2) {
        return isSrc ? TPosition::B1 : TPosition::B2;
    } else if (pos == TPosition::C2) {
        return isSrc ? TPosition::C1 : TPosition::C2;
    } else if (pos == TPosition::CO1) {
        return isSrc ? TPosition::CO1 : TPosition::CO2;
    } else if (pos == TPosition::CO2) {
        return isSrc ? TPosition::CO2 : TPosition::GM;
    } else if (pos == TPosition::VECIN) {
        return isSrc ? TPosition::GM : TPosition::VECIN;
    } else if (pos == TPosition::VECOUT) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::SPM) {
        return isSrc ? TPosition::VECOUT : TPosition::GM;
    } else if (pos == TPosition::C2PIPE2GM) {
        return isSrc ? TPosition::B1 : TPosition::C2PIPE2GM;
    }
    return TPosition::MAX;
}

[aicore] constexpr HardEvent GetQueEvt(Hardware src, Hardware dst, bool fwdDirect, bool nd2nz = false,
                                         bool nz2nd = false)
{
    (void)(nz2nd);

                                                            ;
                                ;
                                ;
    if (src == Hardware::GM) {
                                   ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_V : HardEvent::V_MTE2;
        } else if (dst == Hardware::L1) {

            (void)(nd2nz);






            return fwdDirect ? HardEvent::MTE2_MTE1 : HardEvent::MTE1_MTE2;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE2_M : HardEvent::M_MTE2;
        }
    } else if (src == Hardware::UB) {
                                    ;
                                    ;
                                     ;
                                       ;
        if (dst == Hardware::GM) {
            return fwdDirect ? HardEvent::V_MTE3 : HardEvent::MTE3_V;
        } else if (dst == Hardware::L1) {
            return fwdDirect ? HardEvent::MTE3_MTE1 : HardEvent::MTE1_MTE3;
        } else if (dst == Hardware::L0C) {
            return fwdDirect ? HardEvent::V_V : HardEvent::MAX;
        } else if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE2_MTE3 : HardEvent::MTE3_MTE2;
        }
    } else if (src == Hardware::L1) {
                                   ;
                                   ;
                                    ;




        if (dst == Hardware::UB) {
            return fwdDirect ? HardEvent::MTE1_V : HardEvent::V_MTE1;
        } else if (dst == Hardware::L0A) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::L0B) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        } else if (dst == Hardware::FIXBUF) {
            return fwdDirect ? HardEvent::MTE1_FIX : HardEvent::FIX_MTE1;
        } else if (dst == Hardware::BIAS) {
            return fwdDirect ? HardEvent::MTE1_M : HardEvent::M_MTE1;
        }
    } else if (src == Hardware::L0A) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;
    } else if (src == Hardware::L0B) {
                                    ;
        return fwdDirect ? HardEvent::M_V : HardEvent::V_M;






    } else if (src == Hardware::L0C) {
                                   ;
        return fwdDirect ? HardEvent::M_FIX : HardEvent::FIX_M;
    }
# 582 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
    return HardEvent::MAX;
}
# 729 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
template <MemDsbT arg>
[aicore] inline void DataSyncBarrierImpl()
{
    if constexpr (arg == MemDsbT::UB) {
        if constexpr(g_coreType == AscendC::AIC) {
            return;
        }
    }
    dsb((mem_dsb_t)arg);
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] inline void HSetFlagImpl(int32_t eventID)
{

                                                                                                                     ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
        "For HSetFlag, memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hset_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hset_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hset_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hset_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}

template <HardEvent event, MemoryT memT, bool isVirtual>
[aicore] inline void HWaitFlagImpl(int32_t eventID)
{

                                                                                                                                 ;
    static_assert(((int32_t)memT >= 0 && memT <= MemoryT::BIAS && memT != MemoryT::UB && memT != MemoryT::L1),
                  "For HWaitFlag, memT only support L0A, L0B, L0C, BIAS.");

    event_t e = static_cast<event_t>(eventID);

    switch (event) {
        case HardEvent::MTE1_M:

                                                                         ;
            hwait_flag(PIPE_MTE1, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_MTE1:

                                                                         ;
            hwait_flag(PIPE_M, PIPE_MTE1, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::M_FIX:
                                                                                     ;
            hwait_flag(PIPE_M, PIPE_FIX, e, (mem_t)memT, isVirtual);
            break;
        case HardEvent::FIX_M:
                                                                                     ;
            hwait_flag(PIPE_FIX, PIPE_M, e, (mem_t)memT, isVirtual);
            break;
        default:
                                                                                                           ;
            break;
    }
}


template <HardEvent event>
[aicore] inline void SetFlagImpl(int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
        case HardEvent::MTE2_MTE1:
            SetFlagInternal<PIPE_MTE2, PIPE_MTE1>(e);
            break;
        case HardEvent::MTE1_MTE2:
            SetFlagInternal<PIPE_MTE1, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE2_MTE3:
            SetFlagInternal<PIPE_MTE2, PIPE_MTE3>(e);
            break;
        case HardEvent::MTE3_MTE2:
            SetFlagInternal<PIPE_MTE3, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE1_M:
            SetFlagInternal<PIPE_MTE1, PIPE_M>(e);
            break;
        case HardEvent::M_MTE1:
            SetFlagInternal<PIPE_M, PIPE_MTE1>(e);
            break;
        case HardEvent::MTE2_V:
            SetFlagInternal<PIPE_MTE2, PIPE_V>(e);
            break;
        case HardEvent::V_MTE2:
            SetFlagInternal<PIPE_V, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE3_V:
            SetFlagInternal<PIPE_MTE3, PIPE_V>(e);
            break;
        case HardEvent::V_MTE3:
            SetFlagInternal<PIPE_V, PIPE_MTE3>(e);
            break;
        case HardEvent::M_V:
            SetFlagInternal<PIPE_M, PIPE_V>(e);
            break;
        case HardEvent::M_S:
            SetFlagInternal<PIPE_M, PIPE_S>(e);
            break;
        case HardEvent::V_M:
            SetFlagInternal<PIPE_V, PIPE_M>(e);
            break;
        case HardEvent::S_V:
            SetFlagInternal<PIPE_S, PIPE_V>(e);
            break;
        case HardEvent::V_S:
            SetFlagInternal<PIPE_V, PIPE_S>(e);
            break;

        case HardEvent::V_V:
            PipeBarrierInternal<PIPE_V>();
            return;

        case HardEvent::MTE3_MTE1:
            SetFlagInternal<PIPE_MTE3, PIPE_MTE1>(e);
            break;
        case HardEvent::MTE1_MTE3:
            SetFlagInternal<PIPE_MTE1, PIPE_MTE3>(e);
            break;
        case HardEvent::MTE1_V:
            SetFlagInternal<PIPE_MTE1, PIPE_V>(e);
            break;
        case HardEvent::MTE2_M:
            SetFlagInternal<PIPE_MTE2, PIPE_M>(e);
            break;
        case HardEvent::M_MTE2:
            SetFlagInternal<PIPE_M, PIPE_MTE2>(e);
            break;
        case HardEvent::S_MTE2:
            SetFlagInternal<PIPE_S, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE2_S:
            SetFlagInternal<PIPE_MTE2, PIPE_S>(e);
            break;
        case HardEvent::V_MTE1:
            SetFlagInternal<PIPE_V, PIPE_MTE1>(e);
            break;
        case HardEvent::S_MTE3:
            SetFlagInternal<PIPE_S, PIPE_MTE3>(e);
            break;
        case HardEvent::MTE3_S:
            SetFlagInternal<PIPE_MTE3, PIPE_S>(e);
            break;



        case HardEvent::M_FIX:
            SetFlagInternal<PIPE_M, PIPE_FIX>(e);
            break;
        case HardEvent::FIX_M:
            SetFlagInternal<PIPE_FIX, PIPE_M>(e);
            break;
        case HardEvent::FIX_MTE3:
            SetFlagInternal<PIPE_FIX, PIPE_MTE3>(e);
            break;
        case HardEvent::FIX_MTE2:
            SetFlagInternal<PIPE_FIX, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE2_FIX:
            SetFlagInternal<PIPE_MTE2, PIPE_FIX>(e);
            break;
        case HardEvent::FIX_S:
            SetFlagInternal<PIPE_FIX, PIPE_S>(e);
            break;
        case HardEvent::MTE1_FIX:
            SetFlagInternal<PIPE_MTE1, PIPE_FIX>(e);
            break;
        case HardEvent::FIX_MTE1:
            SetFlagInternal<PIPE_FIX, PIPE_MTE1>(e);
            break;
        case HardEvent::FIX_FIX:
            PipeBarrierInternal<PIPE_FIX>();
            break;

        case HardEvent::MAX:
            break;
        default:
                                                                                                               ;
            break;
    }
}

[aicore] inline void WaitFlagImpl(const HardEvent event, int32_t eventID)
{

                                                                                                                  ;
    event_t e = static_cast<event_t>(eventID);
    switch (event) {
# 970 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
        case HardEvent::MTE2_V:
            WaitFlagInternal<PIPE_MTE2, PIPE_V>(e);
            break;
        case HardEvent::V_MTE2:
            WaitFlagInternal<PIPE_V, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE3_V:
            WaitFlagInternal<PIPE_MTE3, PIPE_V>(e);
            break;
        case HardEvent::V_MTE3:
            WaitFlagInternal<PIPE_V, PIPE_MTE3>(e);
            break;
# 999 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_event.h"
        case HardEvent::FIX_M:
            WaitFlagInternal<PIPE_FIX, PIPE_M>(e);
            break;
        case HardEvent::M_FIX:
            WaitFlagInternal<PIPE_M, PIPE_FIX>(e);
            break;
        case HardEvent::MTE2_FIX:
            WaitFlagInternal<PIPE_MTE2, PIPE_FIX>(e);
            break;
        case HardEvent::FIX_MTE2:
            WaitFlagInternal<PIPE_FIX, PIPE_MTE2>(e);
            break;
        case HardEvent::FIX_S:
            WaitFlagInternal<PIPE_FIX, PIPE_S>(e);
            break;
        case HardEvent::FIX_MTE3:
            WaitFlagInternal<PIPE_FIX, PIPE_MTE3>(e);
            break;
        case HardEvent::MTE1_FIX:
            WaitFlagInternal<PIPE_MTE1, PIPE_FIX>(e);
            break;
        case HardEvent::FIX_MTE1:
            WaitFlagInternal<PIPE_FIX, PIPE_MTE1>(e);
            break;
        case HardEvent::FIX_FIX:
            PipeBarrierInternal<PIPE_FIX>();
            break;

        case HardEvent::MTE3_MTE2:
            WaitFlagInternal<PIPE_MTE3, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE2_MTE3:
            WaitFlagInternal<PIPE_MTE2, PIPE_MTE3>(e);
            break;
        case HardEvent::S_MTE2:
            WaitFlagInternal<PIPE_S, PIPE_MTE2>(e);
            break;
        case HardEvent::MTE2_S:
            WaitFlagInternal<PIPE_MTE2, PIPE_S>(e);
            break;
        case HardEvent::S_MTE3:
            WaitFlagInternal<PIPE_S, PIPE_MTE3>(e);
            break;
        case HardEvent::MTE3_S:
            WaitFlagInternal<PIPE_MTE3, PIPE_S>(e);
            break;
        case HardEvent::M_S:
            WaitFlagInternal<PIPE_M, PIPE_S>(e);
            break;
        case HardEvent::S_V:
            WaitFlagInternal<PIPE_S, PIPE_V>(e);
            break;
        case HardEvent::V_S:
            WaitFlagInternal<PIPE_V, PIPE_S>(e);
            break;
        case HardEvent::V_V:
            return;
        case HardEvent::MAX:
            break;
        default:
            break;
    }
    return;
}
}
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h" 2
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
enum KernelMetaType : uint8_t {
    KERNEL_TYPE_AIV_ONLY,
    KERNEL_TYPE_AIC_ONLY,
    KERNEL_TYPE_MIX_AIV_1_0,
    KERNEL_TYPE_MIX_AIC_1_0,
    KERNEL_TYPE_MIX_AIC_1_1,
    KERNEL_TYPE_MIX_AIC_1_2,
    KERNEL_TYPE_AICORE,
    KERNEL_TYPE_VECTORCORE,
    KERNEL_TYPE_MIX_AICORE,
    KERNEL_TYPE_MIX_VECTOR_CORE,
    KERNEL_TYPE_MAX,
};

enum KernelType {
    K_TYPE_AICORE = 1,
    K_TYPE_AIC = 2,
    K_TYPE_AIV = 3,
    K_TYPE_MIX_AIC_MAIN = 4,
    K_TYPE_MIX_AIV_MAIN = 5,
    K_TYPE_AIC_ROLLBACK = 6,
    K_TYPE_AIV_ROLLBACK = 7,
    K_TYPE_MAX
};

enum BinaryMetaType {
    B_TYPE_BIN_VERSION_INFO = 0,
    B_TYPE_DEBUG_INFO = 1,
    B_TYPE_DYNAMIC_PARAM = 2,
    B_TYPE_OPTIONAL_PARAM = 3
};

struct BaseTlv {
    unsigned short type;
    unsigned short len;
};

struct BinaryMetaVersion {
    BaseTlv head;
    uint32_t version;
};

struct BinaryMetaDebug {
    BaseTlv head;
    uint32_t debugBufSize;
    uint32_t debugOptions;
};

struct BinaryMetaDynamicParam {
    BaseTlv head;
    uint16_t dynamicParamMode;
};

struct BinaryMetaOptionalParam {
    BaseTlv head;
    uint16_t optionalInputMode;
    uint16_t optionalOutputMode;
};

enum FuncMetaType {
    F_TYPE_KTYPE = 1,
    F_TYPE_CROSS_CORE_SYNC = 2,
    F_TYPE_MIX_TASK_RATION = 3,
    F_TYPE_L0_EXCEPTION_DFX = 4,
    F_TYPE_L0_EXCEPTION_DFX_ARGSINFO = 5,
    F_TYPE_L0_EXCEPTION_DFX_IS_TIK = 6,
    F_TYPE_DETERMINISTIC_INFO = 13,
    F_TYPE_FUNCTION_ENTRY_INFO= 14,
    F_TYPE_BLOCK_DIM_INFO = 15,
    F_TYPE_MAX
};

struct FuncMetaDeterministic {
    BaseTlv head;
    uint32_t deterministic;
};

struct FuncMetaFunctionEntry {
    BaseTlv head;
    uint32_t reserve;
    uint64_t functionEntry;
};

struct FuncMetaBlockDim {
    BaseTlv head;
    uint32_t blockDim;
};

enum CrossCoreSyncType {
    C_TYPE_USE_SYNC = 1,
    C_TYPE_MAX
};

struct OpSystemRunCfg {
    uint64_t l2Cacheoffset;
};
# 157 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
[aicore] inline void GetCannVersion(__attribute__((cce_global)) char*& versionStr, uint64_t& version, uint64_t& timeStamp)
{

    versionStr = const_cast<__attribute__((cce_global)) char*>("8.5.0");





    timeStamp = static_cast<uint64_t>(202507);







    version = 0;

}

namespace AscendC {
template <typename T>
[aicore] inline constexpr static auto IsLite(int) -> typename T::LiteType;
template <typename T>
[aicore] inline constexpr static auto IsLite(void*) -> T;

template <typename T>
using PrimT = decltype(IsLite<T>(0));

enum class CacheMode {
    CACHE_MODE_DISABLE = 0,
    CACHE_MODE_NORMAL = 1,
    CACHE_MODE_LAST = 2,
    CACHE_MODE_PERSISTENT = 4
};

enum class CacheRwMode {
    READ = 1,
    WRITE = 2,
    RW = 3
};
# 241 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
template<class T, CacheRwMode rwMode = CacheRwMode::RW>
[aicore] inline __attribute__((cce_global)) T* L2CacheAlter(__attribute__((cce_global)) T* addr, CacheMode mode)
{
# 254 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
    return addr;
}

}

struct FunMetaKType {
    BaseTlv head;
    unsigned int ktype;
};

struct FunMetaCrossCoreType {
    BaseTlv head;
    unsigned int usedCrossCoreSync;
};

struct FunMetaMixCoreType {
    BaseTlv head;
    unsigned short taskRation0;
    unsigned short taskRation1;
};

struct FunLevelKType {
    struct FunMetaKType ktypeMeta;
};

struct FunLevelCrossCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaCrossCoreType crossCoreType;
};

struct FunLevelMixCoreType {
    struct FunMetaKType ktypeMeta;
    struct FunMetaMixCoreType mixCoreType;
};
# 334 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
namespace AscendC {
constexpr size_t DUMP_UINTSIZE = (1024 * 1024);
}
# 419 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_macros.h"
constexpr bool g_gm_overflow_check = false;
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
namespace AscendC {
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
[aicore] constexpr inline uint32_t DivCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] constexpr inline uint32_t AlignUp(uint32_t a, uint32_t b)
{
    return DivCeil(a, b) * b;
}

[aicore] constexpr inline uint32_t ConstCeil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}

[aicore] inline uint32_t Ceil(uint32_t a, uint32_t b)
{
    return (a + b - 1) / b;
}
# 66 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
[aicore] inline int32_t CeilDivision(int32_t num1, int32_t num2)
{
    if (num2 == 0) {
        return 0;
    }
    return (num1 + num2 - 1) / num2;
}



[aicore] inline void WriteBackOverflow(__attribute__((cce_global)) uint8_t* overflowStatus)
{
    (void)overflowStatus;
# 109 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
}

template <typename T>
[aicore] static inline void OOMCheckTensorListRange(__attribute__((cce_global)) T *gmInputAddr, const int inputSize)
{
# 124 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
}

[aicore] static inline bool OOMCheckAddrInTensorList(uint64_t index, uintptr_t gmAddrConvert,
    uintptr_t& inputOutputAddr, uint64_t& inputOutputLen)
{
# 159 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
    (void)index;
    (void)gmAddrConvert;
    (void)inputOutputAddr;
    (void)inputOutputLen;
    return false;
}

template <typename T>
[aicore] static inline void OOMCheckAddrRange(__attribute__((cce_global)) T* gmAddr, const uint64_t gmSize)
{
# 179 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
}

template <typename T>
[aicore] static inline void OOMAddAddrForL2Cache(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) T* oriAddr)
{
# 207 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
}

[aicore] static inline void OOMInit()
{



}

struct TQueConfig {
    bool nd2nz = false;
    bool nz2nd = false;
    bool scmBlockGroup = false;
    uint32_t bufferLen = 0;
    uint32_t bufferNumber = 0;
    uint32_t consumerSize = 0;
    TPosition consumer[8] = {};
    bool enableStaticEvtId = false;
    bool enableLoopQueue = false;
};

[aicore] constexpr TQueConfig GetTQueConfig(bool nd2nzIn, bool nz2ndIn, bool scmBlockGroupIn,
    uint32_t bufferLenIn, uint32_t bufferNumberIn, uint32_t consumerSizeIn,
    const TPosition consumerIn[], bool enableStaticEvtIdIn, bool enableLoopQueueIn)
{
    return {
        .nd2nz = nd2nzIn,
        .nz2nd = nz2ndIn,
        .scmBlockGroup = scmBlockGroupIn,
        .bufferLen = bufferLenIn,
        .bufferNumber = bufferNumberIn,
        .consumerSize = consumerSizeIn,
        .consumer = {consumerIn[0], consumerIn[1], consumerIn[2], consumerIn[3],
            consumerIn[4], consumerIn[5], consumerIn[6], consumerIn[7]},
        .enableStaticEvtId = enableStaticEvtIdIn,
        .enableLoopQueue = enableLoopQueueIn
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const int32_t mask)
{
    return {
        .nd2nz = static_cast<bool>(static_cast<uint32_t>(mask) & 0x1u),
        .nz2nd = static_cast<bool>((static_cast<uint32_t>(mask) & 0x2u) >> 1),
        .scmBlockGroup = static_cast<bool>((static_cast<uint32_t>(mask) & 0x4u) >> 2),
        .bufferLen = 0,
        .bufferNumber = 0,
        .consumerSize = 0,
        .consumer = {TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX,
            TPosition::MAX, TPosition::MAX, TPosition::MAX, TPosition::MAX},
        .enableStaticEvtId = false,
        .enableLoopQueue = false
    };
}

[aicore] constexpr TQueConfig GetTQueConfig(const TQueConfig* conf)
{
    return {
        .nd2nz = conf->nd2nz,
        .nz2nd = conf->nz2nd,
        .scmBlockGroup = conf->scmBlockGroup,
        .bufferLen = conf->bufferLen,
        .bufferNumber = conf->bufferNumber,
        .consumerSize = conf->consumerSize,
        .consumer = {conf->consumer[0], conf->consumer[1], conf->consumer[2], conf->consumer[3],
            conf->consumer[4], conf->consumer[5], conf->consumer[6], conf->consumer[7]},
        .enableStaticEvtId = conf->enableStaticEvtId,
        .enableLoopQueue = conf->enableLoopQueue
    };
}
# 296 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
template <bool b> struct BoolInst {
    using Type = BoolInst<b>;
    static constexpr bool value = b;
};

using TrueType = BoolInst<true>;
using FalseType = BoolInst<false>;

template <typename T, typename U> struct IsSameType : public FalseType {};

template <typename T> struct IsSameType<T, T> : public TrueType {};

template <typename... Arg>
struct Tuple {};

template <typename T, typename U, typename... Args>
[aicore] constexpr bool SupportType()
{
    if constexpr (sizeof...(Args) > 0) {
        return IsSameType<T, U>::value || SupportType<T, Args...>();
    }
    return IsSameType<T, U>::value;
}
# 356 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_ceil_oom_que.h"
template <typename T, int U, int... Args> [aicore] constexpr bool SupportBytes()
{
    if constexpr (sizeof...(Args) > 0) {
        return sizeof(T) == U || SupportBytes<T, Args...>();
    }
    return sizeof(T) == U;
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_dump_constants.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_dump_constants.h"
namespace AscendC {
const int32_t ONE_DUMP_BACKUP_SIZE = 1024;
const int32_t DUMP_UB_SIZE = 256;
const int32_t DUMP_EXC_FLAG = 7;


const uint32_t BLOCK_INFO_LEN_POS = 0;
const uint32_t BLOCK_INFO_CORE_POS = 1;
const uint32_t BLOCK_INFO_BLOCKNUM_POS = 2;
const uint32_t BLOCK_INFO_DUMPOFFSET_POS = 3;
const uint32_t BLOCK_INFO_MAGIC_POS = 4;
const uint32_t BLOCK_INFO_RSV_POS = 5;
const uint32_t BLOCK_INFO_DUMP_ADDR = 6;
const uint32_t BLOCK_INFO_MAGIC_NUM = 0x5aa5bccd;

const uint32_t DUMP_META_TYPE_POS = 0;
const uint32_t DUMP_META_LEN_POS = 4;
const uint16_t DUMP_META_BLOCK_DIM_POS = 8;
const uint16_t DUMP_META_SIMT_THREAD_ID_POS = 8;
const uint8_t DUMP_META_CORE_TYPE_POS = 10;
const uint8_t DUMP_META_TASK_RATION = 11;
const uint32_t DUMP_META_RSV_POS = 12;

const uint32_t DUMP_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_MESSAGE_HEAD_ADDR_POS = 2;
const uint32_t DUMP_MESSAGE_HEAD_DATA_TYPE_POS = 3;
const uint32_t DUMP_MESSAGE_HEAD_DESC_POS = 4;
const uint32_t DUMP_MESSAGE_HEAD_BUFFERID_POS = 5;
const uint32_t DUMP_MESSAGE_HEAD_POSITION_POS = 6;
const uint32_t DUMP_MESSAGE_HEAD_DUMP_SIZE_POS = 7;
const uint32_t DUMP_SCALAR_POS = 8;
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_dump_constants.h"
const uint32_t DUMP_CORE_COUNT = 75;
const uint32_t DUMP_WORKSPACE_SIZE = DUMP_CORE_COUNT * (1024 * 1024);


const uint32_t DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS = 0;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_LEN_POS = 1;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_DIM_POS = 2;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS = 3;
const uint32_t DUMP_SHAPE_MESSAGE_HEAD_RSV_POS = 11;
const uint32_t DUMP_SHAPE_MESSAGE_TL_LEN = 8;

namespace Internal {
enum class DumpTensorDataType : uint32_t {
    ACL_FLOAT = 0,
    ACL_FLOAT16 = 1,
    ACL_INT8 = 2,
    ACL_INT32 = 3,
    ACL_UINT8 = 4,
    ACL_INT16 = 6,
    ACL_UINT16 = 7,
    ACL_UINT32 = 8,
    ACL_INT64 = 9,
    ACL_UINT64 = 10,
    ACL_DOUBLE = 11,
    ACL_BOOL = 12,
    ACL_STRING = 13,
    ACL_COMPLEX64 = 16,
    ACL_COMPLEX128 = 17,
    ACL_BF16 = 27,
    ACL_INT4 = 29,
    ACL_UINT1 = 30,
    ACL_COMPLEX32 = 33,
    ACL_HIFLOAT8 = 34,
    ACL_FLOAT8_E5M2 = 35,
    ACL_FLOAT8_E4M3FN = 36,
    ACL_FLOAT8_E8M0 = 37,
    ACL_FLOAT6_E3M2 = 38,
    ACL_FLOAT6_E2M3 = 39,
    ACL_FLOAT4_E2M1 = 40,
    ACL_FLOAT4_E1M2 = 41,
    ACL_MAX = 42,
};
}
# 120 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_dump_constants.h"
const uint32_t DUMP_TIME_STAMP_LEN = 24;
const uint32_t DUMP_TIME_STAMP_TOTAL_LEN = 32;
const uint32_t DUMP_TIME_STAMP_LEN_POS = 1;
const uint32_t DUMP_TIME_STAMP_ID_POS = 2;
const uint32_t DUMP_TIME_STAMP_CYCLE_POS = 4;
const uint32_t DUMP_TIME_STAMP_PTR_POS = 6;




}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h" 2

namespace AscendC {
const int32_t DEFAULT_BLK_NUM = 8;
const int32_t POWER_MASK_NUM = 8;
const int32_t HALF_FACTOR = 2;
const int32_t DOUBLE_FACTOR = 2;
const int32_t DEFAULT_BLK_STRIDE = 1;
const uint8_t DEFAULT_REPEAT_STRIDE = 8;
const uint8_t HALF_DEFAULT_REPEAT_STRIDE = 4;
const uint8_t ONE_FOURTH_DEFAULT_REPEAT_STRIDE = 2;
const uint64_t FULL_MASK = 0xffffffffffffffff;
const uint64_t CONST_MASK_VALUE = 0x8000000000000000;
const uint16_t MAX_HALF_MASK_LEN = 64;
const int32_t DEFAULT_C0_SIZE = 32;
const int32_t DEFAULT_BLOCK_SIZE = 256;
const int32_t MAX_REPEAT_TIMES = 255;
const int32_t MIN_REPEAT_TIMES = 0;
const bool DEFAULT_REPEAT_STRIDE_MODE = 0;
const bool STRIDE_SIZE_MODE = 0;
const int32_t ONE_BYTE_BIT_SIZE = 8;
const uint16_t MASK_ARRAY_SIZE = 4;
const uint32_t TOTAL_L0A_SIZE = 64 * 1024;
const uint32_t TOTAL_L0B_SIZE = 64 * 1024;
const uint32_t TMP_UB_SIZE = 8 * 1024;
const uint32_t MAX_SLICE_SIZE = 6 * 256;
const uint32_t F32_INF = 0x7f800000;
const uint32_t F32_NEG_INF = 0xff800000;
const uint32_t F32_NAN = 0x7fc00000;

const uint16_t VALUE_512 = 512;
const uint16_t UINT12_MAX = 4095;
const uint16_t UINT15_MAX = 32767;


constexpr int32_t CTRL_46_BIT = 46;
constexpr int32_t CTRL_47_BIT = 47;
constexpr int32_t CTRL_48_BIT = 48;
constexpr int32_t CTRL_53_BIT = 53;


constexpr uint32_t TENSOR_TENSOR_FLOAT_POWER_FACTOR = 4;
constexpr uint32_t TENSOR_TENSOR_INT_POWER_FACTOR = 6;
constexpr uint32_t TENSOR_TENSOR_HALF_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT_POWER_FACTOR = 5;
constexpr uint32_t TENSOR_SCALAR_INT_POWER_FACTOR = 7;
constexpr uint32_t TENSOR_SCALAR_HALF_POWER_FACTOR = 7;
constexpr uint32_t POWER_TWO = 2;
constexpr uint32_t POWER_THREE = 3;
constexpr uint32_t POWER_INT32_BITS = 32;


constexpr uint32_t INT4_TWO = 2;
constexpr uint32_t INT4_BIT_NUM = 4;
# 88 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
constexpr int32_t DEQ_SHIFT_LEFT_17_BIT = 131072;
constexpr float DEQ_SHIFT_RIGHT_17_BIT = 1.0 / DEQ_SHIFT_LEFT_17_BIT;
constexpr int8_t ADDDEQRELU_MASK_MODE_ONE = 1;
constexpr int8_t ADDDEQRELU_MASK_MODE_TWO = 2;
# 112 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
const int32_t TOTAL_VEC_LOCAL_SIZE = 184 * 1024;
const uint32_t TOTAL_UB_SIZE = 192 * 1024;
const uint32_t TMP_UB_OFFSET = 184 * 1024;



const uint32_t TOTAL_L1_SIZE = 512 * 1024 - 128;
const uint32_t SINGLE_MSG_SIZE = 64;
const uint32_t CACHE_LINE_SIZE = 64;
const uint32_t TOTAL_L0C_SIZE = 128 * 1024;
# 157 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
const int32_t BLOCK_CUBE = 16;




const uint16_t ONE_BLK_SIZE = 32;




const int32_t CUBE_MAX_SIZE = 256;
# 188 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
const uint8_t PAD_SIZE = 4;
const uint8_t MRG_SORT_ELEMENT_LEN = 4;
const uint8_t DEFAULT_DATA_COPY_NBURST = 1;
const uint8_t DEFAULT_DATA_COPY_STRIDE = 0;
const int32_t BYTE_PER_FRACTAL = 512;
const int32_t SRC_BURST_LEN_SIZE_ELE = 16;
const int32_t SRC_GAP_SIZE_BYTE = 32;
const int32_t DST_BURST_LEN_SIZE_ELE = 256;
const int32_t VREDUCE_PER_REP_OUTPUT = 2;
const uint16_t ONE_PARAM_SIZE = 8;



const uint16_t AIV_CORE_NUM = 50;

const uint16_t DUMP_MSG_HEAD_SIZE = 24;
const int32_t ONE_REPEAT_BYTE_SIZE = 256;
const int32_t FULL_MASK_LEN = 128;
const int32_t HLAF_MASK_LEN = 64;
const int32_t DEFAULT_REDUCE_DST_REP_SRIDE = 1;
const uint8_t B64_BYTE_SIZE = 8;
const uint8_t B32_BYTE_SIZE = 4;
const uint8_t B16_BYTE_SIZE = 2;
const uint8_t B8_BYTE_SIZE = 1;
const uint8_t B32_DATA_NUM_PER_BLOCK = 8;
const uint8_t B16_DATA_NUM_PER_BLOCK = 16;
const int32_t B16_DATA_NUM_PER_REPEAT = 128;
const int32_t B32_DATA_NUM_PER_REPEAT = 64;







const int32_t BLOCK_STRIDE_POS_IN_SM = 16;
const int32_t PLD_BUFFER_SIZE = 2;
const uint8_t FIXPIPE_DEQ_TENSOR_SIZE = 16;
const uint8_t SET_DATA_EXP_ZERO = 0;
const uint8_t SET_DATA_EXP_ONE = 1;
const uint8_t SET_DATA_EXP_TWO = 2;
const uint8_t SET_DATA_EXP_THREE = 3;
const uint8_t VDEQ_TENSOR_SIZE = 16;






constexpr size_t RESERVED_WORKSPACE = 16 * 1024 * 1024;
# 247 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
const int32_t NCHW_CONV_ADDR_LIST_SIZE = 16;
const int32_t VA_REG_ARRAY_LEN = 8;
const uint8_t CONV2D_IMG_SIZE = 2;
const uint8_t CONV2D_KERNEL_SIZE = 2;
const uint8_t CONV2D_STRIDE = 2;
const uint8_t CONV2D_PAD = 4;
const uint8_t CONV2D_DILATION = 2;
const int32_t K_MAX_DIM = 8;

const uint32_t TWO_OF_STACK_BUFFER = 2;
const uint32_t THREE_OF_STACK_BUFFER = 3;
const uint32_t HALF_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t FLOAT_REPEAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_FLOAT_SIZE = ONE_REPEAT_BYTE_SIZE / B32_BYTE_SIZE;
const uint32_t ONE_REPEAT_HALF_SIZE = ONE_REPEAT_BYTE_SIZE / B16_BYTE_SIZE;
const uint32_t MAX_REPEAT_FLOAT_SIZE = ONE_REPEAT_FLOAT_SIZE * MAX_REPEAT_TIMES;
const uint32_t MAX_REPEAT_HALF_SIZE = ONE_REPEAT_HALF_SIZE * MAX_REPEAT_TIMES;
const uint32_t ONE_BLK_HALF_NUM = ONE_BLK_SIZE / B16_BYTE_SIZE;
const uint32_t ONE_BLK_FLOAT_NUM = ONE_BLK_SIZE / B32_BYTE_SIZE;






const uint32_t BRCB_BROADCAST_NUMBER = 8;
const uint32_t BRCB_MAX_REPEAT_SIZE = BRCB_BROADCAST_NUMBER * MAX_REPEAT_TIMES;
const int32_t MIN_BLOCK_LEN = 1;
const uint32_t PAIR_REDUCE_REPEAT_STRIDE_LEN = 128;
const uint32_t PAIR_REDUCE_SUM_MERGES = 2;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT = 252;
const uint32_t TWO_HUNDRED_FIFTY_TWO_REPEAT_BYTE_SIZE = TWO_HUNDRED_FIFTY_TWO_REPEAT * ONE_REPEAT_BYTE_SIZE;
const uint32_t REDUCEV2_MODE_SEVEN = 7;
const uint32_t DROPOUT_MODE_BYTE_MISALIGN = 1;
const uint32_t DROPOUT_MODE_BYTE_ALIGN = 2;
const uint32_t DROPOUT_MODE_BIT_ALIGN = 3;
const uint32_t DROPOUT_MODE_BIT_MISALIGN = 4;
const uint32_t REDUCEV2_MODE_ONE = 1;
const uint32_t REDUCEV2_MODE_TWO = 2;
const uint32_t REDUCEV2_MODE_THREE = 3;


const int32_t B8_TMP_ELE_LEN = 1024;
const int32_t B16_TMP_ELE_LEN = 256;
const int32_t B32_TMP_ELE_LEN = 128;
const int32_t B8_TRANS_LEN = 1024;
const int32_t B8_TRANS_FRACTAL = 512;
const int32_t B8_TRANS_ROW = 32;
const int32_t B8_COPY_COL = 32;


const uint64_t LOAD_M_START_POSITION = 48;
const uint64_t LOAD_K_START_POSITION = 32;
const uint64_t LOAD_M_EXTENSION = 16;
const uint64_t LOAD_DILATION_FILTER_H = 40;
const uint64_t LOAD_DILATION_FILTER_W = 32;
const uint64_t LOAD_FILTER_H = 24;
const uint64_t LOAD_FILTER_W = 16;
const uint64_t LOAD_STRIDE_H = 8;
# 455 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
template <bool condition, class T1, class T2>
struct Conditional {
    using type = T1;
};

template <class T1, class T2>
struct Conditional<false, T1, T2> {
    using type = T2;
};

template <bool condition1, bool condition2, class T1, class T2, class T3, class T4> struct ConditionalMulti {
    using type = typename Conditional<condition1, typename Conditional<condition2, T1, T2>::type,
        typename Conditional<condition2, T3, T4>::type>::type;
};

template <int bitNum, bool sign = true>
struct IntegerSubType {
    static int const kBits = bitNum;
    static bool const kSigned = sign;

    using T = typename Conditional<kSigned, int8_t, uint8_t>::type;
    using Storage = uint8_t;

    static Storage const mask = Storage(((static_cast<uint64_t>(1)) << static_cast<uint32_t>(kBits)) - 1);
    Storage storage;
    [aicore] inline IntegerSubType() = default;

    [aicore] inline IntegerSubType(uint32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] inline IntegerSubType(int32_t value)
        : storage(reinterpret_cast<Storage const &>(value) & mask) {}

    [aicore] inline operator T() const
    {
        if (kSigned && ((storage & Storage(static_cast<uint64_t>(1) << static_cast<uint32_t>(kBits - 1))) != 0)) {

            return T(storage) | ~T(mask);
        }
        return T(storage);
    }

    [aicore] inline bool operator == (IntegerSubType const &rhs) const
    {
        return storage == rhs.storage;
    }

    [aicore] inline bool operator != (IntegerSubType const &rhs) const
    {
        return storage != rhs.storage;
    }

    [aicore] inline bool operator > (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return this->storage > rhs.storage;
    }

    [aicore] inline bool operator >= (IntegerSubType const &rhs) const
    {
        bool lhsIsNeg = (this->storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(this->kBits - 1)));
        bool rhsIsNeg = (rhs.storage & (static_cast<uint64_t>(1) << static_cast<uint32_t>(rhs.kBits - 1)));
        if (kSigned && (lhsIsNeg != rhsIsNeg)) {
            return (!lhsIsNeg) && rhsIsNeg;
        }
        return storage >= rhs.storage;
    }

    [aicore] inline bool operator < (IntegerSubType const &rhs) const
    {
        return !(*this >= rhs);
    }

    [aicore] inline bool operator <= (IntegerSubType const &rhs) const
    {
        return !(*this > rhs);
    }
};

using int4b_t = IntegerSubType<INT4_BIT_NUM, true>;







using fp8_e8m0_t = uint8_t;
# 569 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
template <typename T>
[aicore] constexpr bool IsHalfByteDataType()
{



    return IsSameType<T, int4b_t>::value;

}

template <typename T> struct SizeOfBits {
    static constexpr uint32_t value = sizeof(T) * ONE_BYTE_BIT_SIZE;
};

template <>
struct SizeOfBits<int4b_t> {
    static int const value = INT4_BIT_NUM;
};
# 604 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
[aicore] inline bool CheckCastOverlappingHigh(const uint64_t dstAddr, const uint64_t srcAddr,
    const uint32_t dstTypeSize, const uint32_t srcTypeSize, const uint32_t count)
{
    uint64_t addrLow = dstAddr > srcAddr ? srcAddr : dstAddr;
    uint64_t addrHigh = dstAddr > srcAddr ? dstAddr : srcAddr;
    uint64_t needSizeLow = dstAddr > srcAddr ? count * srcTypeSize : count * dstTypeSize;

    if ((srcTypeSize < dstTypeSize) && (srcAddr >= AlignUp(dstAddr + count * srcTypeSize, ONE_BLK_SIZE))) {
        return true;
    }
    if (dstTypeSize > srcTypeSize && srcAddr == dstAddr) {
        return false;
    }
    if ((needSizeLow > static_cast<uint64_t>(ONE_REPEAT_BYTE_SIZE)) && (srcAddr != dstAddr) &&
        ((addrLow + needSizeLow > addrHigh))) {
        return false;
    }
    return true;
}
# 670 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_constants.h"
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode_cpu.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode_cpu.h"
namespace AscendC {
class MaskSetter {
public:
    static MaskSetter& Instance()
    {
        static MaskSetter instance;
        return instance;
    };

    void SetMask(bool setMask)
    {
        isSetMask = setMask;
    }

    bool GetMask() const
    {
        return isSetMask;
    }

private:
    MaskSetter(){};
    ~MaskSetter(){};
    bool isSetMask = true;
};

class Int4Setter {
public:
    static Int4Setter& Instance()
    {
        static Int4Setter instance;
        return instance;
    };

    void SetInt4()
    {
        isInt4 = true;
    }

    void SetDstInt4()
    {
        isDstInt4 = true;
    }

    void SetSrcInt4()
    {
        isSrcInt4 = true;
    }

    void ResetInt4()
    {
        isInt4 = false;
    }

    void ResetDstSrcInt4()
    {
        isDstInt4 = false;
        isSrcInt4 = false;
    }

    bool GetInt4() const
    {
        return isInt4;
    }

    bool GetDstInt4() const
    {
        return isDstInt4;
    }

    bool GetSrcInt4() const
    {
        return isSrcInt4;
    }

private:
    Int4Setter(){};
    ~Int4Setter(){};

    bool isInt4 = false;
    bool isDstInt4 = false;
    bool isSrcInt4 = false;
};

enum class BlockMode : uint8_t {
    BLOCK_MODE_NORMAL = 0,
    BLOCK_MODE_MATRIX,
    BLOCK_MODE_VECTOR,
    BLOCK_MODE_SMALL_CHANNEL,
    BLOCK_MODE_DEPTHWISE,
};
# 450 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode_cpu.h"
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h" 2

namespace AscendC {
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h"
union NotNumUnion {
    __attribute__((cce_simd_callee)) NotNumUnion() {}
    float f;
    uint32_t i;
};

union HalfUnion {
    __attribute__((cce_simd_callee)) HalfUnion() {}
    half f;
    uint16_t i;
};

enum class TShapeType : uint8_t {
    DEFAULT,
    NHWC,
    NC1HWC0,
    NHC,
    NCHT,
    ND,
    FRACTAL_NZ,
    HNC,
    HCNT,
    NDHWC,
    FRACTAL_Z_3D,
    NDHC,
    DCHNT,
    FRACTAL_Z,
    NCDHW,
    NDC1HWC0,
    NCDH,
    NDCHT,
    NCHW,
    NCH,
    HWCN,
    HCN,
    CHNT,
    DHWCN,
    DHCN
};

enum class RoundMode : uint8_t {
    CAST_NONE = 0,
    CAST_RINT,
    CAST_FLOOR,
    CAST_CEIL,
    CAST_ROUND,
    CAST_TRUNC,
    CAST_ODD,






};

enum class CMPMODE : uint8_t {
    LT = 0,
    GT,
    EQ,
    LE,
    GE,
    NE,
};

enum class SELMODE : uint8_t {
    VSEL_CMPMASK_SPR = 0,
    VSEL_TENSOR_SCALAR_MODE,
    VSEL_TENSOR_TENSOR_MODE,
};

enum class DeqScale : uint8_t {
    DEQ_NONE = 0,
    DEQ,
    VDEQ,
    DEQ8,
    VDEQ8,
    DEQ16,
    VDEQ16,
};
# 141 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h"
enum class ReduceMode : uint8_t {
    REDUCE_MAX = 0,
    REDUCE_MIN,
    REDUCE_SUM,
};

enum class ReduceOrder : uint8_t {
    ORDER_VALUE_INDEX = 0,
    ORDER_INDEX_VALUE,
    ORDER_ONLY_VALUE,
    ORDER_ONLY_INDEX,
};

enum class DumpType : uint8_t {
    DUMP_DEFAULT = 0,
    DUMP_SCALAR,
    DUMP_TENSOR,
    DUMP_SHAPE,
    DUMP_ASSERT,
    DUMP_META,
    DUMP_TIME_STAMP,
    DUMP_SIMT,
    DUMP_BUFI,
    DUMP_BUFO,
    DUMP_SKIP
};

enum class CLAMPMODE {
    CLAMP_MAX = 0,
    CLAMP_MIN,
};

enum class PcieCtrl : uint64_t {
    WR = 0,
    RD
};

enum class DeQuantMode : uint8_t {
    DEQUANT_WITH_SINGLE_ROW = 0,
    DEQUANT_WITH_MULTI_ROW,
};
# 196 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_mode.h"
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_confusion_pad.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_confusion_pad.h"
namespace AscendC {
struct ConfusionTranspose2NZ012NTiling {
    [aicore] ConfusionTranspose2NZ012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        blockNum = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        alignH = 0;
        alignS = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsBlockCube = 0;
        prehBlockNum = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
    }

    [aicore] ConfusionTranspose2NZ012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t blockNumIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn,
        uint32_t alignHIn, uint32_t alignSIn, uint32_t hnDivBlockNumIn, uint32_t alignHnDivIn, uint32_t gapIn,
        uint32_t alignsBlockCubeIn, uint32_t prehBlockNumIn, uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        blockNum = blockNumIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        alignH = alignHIn;
        alignS = alignSIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsBlockCube = alignsBlockCubeIn;
        prehBlockNum = prehBlockNumIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t blockNum = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignS = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsBlockCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
};

struct ConfusionTranspose2ND012NTiling {
    [aicore] ConfusionTranspose2ND012NTiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignHnDiv = 0;
        gap = 0;
        alignsCube = 0;
        prehBlockNum = 0;
        alignsMulAlignHnDiv = 0;
        alignHnDivCube = 0;
        alignHnDivBlockSize = 0;
        dstBatchOffset = 0;
        srcBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose2ND012NTiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn,
        uint32_t hnDivIn, uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn,
        uint32_t alignHnDivIn, uint32_t gapIn, uint32_t alignsCubeIn, uint32_t prehBlockNumIn,
        uint32_t alignsMulAlignHnDivIn, uint32_t alignHnDivCubeIn, uint32_t alignHnDivBlockSizeIn,
        uint32_t dstBatchOffsetIn, uint32_t srcBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignHnDiv = alignHnDivIn;
        gap = gapIn;
        alignsCube = alignsCubeIn;
        prehBlockNum = prehBlockNumIn;
        alignsMulAlignHnDiv = alignsMulAlignHnDivIn;
        alignHnDivCube = alignHnDivCubeIn;
        alignHnDivBlockSize = alignHnDivBlockSizeIn;
        dstBatchOffset = dstBatchOffsetIn;
        srcBatchOffset = srcBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignHnDiv = 0;
    uint32_t gap = 0;
    uint32_t alignsCube = 0;
    uint32_t prehBlockNum = 0;
    uint32_t alignsMulAlignHnDiv = 0;
    uint32_t alignHnDivCube = 0;
    uint32_t alignHnDivBlockSize = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t blockNum = 0;
};

struct ConfusionTranspose012Tiling {
    [aicore] ConfusionTranspose012Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeN = 0;
        hnDiv = 0;
        shapeH = 0;
        hBlockNum = 0;
        sBlockNum = 0;
        hnDivBlockNum = 0;
        alignH = 0;
        alignsCube = 0;
        alignhBlockCube = 0;
        blockSizeMulAlignH = 0;
        srcBatchOffset = 0;
        dstBatchOffset = 0;
        blockNum = 0;
    }

    [aicore] ConfusionTranspose012Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeNIn, uint32_t hnDivIn,
        uint32_t shapeHIn, uint32_t hBlockNumIn, uint32_t sBlockNumIn, uint32_t hnDivBlockNumIn, uint32_t alignHIn,
        uint32_t alignsCubeIn, uint32_t alignhBlockCubeIn, uint32_t blockSizeMulAlignHIn, uint32_t srcBatchOffsetIn,
        uint32_t dstBatchOffsetIn, uint32_t blockNumIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeN = shapeNIn;
        hnDiv = hnDivIn;
        shapeH = shapeHIn;
        hBlockNum = hBlockNumIn;
        sBlockNum = sBlockNumIn;
        hnDivBlockNum = hnDivBlockNumIn;
        alignH = alignHIn;
        alignsCube = alignsCubeIn;
        alignhBlockCube = alignhBlockCubeIn;
        blockSizeMulAlignH = blockSizeMulAlignHIn;
        srcBatchOffset = srcBatchOffsetIn;
        dstBatchOffset = dstBatchOffsetIn;
        blockNum = blockNumIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeN = 0;
    uint32_t hnDiv = 0;
    uint32_t shapeH = 0;
    uint32_t hBlockNum = 0;
    uint32_t sBlockNum = 0;
    uint32_t hnDivBlockNum = 0;
    uint32_t alignH = 0;
    uint32_t alignsCube = 0;
    uint32_t alignhBlockCube = 0;
    uint32_t blockSizeMulAlignH = 0;
    uint32_t srcBatchOffset = 0;
    uint32_t dstBatchOffset = 0;
    uint32_t blockNum = 0;
};

struct ConfusionTransposeOnlyTiling {
    [aicore] ConfusionTransposeOnlyTiling()
    {
        blockSize = 0;
        height = 0;
        width = 0;
        highBlock = 0;
        stride = 0;
        repeat = 0;
    }

    [aicore] ConfusionTransposeOnlyTiling(uint32_t blockSizeIn, uint32_t heightIn, uint32_t widthIn,
        uint32_t highBlockIn, uint32_t strideIn, uint32_t repeatIn)
    {
        blockSize = blockSizeIn;
        height = heightIn;
        width = widthIn;
        highBlock = highBlockIn;
        stride = strideIn;
        repeat = repeatIn;
    }
    uint32_t blockSize = 0;
    uint32_t height = 0;
    uint32_t width = 0;
    uint32_t highBlock = 0;
    uint32_t stride = 0;
    uint32_t repeat = 0;
};

struct ConfusionTranspose0213Tiling {
    [aicore] ConfusionTranspose0213Tiling()
    {
        blockSize = 0;
        shapeB = 0;
        shapeA1 = 0;
        alignA3 = 0;
        alignA2 = 0;
        widthTiling = 0;
        newPopSize = 0;
        newPopH = 0;
        needSize = 0;
        mainBlocks = 0;
        tailSize = 0;
        alignA2MulAlignA3 = 0;
        batchOffset = 0;
        alignA3MulA1 = 0;
        shapeA1BlockCube = 0;
        mainOffset = 0;
    }

    [aicore] ConfusionTranspose0213Tiling(uint32_t blockSizeIn, uint32_t shapeBIn, uint32_t shapeA1In,
        uint32_t alignA3In, uint32_t alignA2In, uint32_t widthTilingIn, uint32_t newPopSizeIn, uint32_t newPopHIn,
        uint32_t needSizeIn, uint32_t mainBlocksIn, uint32_t tailSizeIn, uint32_t alignA2MulAlignA3In,
        uint32_t batchOffsetIn, uint32_t alignA3MulA1In, uint32_t shapeA1BlockCubeIn, uint32_t mainOffsetIn)
    {
        blockSize = blockSizeIn;
        shapeB = shapeBIn;
        shapeA1 = shapeA1In;
        alignA3 = alignA3In;
        alignA2 = alignA2In;
        widthTiling = widthTilingIn;
        newPopSize = newPopSizeIn;
        newPopH = newPopHIn;
        needSize = needSizeIn;
        mainBlocks = mainBlocksIn;
        tailSize = tailSizeIn;
        alignA2MulAlignA3 = alignA2MulAlignA3In;
        batchOffset = batchOffsetIn;
        alignA3MulA1 = alignA3MulA1In;
        shapeA1BlockCube = shapeA1BlockCubeIn;
        mainOffset = mainOffsetIn;
    }
    uint32_t blockSize = 0;
    uint32_t shapeB = 0;
    uint32_t shapeA1 = 0;
    uint32_t alignA3 = 0;
    uint32_t alignA2 = 0;
    uint32_t widthTiling = 0;
    uint32_t newPopSize = 0;
    uint32_t newPopH = 0;
    uint32_t needSize = 0;
    uint32_t mainBlocks = 0;
    uint32_t tailSize = 0;
    uint32_t alignA2MulAlignA3 = 0;
    uint32_t batchOffset = 0;
    uint32_t alignA3MulA1 = 0;
    uint32_t shapeA1BlockCube = 0;
    uint32_t mainOffset = 0;
};
# 359 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_confusion_pad.h"
struct IntriInfo {
    uint32_t c0Count{ 0 };
    uint32_t repeat{ 0 };
    uint32_t repeatRounding{ 0 };
    uint32_t repeatRemaining{ 0 };
    uint32_t tail{ 0 };
};
# 374 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_confusion_pad.h"
struct PadParams {
    [aicore] PadParams()
    {
        leftPad = 0;
        rightPad = 0;
        padValue = 0;
    }

    [aicore] PadParams(const uint16_t leftPadIn, const uint16_t rightPadIn, const int32_t padValueIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
        padValue = padValueIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
    int32_t padValue = 0;
};

struct UnPadParams {
    [aicore] UnPadParams()
    {
        leftPad = 0;
        rightPad = 0;
    }

    [aicore] UnPadParams(const uint16_t leftPadIn, const uint16_t rightPadIn)
    {
        leftPad = leftPadIn;
        rightPad = rightPadIn;
    }

    uint16_t leftPad = 0;
    uint16_t rightPad = 0;
};






constexpr int32_t AIPP_OFFSET_CSC_ENABLE = 63;
constexpr int32_t AIPP_OFFSET_CH1 = 16;
constexpr int32_t AIPP_OFFSET_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CH3 = 48;
constexpr int32_t AIPP_OFFSET_SWAP_RB = 16;
constexpr int32_t AIPP_OFFSET_SWAP_UV = 17;
constexpr int32_t AIPP_OFFSET_SWAP_AX = 18;
constexpr int32_t AIPP_OFFSET_FORMAT = 19;
constexpr int32_t AIPP_OFFSET_SINGLE_LINE = 24;
constexpr int32_t AIPP_OFFSET_PADDING_MODE = 27;
constexpr int32_t AIPP_OFFSET_CPADDING_MODE = 40;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH0 = 16;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH1 = 24;
constexpr int32_t AIPP_OFFSET_CSC_OUT_CH2 = 32;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH0 = 40;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH1 = 48;
constexpr int32_t AIPP_OFFSET_CSC_IN_CH2 = 56;
constexpr int32_t AIPP_OFFSET_DTC_CH1 = 32;
constexpr int32_t AIPP_OFFSET_DTC_ROUND_MODE = 34;
constexpr int32_t AIPP_OUTPUT_CHANNEL_NUM = 3;
constexpr int32_t AIPP_YUV400_OUTPUT_CHANNEL_NUM = 1;
constexpr int32_t AIPP_YUV420_SRC0_CHANNEL_NUM = 1;
constexpr int32_t AIPP_XRGB8888_OUTPUT_CHANNEL_NUM = 4;
constexpr int32_t AIPP_OUTPUT_PADDING_MODE0_CHANNEL_NUM = 4;


}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_dma_params.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_dma_params.h"
namespace AscendC {

struct QuantParams {
    [aicore] QuantParams() {}
    [aicore] QuantParams(const QuantMode_t quantPreIn) : quantPre(quantPreIn) {}
    [aicore] QuantParams(const QuantMode_t quantPreIn, const uint64_t deqScalarIn)
        : quantPre(quantPreIn), deqScalar(deqScalarIn) {}
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;
};
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_dma_params.h"
struct Nz2NdParams {
    [aicore] Nz2NdParams()
    {
        nz2ndEn = false;
        ndNum = 1;
        srcNdStride = 0;
        dstNdStride = 0;
        originalNSize = 0;
    }

    [aicore] Nz2NdParams(const bool nz2ndEnIn, const uint16_t ndNumIn, const uint16_t srcNdStrideIn,
        const uint16_t dstNdStrideIn, const uint16_t originalNSizeIn)
    {
        nz2ndEn = nz2ndEnIn;
        ndNum = ndNumIn;
        srcNdStride = srcNdStrideIn;
        dstNdStride = dstNdStrideIn;
        originalNSize = originalNSizeIn;
    }

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    uint16_t originalNSize = 0;
};
# 94 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_dma_params.h"
template <typename T = int32_t>
struct FixpipeParams {
    [aicore] FixpipeParams()
    {
        cburstNum = DEFAULT_DATA_COPY_NBURST;
        burstLen = 1;
        srcStride = DEFAULT_DATA_COPY_STRIDE;
        dstStride = DEFAULT_DATA_COPY_STRIDE;
        reluEn = false;
        unitFlag = 0;
    }

    [aicore] FixpipeParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn)
    {
        cburstNum = count;
        burstLen = len;
        dstStride = dstStrideIn;
        srcStride = srcStrideIn;
    }

    uint16_t cburstNum = 0;
    uint16_t burstLen = 0;
    uint32_t dstStride = 0;
    uint16_t srcStride = 0;

    QuantParams quantParams;
    bool reluEn = false;
    Nz2NdParams nz2ndParams;
    uint8_t unitFlag = 0;
};

}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_norm_sort.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_norm_sort.h"
namespace AscendC {
struct MatMulInfo {
    const uint16_t m{ 0 };
    const uint16_t n{ 0 };
    const uint16_t k{ 0 };
    const bool isInitOut{ false };
    const bool isBias{ false };
};

struct DropOutShapeInfo {
    [aicore] DropOutShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

struct SelectWithBytesMaskShapeInfo {
    [aicore] SelectWithBytesMaskShapeInfo(){};
    uint32_t firstAxis = 0;
    uint32_t srcLastAxis = 0;
    uint32_t maskLastAxis = 0;
};

template <typename T> class LocalTensor;
template <typename T> class GlobalTensor;

template <typename T> struct LayerNormParams {
    [aicore] LayerNormParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct BatchNormParams {
    [aicore] BatchNormParams(){};
    float firstDimValueBack = 1.0;
    uint8_t srcRepeatStride = 1;
    uint32_t srcOffset = 1;
    uint32_t basicLoop = 0;
    uint32_t brcRepeatTimes = 0;
    uint32_t oriBloop = 0;
    uint32_t oriBTail = 0;
    uint32_t oriBTmpLoopOffset = 0;
    uint32_t oriBTmpTailOffset = 0;
    uint32_t oriBOutLoopOffset = 0;
    uint32_t oriBOutTailOffset = 0;
    uint32_t reduceAddLoop = 0;
    uint32_t reduceAddTail = 0;
    uint32_t reduceAddTailOffset = 0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct DeepNormParams {
    [aicore] DeepNormParams(){};
    float lastDimValueBack = 1.0;

    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

template <typename T> struct ExpParams {
    [aicore] ExpParams() {};
    uint32_t inputSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t fourthTmpStartPos = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t tailPos = 0;
    uint32_t curDataLength = 0;
    uint32_t expandLevel = 0;

    LocalTensor<T> tempTensorFloorX;
    LocalTensor<T> tempTensorFloorXPow;
    LocalTensor<T> tempTensorRes;
    LocalTensor<T> tempTensorIntPart;
};

template <typename T> struct AntiquantParams {
    [aicore] AntiquantParams() {};
    LocalTensor<T> tempTensorOffset;
    LocalTensor<T> tempTensorScale;
    LocalTensor<T> tempTensorInput;
};

template <typename T, typename U>struct DropOutParams {
    [aicore] DropOutParams() {};
    uint32_t dataSize = 0;
    uint32_t stackBufferSize = 0;
    uint32_t repeatTimes = 1;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t repeatTail = 0;

    LocalTensor<T> firstLocal;
    LocalTensor<U> secondLocal;
};

template <typename T, typename U> struct PowerFParams {
    [aicore] PowerFParams(){};
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<T> tmpTensor3;
    LocalTensor<U> tmpMask1;
    LocalTensor<U> tmpMask2;
    LocalTensor<U> tmpMask3;
    LocalTensor<U> finiteIntegerYMask;
};

template <typename T, typename U> struct PowerIParams {
    [aicore] PowerIParams(){};
    float expIterateSum;

    LocalTensor<T> expUBIterate;
    LocalTensor<T> oriAbsExp;
    LocalTensor<T> recordExpNode;
    LocalTensor<T> tmpTensor1;
    LocalTensor<T> tmpTensor2;
    LocalTensor<U> negMask;
    LocalTensor<U> mask;
    LocalTensor<U> tmpScalar;
};

template <typename T> struct GeluParams {
    [aicore] GeluParams(){};
    uint32_t repeatTimes = 1;

    uint32_t currentSize = 0;
    uint32_t repeatRounding = 0;
    uint32_t repeatRemaining = 0;
    uint32_t tail = 0;

    uint32_t maxRepeatSize = 0;
    uint32_t oneRepeatSize = 0;

    uint32_t dataSize = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
};

template <typename T> struct TanhParams {
    [aicore] TanhParams(){};
    uint32_t repeatTimes = 1;
    uint32_t calCount = 0;
    uint32_t stackSize = 0;
    uint32_t tmpBufferSize = 0;
    LocalTensor<T> sharedTmpBuffer;

    LocalTensor<T> tempTensorConv;
    LocalTensor<T> tmpClip;
};

template <typename T> struct AscendDequantParams {
    [aicore] AscendDequantParams(){};
    uint64_t tmpSize;

    LocalTensor<T> tmpAddrA;
    LocalTensor<T> tmpAddrB;
};

template <typename T> constexpr [aicore] inline uint64_t GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        uint64_t output;
    } data;

    data.input = scalarValue;
    return data.output;
}

template <typename T, typename U> constexpr [aicore] inline U GetScalarBitcodeValue(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        U output;
    } data;

    data.input = scalarValue;
    return static_cast<U>(data.output);
}

template <typename T> constexpr [aicore] inline half GetScalarBitcodeToHalf(T scalarValue)
{
    union ScalarBitcode {
        [aicore] ScalarBitcode() {}
        T input;
        half output;
    } data;

    data.input = scalarValue;
    return data.output;
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_param.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_param.h"
namespace AscendC {
struct ReduceRepeatParams {
    [aicore] ReduceRepeatParams()
    {
        highMask = FULL_MASK;
        lowMask = FULL_MASK;
        repeatTimes = 0;
        dstRepStride = DEFAULT_REDUCE_DST_REP_SRIDE;
        srcBlkStride = DEFAULT_BLK_STRIDE;
        srcRepStride = DEFAULT_REPEAT_STRIDE;
    }

    [aicore] ReduceRepeatParams(const int32_t mask, const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {






        if (mask == HLAF_MASK_LEN) {
            highMask = 0;
            lowMask = FULL_MASK;
        } else if (mask == HLAF_MASK_LEN * DOUBLE_FACTOR) {
            highMask = FULL_MASK;
            lowMask = FULL_MASK;
        } else {
            highMask = (mask > HLAF_MASK_LEN) ?
                (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask - HLAF_MASK_LEN)) - 1) :
                0;
            lowMask =
                (mask > HLAF_MASK_LEN) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(mask)) - 1);
        }

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    [aicore] ReduceRepeatParams(const uint64_t mask[2], const int32_t repeatTimesIn, const int32_t dstRepStrideIn,
        const int32_t srcBlkStrideIn, const int32_t srcRepStrideIn)
    {






        highMask = mask[1];
        lowMask = mask[0];

        repeatTimes = repeatTimesIn;
        dstRepStride = dstRepStrideIn;
        srcBlkStride = srcBlkStrideIn;
        srcRepStride = srcRepStrideIn;
    }

    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    uint64_t bitMask[2] = {0, 0};
    int32_t normalMask = 0;
    int32_t maskMode = 0;
    int32_t repeatTimes = 0;
    int32_t dstRepStride = 0;
    int32_t srcBlkStride = 0;
    int32_t srcRepStride = 0;
};

struct DumpMessageHead {
    [aicore] DumpMessageHead()
    {
        type = 0;
        lenth = 0;
        addr = 0;
        dataType = 0;
        desc = 0;
        bufferId = 0;
        position = 0;
        dumpSize = 0;
    }

    [aicore] DumpMessageHead(uint32_t typeIn, uint32_t lenthIn, uint32_t addrIn, uint32_t dataTypeIn, uint32_t descIn,
        uint32_t bufferIdIn, uint32_t positionIn, uint32_t dumpSizeIn)
    {
        type = typeIn;
        lenth = lenthIn;
        addr = addrIn;
        dataType = dataTypeIn;
        desc = descIn;
        bufferId = bufferIdIn;
        position = positionIn;
        dumpSize = dumpSizeIn;
    }

    uint32_t type = 0;
    uint32_t lenth = 0;
    uint32_t addr = 0;
    uint32_t dataType = 0;
    uint32_t desc = 0;
    uint32_t bufferId = 0;
    uint32_t position = 0;
    uint32_t dumpSize = 0;
};

struct DumpShapeMessageHead {
    [aicore] DumpShapeMessageHead()
    {
        dim = 0;
        rsv = 0;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            shape[idx] = 0;
        }
    }

    [aicore] DumpShapeMessageHead(uint32_t dimIn, uint32_t shapeIn[], uint32_t rsvIn = 0)
    {


          ;
        dim = dimIn;
        rsv = rsvIn;
        for (uint32_t idx = 0; idx < 8; ++idx) {
            if (idx < dim) {
                shape[idx] = shapeIn[idx];
            } else {
                shape[idx] = 0;
            }
        }
    }

    uint32_t dim = 0;
    uint32_t shape[8];
    uint32_t rsv = 0;
};
# 165 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_param.h"
struct ProposalIntriParams {
    [aicore] ProposalIntriParams()
    {
        repeat = 0;
        modeNumber = 0;
    }

    [aicore] ProposalIntriParams(const int32_t repeatTime, const int32_t modeNumberIn)
    {
        repeat = repeatTime;
        modeNumber = modeNumberIn;
    }

    int32_t repeat = 0;
    int32_t modeNumber = 0;
};

struct BlockInfo {
    [aicore] BlockInfo()
    {
        len = 0;
        core = 0;
        blockNum = 0;
        dumpOffset = 0;
        magic = 0;
        rsv = 0;
        dumpAddr = 0;
    }
    [aicore] BlockInfo(uint64_t dumpAddrIn, uint32_t lenIn, uint32_t coreIn, uint32_t blockNumIn,
        uint32_t dumpOffsetIn, uint32_t magicIn, uint32_t rsvIn)
    {
        len = lenIn;
        core = coreIn;
        blockNum = blockNumIn;
        dumpOffset = dumpOffsetIn;
        magic = magicIn;
        rsv = rsvIn;
        dumpAddr = dumpAddrIn;
    }
# 213 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/utils/kernel_utils_struct_param.h"
    uint32_t len = 0;
    uint32_t core = 0;
    uint32_t blockNum = 0;
    uint32_t dumpOffset = 0;
    uint32_t magic = 0;
    uint32_t rsv = 0;
    uint64_t dumpAddr = 0;

};

struct BlockRingBufInfo {
    uint32_t length = 0U;
    uint32_t coreId = 0U;
    uint32_t blockNum = 0U;
    uint32_t ringBufLen = 0U;
    uint16_t magic = 0U;
    uint16_t flag = 0U;
    uint32_t rsv = 0U;
    uint64_t ringBufAddr = 0U;
    uint32_t resvMem[6];
};

struct RingBufWriteInfo {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_BUFI);
    uint32_t length = 0U;
    uint64_t bufOffset = 0U;
    uint64_t packIdx = 0U;
};

struct RingBufReadInfo {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_BUFO);
    uint32_t length = 0U;
    uint64_t bufOffset = 0U;
    uint64_t resv = 0U;
};

struct SkipTlvInfo {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_SKIP);
    uint32_t length = 0U;
};

struct PrintTlvInfoHead {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_SCALAR);
    uint32_t length = 0U;
    uint32_t resvMem[2];
    uint64_t fmtOffset = 0U;
};

struct DumpTensorTlvInfoHead {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    uint32_t length = 0U;
    uint32_t tensorAddr = 0U;
    uint32_t dataType = 0U;
    uint32_t desc = 0U;
    uint32_t bufferId = 0U;
    uint16_t position = 0U;
    uint16_t resv0 = 0U;
    uint32_t dim = 0U;
    uint32_t shape[8];
    uint32_t resv1 = 0U;
    uint32_t dumpSize = 0U;

};

struct DumpShapeTlvInfo {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_SHAPE);
    uint32_t length = 0U;
    uint32_t dim = 0U;
    uint32_t shape[8];
    uint32_t resv;
};

struct TimeStampTlvInfo {
    uint32_t type = static_cast<uint32_t>(DumpType::DUMP_TIME_STAMP);
    uint32_t length = 0U;
    uint32_t descId = 0U;
    uint32_t resv = 0U;
    uint64_t cycle = 0U;
    uint64_t pc = 0U;
    uint64_t entry = 0U;
    uint32_t resvMem[2];
};

struct DumpMeta {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_META);
    uint32_t len = 8;
    uint16_t blockDim = 0;
    uint8_t coreType = 0;
    uint8_t taskRation = 0;
    uint32_t rsv = 0;
};

struct SimtDumpMeta {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_META);
    uint32_t len = 8;
    uint32_t threadId = 0;
    uint32_t rsv = 0;
};

struct DumpTimeStamp {
    uint32_t typeId = static_cast<uint32_t>(DumpType::DUMP_TIME_STAMP);
    uint32_t len = 24;
    uint32_t descId = 0;
    uint32_t rsv = 0;
    uint64_t systemCycle = 0;
    uint64_t pcPtr = 0;
};
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_data_copy.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_data_copy.h"
namespace AscendC {


enum class DataFormat : uint8_t {
    ND = 0,
    NZ,
    NCHW,
    NC1HWC0,
    NHWC,
    NCDHW,
    NDC1HWC0,
    FRACTAL_Z_3D,
};


struct DataCopyParams {
    [aicore] DataCopyParams() {}

    [aicore] DataCopyParams(const uint16_t count, const uint16_t len, const uint16_t srcStrideIn,
        const uint16_t dstStrideIn)
        : blockCount(count),
          blockLen(len),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn)
    {}

    uint16_t blockCount = DEFAULT_DATA_COPY_NBURST;
    uint16_t blockLen = 0;

    union {
        uint16_t srcGap = 0;

        uint16_t srcStride;
    };

    union {
        uint16_t dstGap = 0;

        uint16_t dstStride;
    };
};

struct DataCopyEnhancedParams {
    [aicore] DataCopyEnhancedParams() {}

    [aicore] DataCopyEnhancedParams(const BlockMode blockModeIn, const DeqScale deqScaleIn, const uint64_t deqValueIn,
        const uint8_t sidStoreModeIn, const bool isReluIn, const pad_t padModeIn, const uint64_t padValueIn)
        : blockMode(blockModeIn),
          deqScale(deqScaleIn),
          deqValue(deqValueIn),
          sidStoreMode(sidStoreModeIn),
          isRelu(isReluIn),
          padMode(padModeIn),
          padValue(padValueIn)
    {}

    BlockMode blockMode = BlockMode::BLOCK_MODE_NORMAL;
    DeqScale deqScale = DeqScale::DEQ_NONE;
    uint64_t deqValue = 0;
    uint8_t sidStoreMode = 0;
    bool isRelu = false;
    pad_t padMode = pad_t::PAD_NONE;
    uint64_t padValue = 0;
    uint64_t deqTensorAddr = 0;
};

struct DataCopyCO12DstParams {
    [aicore] DataCopyCO12DstParams() {}

    [aicore] DataCopyCO12DstParams(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint32_t dstStrideIn,
        const uint16_t srcStrideIn, const QuantMode_t quantPreIn, const uint8_t reluPreIn, const bool channelSplitIn,
        const bool nz2ndEnIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          dstStride(dstStrideIn),
          srcStride(srcStrideIn),
          quantPre(quantPreIn),
          reluPre(reluPreIn),
          channelSplit(channelSplitIn),
          nz2ndEn(nz2ndEnIn)
    {}

    uint8_t sid = 0;
    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint32_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint8_t unitFlag = 0;
    uint8_t clipReluPre = 0;
    uint8_t eltWiseOp = 0;
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint8_t reluPre = 0;
    bool channelSplit = false;
    bool nz2ndEn = false;
};

struct DataCopyPadParams {
    [aicore] DataCopyPadParams() {}

    [aicore] DataCopyPadParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        const uint64_t padValue)
        : isPad(isPadValue),
          leftPadding(leftPadValue),
          rightPadding(rightPadValue),
          paddingValue(padValue)
    {}

    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    uint64_t paddingValue = 0;
};

struct DataCopyExtParams {
    [aicore] DataCopyExtParams() {}

    [aicore] DataCopyExtParams(const uint16_t count, const uint32_t len, const uint32_t srcStrideIn,
        const uint32_t dstStrideIn, const uint32_t rsvIn)
        : blockCount(count),
          blockLen(len),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          rsv(rsvIn)
    {}

    uint16_t blockCount = DEFAULT_DATA_COPY_NBURST;
    uint32_t blockLen = 0;
    uint32_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint32_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint32_t rsv = 0;
};

template <typename T>
struct DataCopyPadExtParams {
    [aicore] DataCopyPadExtParams() {}

    [aicore] DataCopyPadExtParams(const bool isPadValue, const uint8_t leftPadValue, const uint8_t rightPadValue,
        T padValue)
        : isPad(isPadValue),
          leftPadding(leftPadValue),
          rightPadding(rightPadValue),
          paddingValue(padValue)
    {}

    bool isPad = false;
    uint8_t leftPadding = 0;
    uint8_t rightPadding = 0;
    T paddingValue = 0;
};

struct Nd2NzParams {
    [aicore] Nd2NzParams() {}

    [aicore] Nd2NzParams(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcDValueIn, const uint16_t dstNzC0StrideIn,
        const uint16_t dstNzNStrideIn, const uint16_t dstNzMatrixStrideIn)
        : ndNum(ndNumIn),
          nValue(nValueIn),
          dValue(dValueIn),
          srcNdMatrixStride(srcNdMatrixStrideIn),
          srcDValue(srcDValueIn),
          dstNzC0Stride(dstNzC0StrideIn),
          dstNzNStride(dstNzNStrideIn),
          dstNzMatrixStride(dstNzMatrixStrideIn)
    {}

    uint16_t ndNum = 0;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 0;
    uint16_t srcDValue = 0;
    uint16_t dstNzC0Stride = 0;
    uint16_t dstNzNStride = 0;
    uint16_t dstNzMatrixStride = 0;
};

struct Nz2NdParamsFull {
    [aicore] Nz2NdParamsFull() {}

    [aicore] Nz2NdParamsFull(const uint16_t ndNumIn, const uint16_t nValueIn, const uint16_t dValueIn,
        const uint16_t srcNdMatrixStrideIn, const uint16_t srcNStrideIn, const uint16_t dstDStrideIn,
        const uint16_t dstNdMatrixStrideIn)
        : ndNum(ndNumIn),
          nValue(nValueIn),
          dValue(dValueIn),
          srcNdMatrixStride(srcNdMatrixStrideIn),
          srcNStride(srcNStrideIn),
          dstDStride(dstDStrideIn),
          dstNdMatrixStride(dstNdMatrixStrideIn)
    {}

    uint16_t ndNum = 1;
    uint16_t nValue = 0;
    uint16_t dValue = 0;
    uint16_t srcNdMatrixStride = 1;
    uint16_t srcNStride = 0;
    uint16_t dstDStride = 0;
    uint16_t dstNdMatrixStride = 1;
};

struct SliceInfo {
    [aicore] SliceInfo() {}

    [aicore] SliceInfo(const uint32_t startIndexIn, const uint32_t endIndexIn, const uint32_t strideIn,
        const uint32_t burstLenIn, const uint32_t shapeValueIn = 0)
        : startIndex(startIndexIn),
          endIndex(endIndexIn),
          stride(strideIn),
          burstLen(burstLenIn),
          shapeValue(shapeValueIn)
    {}

    uint32_t startIndex = 0;
    uint32_t endIndex = ONE_BLK_SIZE - 1;
    uint32_t stride = 0;
    uint32_t burstLen = ONE_BLK_SIZE;
    uint32_t shapeValue = 0;
};

struct CopyRepeatParams {
    [aicore] CopyRepeatParams() {}

    [aicore] CopyRepeatParams(const uint16_t dstStrideIn, const uint16_t srcStrideIn, uint16_t dstRepeatSizeIn,
        uint16_t srcRepeatSizeIn)
        : dstStride(dstStrideIn),
          srcStride(srcStrideIn),
          dstRepeatSize(dstRepeatSizeIn),
          srcRepeatSize(srcRepeatSizeIn)
    {}

    uint16_t dstStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t srcStride = DEFAULT_DATA_COPY_STRIDE;
    uint16_t dstRepeatSize = DEFAULT_REPEAT_STRIDE;
    uint16_t srcRepeatSize = DEFAULT_REPEAT_STRIDE;
};

}




namespace AscendC {

struct DataCopyTrait {};
constexpr DataCopyTrait DEFAULT_DATA_COPY_TRAIT;

}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar_convert.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar_convert.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_type_conversion_utils.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_type_conversion_utils.h"
namespace AscendC {



constexpr uint32_t BF16_TO_FP32_MAN_LEN = 16;
# 426 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_type_conversion_utils.h"
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar_convert.h" 2

namespace AscendC {
# 110 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar_convert.h"
[aicore] inline bfloat16_t ToBfloat16(const float& fVal)
{
    float fNum = fVal;
    union ToBfloat16Union {
        [aicore] ToBfloat16Union() {}
        uint16_t val;
        bfloat16_t bNum;
    } bf16Union;
    union FloattoInt32Union {
        [aicore] FloattoInt32Union() {}
        float ftmp;
        uint32_t uret;
    } int32Union;
    int32Union.ftmp = fNum;
    bf16Union.val = int32Union.uret >> BF16_TO_FP32_MAN_LEN;
    return bf16Union.bNum;
}

[aicore] inline float ToFloat(const bfloat16_t& bVal)
{
    bfloat16_t bNum = bVal;
    union ToFloatUnion {
        [aicore] ToFloatUnion() {}
        uint32_t val;
        float fNum;
    } floatUnion;
    union ToUint16Union {
        [aicore] ToUint16Union() {}
        bfloat16_t uret;
        uint16_t num;
    } u16Union;
    u16Union.uret = bNum;
    floatUnion.val = u16Union.num << BF16_TO_FP32_MAN_LEN;
    return floatUnion.fNum;
}


}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h" 1
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
namespace AscendC {
class AscendCUtils {
public:
    [aicore] static inline int32_t GetBitSize(int32_t byteSize)
    {
        return byteSize * ONE_BYTE_BIT_SIZE;
    }

    [aicore] static inline int32_t GetC0Size()
    {
        return DEFAULT_C0_SIZE;
    }

    [aicore] static inline void InitSocStateImpl()
    {
        set_atomic_none();

        set_mask_norm();
        if constexpr(g_coreType == AscendC::AIC) {
            set_l1_3d_size(static_cast<uint64_t>(0));
            set_padding(static_cast<uint64_t>(0));
        } else {
            set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
        }
# 72 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
    }

    [aicore] static inline int32_t GetC0Count(const int32_t dtypeSize)
    {
                                                                                                 ;
        return GetC0Size() / dtypeSize;
    }

    [aicore] static inline int32_t GetDefaultBlockNum()
    {
        return DEFAULT_BLK_NUM;
    }

    [aicore] static inline int64_t GetRsvdCnt()
    {
        return get_rsvd_cnt();
    }

    template <typename T, bool isSetMask = true>
    [aicore] static inline void SetMask(const uint64_t& maskHigh, const uint64_t& maskLow)
    {
        if constexpr (!isSetMask) {
            return;
        }
# 107 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(maskHigh, maskLow);
        }
    }

    template <typename T, bool isSetMask = true> [aicore] static inline void SetMask(int32_t len)
    {
        if constexpr (!isSetMask) {
            return;
        }

        int32_t typeLen = 0;
        if constexpr (IsSameType<T, int4b_t>::value) {
            typeLen = DEFAULT_BLOCK_SIZE * INT4_TWO;




        } else {
            typeLen = DEFAULT_BLOCK_SIZE / sizeof(T);
        }
        constexpr int32_t halfTypeLen = 64;
        constexpr int32_t lenCoeff = 2;
        if (len == halfTypeLen) {
            SetMask<T>(0, FULL_MASK);
            return;
        } else if (len == typeLen || len >= halfTypeLen * lenCoeff) {
            SetMask<T>(FULL_MASK, FULL_MASK);
            return;
        }
        SetMask<T>(static_cast<uint64_t>(
            (len > halfTypeLen) ? (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len - halfTypeLen)) - 1) : 0),
            static_cast<uint64_t>(
            (len > halfTypeLen) ? FULL_MASK : (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(len)) - 1)));
    }

    template <typename T> [aicore] static inline void SetMaskCount()
    {
        set_mask_count();
    }

    template <typename T> [aicore] static inline void SetMaskNorm()
    {
        set_mask_norm();
    }



    [aicore] static inline void SetOverflow(uint64_t ctrlValue)
    {


        if (ctrlValue == 1) {
            set_ctrl(sbitset1(get_ctrl(), CTRL_48_BIT));
        } else {
            set_ctrl(sbitset0(get_ctrl(), CTRL_48_BIT));
        }
    }
# 179 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
    template <bool isSetMask = true> [aicore] static inline void ResetMask()
    {
        if constexpr (!isSetMask) {
            return;
        }
        if constexpr(g_coreType != AscendC::AIC) {
            set_vector_mask(FULL_MASK, FULL_MASK);
        }
    }

    template <bool isInt4 = false>
    [aicore] inline static IntriInfo CalIntriInfo(
        const uint32_t dtypeSize, const uint32_t count, uint32_t repStride = DEFAULT_BLK_NUM)
    {
        IntriInfo retIntriInfo;
        retIntriInfo.c0Count = GetC0Count(dtypeSize);
        if constexpr (isInt4) {
            retIntriInfo.c0Count = GetC0Size() * INT4_TWO;
        }
        uint32_t repeatCount = repStride * retIntriInfo.c0Count;
        retIntriInfo.repeat = count / repeatCount;
        retIntriInfo.tail = count % repeatCount;
        retIntriInfo.repeatRounding = retIntriInfo.repeat / MAX_REPEAT_TIMES;
        retIntriInfo.repeatRemaining = retIntriInfo.repeat % MAX_REPEAT_TIMES;

        return retIntriInfo;
    }

    template <typename T>
    [aicore] static inline __attribute__((cce_unif_buff)) T* GetTemporaryBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 225 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
        (void)bufferSize;
        __attribute__((cce_unif_buff)) T* addr = reinterpret_cast<__attribute__((cce_unif_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static inline void FreeTemporaryBuffer(__attribute__((cce_unif_buff)) T* addr)
    {
        (void)addr;
    }




    template <typename T>
    [aicore] static inline __attribute__((cce_fixpipe_buff)) T* GetTemporaryFbBufferAddr(const int32_t bufferOffset, const int32_t bufferSize)
    {
# 253 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
        (void)bufferSize;
        __attribute__((cce_fixpipe_buff)) T* addr = reinterpret_cast<__attribute__((cce_fixpipe_buff)) T*>((uint64_t)(0) + bufferOffset);

        return addr;
    }

    template <typename T> [aicore] static inline void FreeTemporaryFbBuffer(__attribute__((cce_fixpipe_buff)) T* addr)
    {
        (void)addr;
    }


    [aicore] static inline uint64_t GetGMLen(const DataCopyParams& intriParams, const bool& isSrc,
                                               const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit
                         + (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static inline uint64_t GetGMLen(const DataCopyExtParams& intriParams, const bool& isSrc,
                                               const bool& isMovAlignIntri)
    {
        uint16_t stride = intriParams.dstStride;
        uint16_t burstLenUnit = 32;
        uint16_t strideUnit = 32;
        if (isSrc) {
            stride = intriParams.srcStride;
        }
        if (isMovAlignIntri) {
            burstLenUnit = 1;
            strideUnit = 1;
        }
        if (intriParams.blockLen == 0) {
            return 0;
        }
        uint64_t gmLen = static_cast<uint64_t>(intriParams.blockCount) * intriParams.blockLen * burstLenUnit
                         + (intriParams.blockCount - 1) * stride * strideUnit;
        return gmLen;
    }

    [aicore] static inline uint64_t GetGMLen(const uint64_t& srcEleSize, const Nd2NzParams& intriParams)
    {
        uint64_t gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * srcEleSize * intriParams.srcNdMatrixStride
                         + (intriParams.nValue - 1) * intriParams.srcDValue * srcEleSize
                         + intriParams.dValue * srcEleSize;
        return gmLen;
    }
# 326 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
    [aicore] static inline bool OOMCheckAddrIsOverflow(uintptr_t gmAddrConvert, const uint64_t& gmLen)
    {
        (void)gmAddrConvert;
        (void)gmLen;
# 356 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
        (void)gmAddrConvert;
        (void)gmLen;
        return false;
    }

    template <typename T>
    [aicore] static inline void CheckGmMemOverflow(__attribute__((cce_global)) T* gmAddr, const bool& isSrc, const uint64_t& gmLen)
    {
# 403 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
    }

    template <typename T>
    [aicore] static inline void CheckGmMemOverflowNormal(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
                                                           const bool isSrc, const bool isMovAlignIntri,
                                                           const DataCopyParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static inline void CheckGmMemOverflowNormal(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace,
                                                           const bool isSrc, const bool isMovAlignIntri,
                                                           const DataCopyExtParams& intriParams)
    {
        (void)(workSpace);
        uint64_t gmLen = GetGMLen(intriParams, isSrc, isMovAlignIntri);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }

    template <typename T>
    [aicore] static inline void CheckGmMemOverflowNd2Nz(__attribute__((cce_global)) T* gmAddr, __attribute__((cce_global)) uint8_t* workSpace, const bool isSrc,
                                                          const Nd2NzParams& intriParams)
    {
        (void)(workSpace);
        uint64_t srcEleSize = sizeof(T);
        uint64_t gmLen = GetGMLen(srcEleSize, intriParams);
        CheckGmMemOverflow(gmAddr, isSrc, gmLen);
    }
# 461 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils_base.h"
};
}
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h" 2







inline __attribute__((cce_global)) void* g_sysFftsAddr = nullptr;
namespace AscendC {
# 219 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_utils.h"
enum class TimeStampId : uint32_t {
    TIME_STAMP_WRAP_FIRST = 0x000,
    TIME_STAMP_WRAP_MC2_CTX,
    TIME_STAMP_WRAP_WK_SPACE,
    TIME_STAMP_WRAP_INIT_DUMP,
    TIME_STAMP_WRAP_FFTS_ADDR,
    TIME_STAMP_WRAP_CLEAR_WK_SPAC,

    TIME_STAMP_TPIPE = 0x030,
    TIME_STAMP_BUFFER,

    TIME_STAMP_MATMUL_SERVER = 0x060,
    TIME_STAMP_MATMUL_SERVER_INIT,
    TIME_STAMP_MATMUL_SERVER_OBJ,
    TIME_STAMP_MATMUL_MATRIX_KFC,
    TIME_STAMP_MATMUL_CLIENT_KFC,
    TIME_STAMP_MATMUL_WAIT_EVE,
    TIME_STAMP_MATMUL_OBJ,

    TIME_STAMP_TILING_DATA = 0x090,
    TIME_STAMP_TILING_DATA_STRUCT,
    TIME_STAMP_TILING_DATA_MEMBER,


    TIME_STAMP_MC2_START = 0x1000,
    TIME_STAMP_MC2_END = 0x1fff,

    TIME_STAMP_MAX = 0xffff,
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_aipp.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_aipp.h"
namespace AscendC {
enum class AippInputFormat : uint8_t {
    YUV420SP_U8 = 0,
    XRGB8888_U8 = 1,
    RGB888_U8 = 4,
    YUV400_U8 = 9,
};

template <typename T>
struct AippPaddingParams {
    uint32_t paddingMode{ 0 };
    T paddingValueCh0{ 0 };
    T paddingValueCh1{ 0 };
    T paddingValueCh2{ 0 };
    T paddingValueCh3{ 0 };
};

struct AippSwapParams {
    bool isSwapRB{ false };
    bool isSwapUV{ false };
    bool isSwapAX{ false };
};

struct AippSingleLineParams {
    bool isSingleLineCopy{ false };
};

struct AippDataTypeConvParams {
    uint8_t dtcMeanCh0{ 0 };
    uint8_t dtcMeanCh1{ 0 };
    uint8_t dtcMeanCh2{ 0 };
    half dtcMinCh0{ 0 };
    half dtcMinCh1{ 0 };
    half dtcMinCh2{ 0 };
    half dtcVarCh0{ 1.0 };
    half dtcVarCh1{ 1.0 };
    half dtcVarCh2{ 1.0 };
    uint32_t dtcRoundMode{ 0 };
};

template <typename T>
struct AippChannelPaddingParams {
    uint32_t cPaddingMode;
    T cPaddingValue;
};

struct AippColorSpaceConvParams {
    bool isEnableCsc{ false };
    int16_t cscMatrixR0C0{ 0 };
    int16_t cscMatrixR0C1{ 0 };
    int16_t cscMatrixR0C2{ 0 };
    int16_t cscMatrixR1C0{ 0 };
    int16_t cscMatrixR1C1{ 0 };
    int16_t cscMatrixR1C2{ 0 };
    int16_t cscMatrixR2C0{ 0 };
    int16_t cscMatrixR2C1{ 0 };
    int16_t cscMatrixR2C2{ 0 };
    uint8_t cscBiasIn0{ 0 };
    uint8_t cscBiasIn1{ 0 };
    uint8_t cscBiasIn2{ 0 };
    uint8_t cscBiasOut0{ 0 };
    uint8_t cscBiasOut1{ 0 };
    uint8_t cscBiasOut2{ 0 };
};




template <typename T> struct AippParams {
    AippPaddingParams<T> paddingParams;
    AippSwapParams swapParams;
    AippSingleLineParams singleLineParams;
    AippDataTypeConvParams dtcParams;
    AippChannelPaddingParams<T> cPaddingParams;
    AippColorSpaceConvParams cscParams;
};

}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h" 2

namespace AscendC {
constexpr uint64_t MASK_PLACEHOLDER = 0;
constexpr uint64_t MASK_PLACEHOLDER_LIST[2] = {0, 0};

enum class MaskMode : uint8_t {
    NORMAL = 0,
    COUNTER
};

template <typename T, MaskMode mode>
[aicore] static inline void SetVectorMaskImpl(const uint64_t maskHigh, const uint64_t maskLow)
{
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(maskHigh, maskLow);
    }
}

template <typename T, MaskMode mode>
[aicore] static inline void SetVectorMaskImpl(int32_t len)
{
    if constexpr (mode == MaskMode::COUNTER) {
        SetVectorMaskImpl<PrimT<T>, mode>(0, len);
        return;
    }
    AscendCUtils::SetMask<PrimT<T>>(len);
}

[aicore] inline void ResetMaskImpl()
{
    if constexpr(g_coreType != AscendC::AIC) {
        set_vector_mask(FULL_MASK, FULL_MASK);
    }
}
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
template <pipe_t pipe> [aicore] inline void PipeBarrierImpl()
{
# 95 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (pipe == PIPE_V) {
            return;
        }
    }

    pipe_barrier(pipe);
}


enum class CacheLine : uint64_t {
    SINGLE_CACHE_LINE = 0,
    ENTIRE_DATA_CACHE
};

enum class DcciDst : uint64_t {
    CACHELINE_ALL = 0,
    CACHELINE_UB,
    CACHELINE_OUT,
    CACHELINE_ATOMIC
};




template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] inline void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] inline void DcciUBImpl(__attribute__((cce_unif_buff)) T* dst)
{
    dcci(static_cast<__attribute__((cce_unif_buff)) void *>(dst), static_cast<uint64_t>(entireType), static_cast<uint64_t>(dcciDst));
}





template <typename T, CacheLine entireType>
[aicore] inline void DcciGMImpl(__attribute__((cce_global)) T* dst)
{
    dcci(static_cast<__attribute__((cce_global)) void *>(dst), static_cast<uint64_t>(entireType));
}


[aicore] inline void SetMaskCountImpl()
{
    set_mask_count();
}

[aicore] inline void SetMaskNormImpl()
{
    set_mask_norm();
}

[aicore] inline void SetLreluMode(bool lreluMode)
{
    if (lreluMode) {
        set_ctrl(sbitset1(get_ctrl(), LEAKY_RELU_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), LEAKY_RELU_MODE_BIT));
    }
}

[aicore] inline void SetHF32ModeImpl(bool hf32Mode)
{
    if (hf32Mode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_MODE_BIT));
    }
}

[aicore] inline void SetHF32TransModeImpl(bool hf32TransMode)
{
    if (hf32TransMode) {
        set_ctrl(sbitset1(get_ctrl(), HF32_TRANS_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), HF32_TRANS_MODE_BIT));
    }
}

[aicore] inline void SetMMLayoutTransformImpl(bool mmLayoutMode)
{
    if (mmLayoutMode) {
        set_ctrl(sbitset1(get_ctrl(), MM_LAYOUT_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), MM_LAYOUT_MODE_BIT));
    }
}

template <bool castMode>
[aicore] inline void SetCastOverflowModeImpl()
{
    if constexpr (castMode) {
        set_ctrl(sbitset1(get_ctrl(), CAST_MODE_BIT));
    } else {
        set_ctrl(sbitset0(get_ctrl(), CAST_MODE_BIT));
    }
}




template <typename T>
[aicore] inline void SetAippFunctionsImpl0(__attribute__((cce_global)) T* src0)
{



    uint64_t aippConfig0 = reinterpret_cast<uint64_t>(src0) & 0xffffffffffff;

    set_aipp_spr_0(aippConfig0);

}

template <typename T, typename U>
[aicore] inline void SetAippFunctionsImpl1(__attribute__((cce_global)) T* src1, AippParams<U>& config)
{






    uint64_t aippConfig1 = reinterpret_cast<uint64_t>(src1) & 0xffffffffffff;

    if (config.cscParams.isEnableCsc) {
        aippConfig1 |= static_cast<uint64_t>(1) << AIPP_OFFSET_CSC_ENABLE;
    }

    set_aipp_spr_1(aippConfig1);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl2(AippParams<T>& config)
{
    uint16_t cscMatrixR0C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C0);
    uint16_t cscMatrixR0C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C1);
    uint16_t cscMatrixR0C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR0C2);
    uint16_t cscMatrixR1C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C0);







    uint64_t aippConfig2 = static_cast<uint64_t>(cscMatrixR0C0);
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C1) << AIPP_OFFSET_CH1;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR0C2) << AIPP_OFFSET_CH2;
    aippConfig2 |= static_cast<uint64_t>(cscMatrixR1C0) << AIPP_OFFSET_CH3;

    set_aipp_spr_2(aippConfig2);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl3(AippParams<T>& config)
{
    uint16_t cscMatrixR1C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C1);
    uint16_t cscMatrixR1C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR1C2);
    uint16_t cscMatrixR2C0 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C0);
    uint16_t cscMatrixR2C1 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C1);







    uint64_t aippConfig3 = static_cast<uint64_t>(cscMatrixR1C1);
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR1C2) << AIPP_OFFSET_CH1;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C0) << AIPP_OFFSET_CH2;
    aippConfig3 |= static_cast<uint64_t>(cscMatrixR2C1) << AIPP_OFFSET_CH3;

    set_aipp_spr_3(aippConfig3);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl4(AippParams<T>& config)
{
    uint16_t cscMatrixR2C2 = GetScalarBitcodeValue(config.cscParams.cscMatrixR2C2);
    uint8_t cscBiasOut0 = GetScalarBitcodeValue(config.cscParams.cscBiasOut0);
    uint8_t cscBiasOut1 = GetScalarBitcodeValue(config.cscParams.cscBiasOut1);
    uint8_t cscBiasOut2 = GetScalarBitcodeValue(config.cscParams.cscBiasOut2);
    uint8_t cscBiasIn0 = GetScalarBitcodeValue(config.cscParams.cscBiasIn0);
    uint8_t cscBiasIn1 = GetScalarBitcodeValue(config.cscParams.cscBiasIn1);
    uint8_t cscBiasIn2 = GetScalarBitcodeValue(config.cscParams.cscBiasIn2);
# 299 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
    uint64_t aippConfig4 = static_cast<uint64_t>(cscMatrixR2C2);
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut0) << AIPP_OFFSET_CSC_OUT_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut1) << AIPP_OFFSET_CSC_OUT_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasOut2) << AIPP_OFFSET_CSC_OUT_CH2;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn0) << AIPP_OFFSET_CSC_IN_CH0;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn1) << AIPP_OFFSET_CSC_IN_CH1;
    aippConfig4 |= static_cast<uint64_t>(cscBiasIn2) << AIPP_OFFSET_CSC_IN_CH2;

    set_aipp_spr_4(aippConfig4);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl5(AippParams<T>& config)
{



    uint8_t dtcMeanCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh0);
    uint8_t dtcMeanCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh1);
    uint8_t dtcMeanCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMeanCh2);






    uint64_t aippConfig5 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_CH1;
    aippConfig5 |= static_cast<uint64_t>(dtcMeanCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_5(aippConfig5);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl6(AippParams<T>& config)
{



    uint16_t dtcMinCh0 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh0);
    uint16_t dtcMinCh1 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh1);
    uint16_t dtcMinCh2 = GetScalarBitcodeValue(config.dtcParams.dtcMinCh2);






    uint64_t aippConfig6 = static_cast<uint64_t>(dtcMinCh0);
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh1) << AIPP_OFFSET_CH1;
    aippConfig6 |= static_cast<uint64_t>(dtcMinCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_6(aippConfig6);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl7(AippParams<T>& config)
{



    uint16_t dtcVarCh0 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh0);
    uint16_t dtcVarCh1 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh1);
    uint16_t dtcVarCh2 = GetScalarBitcodeValue(config.dtcParams.dtcVarCh2);






    uint64_t aippConfig7 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_CH1;
    aippConfig7 |= static_cast<uint64_t>(dtcVarCh2) << AIPP_OFFSET_CH2;

    set_aipp_spr_7(aippConfig7);

}

template <typename T>
[aicore] inline void SetAippFunctionsImpl8(AippParams<T>& config)
{
    uint64_t aippConfig8 = 0;
    if constexpr(IsSameType<T, int8_t>::value || IsSameType<T, uint8_t>::value) {
        uint8_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint8_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint8_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint8_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);







        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;

        set_aipp_spr_8(aippConfig8);

    } else {
        uint16_t paddingValueCh0 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh0);
        uint16_t paddingValueCh1 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh1);
        uint16_t paddingValueCh2 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh2);
        uint16_t paddingValueCh3 = GetScalarBitcodeValue(config.paddingParams.paddingValueCh3);







        aippConfig8 |= static_cast<uint64_t>(paddingValueCh0);
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh1) << AIPP_OFFSET_CH1;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh2) << AIPP_OFFSET_CH2;
        aippConfig8 |= static_cast<uint64_t>(paddingValueCh3) << AIPP_OFFSET_CH3;

        set_aipp_spr_8(aippConfig8);

    }
}

template <typename T>
[aicore] inline void SetAippFunctionsImpl9(AippInputFormat format, AippParams<T>& config)
{
    uint64_t aippConfig9 = 0;

    if constexpr(IsSameType<T, int8_t>::value || IsSameType<T, uint8_t>::value) {
        uint8_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    } else {
        uint16_t cPaddingValue = GetScalarBitcodeValue(config.cPaddingParams.cPaddingValue);
        aippConfig9 |= static_cast<uint64_t>(cPaddingValue);
    }

    if (config.swapParams.isSwapRB) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_RB;
    }
    if (config.swapParams.isSwapUV) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_UV;
    }
    if (config.swapParams.isSwapAX) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SWAP_AX;
    }

    aippConfig9 |= (static_cast<uint64_t>(format) & 0x1f) << AIPP_OFFSET_FORMAT;

    if (config.singleLineParams.isSingleLineCopy) {
        aippConfig9 |= static_cast<uint64_t>(1) << AIPP_OFFSET_SINGLE_LINE;
    }

    aippConfig9 |= (static_cast<uint64_t>(config.paddingParams.paddingMode) & 0x3) << AIPP_OFFSET_PADDING_MODE;





    aippConfig9 |= (static_cast<uint64_t>(config.cPaddingParams.cPaddingMode) & 0x1) << AIPP_OFFSET_CPADDING_MODE;

    set_aipp_spr_9(aippConfig9);
}

template <typename T>
[aicore] inline void SetAippFunctionsImpl18(AippParams<T>& config)
{

    return;

    float dtcVarCh0f = static_cast<float>(config.dtcParams.dtcVarCh0);
    float dtcVarCh1f = static_cast<float>(config.dtcParams.dtcVarCh1);
    uint32_t dtcVarCh0 = GetScalarBitcodeValue(dtcVarCh0f);
    uint32_t dtcVarCh1 = GetScalarBitcodeValue(dtcVarCh1f);

    uint64_t aippConfig18 = static_cast<uint64_t>(dtcVarCh0);
    aippConfig18 |= static_cast<uint64_t>(dtcVarCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_18(aippConfig18);
}

template <typename T>
[aicore] inline void SetAippFunctionsImpl19(AippParams<T>& config)
{

    return;

    float dtcVarCh2f = static_cast<float>(config.dtcParams.dtcVarCh2);
    uint32_t dtcVarCh2 = GetScalarBitcodeValue(dtcVarCh2f);
    uint64_t aippConfig19 = static_cast<uint64_t>(dtcVarCh2);
    set_aipp_spr_19(aippConfig19);
}

template <typename T>
[aicore] inline void SetAippFunctionsImpl20(AippParams<T>& config)
{

    return;

    float dtcMeanCh0f = static_cast<float>(config.dtcParams.dtcMeanCh0 * 1.0f);
    float dtcMeanCh1f = static_cast<float>(config.dtcParams.dtcMeanCh1 * 1.0f);

    uint32_t dtcMeanCh0 = GetScalarBitcodeValue(dtcMeanCh0f);
    uint32_t dtcMeanCh1 = GetScalarBitcodeValue(dtcMeanCh1f);

    uint64_t aippConfig20 = static_cast<uint64_t>(dtcMeanCh0);
    aippConfig20 |= static_cast<uint64_t>(dtcMeanCh1) << AIPP_OFFSET_DTC_CH1;

    set_aipp_spr_20(aippConfig20);
}

template <typename T>
[aicore] inline void SetAippFunctionsImpl21(AippParams<T>& config)
{

    return;

    float dtcMeanCh2f = static_cast<float>(config.dtcParams.dtcMeanCh2 * 1.0f);
    uint32_t dtcMeanCh2 = GetScalarBitcodeValue(dtcMeanCh2f);
    uint64_t aippConfig21 = static_cast<uint64_t>(dtcMeanCh2);
    set_aipp_spr_21(aippConfig21);
}
# 559 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
template <typename T, typename U>
[aicore] inline void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, __attribute__((cce_global)) T* src1,
    AippInputFormat format, AippParams<U>& config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
# 585 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_reg.h"
    SetAippFunctionsImpl0<T>(src0);
    SetAippFunctionsImpl1<T, U>(src1, config);
    SetAippFunctionsImpl2<U>(config);
    SetAippFunctionsImpl3<U>(config);
    SetAippFunctionsImpl4<U>(config);
    SetAippFunctionsImpl5<U>(config);
    SetAippFunctionsImpl6<U>(config);
    SetAippFunctionsImpl7<U>(config);
    SetAippFunctionsImpl8<U>(config);



    SetAippFunctionsImpl9<U>(format, config);


}

template <typename T, typename U>
[aicore] inline void SetAippFunctionsImpl(__attribute__((cce_global)) T* src0, AippInputFormat format, AippParams<U> config)
{

    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }

    SetAippFunctionsImpl(src0, reinterpret_cast<__attribute__((cce_global)) T*>(0), format, config);
}


}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_process_lock.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_tensor_trait.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_tensor_trait.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_coord.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_coord.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_layout.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_layout.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/tuple.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/tuple.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h"
# 1 "/usr/include/c++/11/type_traits" 1 3
# 33 "/usr/include/c++/11/type_traits" 3







namespace std __attribute__ ((__visibility__ ("default")))
{


  template<typename... _Elements>
    class tuple;

  template<typename _Tp>
    class reference_wrapper;
# 64 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp, _Tp __v>
    struct integral_constant
    {
      static constexpr _Tp value = __v;
      typedef _Tp value_type;
      typedef integral_constant<_Tp, __v> type;
      constexpr operator value_type() const noexcept { return value; }




      constexpr value_type operator()() const noexcept { return value; }

    };

  template<typename _Tp, _Tp __v>
    constexpr _Tp integral_constant<_Tp, __v>::value;


  using true_type = integral_constant<bool, true>;


  using false_type = integral_constant<bool, false>;



  template<bool __v>
    using __bool_constant = integral_constant<bool, __v>;






  template<bool __v>
    using bool_constant = integral_constant<bool, __v>;




  template<bool, typename, typename>
    struct conditional;


  template <typename _Type>
    struct __type_identity
    { using type = _Type; };

  template<typename _Tp>
    using __type_identity_t = typename __type_identity<_Tp>::type;

  template<typename...>
    struct __or_;

  template<>
    struct __or_<>
    : public false_type
    { };

  template<typename _B1>
    struct __or_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __or_<_B1, _B2>
    : public conditional<_B1::value, _B1, _B2>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __or_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, _B1, __or_<_B2, _B3, _Bn...>>::type
    { };

  template<typename...>
    struct __and_;

  template<>
    struct __and_<>
    : public true_type
    { };

  template<typename _B1>
    struct __and_<_B1>
    : public _B1
    { };

  template<typename _B1, typename _B2>
    struct __and_<_B1, _B2>
    : public conditional<_B1::value, _B2, _B1>::type
    { };

  template<typename _B1, typename _B2, typename _B3, typename... _Bn>
    struct __and_<_B1, _B2, _B3, _Bn...>
    : public conditional<_B1::value, __and_<_B2, _B3, _Bn...>, _B1>::type
    { };

  template<typename _Pp>
    struct __not_
    : public __bool_constant<!bool(_Pp::value)>
    { };





  template<typename... _Bn>
    inline constexpr bool __or_v = __or_<_Bn...>::value;
  template<typename... _Bn>
    inline constexpr bool __and_v = __and_<_Bn...>::value;




  template<typename... _Bn>
    struct conjunction
    : __and_<_Bn...>
    { };

  template<typename... _Bn>
    struct disjunction
    : __or_<_Bn...>
    { };

  template<typename _Pp>
    struct negation
    : __not_<_Pp>
    { };




  template<typename... _Bn>
    inline constexpr bool conjunction_v = conjunction<_Bn...>::value;

  template<typename... _Bn>
    inline constexpr bool disjunction_v = disjunction<_Bn...>::value;

  template<typename _Pp>
    inline constexpr bool negation_v = negation<_Pp>::value;





  template<typename>
    struct is_reference;
  template<typename>
    struct is_function;
  template<typename>
    struct is_void;
  template<typename>
    struct remove_cv;
  template<typename>
    struct is_const;


  template<typename>
    struct __is_array_unknown_bounds;




  template <typename _Tp, size_t = sizeof(_Tp)>
    constexpr true_type __is_complete_or_unbounded(__type_identity<_Tp>)
    { return {}; }

  template <typename _TypeIdentity,
      typename _NestedType = typename _TypeIdentity::type>
    constexpr typename __or_<
      is_reference<_NestedType>,
      is_function<_NestedType>,
      is_void<_NestedType>,
      __is_array_unknown_bounds<_NestedType>
    >::type __is_complete_or_unbounded(_TypeIdentity)
    { return {}; }






  template<typename _Tp>
    struct __success_type
    { typedef _Tp type; };

  struct __failure_type
  { };


  template<typename _Tp>
    using __remove_cv_t = typename remove_cv<_Tp>::type;



  template<typename>
    struct __is_void_helper
    : public false_type { };

  template<>
    struct __is_void_helper<void>
    : public true_type { };



  template<typename _Tp>
    struct is_void
    : public __is_void_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct __is_integral_helper
    : public false_type { };

  template<>
    struct __is_integral_helper<bool>
    : public true_type { };

  template<>
    struct __is_integral_helper<char>
    : public true_type { };

  template<>
    struct __is_integral_helper<signed char>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned char>
    : public true_type { };





  template<>
    struct __is_integral_helper<wchar_t>
    : public true_type { };
# 310 "/usr/include/c++/11/type_traits" 3
  template<>
    struct __is_integral_helper<char16_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<char32_t>
    : public true_type { };

  template<>
    struct __is_integral_helper<short>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned short>
    : public true_type { };

  template<>
    struct __is_integral_helper<int>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned int>
    : public true_type { };

  template<>
    struct __is_integral_helper<long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long>
    : public true_type { };

  template<>
    struct __is_integral_helper<long long>
    : public true_type { };

  template<>
    struct __is_integral_helper<unsigned long long>
    : public true_type { };
# 391 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp>
    struct is_integral
    : public __is_integral_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct __is_floating_point_helper
    : public false_type { };

  template<>
    struct __is_floating_point_helper<float>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<double>
    : public true_type { };

  template<>
    struct __is_floating_point_helper<long double>
    : public true_type { };
# 421 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp>
    struct is_floating_point
    : public __is_floating_point_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_array
    : public false_type { };

  template<typename _Tp, std::size_t _Size>
    struct is_array<_Tp[_Size]>
    : public true_type { };

  template<typename _Tp>
    struct is_array<_Tp[]>
    : public true_type { };

  template<typename>
    struct __is_pointer_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_pointer_helper<_Tp*>
    : public true_type { };


  template<typename _Tp>
    struct is_pointer
    : public __is_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename>
    struct is_lvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_lvalue_reference<_Tp&>
    : public true_type { };


  template<typename>
    struct is_rvalue_reference
    : public false_type { };

  template<typename _Tp>
    struct is_rvalue_reference<_Tp&&>
    : public true_type { };

  template<typename>
    struct __is_member_object_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_object_pointer_helper<_Tp _Cp::*>
    : public __not_<is_function<_Tp>>::type { };


  template<typename _Tp>
    struct is_member_object_pointer
    : public __is_member_object_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename>
    struct __is_member_function_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_function_pointer_helper<_Tp _Cp::*>
    : public is_function<_Tp>::type { };


  template<typename _Tp>
    struct is_member_function_pointer
    : public __is_member_function_pointer_helper<__remove_cv_t<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_enum
    : public integral_constant<bool, __is_enum(_Tp)>
    { };


  template<typename _Tp>
    struct is_union
    : public integral_constant<bool, __is_union(_Tp)>
    { };


  template<typename _Tp>
    struct is_class
    : public integral_constant<bool, __is_class(_Tp)>
    { };


  template<typename _Tp>
    struct is_function
    : public __bool_constant<!is_const<const _Tp>::value> { };

  template<typename _Tp>
    struct is_function<_Tp&>
    : public false_type { };

  template<typename _Tp>
    struct is_function<_Tp&&>
    : public false_type { };



  template<typename>
    struct __is_null_pointer_helper
    : public false_type { };

  template<>
    struct __is_null_pointer_helper<std::nullptr_t>
    : public true_type { };


  template<typename _Tp>
    struct is_null_pointer
    : public __is_null_pointer_helper<__remove_cv_t<_Tp>>::type
    { };



  template<typename _Tp>
    struct __is_nullptr_t
    : public is_null_pointer<_Tp>
    { } __attribute__ ((__deprecated__ ("use '" "std::is_null_pointer" "' instead")));




  template<typename _Tp>
    struct is_reference
    : public __or_<is_lvalue_reference<_Tp>,
                   is_rvalue_reference<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_arithmetic
    : public __or_<is_integral<_Tp>, is_floating_point<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_fundamental
    : public __or_<is_arithmetic<_Tp>, is_void<_Tp>,
     is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_object
    : public __not_<__or_<is_function<_Tp>, is_reference<_Tp>,
                          is_void<_Tp>>>::type
    { };

  template<typename>
    struct is_member_pointer;


  template<typename _Tp>
    struct is_scalar
    : public __or_<is_arithmetic<_Tp>, is_enum<_Tp>, is_pointer<_Tp>,
                   is_member_pointer<_Tp>, is_null_pointer<_Tp>>::type
    { };


  template<typename _Tp>
    struct is_compound
    : public __not_<is_fundamental<_Tp>>::type { };


  template<typename _Tp>
    struct __is_member_pointer_helper
    : public false_type { };

  template<typename _Tp, typename _Cp>
    struct __is_member_pointer_helper<_Tp _Cp::*>
    : public true_type { };



  template<typename _Tp>
    struct is_member_pointer
    : public __is_member_pointer_helper<__remove_cv_t<_Tp>>::type
    { };

  template<typename, typename>
    struct is_same;


  template<typename _Tp, typename... _Types>
    using __is_one_of = __or_<is_same<_Tp, _Types>...>;


  template<typename _Tp>
    using __is_signed_integer = __is_one_of<__remove_cv_t<_Tp>,
   signed char, signed short, signed int, signed long,
   signed long long
# 637 "/usr/include/c++/11/type_traits" 3
   >;


  template<typename _Tp>
    using __is_unsigned_integer = __is_one_of<__remove_cv_t<_Tp>,
   unsigned char, unsigned short, unsigned int, unsigned long,
   unsigned long long
# 656 "/usr/include/c++/11/type_traits" 3
   >;


  template<typename _Tp>
    using __is_standard_integer
      = __or_<__is_signed_integer<_Tp>, __is_unsigned_integer<_Tp>>;


  template<typename...> using __void_t = void;



  template<typename _Tp, typename = void>
    struct __is_referenceable
    : public false_type
    { };

  template<typename _Tp>
    struct __is_referenceable<_Tp, __void_t<_Tp&>>
    : public true_type
    { };





  template<typename>
    struct is_const
    : public false_type { };

  template<typename _Tp>
    struct is_const<_Tp const>
    : public true_type { };


  template<typename>
    struct is_volatile
    : public false_type { };

  template<typename _Tp>
    struct is_volatile<_Tp volatile>
    : public true_type { };


  template<typename _Tp>
    struct is_trivial
    : public integral_constant<bool, __is_trivial(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_copyable
    : public integral_constant<bool, __is_trivially_copyable(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_standard_layout
    : public integral_constant<bool, __is_standard_layout(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  template<typename _Tp>
    struct

    is_pod
    : public integral_constant<bool, __is_pod(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };




  template<typename _Tp>
    struct
    [[__deprecated__]]
    is_literal_type
    : public integral_constant<bool, __is_literal_type(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_empty
    : public integral_constant<bool, __is_empty(_Tp)>
    { };


  template<typename _Tp>
    struct is_polymorphic
    : public integral_constant<bool, __is_polymorphic(_Tp)>
    { };





  template<typename _Tp>
    struct is_final
    : public integral_constant<bool, __is_final(_Tp)>
    { };



  template<typename _Tp>
    struct is_abstract
    : public integral_constant<bool, __is_abstract(_Tp)>
    { };


  template<typename _Tp,
    bool = is_arithmetic<_Tp>::value>
    struct __is_signed_helper
    : public false_type { };

  template<typename _Tp>
    struct __is_signed_helper<_Tp, true>
    : public integral_constant<bool, _Tp(-1) < _Tp(0)>
    { };



  template<typename _Tp>
    struct is_signed
    : public __is_signed_helper<_Tp>::type
    { };


  template<typename _Tp>
    struct is_unsigned
    : public __and_<is_arithmetic<_Tp>, __not_<is_signed<_Tp>>>
    { };


  template<typename _Tp, typename _Up = _Tp&&>
    _Up
    __declval(int);

  template<typename _Tp>
    _Tp
    __declval(long);


  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0));

  template<typename, unsigned = 0>
    struct extent;

  template<typename>
    struct remove_all_extents;


  template<typename _Tp>
    struct __is_array_known_bounds
    : public integral_constant<bool, (extent<_Tp>::value > 0)>
    { };

  template<typename _Tp>
    struct __is_array_unknown_bounds
    : public __and_<is_array<_Tp>, __not_<extent<_Tp>>>
    { };
# 842 "/usr/include/c++/11/type_traits" 3
  struct __do_is_destructible_impl
  {
    template<typename _Tp, typename = decltype(declval<_Tp&>().~_Tp())>
      static true_type __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_destructible_impl
    : public __do_is_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_destructible_safe;

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, false>
    : public __is_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_destructible_safe<_Tp, false, true>
    : public true_type { };



  template<typename _Tp>
    struct is_destructible
    : public __is_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };







  struct __do_is_nt_destructible_impl
  {
    template<typename _Tp>
      static __bool_constant<noexcept(declval<_Tp&>().~_Tp())>
      __test(int);

    template<typename>
      static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_nt_destructible_impl
    : public __do_is_nt_destructible_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp,
           bool = __or_<is_void<_Tp>,
                        __is_array_unknown_bounds<_Tp>,
                        is_function<_Tp>>::value,
           bool = __or_<is_reference<_Tp>, is_scalar<_Tp>>::value>
    struct __is_nt_destructible_safe;

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, false>
    : public __is_nt_destructible_impl<typename
               remove_all_extents<_Tp>::type>::type
    { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, true, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_destructible_safe<_Tp, false, true>
    : public true_type { };



  template<typename _Tp>
    struct is_nothrow_destructible
    : public __is_nt_destructible_safe<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    struct __is_constructible_impl
    : public __bool_constant<__is_constructible(_Tp, _Args...)>
    { };



  template<typename _Tp, typename... _Args>
    struct is_constructible
      : public __is_constructible_impl<_Tp, _Args...>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_default_constructible
    : public __is_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_constructible_impl;

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, const _Tp&>
    { };



  template<typename _Tp>
    struct is_copy_constructible
    : public __is_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_constructible_impl;

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_constructible_impl<_Tp, true>
    : public __is_constructible_impl<_Tp, _Tp&&>
    { };



  template<typename _Tp>
    struct is_move_constructible
    : public __is_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    using __is_nothrow_constructible_impl
      = __bool_constant<__is_nothrow_constructible(_Tp, _Args...)>;



  template<typename _Tp, typename... _Args>
    struct is_nothrow_constructible
    : public __is_nothrow_constructible_impl<_Tp, _Args...>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_default_constructible
    : public __bool_constant<__is_nothrow_constructible(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_copy_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_copy_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, const _Tp&>
    { };



  template<typename _Tp>
    struct is_nothrow_copy_constructible
    : public __is_nothrow_copy_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nothrow_move_constructible_impl;

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nothrow_move_constructible_impl<_Tp, true>
    : public __is_nothrow_constructible_impl<_Tp, _Tp&&>
    { };



  template<typename _Tp>
    struct is_nothrow_move_constructible
    : public __is_nothrow_move_constructible_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_assignable
    : public __bool_constant<__is_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_copy_assignable_impl;

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_copy_assignable
    : public __is_copy_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_move_assignable_impl;

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_move_assignable
    : public __is_move_assignable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, typename _Up>
    using __is_nothrow_assignable_impl
      = __bool_constant<__is_nothrow_assignable(_Tp, _Up)>;


  template<typename _Tp, typename _Up>
    struct is_nothrow_assignable
    : public __is_nothrow_assignable_impl<_Tp, _Up>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_copy_assignable_impl;

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_copy_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, const _Tp&>
    { };


  template<typename _Tp>
    struct is_nothrow_copy_assignable
    : public __is_nt_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_nt_move_assignable_impl;

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_nt_move_assignable_impl<_Tp, true>
    : public __is_nothrow_assignable_impl<_Tp&, _Tp&&>
    { };


  template<typename _Tp>
    struct is_nothrow_move_assignable
    : public __is_nt_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename... _Args>
    struct is_trivially_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp, _Args...)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_default_constructible
    : public __bool_constant<__is_trivially_constructible(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  struct __do_is_implicitly_default_constructible_impl
  {
    template <typename _Tp>
    static void __helper(const _Tp&);

    template <typename _Tp>
    static true_type __test(const _Tp&,
                            decltype(__helper<const _Tp&>({}))* = 0);

    static false_type __test(...);
  };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_impl
    : public __do_is_implicitly_default_constructible_impl
    {
      typedef decltype(__test(declval<_Tp>())) type;
    };

  template<typename _Tp>
    struct __is_implicitly_default_constructible_safe
    : public __is_implicitly_default_constructible_impl<_Tp>::type
    { };

  template <typename _Tp>
    struct __is_implicitly_default_constructible
    : public __and_<__is_constructible_impl<_Tp>,
      __is_implicitly_default_constructible_safe<_Tp>>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_constructible_impl<_Tp, true>
    : public __and_<__is_copy_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, const _Tp&)>>
    { };


  template<typename _Tp>
    struct is_trivially_copy_constructible
    : public __is_trivially_copy_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_constructible_impl;

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_constructible_impl<_Tp, true>
    : public __and_<__is_move_constructible_impl<_Tp>,
      integral_constant<bool,
   __is_trivially_constructible(_Tp, _Tp&&)>>
    { };


  template<typename _Tp>
    struct is_trivially_move_constructible
    : public __is_trivially_move_constructible_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_trivially_assignable
    : public __bool_constant<__is_trivially_assignable(_Tp, _Up)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_copy_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_copy_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, const _Tp&)>
    { };


  template<typename _Tp>
    struct is_trivially_copy_assignable
    : public __is_trivially_copy_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __is_trivially_move_assignable_impl;

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, false>
    : public false_type { };

  template<typename _Tp>
    struct __is_trivially_move_assignable_impl<_Tp, true>
    : public __bool_constant<__is_trivially_assignable(_Tp&, _Tp&&)>
    { };


  template<typename _Tp>
    struct is_trivially_move_assignable
    : public __is_trivially_move_assignable_impl<_Tp>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_trivially_destructible
    : public __and_<__is_destructible_safe<_Tp>,
      __bool_constant<__has_trivial_destructor(_Tp)>>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    struct has_virtual_destructor
    : public integral_constant<bool, __has_virtual_destructor(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };





  template<typename _Tp>
    struct alignment_of
    : public integral_constant<std::size_t, alignof(_Tp)>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename>
    struct rank
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, std::size_t _Size>
    struct rank<_Tp[_Size]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };

  template<typename _Tp>
    struct rank<_Tp[]>
    : public integral_constant<std::size_t, 1 + rank<_Tp>::value> { };


  template<typename, unsigned _Uint>
    struct extent
    : public integral_constant<std::size_t, 0> { };

  template<typename _Tp, unsigned _Uint, std::size_t _Size>
    struct extent<_Tp[_Size], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? _Size : extent<_Tp,
          _Uint - 1>::value>
    { };

  template<typename _Tp, unsigned _Uint>
    struct extent<_Tp[], _Uint>
    : public integral_constant<std::size_t,
          _Uint == 0 ? 0 : extent<_Tp,
             _Uint - 1>::value>
    { };





  template<typename _Tp, typename _Up>
    struct is_same

    : public integral_constant<bool, __is_same(_Tp, _Up)>



    { };
# 1420 "/usr/include/c++/11/type_traits" 3
  template<typename _Base, typename _Derived>
    struct is_base_of
    : public integral_constant<bool, __is_base_of(_Base, _Derived)>
    { };

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_convertible_helper
    {
      typedef typename is_void<_To>::type type;
    };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1,
        typename = decltype(__test_aux<_To1>(std::declval<_From1>()))>
 static true_type
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      typedef decltype(__test<_From, _To>(0)) type;
    };
#pragma GCC diagnostic pop


  template<typename _From, typename _To>
    struct is_convertible
    : public __is_convertible_helper<_From, _To>::type
    { };


  template<typename _ToElementType, typename _FromElementType>
    using __is_array_convertible
      = is_convertible<_FromElementType(*)[], _ToElementType(*)[]>;

  template<typename _From, typename _To,
           bool = __or_<is_void<_From>, is_function<_To>,
                        is_array<_To>>::value>
    struct __is_nt_convertible_helper
    : is_void<_To>
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  template<typename _From, typename _To>
    class __is_nt_convertible_helper<_From, _To, false>
    {
      template<typename _To1>
 static void __test_aux(_To1) noexcept;

      template<typename _From1, typename _To1>
 static
 __bool_constant<noexcept(__test_aux<_To1>(std::declval<_From1>()))>
 __test(int);

      template<typename, typename>
 static false_type
 __test(...);

    public:
      using type = decltype(__test<_From, _To>(0));
    };
#pragma GCC diagnostic pop
# 1512 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp>
    struct remove_const
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_const<_Tp const>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_volatile
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_volatile<_Tp volatile>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_cv
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<volatile _Tp>
    { using type = _Tp; };

  template<typename _Tp>
    struct remove_cv<const volatile _Tp>
    { using type = _Tp; };


  template<typename _Tp>
    struct add_const
    { typedef _Tp const type; };


  template<typename _Tp>
    struct add_volatile
    { typedef _Tp volatile type; };


  template<typename _Tp>
    struct add_cv
    {
      typedef typename
      add_const<typename add_volatile<_Tp>::type>::type type;
    };






  template<typename _Tp>
    using remove_const_t = typename remove_const<_Tp>::type;


  template<typename _Tp>
    using remove_volatile_t = typename remove_volatile<_Tp>::type;


  template<typename _Tp>
    using remove_cv_t = typename remove_cv<_Tp>::type;


  template<typename _Tp>
    using add_const_t = typename add_const<_Tp>::type;


  template<typename _Tp>
    using add_volatile_t = typename add_volatile<_Tp>::type;


  template<typename _Tp>
    using add_cv_t = typename add_cv<_Tp>::type;





  template<typename _Tp>
    struct remove_reference
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_reference<_Tp&&>
    { typedef _Tp type; };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_lvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_lvalue_reference_helper<_Tp, true>
    { typedef _Tp& type; };


  template<typename _Tp>
    struct add_lvalue_reference
    : public __add_lvalue_reference_helper<_Tp>
    { };

  template<typename _Tp, bool = __is_referenceable<_Tp>::value>
    struct __add_rvalue_reference_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_rvalue_reference_helper<_Tp, true>
    { typedef _Tp&& type; };


  template<typename _Tp>
    struct add_rvalue_reference
    : public __add_rvalue_reference_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_reference_t = typename remove_reference<_Tp>::type;


  template<typename _Tp>
    using add_lvalue_reference_t = typename add_lvalue_reference<_Tp>::type;


  template<typename _Tp>
    using add_rvalue_reference_t = typename add_rvalue_reference<_Tp>::type;







  template<typename _Unqualified, bool _IsConst, bool _IsVol>
    struct __cv_selector;

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, false>
    { typedef _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, false, true>
    { typedef volatile _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, false>
    { typedef const _Unqualified __type; };

  template<typename _Unqualified>
    struct __cv_selector<_Unqualified, true, true>
    { typedef const volatile _Unqualified __type; };

  template<typename _Qualified, typename _Unqualified,
    bool _IsConst = is_const<_Qualified>::value,
    bool _IsVol = is_volatile<_Qualified>::value>
    class __match_cv_qualifiers
    {
      typedef __cv_selector<_Unqualified, _IsConst, _IsVol> __match;

    public:
      typedef typename __match::__type __type;
    };


  template<typename _Tp>
    struct __make_unsigned
    { typedef _Tp __type; };

  template<>
    struct __make_unsigned<char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<signed char>
    { typedef unsigned char __type; };

  template<>
    struct __make_unsigned<short>
    { typedef unsigned short __type; };

  template<>
    struct __make_unsigned<int>
    { typedef unsigned int __type; };

  template<>
    struct __make_unsigned<long>
    { typedef unsigned long __type; };

  template<>
    struct __make_unsigned<long long>
    { typedef unsigned long long __type; };
# 1736 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_unsigned_selector;

  template<typename _Tp>
    class __make_unsigned_selector<_Tp, true, false>
    {
      using __unsigned_type
 = typename __make_unsigned<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };

  class __make_unsigned_selector_base
  {
  protected:
    template<typename...> struct _List { };

    template<typename _Tp, typename... _Up>
      struct _List<_Tp, _Up...> : _List<_Up...>
      { static constexpr size_t __size = sizeof(_Tp); };

    template<size_t _Sz, typename _Tp, bool = (_Sz <= _Tp::__size)>
      struct __select;

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, true>
      { using __type = _Uint; };

    template<size_t _Sz, typename _Uint, typename... _UInts>
      struct __select<_Sz, _List<_Uint, _UInts...>, false>
      : __select<_Sz, _List<_UInts...>>
      { };
  };


  template<typename _Tp>
    class __make_unsigned_selector<_Tp, false, true>
    : __make_unsigned_selector_base
    {

      using _UInts = _List<unsigned char, unsigned short, unsigned int,
      unsigned long, unsigned long long>;

      using __unsigned_type = typename __select<sizeof(_Tp), _UInts>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __unsigned_type>::__type;
    };






  template<>
    struct __make_unsigned<wchar_t>
    {
      using __type
 = typename __make_unsigned_selector<wchar_t, false, true>::__type;
    };
# 1812 "/usr/include/c++/11/type_traits" 3
  template<>
    struct __make_unsigned<char16_t>
    {
      using __type
 = typename __make_unsigned_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_unsigned<char32_t>
    {
      using __type
 = typename __make_unsigned_selector<char32_t, false, true>::__type;
    };






  template<typename _Tp>
    struct make_unsigned
    { typedef typename __make_unsigned_selector<_Tp>::__type type; };


  template<>
    struct make_unsigned<bool>;




  template<typename _Tp>
    struct __make_signed
    { typedef _Tp __type; };

  template<>
    struct __make_signed<char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned char>
    { typedef signed char __type; };

  template<>
    struct __make_signed<unsigned short>
    { typedef signed short __type; };

  template<>
    struct __make_signed<unsigned int>
    { typedef signed int __type; };

  template<>
    struct __make_signed<unsigned long>
    { typedef signed long __type; };

  template<>
    struct __make_signed<unsigned long long>
    { typedef signed long long __type; };
# 1892 "/usr/include/c++/11/type_traits" 3
  template<typename _Tp,
    bool _IsInt = is_integral<_Tp>::value,
    bool _IsEnum = is_enum<_Tp>::value>
    class __make_signed_selector;

  template<typename _Tp>
    class __make_signed_selector<_Tp, true, false>
    {
      using __signed_type
 = typename __make_signed<__remove_cv_t<_Tp>>::__type;

    public:
      using __type
 = typename __match_cv_qualifiers<_Tp, __signed_type>::__type;
    };


  template<typename _Tp>
    class __make_signed_selector<_Tp, false, true>
    {
      typedef typename __make_unsigned_selector<_Tp>::__type __unsigned_type;

    public:
      typedef typename __make_signed_selector<__unsigned_type>::__type __type;
    };






  template<>
    struct __make_signed<wchar_t>
    {
      using __type
 = typename __make_signed_selector<wchar_t, false, true>::__type;
    };
# 1940 "/usr/include/c++/11/type_traits" 3
  template<>
    struct __make_signed<char16_t>
    {
      using __type
 = typename __make_signed_selector<char16_t, false, true>::__type;
    };

  template<>
    struct __make_signed<char32_t>
    {
      using __type
 = typename __make_signed_selector<char32_t, false, true>::__type;
    };






  template<typename _Tp>
    struct make_signed
    { typedef typename __make_signed_selector<_Tp>::__type type; };


  template<>
    struct make_signed<bool>;



  template<typename _Tp>
    using make_signed_t = typename make_signed<_Tp>::type;


  template<typename _Tp>
    using make_unsigned_t = typename make_unsigned<_Tp>::type;





  template<typename _Tp>
    struct remove_extent
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_extent<_Tp[_Size]>
    { typedef _Tp type; };

  template<typename _Tp>
    struct remove_extent<_Tp[]>
    { typedef _Tp type; };


  template<typename _Tp>
    struct remove_all_extents
    { typedef _Tp type; };

  template<typename _Tp, std::size_t _Size>
    struct remove_all_extents<_Tp[_Size]>
    { typedef typename remove_all_extents<_Tp>::type type; };

  template<typename _Tp>
    struct remove_all_extents<_Tp[]>
    { typedef typename remove_all_extents<_Tp>::type type; };



  template<typename _Tp>
    using remove_extent_t = typename remove_extent<_Tp>::type;


  template<typename _Tp>
    using remove_all_extents_t = typename remove_all_extents<_Tp>::type;




  template<typename _Tp, typename>
    struct __remove_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp, typename _Up>
    struct __remove_pointer_helper<_Tp, _Up*>
    { typedef _Up type; };


  template<typename _Tp>
    struct remove_pointer
    : public __remove_pointer_helper<_Tp, __remove_cv_t<_Tp>>
    { };

  template<typename _Tp, bool = __or_<__is_referenceable<_Tp>,
          is_void<_Tp>>::value>
    struct __add_pointer_helper
    { typedef _Tp type; };

  template<typename _Tp>
    struct __add_pointer_helper<_Tp, true>
    { typedef typename remove_reference<_Tp>::type* type; };


  template<typename _Tp>
    struct add_pointer
    : public __add_pointer_helper<_Tp>
    { };



  template<typename _Tp>
    using remove_pointer_t = typename remove_pointer<_Tp>::type;


  template<typename _Tp>
    using add_pointer_t = typename add_pointer<_Tp>::type;


  template<std::size_t _Len>
    struct __aligned_storage_msa
    {
      union __type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__)) { } __align;
      };
    };
# 2076 "/usr/include/c++/11/type_traits" 3
  template<std::size_t _Len, std::size_t _Align =
    __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    struct aligned_storage
    {
      union type
      {
 unsigned char __data[_Len];
 struct __attribute__((__aligned__((_Align)))) { } __align;
      };
    };

  template <typename... _Types>
    struct __strictest_alignment
    {
      static const size_t _S_alignment = 0;
      static const size_t _S_size = 0;
    };

  template <typename _Tp, typename... _Types>
    struct __strictest_alignment<_Tp, _Types...>
    {
      static const size_t _S_alignment =
        alignof(_Tp) > __strictest_alignment<_Types...>::_S_alignment
 ? alignof(_Tp) : __strictest_alignment<_Types...>::_S_alignment;
      static const size_t _S_size =
        sizeof(_Tp) > __strictest_alignment<_Types...>::_S_size
 ? sizeof(_Tp) : __strictest_alignment<_Types...>::_S_size;
    };
# 2115 "/usr/include/c++/11/type_traits" 3
  template <size_t _Len, typename... _Types>
    struct aligned_union
    {
    private:
      static_assert(sizeof...(_Types) != 0, "At least one type is required");

      using __strictest = __strictest_alignment<_Types...>;
      static const size_t _S_len = _Len > __strictest::_S_size
 ? _Len : __strictest::_S_size;
    public:

      static const size_t alignment_value = __strictest::_S_alignment;

      typedef typename aligned_storage<_S_len, alignment_value>::type type;
    };

  template <size_t _Len, typename... _Types>
    const size_t aligned_union<_Len, _Types...>::alignment_value;





  template<typename _Up,
    bool _IsArray = is_array<_Up>::value,
    bool _IsFunction = is_function<_Up>::value>
    struct __decay_selector;


  template<typename _Up>
    struct __decay_selector<_Up, false, false>
    { typedef __remove_cv_t<_Up> __type; };

  template<typename _Up>
    struct __decay_selector<_Up, true, false>
    { typedef typename remove_extent<_Up>::type* __type; };

  template<typename _Up>
    struct __decay_selector<_Up, false, true>
    { typedef typename add_pointer<_Up>::type __type; };



  template<typename _Tp>
    class decay
    {
      typedef typename remove_reference<_Tp>::type __remove_type;

    public:
      typedef typename __decay_selector<__remove_type>::__type type;
    };




  template<typename _Tp>
    struct __strip_reference_wrapper
    {
      typedef _Tp __type;
    };

  template<typename _Tp>
    struct __strip_reference_wrapper<reference_wrapper<_Tp> >
    {
      typedef _Tp& __type;
    };


  template<typename _Tp>
    using __decay_t = typename decay<_Tp>::type;

  template<typename _Tp>
    using __decay_and_strip = __strip_reference_wrapper<__decay_t<_Tp>>;




  template<bool, typename _Tp = void>
    struct enable_if
    { };


  template<typename _Tp>
    struct enable_if<true, _Tp>
    { typedef _Tp type; };




  template<bool _Cond, typename _Tp = void>
    using __enable_if_t = typename enable_if<_Cond, _Tp>::type;


  template<typename... _Cond>
    using _Require = __enable_if_t<__and_<_Cond...>::value>;


  template<typename _Tp>
    using __remove_cvref_t
     = typename remove_cv<typename remove_reference<_Tp>::type>::type;




  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    struct conditional
    { typedef _Iftrue type; };


  template<typename _Iftrue, typename _Iffalse>
    struct conditional<false, _Iftrue, _Iffalse>
    { typedef _Iffalse type; };


  template<typename... _Tp>
    struct common_type;




  struct __do_common_type_impl
  {
    template<typename _Tp, typename _Up>
      using __cond_t
 = decltype(true ? std::declval<_Tp>() : std::declval<_Up>());



    template<typename _Tp, typename _Up>
      static __success_type<__decay_t<__cond_t<_Tp, _Up>>>
      _S_test(int);
# 2255 "/usr/include/c++/11/type_traits" 3
    template<typename, typename>
      static __failure_type
      _S_test_2(...);

    template<typename _Tp, typename _Up>
      static decltype(_S_test_2<_Tp, _Up>(0))
      _S_test(...);
  };


  template<>
    struct common_type<>
    { };


  template<typename _Tp0>
    struct common_type<_Tp0>
    : public common_type<_Tp0, _Tp0>
    { };


  template<typename _Tp1, typename _Tp2,
    typename _Dp1 = __decay_t<_Tp1>, typename _Dp2 = __decay_t<_Tp2>>
    struct __common_type_impl
    {


      using type = common_type<_Dp1, _Dp2>;
    };

  template<typename _Tp1, typename _Tp2>
    struct __common_type_impl<_Tp1, _Tp2, _Tp1, _Tp2>
    : private __do_common_type_impl
    {


      using type = decltype(_S_test<_Tp1, _Tp2>(0));
    };


  template<typename _Tp1, typename _Tp2>
    struct common_type<_Tp1, _Tp2>
    : public __common_type_impl<_Tp1, _Tp2>::type
    { };

  template<typename...>
    struct __common_type_pack
    { };

  template<typename, typename, typename = void>
    struct __common_type_fold;


  template<typename _Tp1, typename _Tp2, typename... _Rp>
    struct common_type<_Tp1, _Tp2, _Rp...>
    : public __common_type_fold<common_type<_Tp1, _Tp2>,
    __common_type_pack<_Rp...>>
    { };




  template<typename _CTp, typename... _Rp>
    struct __common_type_fold<_CTp, __common_type_pack<_Rp...>,
         __void_t<typename _CTp::type>>
    : public common_type<typename _CTp::type, _Rp...>
    { };


  template<typename _CTp, typename _Rp>
    struct __common_type_fold<_CTp, _Rp, void>
    { };

  template<typename _Tp, bool = is_enum<_Tp>::value>
    struct __underlying_type_impl
    {
      using type = __underlying_type(_Tp);
    };

  template<typename _Tp>
    struct __underlying_type_impl<_Tp, false>
    { };



  template<typename _Tp>
    struct underlying_type
    : public __underlying_type_impl<_Tp>
    { };


  template<typename _Tp>
    struct __declval_protector
    {
      static const bool __stop = false;
    };






  template<typename _Tp>
    auto declval() noexcept -> decltype(__declval<_Tp>(0))
    {
      static_assert(__declval_protector<_Tp>::__stop,
      "declval() must not be used!");
      return __declval<_Tp>(0);
    }


  template<typename _Signature>
    struct result_of;






  struct __invoke_memfun_ref { };
  struct __invoke_memfun_deref { };
  struct __invoke_memobj_ref { };
  struct __invoke_memobj_deref { };
  struct __invoke_other { };


  template<typename _Tp, typename _Tag>
    struct __result_of_success : __success_type<_Tp>
    { using __invoke_type = _Tag; };


  struct __result_of_memfun_ref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      (std::declval<_Tp1>().*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_ref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_ref
    : private __result_of_memfun_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memfun_deref_impl
  {
    template<typename _Fp, typename _Tp1, typename... _Args>
      static __result_of_success<decltype(
      ((*std::declval<_Tp1>()).*std::declval<_Fp>())(std::declval<_Args>()...)
      ), __invoke_memfun_deref> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun_deref
    : private __result_of_memfun_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg, _Args...>(0)) type;
    };


  struct __result_of_memobj_ref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      std::declval<_Tp1>().*std::declval<_Fp>()
      ), __invoke_memobj_ref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_ref
    : private __result_of_memobj_ref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };


  struct __result_of_memobj_deref_impl
  {
    template<typename _Fp, typename _Tp1>
      static __result_of_success<decltype(
      (*std::declval<_Tp1>()).*std::declval<_Fp>()
      ), __invoke_memobj_deref> _S_test(int);

    template<typename, typename>
      static __failure_type _S_test(...);
  };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj_deref
    : private __result_of_memobj_deref_impl
    {
      typedef decltype(_S_test<_MemPtr, _Arg>(0)) type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_memobj;

  template<typename _Res, typename _Class, typename _Arg>
    struct __result_of_memobj<_Res _Class::*, _Arg>
    {
      typedef __remove_cvref_t<_Arg> _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<__or_<is_same<_Argval, _Class>,
        is_base_of<_Class, _Argval>>::value,
        __result_of_memobj_ref<_MemPtr, _Arg>,
        __result_of_memobj_deref<_MemPtr, _Arg>
      >::type::type type;
    };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_memfun;

  template<typename _Res, typename _Class, typename _Arg, typename... _Args>
    struct __result_of_memfun<_Res _Class::*, _Arg, _Args...>
    {
      typedef typename remove_reference<_Arg>::type _Argval;
      typedef _Res _Class::* _MemPtr;
      typedef typename conditional<is_base_of<_Class, _Argval>::value,
        __result_of_memfun_ref<_MemPtr, _Arg, _Args...>,
        __result_of_memfun_deref<_MemPtr, _Arg, _Args...>
      >::type::type type;
    };






  template<typename _Tp, typename _Up = __remove_cvref_t<_Tp>>
    struct __inv_unwrap
    {
      using type = _Tp;
    };

  template<typename _Tp, typename _Up>
    struct __inv_unwrap<_Tp, reference_wrapper<_Up>>
    {
      using type = _Up&;
    };

  template<bool, bool, typename _Functor, typename... _ArgTypes>
    struct __result_of_impl
    {
      typedef __failure_type type;
    };

  template<typename _MemPtr, typename _Arg>
    struct __result_of_impl<true, false, _MemPtr, _Arg>
    : public __result_of_memobj<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type>
    { };

  template<typename _MemPtr, typename _Arg, typename... _Args>
    struct __result_of_impl<false, true, _MemPtr, _Arg, _Args...>
    : public __result_of_memfun<__decay_t<_MemPtr>,
    typename __inv_unwrap<_Arg>::type, _Args...>
    { };


  struct __result_of_other_impl
  {
    template<typename _Fn, typename... _Args>
      static __result_of_success<decltype(
      std::declval<_Fn>()(std::declval<_Args>()...)
      ), __invoke_other> _S_test(int);

    template<typename...>
      static __failure_type _S_test(...);
  };

  template<typename _Functor, typename... _ArgTypes>
    struct __result_of_impl<false, false, _Functor, _ArgTypes...>
    : private __result_of_other_impl
    {
      typedef decltype(_S_test<_Functor, _ArgTypes...>(0)) type;
    };


  template<typename _Functor, typename... _ArgTypes>
    struct __invoke_result
    : public __result_of_impl<
        is_member_object_pointer<
          typename remove_reference<_Functor>::type
        >::value,
        is_member_function_pointer<
          typename remove_reference<_Functor>::type
        >::value,
 _Functor, _ArgTypes...
      >::type
    { };


  template<typename _Functor, typename... _ArgTypes>
    struct result_of<_Functor(_ArgTypes...)>
    : public __invoke_result<_Functor, _ArgTypes...>
    { };



  template<size_t _Len, size_t _Align =
     __alignof__(typename __aligned_storage_msa<_Len>::__type)>
    using aligned_storage_t = typename aligned_storage<_Len, _Align>::type;

  template <size_t _Len, typename... _Types>
    using aligned_union_t = typename aligned_union<_Len, _Types...>::type;


  template<typename _Tp>
    using decay_t = typename decay<_Tp>::type;


  template<bool _Cond, typename _Tp = void>
    using enable_if_t = typename enable_if<_Cond, _Tp>::type;


  template<bool _Cond, typename _Iftrue, typename _Iffalse>
    using conditional_t = typename conditional<_Cond, _Iftrue, _Iffalse>::type;


  template<typename... _Tp>
    using common_type_t = typename common_type<_Tp...>::type;


  template<typename _Tp>
    using underlying_type_t = typename underlying_type<_Tp>::type;


  template<typename _Tp>
    using result_of_t = typename result_of<_Tp>::type;





  template<typename...> using void_t = void;





  template<typename _Default, typename _AlwaysVoid,
    template<typename...> class _Op, typename... _Args>
    struct __detector
    {
      using value_t = false_type;
      using type = _Default;
    };


  template<typename _Default, template<typename...> class _Op,
     typename... _Args>
    struct __detector<_Default, __void_t<_Op<_Args...>>, _Op, _Args...>
    {
      using value_t = true_type;
      using type = _Op<_Args...>;
    };


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or = __detector<_Default, void, _Op, _Args...>;


  template<typename _Default, template<typename...> class _Op,
    typename... _Args>
    using __detected_or_t
      = typename __detected_or<_Default, _Op, _Args...>::type;
# 2649 "/usr/include/c++/11/type_traits" 3
  template <typename _Tp>
    struct __is_swappable;

  template <typename _Tp>
    struct __is_nothrow_swappable;

  template<typename>
    struct __is_tuple_like_impl : false_type
    { };

  template<typename... _Tps>
    struct __is_tuple_like_impl<tuple<_Tps...>> : true_type
    { };


  template<typename _Tp>
    struct __is_tuple_like
    : public __is_tuple_like_impl<__remove_cvref_t<_Tp>>::type
    { };


  template<typename _Tp>

    inline
    _Require<__not_<__is_tuple_like<_Tp>>,
      is_move_constructible<_Tp>,
      is_move_assignable<_Tp>>
    swap(_Tp&, _Tp&)
    noexcept(__and_<is_nothrow_move_constructible<_Tp>,
             is_nothrow_move_assignable<_Tp>>::value);

  template<typename _Tp, size_t _Nm>

    inline
    __enable_if_t<__is_swappable<_Tp>::value>
    swap(_Tp (&__a)[_Nm], _Tp (&__b)[_Nm])
    noexcept(__is_nothrow_swappable<_Tp>::value);


  namespace __swappable_details {
    using std::swap;

    struct __do_is_swappable_impl
    {
      template<typename _Tp, typename
               = decltype(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))>
        static true_type __test(int);

      template<typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_impl
    {
      template<typename _Tp>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp&>(), std::declval<_Tp&>()))
        > __test(int);

      template<typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp>
    struct __is_swappable_impl
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_nothrow_swappable_impl
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp>(0)) type;
    };

  template<typename _Tp>
    struct __is_swappable
    : public __is_swappable_impl<_Tp>::type
    { };

  template<typename _Tp>
    struct __is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    { };







  template<typename _Tp>
    struct is_swappable
    : public __is_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    struct is_nothrow_swappable
    : public __is_nothrow_swappable_impl<_Tp>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp>
    inline constexpr bool is_swappable_v =
      is_swappable<_Tp>::value;


  template<typename _Tp>
    inline constexpr bool is_nothrow_swappable_v =
      is_nothrow_swappable<_Tp>::value;



  namespace __swappable_with_details {
    using std::swap;

    struct __do_is_swappable_with_impl
    {
      template<typename _Tp, typename _Up, typename
               = decltype(swap(std::declval<_Tp>(), std::declval<_Up>())),
               typename
               = decltype(swap(std::declval<_Up>(), std::declval<_Tp>()))>
        static true_type __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

    struct __do_is_nothrow_swappable_with_impl
    {
      template<typename _Tp, typename _Up>
        static __bool_constant<
          noexcept(swap(std::declval<_Tp>(), std::declval<_Up>()))
          &&
          noexcept(swap(std::declval<_Up>(), std::declval<_Tp>()))
        > __test(int);

      template<typename, typename>
        static false_type __test(...);
    };

  }

  template<typename _Tp, typename _Up>
    struct __is_swappable_with_impl
    : public __swappable_with_details::__do_is_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };

  template<typename _Tp, typename _Up>
    struct __is_nothrow_swappable_with_impl
    : public __swappable_with_details::__do_is_nothrow_swappable_with_impl
    {
      typedef decltype(__test<_Tp, _Up>(0)) type;
    };


  template<typename _Tp>
    struct __is_nothrow_swappable_with_impl<_Tp&, _Tp&>
    : public __swappable_details::__do_is_nothrow_swappable_impl
    {
      typedef decltype(__test<_Tp&>(0)) type;
    };



  template<typename _Tp, typename _Up>
    struct is_swappable_with
    : public __is_swappable_with_impl<_Tp, _Up>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "first template argument must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Up>{}),
 "second template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp, typename _Up>
    struct is_nothrow_swappable_with
    : public __is_nothrow_swappable_with_impl<_Tp, _Up>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "first template argument must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Up>{}),
 "second template argument must be a complete class or an unbounded array");
    };



  template<typename _Tp, typename _Up>
    inline constexpr bool is_swappable_with_v =
      is_swappable_with<_Tp, _Up>::value;


  template<typename _Tp, typename _Up>
    inline constexpr bool is_nothrow_swappable_with_v =
      is_nothrow_swappable_with<_Tp, _Up>::value;
# 2876 "/usr/include/c++/11/type_traits" 3
  template<typename _Result, typename _Ret,
    bool = is_void<_Ret>::value, typename = void>
    struct __is_invocable_impl
    : false_type
    {
      using __nothrow_type = false_type;
    };


  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                true,
          __void_t<typename _Result::type>>
    : true_type
    {
      using __nothrow_type = true_type;
    };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"

  template<typename _Result, typename _Ret>
    struct __is_invocable_impl<_Result, _Ret,
                                false,
          __void_t<typename _Result::type>>
    {
    private:



      static typename _Result::type _S_get() noexcept;

      template<typename _Tp>
 static void _S_conv(_Tp) noexcept;


      template<typename _Tp, bool _Check_Noex = false,
        typename = decltype(_S_conv<_Tp>(_S_get())),
        bool _Noex = noexcept(_S_conv<_Tp>(_S_get()))>
 static __bool_constant<_Check_Noex ? _Noex : true>
 _S_test(int);

      template<typename _Tp, bool = false>
 static false_type
 _S_test(...);

    public:

      using type = decltype(_S_test<_Ret>(1));


      using __nothrow_type = decltype(_S_test<_Ret, true>(1));
    };
#pragma GCC diagnostic pop

  template<typename _Fn, typename... _ArgTypes>
    struct __is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    { };

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept((std::declval<_Up>().*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp, typename... _Args>
    constexpr bool __call_is_nt(__invoke_memfun_deref)
    {
      return noexcept(((*std::declval<_Tp>()).*std::declval<_Fn>())(
     std::declval<_Args>()...));
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_ref)
    {
      using _Up = typename __inv_unwrap<_Tp>::type;
      return noexcept(std::declval<_Up>().*std::declval<_Fn>());
    }

  template<typename _Fn, typename _Tp>
    constexpr bool __call_is_nt(__invoke_memobj_deref)
    {
      return noexcept((*std::declval<_Tp>()).*std::declval<_Fn>());
    }

  template<typename _Fn, typename... _Args>
    constexpr bool __call_is_nt(__invoke_other)
    {
      return noexcept(std::declval<_Fn>()(std::declval<_Args>()...));
    }

  template<typename _Result, typename _Fn, typename... _Args>
    struct __call_is_nothrow
    : __bool_constant<
 std::__call_is_nt<_Fn, _Args...>(typename _Result::__invoke_type{})
      >
    { };

  template<typename _Fn, typename... _Args>
    using __call_is_nothrow_
      = __call_is_nothrow<__invoke_result<_Fn, _Args...>, _Fn, _Args...>;


  template<typename _Fn, typename... _Args>
    struct __is_nothrow_invocable
    : __and_<__is_invocable<_Fn, _Args...>,
             __call_is_nothrow_<_Fn, _Args...>>::type
    { };

#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wctor-dtor-privacy"
  struct __nonesuchbase {};
  struct __nonesuch : private __nonesuchbase {
    ~__nonesuch() = delete;
    __nonesuch(__nonesuch const&) = delete;
    void operator=(__nonesuch const&) = delete;
  };
#pragma GCC diagnostic pop






  template<typename _Functor, typename... _ArgTypes>
    struct invoke_result
    : public __invoke_result<_Functor, _ArgTypes...>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Functor>{}),
 "_Functor must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Fn, typename... _Args>
    using invoke_result_t = typename invoke_result<_Fn, _Args...>::type;


  template<typename _Fn, typename... _ArgTypes>
    struct is_invocable
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_invocable_r
    : __is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Ret>{}),
 "_Ret must be a complete class or an unbounded array");
    };


  template<typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable
    : __and_<__is_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, void>,
      __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
    };


  template<typename _Result, typename _Ret>
    using __is_nt_invocable_impl
      = typename __is_invocable_impl<_Result, _Ret>::__nothrow_type;



  template<typename _Ret, typename _Fn, typename... _ArgTypes>
    struct is_nothrow_invocable_r
    : __and_<__is_nt_invocable_impl<__invoke_result<_Fn, _ArgTypes...>, _Ret>,
             __call_is_nothrow_<_Fn, _ArgTypes...>>::type
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Fn>{}),
 "_Fn must be a complete class or an unbounded array");
      static_assert((std::__is_complete_or_unbounded(
 __type_identity<_ArgTypes>{}) && ...),
 "each argument type must be a complete class or an unbounded array");
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Ret>{}),
 "_Ret must be a complete class or an unbounded array");
    };
# 3094 "/usr/include/c++/11/type_traits" 3
template <typename _Tp>
  inline constexpr bool is_void_v = is_void<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_null_pointer_v = is_null_pointer<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_integral_v = is_integral<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_floating_point_v = is_floating_point<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_array_v = is_array<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_pointer_v = is_pointer<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_lvalue_reference_v =
    is_lvalue_reference<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_rvalue_reference_v =
    is_rvalue_reference<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_member_object_pointer_v =
    is_member_object_pointer<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_member_function_pointer_v =
    is_member_function_pointer<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_enum_v = is_enum<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_union_v = is_union<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_class_v = is_class<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_function_v = is_function<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_reference_v = is_reference<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_arithmetic_v = is_arithmetic<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_fundamental_v = is_fundamental<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_object_v = is_object<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_scalar_v = is_scalar<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_compound_v = is_compound<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_member_pointer_v = is_member_pointer<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_const_v = is_const<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_volatile_v = is_volatile<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivial_v = is_trivial<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_copyable_v =
    is_trivially_copyable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_standard_layout_v = is_standard_layout<_Tp>::value;
#pragma GCC diagnostic push
#pragma GCC diagnostic ignored "-Wdeprecated-declarations"
template <typename _Tp>

  inline constexpr bool is_pod_v = is_pod<_Tp>::value;
template <typename _Tp>
  [[__deprecated__]]
  inline constexpr bool is_literal_type_v = is_literal_type<_Tp>::value;
#pragma GCC diagnostic pop
 template <typename _Tp>
  inline constexpr bool is_empty_v = is_empty<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_polymorphic_v = is_polymorphic<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_abstract_v = is_abstract<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_final_v = is_final<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_signed_v = is_signed<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_unsigned_v = is_unsigned<_Tp>::value;
template <typename _Tp, typename... _Args>
  inline constexpr bool is_constructible_v =
    is_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  inline constexpr bool is_default_constructible_v =
    is_default_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_copy_constructible_v =
    is_copy_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_move_constructible_v =
    is_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  inline constexpr bool is_assignable_v = is_assignable<_Tp, _Up>::value;
template <typename _Tp>
  inline constexpr bool is_copy_assignable_v = is_copy_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_move_assignable_v = is_move_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_destructible_v = is_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  inline constexpr bool is_trivially_constructible_v =
    is_trivially_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_default_constructible_v =
    is_trivially_default_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_copy_constructible_v =
    is_trivially_copy_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_move_constructible_v =
    is_trivially_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  inline constexpr bool is_trivially_assignable_v =
    is_trivially_assignable<_Tp, _Up>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_copy_assignable_v =
    is_trivially_copy_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_move_assignable_v =
    is_trivially_move_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_trivially_destructible_v =
    is_trivially_destructible<_Tp>::value;
template <typename _Tp, typename... _Args>
  inline constexpr bool is_nothrow_constructible_v =
    is_nothrow_constructible<_Tp, _Args...>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_default_constructible_v =
    is_nothrow_default_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_copy_constructible_v =
    is_nothrow_copy_constructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_move_constructible_v =
    is_nothrow_move_constructible<_Tp>::value;
template <typename _Tp, typename _Up>
  inline constexpr bool is_nothrow_assignable_v =
    is_nothrow_assignable<_Tp, _Up>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_copy_assignable_v =
    is_nothrow_copy_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_move_assignable_v =
    is_nothrow_move_assignable<_Tp>::value;
template <typename _Tp>
  inline constexpr bool is_nothrow_destructible_v =
    is_nothrow_destructible<_Tp>::value;
template <typename _Tp>
  inline constexpr bool has_virtual_destructor_v =
    has_virtual_destructor<_Tp>::value;
template <typename _Tp>
  inline constexpr size_t alignment_of_v = alignment_of<_Tp>::value;
template <typename _Tp>
  inline constexpr size_t rank_v = rank<_Tp>::value;
template <typename _Tp, unsigned _Idx = 0>
  inline constexpr size_t extent_v = extent<_Tp, _Idx>::value;

template <typename _Tp, typename _Up>
  inline constexpr bool is_same_v = __is_same(_Tp, _Up);




template <typename _Base, typename _Derived>
  inline constexpr bool is_base_of_v = is_base_of<_Base, _Derived>::value;
template <typename _From, typename _To>
  inline constexpr bool is_convertible_v = is_convertible<_From, _To>::value;
template<typename _Fn, typename... _Args>
  inline constexpr bool is_invocable_v = is_invocable<_Fn, _Args...>::value;
template<typename _Fn, typename... _Args>
  inline constexpr bool is_nothrow_invocable_v
    = is_nothrow_invocable<_Fn, _Args...>::value;
template<typename _Ret, typename _Fn, typename... _Args>
  inline constexpr bool is_invocable_r_v
    = is_invocable_r<_Ret, _Fn, _Args...>::value;
template<typename _Ret, typename _Fn, typename... _Args>
  inline constexpr bool is_nothrow_invocable_r_v
    = is_nothrow_invocable_r<_Ret, _Fn, _Args...>::value;





  template<typename _Tp>
    struct has_unique_object_representations
    : bool_constant<__has_unique_object_representations(
      remove_cv_t<remove_all_extents_t<_Tp>>
      )>
    {
      static_assert(std::__is_complete_or_unbounded(__type_identity<_Tp>{}),
 "template argument must be a complete class or an unbounded array");
    };


  template<typename _Tp>
    inline constexpr bool has_unique_object_representations_v
      = has_unique_object_representations<_Tp>::value;





  template<typename _Tp>
    struct is_aggregate
    : bool_constant<__is_aggregate(remove_cv_t<_Tp>)>
    { };


  template<typename _Tp>
    inline constexpr bool is_aggregate_v = is_aggregate<_Tp>::value;
# 3599 "/usr/include/c++/11/type_traits" 3
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/ignore.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/ignore.h"
namespace AscendC {
namespace Std {

struct ignore_t
{
    [host, aicore] inline ignore_t() = default;

    template <typename Tp, typename...Ts>
    [host, aicore] inline constexpr ignore_t(const Tp&, const Ts&...) noexcept {}

    template <typename Tp>
    [host, aicore] inline constexpr const ignore_t& operator=(const Tp&) const noexcept
    {
        return *this;
    }
};

constexpr ignore_t ignore{};

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/move.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/move.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_reference.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_reference {
    using type = Tp;
};

template <typename Tp>
struct remove_reference<Tp&> {
    using type = Tp;
};

template <typename Tp>
struct remove_reference<Tp&&> {
    using type = Tp;
};

template <typename Tp>
using remove_reference_t = typename remove_reference<Tp>::type;

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/move.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
[host, aicore] inline constexpr remove_reference_t<Tp>&& move(Tp&& t) noexcept
{
    using Up = remove_reference_t<Tp> ;
    return static_cast<Up&&>(t);
}

}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/forward.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/forward.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_reference.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_reference.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/integral_constant.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/integral_constant.h"
namespace AscendC {
namespace Std {

template <typename Tp, Tp v>
struct integral_constant
{
    static constexpr const Tp value = v;

    using value_type = Tp;
    using type = integral_constant;

    [host, aicore] inline constexpr operator value_type() const noexcept {
        return value;
    }

    [host, aicore] inline constexpr value_type operator()() const noexcept {
        return value;
    }
};

template <typename Tp, Tp v>
constexpr const Tp integral_constant<Tp, v>::value;

using true_type = integral_constant<bool, true>;
using false_type = integral_constant<bool, false>;

template <bool b>
using bool_constant = integral_constant<bool, b>;

template <size_t v>
using Int = integral_constant<size_t, v>;







template <auto t, auto u> [aicore] inline constexpr Int<(t + u)> operator + (Int<t>, Int<u>) { return {}; };
template <auto t, auto u> [aicore] inline constexpr Int<(t - u)> operator - (Int<t>, Int<u>) { return {}; };
template <auto t, auto u> [aicore] inline constexpr Int<(t * u)> operator * (Int<t>, Int<u>) { return {}; };
template <auto t, auto u> [aicore] inline constexpr Int<(t / u)> operator / (Int<t>, Int<u>) { return {}; };
template <auto t, auto u> [aicore] inline constexpr Int<(t % u)> operator % (Int<t>, Int<u>) { return {}; };


}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_reference.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct is_lvalue_reference : public false_type {};

template <typename Tp>
struct is_lvalue_reference<Tp&> : public true_type {};

template <typename Tp>
struct is_rvalue_reference : public false_type {};

template <typename Tp>
struct is_rvalue_reference<Tp&&> : public true_type {};

template <typename Tp>
struct is_reference : public false_type {};

template <typename Tp>
struct is_reference<Tp&> : public true_type {};

template <typename Tp>
struct is_reference<Tp&&> : public true_type {};

template <typename Tp>
constexpr bool is_lvalue_reference_v = is_lvalue_reference<Tp>::value;

template <typename Tp>
constexpr bool is_rvalue_reference_v = is_rvalue_reference<Tp>::value;

template <typename Tp>
constexpr bool is_reference_v = is_reference<Tp>::value;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/forward.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
[host, aicore] inline constexpr Tp&& forward(remove_reference_t<Tp>& t) noexcept
{
    return static_cast<Tp&&>(t);
}

template <typename Tp>
[host, aicore] inline constexpr Tp&& forward(remove_reference_t<Tp>&& t) noexcept
{
    static_assert(!is_lvalue_reference<Tp>::value, "cannot forward an rvalue as an lvalue");
    return static_cast<Tp&&>(t);
}

}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/add_pointer.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/add_pointer.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_referenceable.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_referenceable.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_same.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_same.h"
namespace AscendC {
namespace Std {

template <typename TP, typename Up>
struct is_same : public false_type {};

template <typename TP>
struct is_same<TP, TP> : public true_type {};

template <typename TP, typename Up>
constexpr bool is_same_v = false;

template <typename TP>
constexpr bool is_same_v<TP, TP> = true;

template <typename Tp, typename Up>
using IsSame = bool_constant<is_same_v<Tp, Up>>;

template <typename Tp, typename Up>
using IsNotSame = bool_constant<!is_same_v<Tp, Up>>;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_referenceable.h" 2

namespace AscendC {
namespace Std {

struct IsReferenceableImpl
{
    template <typename Tp>
    [host, aicore] inline static Tp& Test(int32_t);

    template <typename Tp>
    [host, aicore] inline static false_type Test(uint32_t);
};

template <typename Tp>
struct is_referenceable
    : integral_constant<bool, IsNotSame<decltype(IsReferenceableImpl::Test<Tp>(0)), false_type>::value> {};

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/add_pointer.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_void.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_void.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_cv.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_cv.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_const.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_const {
    using type = Tp;
};

template <typename Tp>
struct remove_const<const Tp> {
    using type = Tp ;
};

template <typename Tp>
using remove_const_t = typename remove_const<Tp>::type;

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_cv.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_volatile.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_volatile.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_volatile {
    using type = Tp;
};

template <typename Tp>
struct remove_volatile<volatile Tp> {
    using type = Tp;
};

template <typename Tp>
using remove_volatile_t = typename remove_volatile<Tp>::type;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_cv.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_cv {
    using type = remove_volatile_t<remove_const_t<Tp>>;
};

template <typename Tp>
using remove_cv_t = remove_volatile_t<remove_const_t<Tp>>;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_void.h" 2

namespace AscendC {
namespace Std {

template <typename Tp>
struct is_void : public is_same<remove_cv_t<Tp>, void> {};

template <typename Tp>
constexpr bool is_void_v = is_void<Tp>::value;

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/add_pointer.h" 2



namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value || is_void<Tp>::value>
struct AddPointerImpl {
    using type = remove_reference_t<Tp>*;
};

template <typename Tp>
struct AddPointerImpl<Tp, false> {
    using type = Tp;
};

template <typename Tp>
using add_pointer_t = typename AddPointerImpl<Tp>::type;

template <typename Tp>
struct add_pointer {
    using type = add_pointer_t<Tp>;
};

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/conditional.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/conditional.h"
namespace AscendC {
namespace Std {

namespace conditional_impl {

template <bool>
struct IfImpl;

template <>
struct IfImpl<true>
{
    template <typename IfRes, typename ElseRes>
    using Select = IfRes;
};

template <>
struct IfImpl<false>
{
    template <typename IfRes, typename ElseRes>
    using Select = ElseRes;
};

template <bool Cond, typename IfRes, typename ElseRes>
using If = typename IfImpl<Cond>::template Select<IfRes, ElseRes>;

}

template <bool Bp, typename If, typename Then>
struct conditional {
    using type = If;
};

template <typename If, typename Then>
struct conditional<false, If, Then> {
    using type = Then;
};

template <bool Bp, typename If, typename Then>
using conditional_t = typename conditional<Bp, If, Then>::type;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_array.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_array.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_array : public false_type {};

template <typename Tp>
struct is_array<Tp[]> : public true_type {};

template <typename Tp, size_t Np>
struct is_array<Tp[Np]> : public true_type {};

template <typename Tp>
constexpr bool is_array_v = is_array<Tp>::value;

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_function.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_function.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_const.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_const : public false_type {};

template <typename Tp>
struct is_const<Tp const> : public true_type {};

template <typename Tp>
constexpr bool is_const_v = is_const<Tp>::value;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_function.h" 2


namespace AscendC {
namespace Std {

template <typename T>
struct is_function : public bool_constant<!(is_reference_v<T> || is_const_v<const T>)> {};

template <typename Tp>
constexpr bool is_function_v = is_function<Tp>::value;

}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_extent.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/remove_extent.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_extent {
  using type = Tp;
};

template <typename Tp>
struct remove_extent<Tp[]> {
  using type = Tp;
};

template <typename Tp, size_t Np>
struct remove_extent<Tp[Np]> {
  using type = Tp;
};

template <typename Tp>
using remove_extent_t = typename remove_extent<Tp>::type;

}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/decay.h" 2


namespace AscendC {
namespace Std {

template <typename Up, bool>
struct DecayImpl {
    using type = remove_cv_t<Up>;
};

template <typename Up>
struct DecayImpl<Up, true>
{
public:
    using type = conditional_t<is_array<Up>::value, remove_extent_t<Up>*,
    conditional_t<is_function<Up>::value, add_pointer_t<Up>, remove_cv_t<Up>>>;
};

template <typename Tp>
struct decay
{
private:
    using Up = remove_reference_t<Tp>;

public:
    using type = typename DecayImpl<Up, is_referenceable<Up>::value>::type;
};

template <typename Tp>
using decay_t = typename decay<Tp>::type;

}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/enable_if.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../type_traits/enable_if.h"
namespace AscendC {
namespace Std {

template <bool, typename Tp = void>
struct enable_if {};

template <typename Tp>
struct enable_if<true, Tp> {
    using type = Tp;
};

template <bool Bp, typename Tp = void>
using enable_if_t = typename enable_if<Bp, Tp>::type;

}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/tuple_impl.h" 2







namespace AscendC {
namespace Std {

constexpr uint32_t ASCENDC_STD_TUPLE_STACK_DEEP = 64;

template <size_t N = 0, typename ...Tps>
[host, aicore] inline void tuple_static_assert()
{
    static_assert(N < ASCENDC_STD_TUPLE_STACK_DEEP, "Index overflow. The index must be smaller than 64!");
    static_assert(sizeof...(Tps) <= ASCENDC_STD_TUPLE_STACK_DEEP, "The number of template elements must be <= 64!");
}

template <typename ...Tps>
class tuple;

template <>
class tuple <> {};

template <typename Tp, typename ...Tps>
struct tuple_constraints
{
    using removeType = typename remove_reference<Tp>::type;
    static constexpr bool variadic_copy_constructible = !is_same_v<Tp, removeType&&>;
};

template <typename Tp, typename ...Tps>
class tuple<Tp, Tps...> : public tuple<Tps...>
{
public:
    [host, aicore] inline tuple() : tuple<Tps...>(), value() {
        tuple_static_assert<0, Tp, Tps...>();
    }

    template <typename Constraints = tuple_constraints<Tp, Tps...>,
        enable_if_t<Constraints::variadic_copy_constructible, int> = 0>
    [host, aicore] inline tuple(const Tp& val, const Tps& ...params) : tuple<Tps...>(params...), value(val) {
        tuple_static_assert<0, Tp, Tps...>();
    }

    template <typename Constraints = tuple_constraints<Tp, Tps...>,
        enable_if_t<!Constraints::variadic_copy_constructible, int> = 0>
    [host, aicore] inline tuple(Tp&& val, Tps&& ...params) : tuple<Tps...>(forward<Tps>(params)...), value(forward<Tp>(val)) {
        tuple_static_assert<0, Tp, Tps...>();
    }

    [host, aicore] inline Tp& GetValue() noexcept {
        return value;
    }

    [host, aicore] inline const Tp& GetValue() const noexcept {
        return value;
    }

    template <typename Head, typename ...Args>
    [host, aicore] inline tuple<Tp, Tps...>& operator=(const tuple<Head, Args...>& t)
    {
        static_assert(sizeof...(Tps) == sizeof...(Args), "Both tuples must have the same number of elements");
        this->value = t.value;
        tuple<Tps...>(*this) = tuple<Args...>(t);
        return *this;
    }

private:
    template <typename...> friend class tuple;
    Tp value;
};


template <typename ...Tps>
struct tuple_size;

template <typename ...Tps>
struct tuple_size<tuple<Tps...>> : integral_constant<size_t, sizeof...(Tps)> {};

template <typename T>
struct tuple_size<const T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
struct tuple_size<volatile T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
struct tuple_size<const volatile T> : public integral_constant<size_t, tuple_size<T>::value> {};

template <typename T>
constexpr size_t tuple_size_v = tuple_size<T>::value;


template <size_t N, typename ...Tps>
struct tuple_element;

template <size_t N>
struct tuple_element<N, tuple<>> {
    static_assert(N < 0, "The index(N) is greater than the number of elements!");
};

template <size_t N, typename Tp, typename ...Tps>
struct tuple_element<N, tuple<Tp, Tps...>> : public tuple_element <N - 1, tuple<Tps...>>{};

template <typename Tp, typename ...Tps>
struct tuple_element<0, tuple<Tp, Tps...>> {
    using type = Tp;
    using tuple_t = tuple<Tp, Tps...>;
};

template <size_t N, typename T>
struct tuple_element<N, const T> {
    using type = const typename remove_const<typename tuple_element<N, T>::type>::type;
};

template <size_t N, typename T>
struct tuple_element<N, volatile T> {
    using type = volatile typename remove_volatile<typename tuple_element<N, T>::type>::type;
};

template <size_t N, typename T>
struct tuple_element<N, const volatile T> {
    using type = const volatile typename remove_cv<typename tuple_element<N, T>::type>::type;
};


template <typename T>
struct unwrap_refwrapper {
    using type = T;
};

template <typename T>
struct unwrap_refwrapper<std::reference_wrapper<T>> {
    using type = T&;
};

template <typename T>
using unwrap_decay_t = typename unwrap_refwrapper<decay_t<T>>::type;

template <typename ...Tps>
[host, aicore] inline constexpr tuple<unwrap_decay_t<Tps>...> make_tuple(Tps&& ...args)
{
    tuple_static_assert<0, Tps...>();
    return tuple<unwrap_decay_t<Tps>...>(forward<Tps>(args)...);
}


template <typename ...Tps>
[host, aicore] inline constexpr tuple<Tps& ...> tie(Tps& ...args) noexcept
{
    tuple_static_assert<0, Tps...>();
    return tuple<Tps&...>(args...);
}


template <typename ...Tps>
[host, aicore] inline constexpr tuple<Tps&&...> forward_as_tuple(Tps&& ...args) noexcept
{
    tuple_static_assert<0, Tps...>();
    return tuple<Tps&&...>(forward<Tps>(args)...);
}


template <size_t N, typename ...Tps>
[host, aicore] inline typename tuple_element<N, tuple<Tps...> >::type& get(tuple<Tps...>& t) noexcept
{
    tuple_static_assert<N, Tps...>();
    using type = typename tuple_element<N, tuple<Tps...> >::type;
    using tuple_t = typename tuple_element<N, tuple<Tps...> >::tuple_t;
    return static_cast<type&>(static_cast<tuple_t &>(t).GetValue());
}

template <size_t N, typename ...Tps>
[host, aicore] inline const typename tuple_element<N, tuple<Tps...> >::type& get(const tuple<Tps...>& t) noexcept
{
    tuple_static_assert<N, Tps...>();
    using type = const typename tuple_element<N, tuple<Tps...> >::type;
    using tuple_t = const typename tuple_element<N, tuple<Tps...> >::tuple_t;
    return static_cast<type&>(static_cast<tuple_t &>(t).GetValue());
}

template <size_t N, typename ...Tps>
[host, aicore] inline typename tuple_element<N, tuple<Tps...> >::type&& get(tuple<Tps...>&& t) noexcept
{
    using type = typename tuple_element<N, tuple<Tps...> >::type;
    return static_cast<type&&>(get<N, Tps...>(static_cast<tuple<Tps...>&>(t)));
}

template <size_t N, typename ...Tps>
[host, aicore] inline const typename tuple_element<N, tuple<Tps...> >::type&& get(const tuple<Tps...>&& t) noexcept
{
    using type = const typename tuple_element<N, tuple<Tps...> >::type;
    return static_cast<type&&>(get<N, Tps...>(static_cast<const tuple<Tps...>&>(t)));
}

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/tuple.h" 2

namespace AscendC {
namespace Std {


template <typename ...Tps>
class tuple;


template <typename ...Tps>
struct tuple_size;


template <size_t N, typename ...Tps>
struct tuple_element;


template <typename ...Tps>
[host, aicore] inline constexpr tuple<unwrap_decay_t<Tps>...> make_tuple(Tps&& ...args);


template <typename ...Tps>
[host, aicore] inline constexpr tuple<Tps& ...> tie(Tps& ...args) noexcept;


template <size_t N, typename ...Tps>
[host, aicore] inline typename tuple_element<N, tuple<Tps...> >::type& get(tuple<Tps...>& t) noexcept;

template <size_t N, typename ...Tps>
[host, aicore] inline const typename tuple_element<N, tuple<Tps...> >::type& get(const tuple<Tps...>& t) noexcept;

template <size_t N, typename ...Tps>
[host, aicore] inline typename tuple_element<N, tuple<Tps...> >::type&& get(tuple<Tps...>&& t) noexcept;

template <size_t N, typename ...Tps>
[host, aicore] inline const typename tuple_element<N, tuple<Tps...> >::type&& get(const tuple<Tps...>&& t) noexcept;

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_layout.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_const.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_const.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_const {
    using type = const Tp;
};

template <typename Tp>
using add_const_t = typename add_const<Tp>::type;

}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_cv.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_cv.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_cv {
    using type = const volatile Tp;
};

template <typename Tp>
using add_cv_t = typename add_cv<Tp>::type;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_lvalue_reference.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_lvalue_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value>
struct AddLvalueReferenceImpl {
    using type = Tp;
};

template <typename Tp>
struct AddLvalueReferenceImpl<Tp, true> {
    using type = Tp&;
};

template <typename Tp>
using add_lvalue_reference_t = typename AddLvalueReferenceImpl<Tp>::type;

template <typename Tp>
struct add_lvalue_reference {
    using type = add_lvalue_reference_t<Tp>;
};

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_rvalue_reference.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_rvalue_reference.h"
namespace AscendC {
namespace Std {

template <typename Tp, bool = is_referenceable<Tp>::value>
struct AddRvalueReferenceImpl {
    using type = Tp;
};

template <typename Tp>
struct AddRvalueReferenceImpl<Tp, true> {
    using type = Tp&&;
};

template <typename Tp>
using add_rvalue_reference_t = typename AddRvalueReferenceImpl<Tp>::type;

template <typename Tp>
struct add_rvalue_reference {
    using type = add_rvalue_reference_t<Tp>;
};

}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_volatile.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/add_volatile.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct add_volatile {
    using type = volatile Tp;
};

template <typename Tp>
using add_volatile_t = typename add_volatile<Tp>::type;

}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2





# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_base_of.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_base_of.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_class.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_class.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_union.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_union.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct is_union : bool_constant<std::is_union<Tp>::value> {};

template <typename Tp>
constexpr bool is_union_v = is_union<Tp>::value;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/is_class.h" 2

namespace AscendC {
namespace Std {

namespace IsClassImpl {

template <typename Tp>
[host, aicore] inline bool_constant<!is_union_v<Tp>> Test(int32_t Tp::*);

template <typename Tp>
[host, aicore] inline false_type Test(uint32_t);

}

template <typename Tp>
struct is_class : decltype(IsClassImpl::Test<Tp>(nullptr)) {};

template <typename Tp>
constexpr bool is_class_v = is_class<Tp>::value;

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_base_of.h" 2

namespace AscendC {
namespace Std {

template <typename Base, typename Derived>
struct IsBaseOfImpl {
private:
    template <typename B>
    [host, aicore] inline static true_type TestPtrConv(const volatile B *);

    template <typename B>
    [host, aicore] inline static false_type TestPtrConv(const volatile void *);

    template <typename B, typename D>
    [host, aicore] inline static auto IsBaseOf(int32_t) -> decltype(TestPtrConv<B>(static_cast<D *>(nullptr)));

    template <typename B, typename D>
    [host, aicore] inline static auto IsBaseOf(uint32_t) -> true_type;

public:
    static constexpr bool value = decltype(IsBaseOf<Base, Derived>(0))::value;
};

template <typename Base, typename Derived>
struct is_base_of : bool_constant<is_class_v<Base> && is_class_v<Derived> && IsBaseOfImpl<Base, Derived>::value> {};

template <typename Base, typename Derived>
constexpr bool is_base_of_v = is_base_of<Base, Derived>::value;

}
}
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_constant.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_constant.h"
 namespace AscendC {
 namespace Std {

 template <auto n, typename T>
 struct is_constant : false_type {};

 template <auto n, typename T>
 struct is_constant<n, T const> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T const&> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T &> : is_constant<n,T> {};

 template <auto n, typename T>
 struct is_constant<n, T &&> : is_constant<n,T> {};

 template <auto n, typename T, T v>
 struct is_constant<n, integral_constant<T,v>> : bool_constant<v == n> {};

 }
 }
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_convertible.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_convertible.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/../utility/declval.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/tuple/../utility/../type_traits/../utility/declval.h"
namespace AscendC {
namespace Std {

template <typename T>
[host, aicore] typename add_rvalue_reference<T>::type declval() noexcept
{
    static_assert(!std::is_abstract<T>::value || std::is_polymorphic<T>::value,
        "Std::declval() cannot be used with polymorphic and abstract types !");
    return static_cast<typename add_rvalue_reference<T>::type>(*static_cast<T*>(nullptr));
}

}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_convertible.h" 2

namespace AscendC {
namespace Std {

template <typename From, typename To>
struct IsConvertibleImpl {
private:
    template <typename T>
    [host, aicore] inline static auto TestReturnable(int32_t) -> decltype(void(static_cast<T (*)()>(nullptr)), true_type{});

    template <typename T>
    [host, aicore] inline static auto TestReturnable(uint32_t) -> false_type;

    template <typename F, typename T>
    [host, aicore] inline static auto TestImplicitlyConvertible(int32_t) ->
        decltype(void(declval<void (&)(T)>()(declval<F>())), true_type{});

    template <typename F, typename T>
    [host, aicore] inline static auto TestImplicitlyConvertible(uint32_t) -> false_type;

public:
    static constexpr bool value =
        decltype(TestReturnable<To>(0))::value && decltype(TestImplicitlyConvertible<From, To>(0))::value;
};

template <typename From, typename To>
struct is_convertible : bool_constant<(is_void_v<From> && is_void_v<To>) || IsConvertibleImpl<From, To>::value> {};

template <typename From, typename To>
constexpr bool is_convertible_v = is_convertible<From, To>::value;

template <typename Ty>
struct is_convertible<Ty&, volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<volatile Ty&, volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<Ty&, const volatile Ty&> : true_type {};

template <typename Ty>
struct is_convertible<volatile Ty&, const volatile Ty&> : true_type {};

template <typename Ty>
constexpr bool is_convertible_v<Ty&, volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<volatile Ty&, volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<Ty&, const volatile Ty&> = true;

template <typename Ty>
constexpr bool is_convertible_v<volatile Ty&, const volatile Ty&> = true;

}
}
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_floating_point.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_floating_point.h"
namespace AscendC {
namespace Std {

template <typename T>
struct is_floating_point {
private:
    template <typename Head, typename... Args>
    [host, aicore] inline static constexpr bool IsUnqualifiedAnyOf() {
        return (... || is_same_v<remove_cv_t<Head>, Args>);
    }

public:
    static constexpr bool value = IsUnqualifiedAnyOf<T, float, double, long double, half>();
};

template <typename Tp>
constexpr bool is_floating_point_v = is_floating_point<Tp>::value;

}
}
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_integral.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_integral.h"
namespace AscendC {
namespace Std {

template <typename T>
struct is_integral {
private:
    template <typename Tp, typename... Tps>
    [host, aicore] inline static constexpr bool IsUnqualifiedAnyOf() {
        return (... || is_same_v<remove_cv_t<Tp>, Tps>);
    }

public:
    static constexpr bool value = IsUnqualifiedAnyOf<T,
        bool, unsigned long long, long long, unsigned long, long,
            unsigned int, int, unsigned short, short, unsigned char, signed char, char>();
};

template <typename T, T v>
struct is_integral<integral_constant<T,v>> : true_type {};

template <typename T>
constexpr bool is_integral_v = is_integral<T>::value;

}
}
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_pointer.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_pointer.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct IsPointerImpl : public false_type {};

template <typename Tp>
struct IsPointerImpl<Tp*> : public true_type {};

template <typename Tp>
struct is_pointer : public IsPointerImpl<remove_cv_t<Tp>> {};

template <typename Tp>
constexpr bool is_pointer_v = is_pointer<Tp>::value;

}
}
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_tuple.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_tuple.h"
namespace AscendC {
namespace Std {

template <typename T>
struct IsTupleImpl {
    private:
        template <typename Ts>
        [host, aicore] inline static auto HasTupleSize(int32_t) -> bool_constant<(tuple_size<Ts>::value >= 0)>;

        template <typename Ts>
        [host, aicore] inline static auto HasTupleSize(uint32_t) -> false_type;

    public:
        static constexpr bool value = decltype(HasTupleSize<T>(static_cast<int32_t>(0)))::value;
};

template <typename T>
struct is_tuple : bool_constant<IsTupleImpl<T>::value> {};

template <typename T>
constexpr bool is_tuple_v = is_tuple<T>::value;

}
}
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2




# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/remove_cvref.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/remove_cvref.h"
namespace AscendC {
namespace Std {

template <typename Tp>
using remove_cvref_t = remove_cv_t<remove_reference_t<Tp>>;

template <typename Tp>
struct remove_cvref {
    using type = remove_cvref_t<Tp>;
};

template <typename Tp, typename Up>
struct is_same_uncvref : IsSame<remove_cvref_t<Tp>, remove_cvref_t<Up>> {};

}
}
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/remove_pointer.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/remove_pointer.h"
namespace AscendC {
namespace Std {

template <typename Tp>
struct remove_pointer {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp*> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* const> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* volatile> {
    using type = Tp;
};

template <typename Tp>
struct remove_pointer<Tp* const volatile> {
    using type = Tp;
};

template <typename Tp>
using remove_pointer_t = typename remove_pointer<Tp>::type;

}
}
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_one_of.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/type_traits/is_one_of.h"
namespace AscendC {
namespace Std {
template <typename T, typename... Args> struct is_one_of : public false_type {};

template <typename T, typename Head, typename... Tail>
struct is_one_of<T, Head, Tail...>
    : Std::conditional_t<Std::is_same_v<T, Head>, Std::true_type, is_one_of<T, Tail...>> {};

template <typename T, typename... Args> inline constexpr bool is_one_of_v = is_one_of<T, Args...>::value;
}
}
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/type_traits.h" 2

namespace AscendC {
namespace Std {


template <bool, typename Tp>
struct enable_if;


template <bool Bp, typename If, typename Then>
struct conditional;


template <typename From, typename To>
struct is_convertible;


template <typename Base, typename Derived>
struct is_base_of;


template <typename Tp, typename Up>
struct is_same;


template <typename Tp, Tp v>
struct integral_constant;

}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_layout.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/utility.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/utility.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/utility/integer_sequence.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/utility/integer_sequence.h"
namespace AscendC {
namespace Std {

# 1 "/usr/include/c++/11/cstddef" 1 3
# 43 "/usr/include/c++/11/cstddef" 3







# 1 "/home/lnick/Ascend/cann-8.5.0/tools/bisheng_compiler/lib/clang/15.0.5/include/stddef.h" 1 3
# 51 "/usr/include/c++/11/cstddef" 2 3

extern "C++"
{

namespace std
{

  using ::max_align_t;
}



namespace std
{




  enum class byte : unsigned char {};

  template<typename _IntegerType> struct __byte_operand { };
  template<> struct __byte_operand<bool> { using __type = byte; };
  template<> struct __byte_operand<char> { using __type = byte; };
  template<> struct __byte_operand<signed char> { using __type = byte; };
  template<> struct __byte_operand<unsigned char> { using __type = byte; };

  template<> struct __byte_operand<wchar_t> { using __type = byte; };




  template<> struct __byte_operand<char16_t> { using __type = byte; };
  template<> struct __byte_operand<char32_t> { using __type = byte; };
  template<> struct __byte_operand<short> { using __type = byte; };
  template<> struct __byte_operand<unsigned short> { using __type = byte; };
  template<> struct __byte_operand<int> { using __type = byte; };
  template<> struct __byte_operand<unsigned int> { using __type = byte; };
  template<> struct __byte_operand<long> { using __type = byte; };
  template<> struct __byte_operand<unsigned long> { using __type = byte; };
  template<> struct __byte_operand<long long> { using __type = byte; };
  template<> struct __byte_operand<unsigned long long> { using __type = byte; };
# 110 "/usr/include/c++/11/cstddef" 3
  template<typename _IntegerType>
    struct __byte_operand<const _IntegerType>
    : __byte_operand<_IntegerType> { };
  template<typename _IntegerType>
    struct __byte_operand<volatile _IntegerType>
    : __byte_operand<_IntegerType> { };
  template<typename _IntegerType>
    struct __byte_operand<const volatile _IntegerType>
    : __byte_operand<_IntegerType> { };

  template<typename _IntegerType>
    using __byte_op_t = typename __byte_operand<_IntegerType>::__type;

  template<typename _IntegerType>
    [[__gnu__::__always_inline__]]
    constexpr __byte_op_t<_IntegerType>
    operator<<(byte __b, _IntegerType __shift) noexcept
    { return (byte)(unsigned char)((unsigned)__b << __shift); }

  template<typename _IntegerType>
    [[__gnu__::__always_inline__]]
    constexpr __byte_op_t<_IntegerType>
    operator>>(byte __b, _IntegerType __shift) noexcept
    { return (byte)(unsigned char)((unsigned)__b >> __shift); }

  [[__gnu__::__always_inline__]]
  constexpr byte
  operator|(byte __l, byte __r) noexcept
  { return (byte)(unsigned char)((unsigned)__l | (unsigned)__r); }

  [[__gnu__::__always_inline__]]
  constexpr byte
  operator&(byte __l, byte __r) noexcept
  { return (byte)(unsigned char)((unsigned)__l & (unsigned)__r); }

  [[__gnu__::__always_inline__]]
  constexpr byte
  operator^(byte __l, byte __r) noexcept
  { return (byte)(unsigned char)((unsigned)__l ^ (unsigned)__r); }

  [[__gnu__::__always_inline__]]
  constexpr byte
  operator~(byte __b) noexcept
  { return (byte)(unsigned char)~(unsigned)__b; }

  template<typename _IntegerType>
    [[__gnu__::__always_inline__]]
    constexpr __byte_op_t<_IntegerType>&
    operator<<=(byte& __b, _IntegerType __shift) noexcept
    { return __b = __b << __shift; }

  template<typename _IntegerType>
    [[__gnu__::__always_inline__]]
    constexpr __byte_op_t<_IntegerType>&
    operator>>=(byte& __b, _IntegerType __shift) noexcept
    { return __b = __b >> __shift; }

  [[__gnu__::__always_inline__]]
  constexpr byte&
  operator|=(byte& __l, byte __r) noexcept
  { return __l = __l | __r; }

  [[__gnu__::__always_inline__]]
  constexpr byte&
  operator&=(byte& __l, byte __r) noexcept
  { return __l = __l & __r; }

  [[__gnu__::__always_inline__]]
  constexpr byte&
  operator^=(byte& __l, byte __r) noexcept
  { return __l = __l ^ __r; }

  template<typename _IntegerType>
    [[nodiscard,__gnu__::__always_inline__]]
    constexpr _IntegerType
    to_integer(__byte_op_t<_IntegerType> __b) noexcept
    { return _IntegerType(__b); }


}

}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/utility/integer_sequence.h" 2

namespace Impl {
constexpr size_t MaxIntegerSequenceSize = 64;
constexpr size_t SpiltSize = 2;
};

template <class T, T... Ns> struct IntegerSequence {
  using type = IntegerSequence;
  using valueType = T;
  static_assert((0 <= sizeof...(Ns) && sizeof...(Ns) <= Impl::MaxIntegerSequenceSize), "Std::index_sequence size must be within [0,64].");
  [host, aicore] inline static constexpr size_t size() { return sizeof...(Ns); }
};

namespace Impl {

template <class T, class Seq0, class Seq1> struct MergeSeq {};

template <class T, T... Ns0, T... Ns1>
struct MergeSeq<T, IntegerSequence<T, Ns0...>, IntegerSequence<T, Ns1...>>
    : IntegerSequence<T, Ns0..., (sizeof...(Ns0) + Ns1)...> {};

template <class T, size_t N>
struct MakeIntegerSequence
    : Impl::MergeSeq<T, typename MakeIntegerSequence<T, N / SpiltSize>::type,
                     typename MakeIntegerSequence<T, N - N / SpiltSize>::type> {};

template <class T> struct MakeIntegerSequence<T, 0> : IntegerSequence<T> {};

template <class T> struct MakeIntegerSequence<T, 1> : IntegerSequence<T, 0> {};

};

template <class T, T N>
using MakeIntegerSequenceNoChecked = typename Impl::MakeIntegerSequence<T, N>::type;

template <class T, T N> struct MakeIntegerSequenceChecked {
  static_assert(0 <= N && N <= Impl::MaxIntegerSequenceSize,
                "Std::make_index_sequence must be within [0,64].");
  using type = MakeIntegerSequenceNoChecked<T, 0 <= N ? N : 0>;
};

template <class T, T N>
using MakeIntegerSequence = typename MakeIntegerSequenceChecked<T, N>::type;

}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/utils/std/utility.h" 2

namespace AscendC {
namespace Std {
template <size_t... Idx>
using index_sequence = IntegerSequence<size_t, Idx...>;
template <size_t N>
using make_index_sequence = MakeIntegerSequence<size_t, N>;
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_layout.h" 2

namespace AscendC {
namespace LayoutInternal {
constexpr size_t TWO_DIM_DEPTH = 2;
constexpr size_t FOUR_DIM_DEPTH = 4;
}

template <typename T, typename U, typename S>
[aicore] inline constexpr auto Crd2Idx(const T& coord, const U& shape, const S& stride);

template <typename... Shapes>
using Shape = Std::tuple<Shapes...>;

template <typename... Strides>
using Stride = Std::tuple<Strides...>;

template <typename T>
struct nesting_depth {
    static constexpr size_t value = 1;
};

template <>
struct nesting_depth<Std::tuple<>> {
    static constexpr size_t value = 0;
};

template <typename... Args>
struct nesting_depth<Std::tuple<Args...>> {
    static constexpr size_t value = (nesting_depth<Args>::value + ...);
};

template <typename T>
constexpr size_t nesting_depth_v = nesting_depth<T>::value;

template <size_t Dim, typename T, typename U>
struct IsStaticLayout {
private:
    template<typename T1>
    struct include_dynamic_type : Std::true_type {};

    template<size_t v>
    struct include_dynamic_type<Std::Int<v>> : Std::false_type {};

    template <typename... Args>
    struct include_dynamic_type<Std::tuple<Args...>> : Std::bool_constant<(include_dynamic_type<Args>::value || ...)> {};

    [aicore] inline static constexpr auto TestStaticLayout()
    {
        if constexpr (nesting_depth_v<T> == Dim &&
            !(include_dynamic_type<T>::value || include_dynamic_type<U>::value)) {
            return true;
        }
        return false;
    }
public:
    static constexpr bool value = TestStaticLayout();
};

template<typename T, typename U>
struct StaticLayoutSize {
private:
    [aicore] inline static constexpr auto GetFourDimStaticLayoutSize()
    {
        using rowShapeType = typename Std::tuple_element<0, T>::type;
        using colShapeType = typename Std::tuple_element<1, T>::type;
        using rowStrideType = typename Std::tuple_element<0, U>::type;
        using colStrideType = typename Std::tuple_element<1, U>::type;

        using outterRowNumType = typename Std::tuple_element<1, rowShapeType>::type;
        using outterRowStrideType = typename Std::tuple_element<1, rowStrideType>::type;
        using outterColNumType = typename Std::tuple_element<1, colShapeType>::type;
        using outterColStrideType = typename Std::tuple_element<1, colStrideType>::type;

        return (outterRowNumType {} * outterRowStrideType {}) > (outterColNumType {} * outterColStrideType {}) ?
            (outterRowNumType {} * outterRowStrideType {}) : (outterColNumType {} * outterColStrideType {});
    }

    [aicore] inline static constexpr auto GetTwoDimStaticLayoutSize()
    {
        using rowNumType = typename Std::tuple_element<0, T>::type;
        using colNumType = typename Std::tuple_element<1, T>::type;
        using rowStrideType = typename Std::tuple_element<0, U>::type;
        using colStrideType = typename Std::tuple_element<1, U>::type;

        return (rowNumType {} * rowStrideType {}) > (colNumType {} * colStrideType {}) ?
            (rowNumType {} * rowStrideType {}) : (colNumType {} * colStrideType {});
    }

    [aicore] inline static constexpr auto GetStaticLayoutSize() {
        if constexpr (IsStaticLayout<LayoutInternal::FOUR_DIM_DEPTH, T, U>::value) {
            return GetFourDimStaticLayoutSize();
        } else if constexpr (IsStaticLayout<LayoutInternal::TWO_DIM_DEPTH, T, U>::value) {
            return GetTwoDimStaticLayoutSize();
        } else {
            return Std::Int<0>{};
        }
    }
public:
    static constexpr size_t size = GetStaticLayoutSize();
};

template <typename... Ts>
[aicore] inline constexpr Shape<Ts...> MakeShape(const Ts&... t)
{
    return {t...};
}

template <typename... Ts>
[aicore] inline constexpr Stride<Ts...> MakeStride(const Ts&... t)
{
    return {t...};
}

template <typename T, typename U>
struct Layout : private Std::tuple<T, U>
{
    static constexpr auto size = StaticLayoutSize<T, U>::size;

    [aicore] inline constexpr Layout(const T& shape = {}, const U& stride = {})
        : Std::tuple<T, U>(shape, stride)
    {
        static_assert(Std::is_tuple_v<T> && Std::is_tuple_v<U>, "Shape or Stride is not tuple!");
    }

    [aicore] inline constexpr decltype(auto) GetSize() const
    {
        return GetLayoutSize();
    }

    [aicore] inline constexpr decltype(auto) layout()
    {
        return *this;
    }

    [aicore] inline constexpr decltype(auto) layout() const
    {
        return *this;
    }

    template <size_t... I>
    [aicore] inline constexpr decltype(auto) GetShape()
    {
        return GetValue<0, I...>(static_cast<Std::tuple<T, U>&>(*this));
    }

    template <size_t... I>
    [aicore] inline constexpr decltype(auto) GetShape() const
    {
        return GetValue<0, I...>(static_cast<const Std::tuple<T, U>&>(*this));
    }

    template <size_t... I>
    [aicore] inline constexpr decltype(auto) GetStride()
    {
        return GetValue<1, I...>(static_cast<Std::tuple<T, U>&>(*this));
    }

    template <size_t... I>
    [aicore] inline constexpr decltype(auto) GetStride() const
    {
        return GetValue<1, I...>(static_cast<const Std::tuple<T, U>&>(*this));
    }

    template <typename S>
    [aicore] inline constexpr auto operator()(const S& coord) const
    {
        return Crd2Idx(coord, GetShape(), GetStride());
    }

private:
    template<size_t index, size_t I, size_t... Is, typename Tuple>
    [aicore] inline constexpr decltype(auto) GetValue(const Tuple& t)
    {
        auto tupleEle = Std::get<index>(t);
        return Std::make_tuple(Std::get<I>(tupleEle), Std::get<Is>(tupleEle)...);
    }

    template<size_t index, size_t I, size_t... Is, typename Tuple>
    [aicore] inline constexpr decltype(auto) GetValue(const Tuple& t) const
    {
        auto tupleEle = Std::get<index>(t);
        return Std::make_tuple(Std::get<I>(tupleEle), Std::get<Is>(tupleEle)...);
    }

    template<size_t index, typename Tuple>
    [aicore] inline constexpr decltype(auto) GetValue(const Tuple& t)
    {
        return Std::get<index>(t);
    }

    template<size_t index, typename Tuple>
    [aicore] inline constexpr decltype(auto) GetValue(const Tuple& t) const
    {
        return Std::get<index>(t);
    }

    [aicore] inline constexpr decltype(auto) GetLayoutSize() const
    {
        uint32_t ret = -1;
        auto t = static_cast<const Std::tuple<T, U>&>(*this);
        if constexpr (nesting_depth_v<T> == LayoutInternal::FOUR_DIM_DEPTH) {
            auto rowShape = Std::get<0>(Std::get<0>(t));
            auto StrideInRow = Std::get<0>(Std::get<1>(t));
            auto colShape = Std::get<1>(Std::get<0>(t));
            auto StrideInCol = Std::get<1>(Std::get<1>(t));

            auto rowNum = Std::get<1>(rowShape);
            auto rowStride = Std::get<1>(StrideInRow);
            auto colNum = Std::get<1>(colShape);
            auto colStride = Std::get<1>(StrideInCol);

            auto size1 = rowNum * rowStride;
            auto size2 = colNum * colStride;
            ret = size1 > size2 ? size1 : size2;
        } else if constexpr (nesting_depth_v<T> == LayoutInternal::TWO_DIM_DEPTH) {
            auto shape = Std::get<0>(t);
            auto stride = Std::get<1>(t);

            auto rowNum = Std::get<0>(shape);
            auto rowStride = Std::get<0>(stride);
            auto colNum = Std::get<1>(shape);
            auto colStride = Std::get<1>(stride);

            auto size1 = rowNum * rowStride;
            auto size2 = colNum * colStride;
            ret = size1 > size2 ? size1 : size2;
        }
        return ret;
    }
};

template <typename T, typename U>
[aicore] inline constexpr auto MakeLayout(const T& shape, const U& stride)
{
    static_assert(Std::is_tuple_v<T> && Std::is_tuple_v<U>, "Shape or Stride is not tuple!");
    return Layout<T, U>(shape, stride);
}

template <typename T>
struct is_layout : Std::false_type {};

template <typename T, typename U>
struct is_layout<Layout<T, U>> : Std::true_type {};

template <typename T>
constexpr bool is_layout_v = is_layout<T>::value;

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_coord.h" 2

namespace AscendC {

namespace CoordImpl {
using IntZero = Std::Int<0>;
}

template <typename... Coords>
using Coord = Std::tuple<Coords...>;

template <typename... Ts>
[aicore] inline constexpr Coord<Ts...> MakeCoord(Ts const&... t) {
    return {t...};
}

template <typename TupleType>
using tuple_sequence = Std::make_index_sequence<Std::tuple_size_v<Std::remove_cvref_t<TupleType>>>;

template <typename T, typename F, typename G, size_t... I>
[aicore] inline constexpr auto TupleApply(T&& t, F&& f, G&& g, Std::index_sequence<I...>)
{
    return g(f(Std::get<I>(static_cast<T&&>(t)))...);
}

template <typename T, typename F, typename G>
[aicore] inline constexpr auto TransformApply(T&& t, F&& f, G&& g)
{
    if constexpr (Std::is_tuple_v<Std::remove_cvref_t<T>>) {
        return TupleApply(static_cast<T&&>(t), f, g, tuple_sequence<T>{});
    } else {
        return g(f(static_cast<T&&>(t)));
    }
}

struct MultipliesUnaryLeftFold {
    template <typename... T>
    [aicore] inline constexpr auto operator()(T&&... t) const {
        return (... * t);
    }
};

struct Product {
    template <typename T>
    [aicore] inline constexpr auto operator()(const T& intT) const
    {
        if constexpr (Std::is_tuple_v<T>) {
            if constexpr (Std::tuple_size_v<T> == 0) {
                return Std::Int<1>{};
            } else {
                return TransformApply(intT, Product{}, MultipliesUnaryLeftFold{});
            }
        } else if constexpr (Std::is_integral<T>::value) {
            return intT;
        } else {
            static_assert(sizeof(T) == 0, "Invalid Product parameters");
        }
    }
};

template <typename T, typename U, typename S>
[aicore] inline constexpr auto Crd2Idx(const T& coord, const U& shape, const S& stride);

template <typename T, typename U, typename S, size_t... Is>
[aicore] inline constexpr auto Crd2IdxTTT(const T& coord, const U& shape, const S& stride,
    Std::index_sequence<Is...>)
{
    return (... + Crd2Idx(Std::get<Is>(coord), Std::get<Is>(shape), Std::get<Is>(stride)));
}

template <typename T, typename U, typename S, size_t I0, size_t... Is>
[aicore] inline constexpr auto Crd2IdxITT(const T& coord, const U& shape, const S& stride,
    Std::index_sequence<I0,Is...>)
{
    if constexpr (sizeof...(Is) == 0) {
        return Crd2Idx(coord, Std::get<I0>(shape), Std::get<I0>(stride));
    } else if constexpr (Std::is_constant<0, T>::value) {
        return Crd2Idx(CoordImpl::IntZero{}, Std::get<I0>(shape), Std::get<I0>(stride)) +
            (CoordImpl::IntZero{} + ... + Crd2Idx(CoordImpl::IntZero{}, Std::get<Is>(shape), Std::get<Is>(stride)));
    } else {
        auto prod = Product{}(Std::get<I0>(shape));
        auto div = coord / prod;
        auto mod = coord % prod;
        return Crd2Idx(mod, Std::get<I0>(shape), Std::get<I0>(stride)) +
            Crd2IdxITT(div, shape, stride, Std::index_sequence<Is...>{});
    }
}

template <typename T, typename U, typename S>
[aicore] inline constexpr auto Crd2Idx(const T& coord, const U& shape, const S& stride)
{
    if constexpr (Std::is_tuple_v<T>) {
        if constexpr (Std::is_tuple_v<U>) {
            static_assert(Std::tuple_size_v<T> == Std::tuple_size_v<U>, "Shape and Coord Mismatched Ranks");
            static_assert(Std::tuple_size_v<T> == Std::tuple_size_v<S>, "Stride and Coord Mismatched Ranks");
            return Crd2IdxTTT(coord, shape, stride, tuple_sequence<T>{});
        } else {
            static_assert(sizeof(T) == 0, "Invalid parameters, U is not tuple!");
        }
    } else {
        if constexpr (Std::is_tuple_v<U>) {
            static_assert(Std::tuple_size_v<U> == Std::tuple_size_v<S>, "Shape and Stride Mismatched Ranks");
            return Crd2IdxITT(coord, shape, stride, tuple_sequence<U>{});
        } else {
            return coord * stride;
        }
    }
}

template <typename T, typename U, typename S>
[aicore] inline constexpr auto Crd2Idx(const T& coord, const Layout<U, S>& layout)
{
    return Crd2Idx(coord, layout.GetShape(), layout.GetStride());
}

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_tensor_trait.h" 2


namespace AscendC {







template <typename T, TPosition pos = TPosition::GM, typename LayoutType = Layout<Shape<>, Stride<>>>
struct TensorTrait {
    using LiteType = T;
    using LiteLayoutType = LayoutType;
    static constexpr const TPosition tPos = pos;
public:
    [aicore] inline TensorTrait(const LayoutType& t = {})
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        this->layout_ = t;
    }

    [aicore] inline LayoutType& GetLayout()
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_;
    }

    [aicore] inline const LayoutType& GetLayout() const
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_;
    }

    [aicore] inline void SetLayout(const LayoutType& t)
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        this->layout_ = t;
    }

    [aicore] inline decltype(auto) GetShape() const
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_.GetShape();
    }

    [aicore] inline decltype(auto) GetStride() const
    {
        static_assert(is_layout_v<LayoutType>, "TensorTrait without layout instantiation!");
        return layout_.GetStride();
    }
private:
    LayoutType layout_ = {};
};

template <typename T, TPosition pos, typename LayoutType>
[aicore] inline constexpr auto MakeTensorTrait(const LayoutType& t)
{
    static_assert(is_layout_v<LayoutType>, "Input parameters does not contain the layout type!");
    return TensorTrait<T, pos, LayoutType>(t);
}

template <typename T>
struct is_tensorTrait : Std::false_type {};

template <typename T, TPosition pos, typename ShapeType, typename StrideType>
struct is_tensorTrait<TensorTrait<T, pos, Layout<ShapeType, StrideType>>> : Std::true_type {};

template <typename T>
constexpr bool is_tensorTrait_v = is_tensorTrait<T>::value;

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_cache_intf.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_cache_intf.h"
namespace AscendC {

template <typename T>
[aicore] inline void DataCachePreload(const GlobalTensor<uint64_t>& src, const T cacheOffset);



template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] inline void DataCacheCleanAndInvalid(const GlobalTensor<T>& dst)
{
    DcciGMImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_global)) T*>(dst.GetPhyAddr()));
}

template <typename T, CacheLine entireType, DcciDst dcciDst>
[aicore] inline void DataCacheCleanAndInvalid(const LocalTensor<T>& dst)
{
    DcciUBImpl<T, entireType, dcciDst>(const_cast<__attribute__((cce_unif_buff)) T*>(dst.GetPhyAddr()));
}




template <typename T, CacheLine entireType>
[aicore] inline void DataCacheCleanAndInvalid(const GlobalTensor<T>& dst)
{
    DcciGMImpl<T, entireType>(const_cast<__attribute__((cce_global)) T*>(dst.GetPhyAddr()));
}


[aicore] inline void ICachePreLoad(const int64_t preFetchLen);

[aicore] inline int64_t GetICachePreloadStatus();

}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_block_sync_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_block_sync_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_base.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_base.h"
namespace AscendC {
using TBufHandle = uint8_t*;
using TEventID = int8_t;
using TTagType = int32_t;

template <typename T>
struct GetTypeFromTrait;

template <typename T, TPosition pos, typename Shape, typename Stride>
struct GetTypeFromTrait<TensorTrait<T, pos, Layout<Shape, Stride>>> {
    using LayoutType = Layout<Shape, Stride>;
    using ShapeType = Shape;
    using StrideType = Stride;
    using ShapeRows = typename Std::tuple_element<0, Shape>::type;
    using ShapeColumns = typename Std::tuple_element<1, Shape>::type;
    using StrideRows = typename Std::tuple_element<0, Stride>::type;
    using StrideColumns = typename Std::tuple_element<1, Stride>::type;
};

template <typename T, TPosition pos, typename Shape, typename Stride>
struct GetTypeFromTrait<LocalTensor<TensorTrait<T, pos, Layout<Shape, Stride>>>>
    : public GetTypeFromTrait<TensorTrait<T, pos, Layout<Shape, Stride>>> {};

template <typename T, TPosition pos, typename Shape, typename Stride>
struct GetTypeFromTrait<GlobalTensor<TensorTrait<T, pos, Layout<Shape, Stride>>>>
    : public GetTypeFromTrait<TensorTrait<T, pos, Layout<Shape, Stride>>> {};

template <typename T>
using GetLayoutType = Std::remove_cvref_t<typename GetTypeFromTrait<T>::LayoutType>;

template <typename T>
using GetShapeType = Std::remove_cvref_t<typename GetTypeFromTrait<T>::ShapeType>;

template <typename T>
using GetStrideType = Std::remove_cvref_t<typename GetTypeFromTrait<T>::StrideType>;

template <typename T>
using GetShapeRows = Std::remove_cvref_t<typename GetTypeFromTrait<T>::ShapeRows>;

template <typename T>
using GetShapeColumns = Std::remove_cvref_t<typename GetTypeFromTrait<T>::ShapeColumns>;

template <typename T>
using GetStrideRows = Std::remove_cvref_t<typename GetTypeFromTrait<T>::StrideRows>;

template <typename T>
using GetStrideColumns = Std::remove_cvref_t<typename GetTypeFromTrait<T>::StrideColumns>;

enum class TBufState : uint8_t {
    FREE = 0,
    OCCUPIED,
    ENQUE,
    DEQUE,
};

struct TBufType {
    TBufState state;
    HardEvent freeBufEvt;
# 91 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_base.h"
    TEventID enQueEvtID;
    TEventID freeBufEvtID;

    uint32_t address;
    uint32_t dataLen;
    TTagType usertag;
                                      ;
};

struct TBuffAddr {
    uint32_t dataLen;
    uint32_t bufferAddr;
    TBufHandle bufferHandle;
    uint8_t logicPos;



};

template <typename T> class BaseLocalTensor {
public:
    using PrimType = PrimT<T>;
# 181 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_base.h"
    [aicore] inline void SetAddr(const TBuffAddr& bufferAddr)
    {
        this->address_ = bufferAddr;
    }
    [[deprecated("NOTICE: InitBuffer has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] inline void InitBuffer(const uint32_t bufferOffset, const uint32_t bufferSize)
    {
        this->address_.bufferAddr = (uint64_t)(0) + bufferOffset;
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            this->address_.dataLen = bufferSize * INT4_BIT_NUM / ONE_BYTE_BIT_SIZE;




        } else {
            this->address_.dataLen = bufferSize * sizeof(PrimType);
        }
    }

    [aicore] inline TBufHandle GetBufferHandle() const
    {
        return address_.bufferHandle;
    }
public:
    TBuffAddr address_;
};

template <typename T> class BaseGlobalTensor {
public:
    using PrimType = PrimT<T>;
    [aicore] inline void SetAddr(const uint64_t offset)
    {
        if constexpr (IsSameType<PrimType, int4b_t>::value) {
            address_ = address_ + offset / INT4_TWO;
            oriAddress_ = oriAddress_ + offset / INT4_TWO;





        } else {
            address_ = address_ + offset;
            oriAddress_ = oriAddress_ + offset;
        }
    }
public:
    __attribute__((cce_global)) PrimType* address_;
    __attribute__((cce_global)) PrimType* oriAddress_;
};

template <typename T> class BaseTensor {};

template <typename T> class BaseTensorTraitTensor {};

template <typename T, TPosition pos, typename LayoutType>
class BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>> {
public:
    [aicore] inline TensorTrait<T, pos, LayoutType>& GetTensorTrait();
    [aicore] inline const TensorTrait<T, pos, LayoutType>& GetTensorTrait() const;
    [aicore] inline void SetTensorTrait(const TensorTrait<T, pos, LayoutType>& newTrait);
    [aicore] inline decltype(auto) GetLayout() const { return trait.GetLayout(); }
    [aicore] inline decltype(auto) GetShape() const { return trait.GetShape(); }
    [aicore] inline decltype(auto) GetStride() const { return trait.GetStride(); }
private:
    TensorTrait<T, pos, LayoutType> trait = {};
};

template <typename T, TPosition pos, typename LayoutType>
[aicore] inline TensorTrait<T, pos, LayoutType>& BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::GetTensorTrait() {
    return this->trait;
}

template <typename T, TPosition pos, typename LayoutType>
[aicore] inline const TensorTrait<T, pos, LayoutType>& BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::GetTensorTrait() const {
    return this->trait;
}

template <typename T, TPosition pos, typename LayoutType>
[aicore] inline void BaseTensorTraitTensor<TensorTrait<T, pos, LayoutType>>::SetTensorTrait(const TensorTrait<T, pos, LayoutType>& newTrait) {
    this->trait = newTrait;
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_block_sync_intf.h" 2

namespace AscendC {

template <HardEvent event>
[aicore] inline void SetFlag(int32_t eventID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (event == HardEvent::MTE2_V || event == HardEvent::V_MTE2 || event == HardEvent::MTE3_V
                      || event == HardEvent::V_MTE3 || event == HardEvent::V_V || event == HardEvent::S_V ||
                      event == HardEvent::V_S) {
            return;
        }
    }
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr ((event == HardEvent::MTE2_MTE1) || (event == HardEvent::MTE1_MTE2) ||
                      (event == HardEvent::MTE1_M) || (event == HardEvent::M_MTE1) || (event == HardEvent::M_FIX) ||
                      (event == HardEvent::FIX_M)) {
            return;
        }
    }
    SetFlagImpl<event>(eventID);
}

template <HardEvent event>
[aicore] inline void WaitFlag(int32_t eventID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (event == HardEvent::MTE2_V || event == HardEvent::V_MTE2 || event == HardEvent::MTE3_V
                      || event == HardEvent::V_MTE3 || event == HardEvent::V_V || event == HardEvent::S_V ||
                      event == HardEvent::V_S) {
            return;
        }
    }
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr ((event == HardEvent::MTE2_MTE1) || (event == HardEvent::MTE1_MTE2) ||
                      (event == HardEvent::MTE1_M) || (event == HardEvent::M_MTE1) || (event == HardEvent::M_FIX) ||
                      (event == HardEvent::FIX_M)) {
            return;
        }
    }
    WaitFlagImpl(event, eventID);
}

template <pipe_t pipe>
[aicore] inline void PipeBarrier()
{
    PipeBarrierImpl<pipe>();
}



template <MemDsbT arg0>
[aicore] inline void DataSyncBarrier()
{
    DataSyncBarrierImpl<arg0>();
}
# 87 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_block_sync_intf.h"
template <bool isAIVOnly = true>
[aicore] inline void IBSet(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                  int32_t blockIdx, int32_t eventID);

template <bool isAIVOnly = true>
[aicore] inline void IBWait(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                   int32_t blockIdx, int32_t eventID);






template <bool isAIVOnly = true>
[aicore] inline void SyncAll(const GlobalTensor<int32_t>& gmWorkspace, const LocalTensor<int32_t>& ubWorkspace,
                                 const int32_t usedCores = 0);

template <bool isAIVOnly = true>
[aicore] inline void SyncAll();

template <uint8_t modeId, pipe_t pipe>
[aicore] inline void CrossCoreSetFlag(uint16_t flagId);

template <uint8_t modeId = 0, pipe_t pipe = PIPE_S>
[aicore] inline void CrossCoreWaitFlag(uint16_t flagId);

template<pipe_t src, pipe_t dst>
class TQueSync {
public:
    [aicore] inline void SetFlag(TEventID id);
    [aicore] inline void WaitFlag(TEventID id);
};

}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_sys_var_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_sys_var_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_mm.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_mm.h"
namespace AscendC {

using LoadData2dParams = struct LoadData2DParams;
struct LoadData2DParams {
    [aicore] LoadData2DParams() {}

    [aicore] LoadData2DParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn, const uint16_t srcStrideIn,
        const uint8_t sidIn, const uint16_t dstGapIn, const bool ifTransposeIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          sid(sidIn),
          dstGap(dstGapIn),
          ifTranspose(ifTransposeIn),
          addrMode(addrModeIn)
    {}

    uint16_t startIndex = 0;
    uint16_t dstGap = 0;
    uint16_t srcStride = 0;
    bool ifTranspose = 0;
    uint8_t repeatTimes = 0;

    uint8_t sid = 0;
    uint8_t addrMode = 0;
};


struct LoadData2DParamsV2 {
    [aicore] LoadData2DParamsV2() {}

    [aicore] LoadData2DParamsV2(const uint32_t mStartPositionIn, const uint32_t kStartPositionIn,
        const uint16_t mStepIn, const uint16_t kStepIn, const int32_t srcStrideIn, const uint16_t dstStrideIn,
        const bool ifTransposeIn, const uint8_t sidIn)
        : mStartPosition(mStartPositionIn),
          kStartPosition(kStartPositionIn),
          mStep(mStepIn),
          kStep(kStepIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          ifTranspose(ifTransposeIn),
          sid(sidIn)
    {}

    uint32_t mStartPosition = 0;
    uint32_t kStartPosition = 0;
    uint16_t mStep = 0;
    uint16_t kStep = 0;
    int32_t srcStride = 0;
    uint16_t dstStride = 0;
    bool ifTranspose = false;
    uint8_t sid = 0;
};

struct LoadData2dTransposeParams {
    [aicore] LoadData2dTransposeParams() {}

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstfracGapIn),
          addrMode(addrModeIn)
    {}

    [aicore] LoadData2dTransposeParams(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstfracGapIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstfracGapIn)
    {}

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint8_t addrMode = 0;
};

template <typename T>
struct LoadData3DParamsV1 {
    [aicore] LoadData3DParamsV1()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
    }

    [aicore] LoadData3DParamsV1(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t c1IndexIn, const uint8_t fetchFilterWIn, const uint8_t fetchFilterHIn, const int16_t leftTopWIn,
        const int16_t leftTopHIn, const uint8_t strideWIn, const uint8_t strideHIn, const uint8_t filterWIn,
        const uint8_t filterHIn, const uint8_t dilationFilterWIn, const uint8_t dilationFilterHIn,
        const uint8_t jumpStrideIn, const uint8_t repeatModeIn, const uint8_t repeatTimeIn, const uint8_t cSizeIn,
        const T padValueIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          c1Index(c1IndexIn),
          fetchFilterW(fetchFilterWIn),
          fetchFilterH(fetchFilterHIn),
          leftTopW(leftTopWIn),
          leftTopH(leftTopHIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          jumpStride(jumpStrideIn),
          repeatMode(repeatModeIn),
          repeatTime(repeatTimeIn),
          cSize(cSizeIn),
          padValue(padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint8_t strideW = 0;
    uint8_t strideH = 0;
    uint8_t filterW = 0;
    uint8_t filterH = 0;
    uint8_t dilationFilterW = 0;
    uint8_t dilationFilterH = 0;
    uint8_t jumpStride = 0;
    uint8_t repeatMode = 0;
    uint8_t repeatTime = 0;
    uint8_t cSize = 0;
    T padValue = 0;
    uint8_t fetchFilterW = 0;
    uint8_t fetchFilterH = 0;
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t c1Index = 0;
    int16_t leftTopW = 0;
    int16_t leftTopH = 0;
};

template <typename T>
struct LoadData3DParamsV2 {
    [aicore] LoadData3DParamsV2()
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = 0;
        }
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          channelSize(channelSizeIn),
          kExtension(kExtensionIn),
          mExtension(mExtensionIn),
          kStartPt(kStartPtIn),
          mStartPt(mStartPtIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          padValue(padValueIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    [aicore] LoadData3DParamsV2(const uint8_t padListIn[PAD_SIZE], const uint16_t l1HIn, const uint16_t l1WIn,
        const uint16_t channelSizeIn, const uint16_t kExtensionIn, const uint16_t mExtensionIn,
        const uint16_t kStartPtIn, const uint16_t mStartPtIn, const uint8_t strideWIn, const uint8_t strideHIn,
        const uint8_t filterWIn, const uint8_t filterHIn, const uint8_t dilationFilterWIn,
        const uint8_t dilationFilterHIn, const bool enTransposeIn, const bool enSmallKIn, const T padValueIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn)
        : l1H(l1HIn),
          l1W(l1WIn),
          channelSize(channelSizeIn),
          kExtension(kExtensionIn),
          mExtension(mExtensionIn),
          kStartPt(kStartPtIn),
          mStartPt(mStartPtIn),
          strideW(strideWIn),
          strideH(strideHIn),
          filterW(filterWIn),
          filterH(filterHIn),
          dilationFilterW(dilationFilterWIn),
          dilationFilterH(dilationFilterHIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          padValue(padValueIn),
          filterSizeW(filterSizeWIn),
          filterSizeH(filterSizeHIn),
          fMatrixCtrl(fMatrixCtrlIn)
    {
        for (int32_t i = 0; i < PAD_SIZE; ++i) {
            padList[i] = padListIn[i];
        }
    }

    uint8_t padList[PAD_SIZE] = {0};
    uint16_t l1H = 0;
    uint16_t l1W = 0;
    uint16_t channelSize = 0;
    uint16_t kExtension = 0;
    uint16_t mExtension = 0;
    uint16_t kStartPt = 0;
    uint16_t mStartPt = 0;

    uint8_t strideW = 1;
    uint8_t strideH = 1;
    uint8_t filterW = 1;
    uint8_t filterH = 1;
    uint8_t dilationFilterW = 1;
    uint8_t dilationFilterH = 1;
    bool enTranspose = false;
    bool enSmallK = false;
    T padValue = 0;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
};
struct LoadData3DParamsV2Pro {
    [aicore] LoadData3DParamsV2Pro()
    {
    }

    [aicore] LoadData3DParamsV2Pro(const uint16_t channelSizeIn, const bool enTransposeIn, const bool enSmallKIn,
        const bool filterSizeWIn, const bool filterSizeHIn, const bool fMatrixCtrlIn, const uint64_t extConfigIn,
        const uint64_t filterConfigIn)
        : channelSize(channelSizeIn),
          enTranspose(enTransposeIn),
          enSmallK(enSmallKIn),
          filterSizeW(filterSizeWIn),
          filterSizeH(filterSizeHIn),
          fMatrixCtrl(fMatrixCtrlIn),
          extConfig(extConfigIn),
          filterConfig(filterConfigIn)
    {}

    uint16_t channelSize = 0;
    bool enTranspose = false;
    bool enSmallK = false;
    bool filterSizeW = false;
    bool filterSizeH = false;
    bool fMatrixCtrl = false;
    uint64_t extConfig = 0;
    uint64_t filterConfig = 0X10101010101;
};

struct LoadData2dTransposeParamsV2 {
    [aicore] LoadData2dTransposeParamsV2() {}

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstFracGapIn,
        const uint16_t srcFracGapIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstFracGapIn),
          srcFracGap(srcFracGapIn)
    {}

    [aicore] LoadData2dTransposeParamsV2(const uint16_t startIndexIn, const uint8_t repeatTimesIn,
        const uint16_t srcStrideIn, const uint16_t dstGapIn, const uint16_t dstFracGapIn,
        const uint16_t srcFracGapIn, const uint8_t addrModeIn)
        : startIndex(startIndexIn),
          repeatTimes(repeatTimesIn),
          srcStride(srcStrideIn),
          dstGap(dstGapIn),
          dstFracGap(dstFracGapIn),
          srcFracGap(srcFracGapIn),
          addrMode(addrModeIn)
    {}

    uint16_t startIndex = 0;
    uint8_t repeatTimes = 0;
    uint16_t srcStride = 0;
    uint16_t dstGap = 0;
    uint16_t dstFracGap = 0;
    uint16_t srcFracGap = 0;
    uint8_t addrMode = 0;
};

struct MmadParams {
    [aicore] MmadParams() {}

    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const bool isBiasIn,
        const int32_t fmOffsetIn, const bool enSsparseIn, const bool enWinogradAIn, const bool enWinogradBIn)
        : m(mIn),
          n(nIn),
          k(kIn),
          isBias(isBiasIn),
          fmOffset(fmOffsetIn),
          enSsparse(enSsparseIn),
          enWinogradA(enWinogradAIn),
          enWinogradB(enWinogradBIn)
    {}

    [aicore] MmadParams(const uint16_t mIn, const uint16_t nIn, const uint16_t kIn, const uint8_t unitFlagIn,
        const bool cmatrixSourceIn, const bool cmatrixInitValIn)
        : m(mIn),
          n(nIn),
          k(kIn),
          unitFlag(unitFlagIn),
          cmatrixSource(cmatrixSourceIn),
          cmatrixInitVal(cmatrixInitValIn)
    {}

    uint16_t m = 0;
    uint16_t n = 0;
    uint16_t k = 0;


    bool isBias = false;

    int32_t fmOffset = 0;

    bool enSsparse = false;

    bool enWinogradA = false;

    bool enWinogradB = false;
    uint8_t unitFlag = 0;

    bool kDirectionAlign = false;

    bool cmatrixSource = false;

    bool cmatrixInitVal = true;
};

template <typename T>
struct InitConstValueParams {
    [aicore] InitConstValueParams() {}

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn,
        const uint16_t blockNumIn, const uint16_t dstGapIn, const T initValueIn)
        : repeatTimes(repeatTimesIn),
          blockNum(blockNumIn),
          dstGap(dstGapIn),
          initValue(initValueIn)
    {}

    [aicore] InitConstValueParams(const uint16_t repeatTimesIn, const T initValueIn)
        : repeatTimes(repeatTimesIn),
          initValue(initValueIn)
    {}

    uint16_t repeatTimes = 0;
    uint16_t blockNum = 0;
    uint16_t dstGap = 0;
    T initValue = 0;
};

enum class FmatrixMode : uint8_t {
    FMATRIX_LEFT = 0,
    FMATRIX_RIGHT = 1,
};
# 409 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_mm.h"
struct LoadDataRepeatParam {
    [aicore] LoadDataRepeatParam() {}

    [aicore] LoadDataRepeatParam(const uint16_t repeatStrideIn, const uint8_t repeatTimeIn,
        const uint8_t repeatModeIn)
        : repeatStride(repeatStrideIn),
          repeatTime(repeatTimeIn),
          repeatMode(repeatModeIn)
    {}

    uint16_t repeatStride = 0;
    uint8_t repeatTime = 1;
    uint8_t repeatMode = 0;
    uint8_t reserved = 0;
};


struct LoadImageToLocalParams {
    [aicore] LoadImageToLocalParams() {}

    [aicore] LoadImageToLocalParams(const uint16_t horizSizeIn, const uint16_t vertSizeIn,
        const uint16_t horizStartPosIn, const uint16_t vertStartPosIn, const uint16_t srcHorizSizeIn,
        const uint8_t topPadSizeIn, const uint8_t botPadSizeIn, const uint16_t leftPadSizeIn,
        const uint16_t rightPadSizeIn)
        : horizSize(horizSizeIn),
          vertSize(vertSizeIn),
          horizStartPos(horizStartPosIn),
          vertStartPos(vertStartPosIn),
          srcHorizSize(srcHorizSizeIn),
          topPadSize(topPadSizeIn),
          botPadSize(botPadSizeIn),
          leftPadSize(leftPadSizeIn),
          rightPadSize(rightPadSizeIn)
    {}

    uint16_t horizSize = 0;
    uint16_t vertSize = 0;
    uint16_t horizStartPos = 0;
    uint16_t vertStartPos = 0;
    uint16_t srcHorizSize = 0;
    uint8_t topPadSize = 0;
    uint8_t botPadSize = 0;
    uint16_t leftPadSize = 0;
    uint16_t rightPadSize = 0;
    uint8_t sid = 0;
};

struct CheckLocalMemoryIAParam {
    [aicore] CheckLocalMemoryIAParam() {}

    [aicore] CheckLocalMemoryIAParam(const uint8_t enableBitIn, const uint32_t startAddrIn, const uint32_t endAddrIn,
        const bool isScalarReadIn, const bool isScalarWriteIn, const bool isVectorReadIn, const bool isVectorWriteIn,
        const bool isMteReadIn, const bool isMteWriteIn, const bool isEnableIn)
        : enableBit(enableBitIn),
          startAddr(startAddrIn),
          endAddr(endAddrIn),
          isScalarRead(isScalarReadIn),
          isScalarWrite(isScalarWriteIn),
          isVectorRead(isVectorReadIn),
          isVectorWrite(isVectorWriteIn),
          isMteRead(isMteReadIn),
          isMteWrite(isMteWriteIn),
          isEnable(isEnableIn)
    {}

    uint8_t enableBit = 0;
    uint32_t startAddr = 0;
    uint32_t endAddr = 0;
    bool isScalarRead = false;
    bool isScalarWrite = false;
    bool isVectorRead = false;
    bool isVectorWrite = false;
    bool isMteRead = false;
    bool isMteWrite = false;
    bool isEnable = false;
    uint32_t reserved = 0;
};
}




namespace AscendC {

struct LoadDataTrait {
    [aicore] constexpr LoadDataTrait() {}

    [aicore] constexpr LoadDataTrait(const bool transposedIn) : transposed(transposedIn) {}

    bool transposed = false;
};
constexpr LoadDataTrait DEFAULT_LOAD_DATA_TRAIT{};

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_sys_var_intf.h" 2

namespace AscendC {

[aicore] inline int64_t GetBlockNum();

[aicore] inline int64_t GetBlockIdx();

[aicore] inline int64_t GetSubBlockIdx();

[aicore] inline int64_t GetTaskRatio();

[aicore] inline constexpr int16_t GetDataBlockSizeInBytes()
{
    return ONE_BLK_SIZE;
}

[aicore] inline void GetArchVersion(uint32_t& coreVersion);

[aicore] inline int64_t GetSubBlockNum();

[aicore] inline int64_t GetProgramCounter();

[aicore] inline void Trap();

[aicore] inline int64_t GetSystemCycle();
}
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_sys_var_intf_impl.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_sys_var_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sys_var_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sys_var_impl.h"
namespace AscendC {

[aicore] inline int64_t GetSubBlockNumImpl()
{
    if constexpr(g_coreType == AscendC::AIC) {
        return 1;
    } else {



        return get_subblockdim();

    }
}

[aicore] inline int64_t GetSystemCycleImpl()
{







    uint64_t sysCnt = 0;
    asm volatile("MOV %0, SYS_CNT\n" : "+l"(sysCnt));
    return (int64_t)(sysCnt);

}

[aicore] inline void GetArchVersionImpl(uint32_t& coreVersion)
{
    const int32_t coreVersionOffset = 32;
    coreVersion = static_cast<uint32_t>((static_cast<uint64_t>(get_arch_ver()) >> coreVersionOffset) & 0xFFF);
}

[aicore] inline int64_t GetProgramCounterImpl()
{
    int64_t pc = static_cast<int64_t>(static_cast<uint64_t>(get_pc()) & 0xFFFFFFFFFFFF);
    return pc;
}

[aicore] inline void TrapImpl()
{
    trap();
}
}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_sys_var_intf_impl.h" 2
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_sys_var_intf_impl.h"
namespace AscendC {
[aicore] inline void GetArchVersion(uint32_t& coreVersion)
{
    GetArchVersionImpl(coreVersion);
}

[aicore] inline int64_t GetSubBlockNum()
{
    return GetSubBlockNumImpl();
}

[aicore] inline int64_t GetProgramCounter()
{
    return GetProgramCounterImpl();
}

[aicore] inline void Trap()
{
    TrapImpl();
}

[aicore] inline int64_t GetSystemCycle()
{
    return GetSystemCycleImpl();
}
}
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_sys_var_intf.h" 2
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_swap_mem_intf.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_swap_mem_intf.h"
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_sysWorkspaceReserved;




[aicore] inline __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetSysWorkSpacePtr()
{



    return g_sysWorkspaceReserved;

}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h" 2

namespace AscendC {
class TPipe;
class KfcCommClient;
}
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h"
[[block_local]] __inline__ AscendC::TPipe* g_vecTPipePtr;
# 54 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_common.h"
[[block_local]] __inline__ half g_deqValue;


[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_hcclContextReserved[2];




[aicore] inline AscendC::TPipe* GetTPipePtr()
{




    return g_vecTPipePtr;






}


namespace AscendC {
template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static inline void SetVectorMask(const uint64_t maskHigh, const uint64_t maskLow)
{





    SetVectorMaskImpl<T, mode>(maskHigh, maskLow);
}

template <typename T, MaskMode mode = MaskMode::NORMAL>
[aicore] static inline void SetVectorMask(int32_t len)
{



    SetVectorMaskImpl<T, mode>(len);
}

[aicore] inline void ResetMask()
{



    ResetMaskImpl();
}

[aicore] inline void SetMaskCount()
{
    SetMaskCountImpl();
}

[aicore] inline void SetMaskNorm()
{
    SetMaskNormImpl();
}

[aicore] inline void SetHF32Mode(bool hf32Mode)
{
    SetHF32ModeImpl(hf32Mode);
}

[aicore] inline void SetHF32TransMode(bool hf32TransMode)
{
    SetHF32TransModeImpl(hf32TransMode);
}

[aicore] inline void SetMMLayoutTransform(bool mmLayoutMode)
{
    SetMMLayoutTransformImpl(mmLayoutMode);
}

template <uint32_t index>
[aicore] inline void SetHcclContext(__attribute__((cce_global)) uint8_t* context)
{
    if constexpr (index > 1) {
        return;
    }
    g_hcclContextReserved[index] = context;
}

template <uint32_t index>
[aicore] inline __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetHcclContext(void)
{
    if constexpr (index > 1) {
        return nullptr;
    }
    return g_hcclContextReserved[index];
}



template <typename T, typename U>
[aicore] inline void SetAippFunctions(const GlobalTensor<T>& src0, AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<PrimT<T>, U>(const_cast<__attribute__((cce_global)) PrimT<T>*>(src0.GetPhyAddr()), format, config);
}

template <typename T, typename U>
[aicore] inline void SetAippFunctions(const GlobalTensor<T>& src0, const GlobalTensor<T>& src1,
                                        AippInputFormat format, AippParams<U> config)
{
    SetAippFunctionsImpl<PrimT<T>, U>(const_cast<__attribute__((cce_global)) PrimT<T>*>(src0.GetPhyAddr()),
                                      const_cast<__attribute__((cce_global)) PrimT<T>*>(src1.GetPhyAddr()), format, config);
}

}

[[deprecated("NOTICE: SetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
             "Please do not use it!")]]
[aicore] inline __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) SetDumpWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    return g_dumpWorkspaceReserved = workspace;
}
[[deprecated("NOTICE: GetDumpWorkSpacePtr has been deprecated and will be removed in the next version. "
             "Please do not use it!")]]
[aicore] inline __attribute__((cce_global)) uint8_t* __attribute__((cce_global)) GetDumpWorkSpacePtr()
{
    return g_dumpWorkspaceReserved;
}



[[deprecated(
    "NOTICE: SetSysWorkSpacePtr has been deprecated and will be removed in the next version.")]]
[aicore] inline void SetSysWorkSpacePtr(__attribute__((cce_global)) uint8_t* workspace)
{
    g_sysWorkspaceReserved = workspace;
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h" 2


namespace AscendC {
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h"
struct ShapeInfo {
public:
    [aicore] inline ShapeInfo() {}
    [aicore] inline ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[],
        const uint8_t inputOriginalShapeDim, const uint32_t inputOriginalShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputOriginalShapeDim), dataFormat(inputFormat)
    {




          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
        }
        for (int index = 0; index < originalShapeDim; ++index) {
            originalShape[index] = inputOriginalShape[index];
        }
    }
    [aicore] inline ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[], const DataFormat inputFormat)
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(inputFormat)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }

    [aicore] inline ShapeInfo(const uint8_t inputShapeDim, const uint32_t inputShape[])
        : shapeDim(inputShapeDim), originalShapeDim(inputShapeDim), dataFormat(DataFormat::ND)
    {



          ;
        for (int index = 0; index < shapeDim; ++index) {
            shape[index] = inputShape[index];
            originalShape[index] = inputShape[index];
        }
    }
    uint8_t shapeDim;
    uint8_t originalShapeDim;
    uint32_t shape[8];
    uint32_t originalShape[8];
    DataFormat dataFormat;
};






template <typename T, typename U>
struct ShapeInfoParams {
    [aicore] ShapeInfoParams() {};
    using Params = ShapeInfo;
};
template <typename T>
struct ShapeInfoParams<TensorTrait<T>, T> {
    [aicore] ShapeInfoParams() {};
    using Params = int8_t;
};

[aicore] inline uint64_t GetShapeSize(const ShapeInfo& shapeInfo)
{
    int shapeSize = 1;
    for (int index = 0; index < shapeInfo.shapeDim; ++index) {
        shapeSize *= shapeInfo.shape[index];
    }
    return shapeSize;
}

template <typename T> class SymbolOverrideAdd;
template <typename T> class SymbolOverrideSub;
template <typename T> class SymbolOverrideDiv;
template <typename T> class SymbolOverrideMul;
template <typename T> class SymbolOverrideOr;
template <typename T> class SymbolOverrideAnd;
template <typename T> class SymbolOverrideCompare;

template <typename T> class LocalTensor : public BaseLocalTensor<T>, public BaseTensorTraitTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] inline LocalTensor<T>() {};
# 140 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tensor.h"
    [aicore] inline uint64_t GetPhyAddr() const;
    [aicore] inline uint64_t GetPhyAddr(const uint32_t offset) const;
    [aicore] inline __attribute__((inout_pipe("S"))) PrimType GetValue(const uint32_t index) const;
    [aicore] inline __attribute__((inout_pipe("S"))) __attribute__((cce_unif_buff)) PrimType& operator()(const uint32_t offset) const;
    template <typename U> [aicore] inline LocalTensor<U> ReinterpretCast() const;
    template <typename S> [aicore] inline __attribute__((inout_pipe("S")))
        void SetValue(const uint32_t index, const S value) const;
    [aicore] inline LocalTensor operator[](const uint32_t offset) const;

    template <typename S>
    [[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
    [aicore] inline void SetAddrWithOffset(LocalTensor<S> &src, uint32_t offset);

    [aicore] inline LocalTensor<T>(TPosition pos, uint32_t addr, uint32_t tileSize);
    template <typename U>
    [aicore] inline LocalTensor<T>(uint32_t addr, const U& layout);
    [aicore] inline LocalTensor<T>(uint32_t addr);
    [aicore] inline int32_t GetPosition() const;
    [aicore] inline void SetSize(const uint32_t size);
    [aicore] inline uint32_t GetSize() const;

    [[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                 "it!")]]
    [aicore] inline uint32_t GetLength() const;

    [aicore] inline void SetBufferLen(uint32_t dataLen);
    [aicore] inline void SetUserTag(const TTagType tag);
    [aicore] inline TTagType GetUserTag() const;

    [aicore] inline void operator = (const SymbolOverrideAdd<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideSub<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideMul<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideDiv<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideOr<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideAnd<T>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideCompare<float>& symbolOverride);
    [aicore] inline void operator = (const SymbolOverrideCompare<half>& symbolOverride);
    [aicore] inline SymbolOverrideAdd<T> operator + (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideSub<T> operator - (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideMul<T> operator *(const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideDiv<T> operator / (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideOr<T> operator | (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideAnd<T> operator & (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator < (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator > (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator != (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator == (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator <= (const LocalTensor<T>& src1) const;
    [aicore] inline SymbolOverrideCompare<T> operator >= (const LocalTensor<T>& src1) const;
    [aicore] inline void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] inline ShapeInfo GetShapeInfo() const;

public:

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;





private:
    template <typename S>
    [aicore] inline void CreateTensor(TPosition pos, uint32_t addr, uint32_t tileSize);



};

template <typename T> class GlobalTensor : public BaseGlobalTensor<T>, public BaseTensorTraitTensor<T> {
public:
    using PrimType = PrimT<T>;
    [aicore] inline GlobalTensor<T>();



    [aicore] inline void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer, uint64_t bufferSize);
    [aicore] inline void SetGlobalBuffer(__attribute__((cce_global)) PrimType* buffer);
    [aicore] inline const __attribute__((cce_global)) PrimType* GetPhyAddr() const;
    [aicore] inline __attribute__((cce_global)) PrimType* GetPhyAddr(const uint64_t offset) const;

    [aicore] inline __attribute__((inout_pipe("S"))) PrimType GetValue(const uint64_t offset) const;
    [aicore] inline __attribute__((inout_pipe("S"))) __attribute__((cce_global)) PrimType& operator()(const uint64_t offset) const;




    [aicore] inline void SetValue(const uint64_t offset, PrimType value);

    [aicore] inline uint64_t GetSize() const;
    [aicore] inline GlobalTensor operator[](const uint64_t offset) const;
    [aicore] inline void SetShapeInfo(const ShapeInfo& shapeInfo);
    [aicore] inline ShapeInfo GetShapeInfo() const;
    template<CacheRwMode rwMode = CacheRwMode::RW>
    [aicore] inline void SetL2CacheHint(CacheMode mode);

public:

    uint64_t bufferSize_;

    typename ShapeInfoParams<T, PrimType>::Params shapeInfo_;

    CacheMode cacheMode_ = CacheMode::CACHE_MODE_NORMAL;




private:




};

template<Hardware hard = Hardware::UB>
class LocalMemAllocator {
public:
    [aicore] inline LocalMemAllocator();
    [aicore] inline uint32_t GetCurAddr() const;
    template <TPosition pos, class DataType, uint32_t tileSize> [aicore] inline LocalTensor<DataType> Alloc();
    template <TPosition pos, class DataType> LocalTensor<DataType> [aicore] inline Alloc(uint32_t tileSize);
    template <class DataType, uint32_t tileSize> LocalTensor<DataType> [aicore] inline Alloc();
    template <class DataType> LocalTensor<DataType> [aicore] inline Alloc(uint32_t tileSize);
    template <class TensorTraitType> LocalTensor<TensorTraitType> [aicore] inline Alloc();
    template <class TensorTraitType, typename LayoutType> LocalTensor<TensorTraitType> [aicore] inline Alloc(const LayoutType& layout);

private:
    uint32_t head_ = 0;
};
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_symbol_override_impl.h" 1
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_symbol_override_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmp_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmp_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_binary.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_binary.h"
namespace AscendC {
struct BinaryRepeatParams {
    [aicore] BinaryRepeatParams() {}

    [aicore] BinaryRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t src0BlkStrideIn,
        const uint8_t src1BlkStrideIn, const uint8_t dstRepStrideIn, const uint8_t src0RepStrideIn,
        const uint8_t src1RepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          src0BlkStride(src0BlkStrideIn),
          src1BlkStride(src1BlkStrideIn),
          dstRepStride(dstRepStrideIn),
          src0RepStride(src0RepStrideIn),
          src1RepStride(src1RepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmp_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_unary.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_unary.h"
namespace AscendC {
struct UnaryRepeatParams {
    [aicore] UnaryRepeatParams() {}

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          srcBlkStride(srcBlkStrideIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn)
    {}

    [aicore] UnaryRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t srcBlkStrideIn,
        const uint8_t dstRepStrideIn, const uint8_t srcRepStrideIn, const bool halfBlockIn)
        : dstBlkStride(dstBlkStrideIn),
          srcBlkStride(srcBlkStrideIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn),
          halfBlock(halfBlockIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint16_t srcBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t srcRepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
    bool halfBlock = false;
};
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmp_impl.h" 2

namespace AscendC {



template <typename T>
[aicore] inline void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpv_lt(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmpv_gt(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmpv_eq(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmpv_le(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmpv_ge(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmpv_ne(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
                repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
                repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

template <typename T>
[aicore] inline void VcmpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const BinaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmp_lt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GT: {
            vcmp_gt(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::EQ: {
            vcmp_eq(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::LE: {
            vcmp_le(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::GE: {
            vcmp_ge(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        case CMPMODE::NE: {
            vcmp_ne(src0, src1, 1,
                repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
                repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] inline void VcmpvIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, __attribute__((cce_unif_buff)) int32_t* src1,
    CMPMODE cmpMode, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{


                                                                                               ;
    vcmpv_eq(dst, src0, src1, repeatTime, repeatParams.dstBlkStride,
        repeatParams.src0BlkStride, repeatParams.src1BlkStride, repeatParams.dstRepStride,
        repeatParams.src0RepStride, repeatParams.src1RepStride);
}

template <typename T, typename U>
[aicore] inline void CompareCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct BinaryRepeatParams repeatParams;
        uint32_t sumRepeat = count * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;

        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1 + i * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1 + repeatRound * srcOffset, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void VcmpImpl(__attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        VcmpIntrinsicsImpl(src0, src1, cmpMode, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void VcmpvImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, CMPMODE cmpMode,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (IsSameType<T, int32_t>::value) {


                                                                  ;
        }
        CompareCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, count);
    }
}




template <typename T>
[aicore] inline void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (cmpMode) {
        case CMPMODE::LT: {
            vcmpvs_lt(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GT: {
            vcmpvs_gt(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::EQ: {
            vcmpvs_eq(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::LE: {
            vcmpvs_le(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::GE: {
            vcmpvs_ge(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        case CMPMODE::NE: {
            vcmpvs_ne(dst, src0, src1, repeatTime,
                static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
            break;
        }
        default:

                                                                                                                   ;
            break;
    }
}

[aicore] inline void VcmpvsIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int32_t* src0, int32_t src1,
    CMPMODE cmpMode, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{


                                                                                                     ;
    vcmpvs_eq(dst, src0, src1, repeatTime,
        static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}

template <typename T, typename U>
[aicore] inline void CompareScalarCompute(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        struct UnaryRepeatParams repeatParams;
        uint32_t sumRepeat = count * sizeof(T) / ONE_REPEAT_BYTE_SIZE;
        constexpr uint32_t repeatNormal = 252;
        uint32_t repeatRound = sumRepeat / repeatNormal;
        uint32_t repeatTail = sumRepeat % repeatNormal;
        uint32_t srcOffset = repeatNormal * ONE_REPEAT_BYTE_SIZE / sizeof(T);
        uint32_t dstOffset = srcOffset / ONE_BYTE_BIT_SIZE;
        for (uint32_t i = 0; i < repeatRound; ++i) {
            VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + i * dstOffset,
                src0 + i * srcOffset,
                src1, cmpMode, MASK_PLACEHOLDER, repeatNormal,
                repeatParams);
        }
        VcmpvsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst) + repeatRound * dstOffset,
            src0 + repeatRound * srcOffset,
            src1, cmpMode, MASK_PLACEHOLDER, repeatTail,
            repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTime, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint64_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    (void)(mask);
    if constexpr(g_coreType == AscendC::AIV) {
        VcmpvsIntrinsicsImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode,
            repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void VcmpvsImpl(__attribute__((cce_unif_buff)) U* dst, __attribute__((cce_unif_buff)) T* src0, T src1, CMPMODE cmpMode,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (IsSameType<T, int32_t>::value) {


                                                                  ;
        }
        CompareScalarCompute(reinterpret_cast<__attribute__((cce_unif_buff)) uint8_t*>(dst), src0, src1, cmpMode, count);
    }
}

template <typename T>
[aicore] inline void GetCmpMaskImpl(__attribute__((cce_unif_buff)) T* dst)
{
    get_cmpmask(dst);
}

template <typename T>
[aicore] inline void SetCmpMaskImpl(__attribute__((cce_unif_buff)) T* src)
{
    set_cmpmask(src);
}
}
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_symbol_override_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_binary_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_binary_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void AddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Add, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadd(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void AddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, count);
        vadd(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void SubIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Sub, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vsub(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void SubImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, count);
        vsub(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] inline void MulIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Mul, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmul(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void MulImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, count);
        vmul(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T>
[aicore] inline void DivIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Div, current api support dtype combination "
        "is src and dst both: half / float.");
    vdiv(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DivIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DivIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void DivImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                          ;
        set_mask_count();
        set_vector_mask(0, count);
        vdiv(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void MaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Max, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmax(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void MaxImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, count);
        vmax(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void MinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Min, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmin(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void MinImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                    ;
        set_mask_count();
        set_vector_mask(0, count);
        vmin(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void AndIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in And, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vand(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AndIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AndIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void AndImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                ;
        set_mask_count();
        set_vector_mask(0, count);

        vand((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void OrIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Or, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vor(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        OrIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        OrIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void OrImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                        ;
        set_mask_count();
        set_vector_mask(0, count);

        vor((__attribute__((cce_unif_buff)) int16_t*)dst, (__attribute__((cce_unif_buff)) int16_t*)src0, (__attribute__((cce_unif_buff)) int16_t*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}





template <typename T>
[aicore] inline void AddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, half, float>(), "Failed to check dtype in AddRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vaddrelu(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void AddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                             ;
        set_mask_count();
        set_vector_mask(0, count);
        vaddrelu((__attribute__((cce_unif_buff)) T*)dst, (__attribute__((cce_unif_buff)) T*)src0, (__attribute__((cce_unif_buff)) T*)src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




struct AddDeqReluParams {
    [aicore] AddDeqReluParams(){};

    uint32_t needTmpSize = 0;
    uint32_t calcSize = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;
    uint32_t dstOffset = 0;
    uint32_t tailSrc0Offset = 0;
    uint32_t tailSrc1Offset = 0;
    uint32_t tailDstOffset = 0;
    uint64_t mask1;
    uint64_t mask2[2];
    uint8_t maskMode = 0;
    uint16_t mainBlock = 0;
    uint16_t tailSize = 0;

    uint8_t repeat = 0;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
};

[aicore] inline void SetAddDeqReluMaskCal(AddDeqReluParams &params)
{
    if (params.maskMode == ADDDEQRELU_MASK_MODE_ONE) {
        AscendCUtils::SetMask<half>(params.mask1);
    } else if (params.maskMode == ADDDEQRELU_MASK_MODE_TWO) {
        AscendCUtils::SetMask<half>(params.mask2[1], params.mask2[0]);
    }
}

template <bool isSetMask = true>
[aicore] inline void AddDeqReluComput(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    __attribute__((cce_unif_buff)) int32_t *sharedTmpBuffer, AddDeqReluParams &params)
{

    vadd(sharedTmpBuffer, src0, src1, params.repeat, DEFAULT_BLK_STRIDE, params.src0BlkStride,
        params.src1BlkStride, DEFAULT_REPEAT_STRIDE, params.src0RepStride, params.src1RepStride);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) float *src0FloatTmp = reinterpret_cast<__attribute__((cce_unif_buff)) float *>(src0);
    vconv_s322f32(src0FloatTmp, sharedTmpBuffer, params.repeat, static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_RIGHT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(g_deqValue), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    vmuls(src0FloatTmp, src0FloatTmp, static_cast<float>(DEQ_SHIFT_LEFT_17_BIT), params.repeat,
        static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
        static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint16_t>(DEFAULT_REPEAT_STRIDE));
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    __attribute__((cce_unif_buff)) half *src1HalfTmp = reinterpret_cast<__attribute__((cce_unif_buff)) half *>(src1);
    vconv_f322f16(src1HalfTmp, src0FloatTmp, params.repeat, 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    pipe_barrier(PIPE_V);

    __attribute__((cce_unif_buff)) half *tmpBufferHalf = reinterpret_cast<__attribute__((cce_unif_buff)) half *>(sharedTmpBuffer);
    if (params.maskMode != 0) {
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
    }
    vector_dup(tmpBufferHalf, static_cast<half>(0), 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), 1,
        DEFAULT_REPEAT_STRIDE, 0);
    if (params.maskMode != 0) {
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
    pipe_barrier(PIPE_V);

    if constexpr (isSetMask) {
        SetAddDeqReluMaskCal(params);
    }
    if (params.maskMode == 0) {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    } else {
        vmax(dst, tmpBufferHalf, src1HalfTmp, params.repeat, params.dstBlkStride, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, params.dstRepStride, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    }
    pipe_barrier(PIPE_V);
}

[aicore] inline void GetAddDeqReluParamCal(AddDeqReluParams &params, uint8_t repeatTime)
{
    if (params.needTmpSize <= TMP_UB_SIZE / sizeof(int32_t)) {
        params.calcSize = params.needTmpSize;
        if (params.maskMode != 0) {
            params.repeat = repeatTime;
        }
    } else {
        params.calcSize = TMP_UB_SIZE / sizeof(int32_t);
        if (params.maskMode != 0) {
            params.repeat = params.calcSize / B32_DATA_NUM_PER_REPEAT;
        }
    }
    if (params.maskMode == 0) {
        params.repeat = repeatTime;
    }
    params.mainBlock = params.needTmpSize / params.calcSize;
    params.tailSize = params.needTmpSize % params.calcSize;
    if (params.maskMode == 0) {
        params.src0Offset = params.calcSize;
        params.src1Offset = params.calcSize;
        params.dstOffset = params.calcSize;
    } else {
        params.src0Offset = params.repeat * params.src0RepStride * B32_DATA_NUM_PER_BLOCK;
        params.src1Offset = params.repeat * params.src1RepStride * B32_DATA_NUM_PER_BLOCK;
        params.dstOffset = params.repeat * params.dstRepStride * B16_DATA_NUM_PER_BLOCK;
    }
    params.tailSrc0Offset = params.mainBlock * params.src0Offset;
    params.tailSrc1Offset = params.mainBlock * params.src1Offset;
    params.tailDstOffset = params.mainBlock * params.dstOffset;
}

[aicore] inline void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const int32_t &count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.needTmpSize = count;
        GetAddDeqReluParamCal(params, 1);
        __attribute__((cce_unif_buff)) int32_t *sharedTmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        set_mask_count();
        set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<false>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, sharedTmpBuffer, params);
        }
        if (params.tailSize != 0) {
            params.calcSize = params.tailSize;
            set_vector_mask(0, static_cast<uint64_t>(params.calcSize));
            AddDeqReluComput<false>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, sharedTmpBuffer, params);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <bool isSetMask = true>
[aicore] inline void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_ONE;
        params.mask1 = mask;
        params.needTmpSize = repeatTime * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;
        GetAddDeqReluParamCal(params, repeatTime);
        __attribute__((cce_unif_buff)) int32_t *sharedTmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, sharedTmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask);
            }
            params.repeat = repeatTime - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, sharedTmpBuffer, params);
        }
    }
}

template <bool isSetMask = true>
[aicore] inline void AddDeqReluImpl(__attribute__((cce_unif_buff)) half *dst, __attribute__((cce_unif_buff)) int32_t *src0, __attribute__((cce_unif_buff)) int32_t *src1,
    const uint64_t mask[], const uint8_t repeatTime, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AddDeqReluParams params;
        params.maskMode = ADDDEQRELU_MASK_MODE_TWO;
        params.mask2[0] = mask[0];
        params.mask2[1] = mask[1];
        params.needTmpSize = repeatTime * DEFAULT_BLOCK_SIZE / sizeof(int32_t);

        params.dstBlkStride = repeatParams.dstBlkStride;
        params.src0BlkStride = repeatParams.src0BlkStride;
        params.src1BlkStride = repeatParams.src1BlkStride;
        params.dstRepStride = repeatParams.dstRepStride;
        params.src0RepStride = repeatParams.src0RepStride;
        params.src1RepStride = repeatParams.src1RepStride;

        GetAddDeqReluParamCal(params, repeatTime);
        __attribute__((cce_unif_buff)) int32_t *sharedTmpBuffer = AscendCUtils::GetTemporaryBufferAddr<int32_t>(TMP_UB_OFFSET, params.calcSize);
        if constexpr (isSetMask) {
            AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
        }
        for (int i = 0; i < params.mainBlock; i++) {
            AddDeqReluComput<isSetMask>(dst + i * params.dstOffset, src0 + i * params.src0Offset,
                src1 + i * params.src1Offset, sharedTmpBuffer, params);
        }
        if (params.tailSize != 0) {
            if constexpr (isSetMask) {
                AscendCUtils::SetMask<int32_t>(mask[1], mask[0]);
            }
            params.repeat = repeatTime - params.repeat * params.mainBlock;
            AddDeqReluComput<isSetMask>(dst + params.tailDstOffset, src0 + params.tailSrc0Offset,
                src1 + params.tailSrc1Offset, sharedTmpBuffer, params);
        }
    }
}



template <typename T>
[aicore] inline void FusedMulAddIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in FusedMulAdd, current api support dtype "
        "combination is src and dst both: half / float.");
    vmadd(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void FusedMulAddImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                       ;
        set_mask_count();
        set_vector_mask(0, count);
        vmadd(dst, src0, src1, 1,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T, typename U>
[aicore] inline void MulAddDstIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<T, U>, Tuple<half, half>, Tuple<float, float>, Tuple<float, half>>(), "Failed to "
        "check dtype in MulAddDst, current api support dtype combination is src: half, dst: half / float; src: float, "
        "dst: float.");
    vmla(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulAddDstIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void MulAddDstImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                       ;
        set_mask_count();
        set_vector_mask(0, count);
        if constexpr (sizeof(T) == sizeof(U)) {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        } else {
            vmla(dst, src0, src1, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void FusedMulAddReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in FusedMulAddRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vmaddrelu(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask[], const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    const uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        FusedMulAddReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T>
[aicore] inline void FusedMulAddReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                           ;
        set_mask_count();
        set_vector_mask(0, count);
        vmaddrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}




template <typename T>
[aicore] inline void SubReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, half, float>(), "Failed to check dtype in SubRelu, current api support dtype "
        "combination is src and dst both: int16_t / half / float.");
    vsubrelu(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
        repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
}


template <typename T>
[aicore] inline void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                             ;
        set_mask_count();
        set_vector_mask(0, count);
        vsubrelu(dst, src0, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask[],
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void SubReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint64_t mask,
    const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SubReluIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}
}
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_symbol_override_impl.h" 2
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_symbol_override_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T> class LocalTensor;


template <typename T> class SymbolOverrideAdd {
public:
    [aicore] inline SymbolOverrideAdd(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        AddImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};

template <typename T> class SymbolOverrideSub {
public:
    [aicore] inline SymbolOverrideSub(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        SubImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};

template <typename T> class SymbolOverrideMul {
public:
    [aicore] inline SymbolOverrideMul(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        MulImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};

template <typename T> class SymbolOverrideDiv {
public:
    [aicore] inline SymbolOverrideDiv(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        DivImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};


template <typename T> class SymbolOverrideAnd {
public:
    [aicore] inline SymbolOverrideAnd(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        if constexpr(SupportType<T, int32_t, uint32_t>()) {
            AndImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize() * 2);
        } else {
            AndImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
        }
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};

template <typename T> class SymbolOverrideOr {
public:
    [aicore] inline SymbolOverrideOr(const LocalTensor<T> &src0, const LocalTensor<T> &src1)
        : src0_(src0), src1_(src1)
    {}

    [aicore] inline void Process(const LocalTensor<T> &dst) const
    {




        if constexpr(SupportType<T, int32_t, uint32_t>()) {
            OrImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize() * 2);
        } else {
            OrImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)this->src0_.GetPhyAddr(),
                (__attribute__((cce_unif_buff)) PrimT<T>*)this->src1_.GetPhyAddr(), dst.GetSize());
        }
    }

private:
    const LocalTensor<T> &src0_;
    const LocalTensor<T> &src1_;
};


template <typename T> class SymbolOverrideCompare {
public:
    [aicore] inline SymbolOverrideCompare(const LocalTensor<T> &src0, const LocalTensor<T> &src1,
        CMPMODE cmpMode)
        : src0_(src0), src1_(src1), cmpMode_(cmpMode)
    {}

    template <typename U> [aicore] inline void Process(const LocalTensor<U> &dst) const
    {




        VcmpvImpl((__attribute__((cce_unif_buff)) PrimT<U> *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T> *)this->src0_.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) PrimT<T> *)this->src1_.GetPhyAddr(), cmpMode_, this->src0_.GetSize());
    }

private:
    const LocalTensor<T> src0_;
    const LocalTensor<T> src1_;
    CMPMODE cmpMode_;
};
}
#pragma end_pipe
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h" 2

namespace AscendC {
template <typename Shape, size_t... Is>
[aicore] inline constexpr auto ProdImpl(const Shape& t, Std::index_sequence<Is...>) {
    return (Std::get<Is>(t) * ... * 1);
}

template <typename... Args>
[aicore] inline constexpr auto Prod(const Shape<Args...>& t) {
    return ProdImpl(t, Std::make_index_sequence<sizeof...(Args)>{});
}
# 548 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <typename T> [aicore] inline uint64_t LocalTensor<T>::GetPhyAddr() const
{
    return GetPhyAddr(0);
}
template <typename T> [aicore] inline uint64_t LocalTensor<T>::GetPhyAddr(const uint32_t offset) const
{
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        return this->address_.bufferAddr + offset / INT4_TWO;




    } else {
        return this->address_.bufferAddr + offset * sizeof(PrimType);
    }
}
template <typename T> [aicore] inline __attribute__((inout_pipe("S")))
    typename LocalTensor<T>::PrimType LocalTensor<T>::GetValue(const uint32_t index) const
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (GetPhyType(AscendC::TPosition(this->GetPosition())) == Hardware::UB) {
# 577 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
            return PrimType(0);

        }
    }
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmp = this->ReinterpretCast<uint8_t>();
        uint8_t val = tmp.GetValue(index / INT4_TWO);
        return static_cast<int4b_t>(val >> (INT4_BIT_NUM * (index % INT4_TWO)));
# 601 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    } else {
        return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(index)));
    }
}
template <typename T> [aicore] inline __attribute__((inout_pipe("S")))
    __attribute__((cce_unif_buff)) typename LocalTensor<T>::PrimType& LocalTensor<T>::operator()(const uint32_t offset) const
{
    return *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(GetPhyAddr(offset)));
}

template <typename T>
template <typename U> [aicore] inline __attribute__((sync_alias)) LocalTensor<U> LocalTensor<T>::ReinterpretCast() const
{
    LocalTensor<U> output;
    output.address_.logicPos = static_cast<uint8_t>(this->GetPosition());
    output.address_.bufferHandle = this->GetBufferHandle();
    if constexpr (IsHalfByteDataType<PrimType>()) {
        output.address_.dataLen = this->GetSize() / INT4_TWO;




    } else {
        output.address_.dataLen = this->GetSize() * sizeof(PrimType);
    }
    output.address_.bufferAddr = this->address_.bufferAddr;
    if constexpr (is_tensorTrait_v<T> && is_tensorTrait_v<U>) {
        output.SetTensorTrait(this->GetTensorTrait());
    }
    return output;
}

template <typename T>
template <typename U> [aicore] inline __attribute__((inout_pipe("S")))
    void LocalTensor<T>::SetValue(const uint32_t index, const U value) const
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (GetPhyType(AscendC::TPosition(this->GetPosition())) == Hardware::UB) {
            return;
        }
    }
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        LocalTensor<uint8_t> tmp = this->ReinterpretCast<uint8_t>();
        uint8_t mask = (index % INT4_TWO == 0)? 0xf0 : 0xf;
        uint32_t idx = index / INT4_TWO;
        uint8_t val = tmp.GetValue(idx) & mask;
        uint8_t shift = (index % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        tmp.SetValue(idx, val + (value.storage << shift));
# 673 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    } else {
        *(reinterpret_cast<__attribute__((cce_unif_buff)) PrimType*>(static_cast<uint64_t>(this->address_.bufferAddr))
            + index) = static_cast<PrimType>(value);
    }
}

template <typename T> [aicore] inline LocalTensor<T> LocalTensor<T>::operator[](const uint32_t offset) const
{
    LocalTensor result = *this;
    if constexpr (IsHalfByteDataType<PrimType>()) {
        result.address_.dataLen -= (offset / INT4_TWO);
        result.address_.bufferAddr = result.address_.bufferAddr + offset / INT4_TWO;





    } else {
        result.address_.dataLen -= (offset * sizeof(PrimType));
        result.address_.bufferAddr = result.address_.bufferAddr + offset * sizeof(PrimType);
    }
    return result;
}

template <typename T>
template <typename U>
[[deprecated("NOTICE: SetAddrWithOffset has been deprecated and will be removed in the next version. "
    "Please do not use it!")]]
[aicore] inline void LocalTensor<T>::SetAddrWithOffset(LocalTensor<U> &src, uint32_t offset)
{
    this->address_ = src.address_;
    this->address_.bufferAddr += offset * sizeof(PrimT<U>);
}

template <typename T> [aicore] inline int32_t LocalTensor<T>::GetPosition() const
{
    return this->address_.logicPos;
}

template <typename T> [aicore] inline void LocalTensor<T>::SetSize(const uint32_t size)
{
# 722 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if constexpr (IsHalfByteDataType<PrimType>()) {
        this->address_.dataLen = size / INT4_TWO;




    } else {
        this->address_.dataLen = size * sizeof(PrimType);
    }
}
template <typename T>
[aicore] inline uint32_t LocalTensor<T>::GetSize() const
{
    if constexpr (IsHalfByteDataType<PrimType>()) {
        return this->address_.dataLen * INT4_TWO;




    } else {
        return this->address_.dataLen / sizeof(PrimType);
    }
}

template <typename T>
[[deprecated("NOTICE: GetLength has been deprecated and will be removed in the next version. Please do not use "
                "it!")]]
[aicore] inline uint32_t LocalTensor<T>::GetLength() const
{
    return this->address_.dataLen;
}

template <typename T>
[aicore] inline void LocalTensor<T>::SetBufferLen(uint32_t dataLen)
{
    this->address_.dataLen = dataLen;





}
template <typename T> [aicore] inline void LocalTensor<T>::SetUserTag(const TTagType tag)
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                       ;
    ptr->usertag = tag;
}
template <typename T> [aicore] inline TTagType LocalTensor<T>::GetUserTag() const
{
    auto ptr = reinterpret_cast<TBufType*>(this->address_.bufferHandle);

                                                                       ;
    return ptr->usertag;
}

template <typename T>
template <typename U>
[aicore] inline void LocalTensor<T>::CreateTensor(AscendC::TPosition pos, uint32_t addr, uint32_t tileSize)
{
# 812 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    this->address_.dataLen = SizeOfBits<U>::value * tileSize / SizeOfBits<uint8_t>::value;
    this->address_.bufferAddr = addr;
    this->address_.logicPos = static_cast<uint8_t>(pos);
}

template <typename T>
[aicore] inline LocalTensor<T>::LocalTensor(AscendC::TPosition pos, uint32_t addr, uint32_t tileSize)
{
    static_assert(!is_tensorTrait_v<T>, "currently not support TensorTrait type!!");
    CreateTensor<T>(pos, addr, tileSize);
}

template <typename T>
[aicore] inline LocalTensor<T>::LocalTensor(uint32_t addr)
{
    static_assert(is_tensorTrait_v<T>, "only support TensorTrait type Tensor!");
    static_assert((T::tPos != TPosition::GM) && (T::tPos != TPosition::MAX),
            "TensorTrait position should not be GM or MAX!");
    using LayoutType = GetLayoutType<T>;
    using type = typename T::LiteType;
    constexpr uint32_t tensorSize = LayoutType::size;
    static_assert((tensorSize * sizeof(type)) % ONE_BLK_SIZE == 0, "The size of localTensor must be align to 32Bytes");
    CreateTensor<typename T::LiteType>(T::tPos, addr, tensorSize);
}

template <typename T>
template <typename U>
[aicore] inline LocalTensor<T>::LocalTensor(uint32_t addr, const U& layout)
{
    static_assert(is_tensorTrait_v<T>, "only support TensorTrait type Tensor!");
    static_assert((T::tPos != TPosition::GM) && (T::tPos != TPosition::MAX),
            "TensorTrait position should not be GM or MAX!");
    uint32_t tensorSize = layout.GetSize();
    CreateTensor<typename T::LiteType>(T::tPos, addr, tensorSize);
}


template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideAdd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideSub<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideMul<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideDiv<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideOr<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void LocalTensor<T>::operator = (const SymbolOverrideAnd<T>& symbolOverride)
{
    symbolOverride.Process(*this);
}
# 882 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <typename T> [aicore] inline void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<float>& symbolOverride)
{
    symbolOverride.Process(*this);
}
template <typename T> [aicore] inline void
    LocalTensor<T>::operator = (const SymbolOverrideCompare<half>& symbolOverride)
{
    symbolOverride.Process(*this);
}


template <typename T> [aicore] inline SymbolOverrideAdd<T>
    LocalTensor<T>::operator + (const LocalTensor<T>& src1) const
{
    return SymbolOverrideAdd<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideSub<T>
    LocalTensor<T>::operator - (const LocalTensor<T>& src1) const
{
    return SymbolOverrideSub<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideMul<T>
    LocalTensor<T>::operator *(const LocalTensor<T>& src1) const
{
    return SymbolOverrideMul<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideDiv<T>
    LocalTensor<T>::operator / (const LocalTensor<T>& src1) const
{
    return SymbolOverrideDiv<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideOr<T>
    LocalTensor<T>::operator | (const LocalTensor<T>& src1) const
{
    return SymbolOverrideOr<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideAnd<T>
    LocalTensor<T>::operator & (const LocalTensor<T>& src1) const
{
    return SymbolOverrideAnd<T>(*this, src1);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator < (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::LT);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator > (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::GT);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator != (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::NE);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator == (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::EQ);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator <= (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::LE);
}
template <typename T> [aicore] inline SymbolOverrideCompare<T>
    LocalTensor<T>::operator >= (const LocalTensor<T>& src1) const
{
    return SymbolOverrideCompare<T>(*this, src1, CMPMODE::GE);
}
template <typename T> [aicore] inline void
    LocalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "only primitive type Tensor has shape info!");

        shapeInfo_ = shapeInfo;

}
template <typename T> [aicore] inline ShapeInfo LocalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "only primitive type Tensor has shape info!");

        return shapeInfo_;




}

template <typename T> [aicore] inline void
    GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer, uint64_t bufferSize)
{
# 987 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;

    bufferSize_ = bufferSize;
}

template <typename T>
[aicore] inline void GlobalTensor<T>::SetGlobalBuffer(__attribute__((cce_global)) typename GlobalTensor<T>::PrimType* buffer)
{
# 1011 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if (this->cacheMode_ == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = buffer;
    } else {
        this->address_ = L2CacheAlter<PrimType, CacheRwMode::RW>(buffer, cacheMode_);
    }
    this->oriAddress_ = buffer;




}

template <typename T> [aicore] inline
    const __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr() const
{



    return this->address_;

}

template <typename T> [aicore] inline
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType* GlobalTensor<T>::GetPhyAddr(const uint64_t offset) const
{
# 1051 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if constexpr (IsHalfByteDataType<PrimType>()) {

                                                                                                 ;
        return this->address_ + offset / INT4_TWO;
    } else {
        return this->address_ + offset;
    }

}
# 1117 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <typename T> [aicore] inline __attribute__((inout_pipe("S")))
    typename GlobalTensor<T>::PrimType GlobalTensor<T>::GetValue(const uint64_t offset) const
{
# 1140 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if constexpr (IsHalfByteDataType<PrimType>()) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;





        return static_cast<PrimType>((*addr) >> (INT4_BIT_NUM * (offset % INT4_TWO)));
    } else {





        return this->oriAddress_[offset];
    }

}
# 1178 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <typename T> [aicore] inline __attribute__((inout_pipe("S")))
    __attribute__((cce_global)) typename GlobalTensor<T>::PrimType& GlobalTensor<T>::operator()(const uint64_t offset) const

{
# 1191 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    return this->oriAddress_[offset];

}

template <typename T> [aicore] inline
    void GlobalTensor<T>::SetValue(const uint64_t offset, typename GlobalTensor<T>::PrimType value)
{
# 1222 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
    if constexpr (IsHalfByteDataType<PrimType>()) {
        __attribute__((cce_global)) uint8_t *addr = reinterpret_cast<__attribute__((cce_global)) uint8_t *>(this->oriAddress_) + offset / INT4_TWO;
        uint8_t mask = (offset % INT4_TWO == 0)? 0xf0 : 0xf;
# 1238 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
        uint8_t val = (*addr) & mask;
        uint8_t shift = (offset % INT4_TWO == 0)? 0 : INT4_BIT_NUM;
        *addr = val + (value.storage << shift);
    } else {
# 1254 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
        this->oriAddress_[offset] = value;
    }

}

template <typename T> [aicore] inline GlobalTensor<T>::GlobalTensor()
{




}
# 1277 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <typename T> [aicore] inline uint64_t GlobalTensor<T>::GetSize() const
{
    return bufferSize_;
}

template <typename T> [aicore] inline GlobalTensor<T> GlobalTensor<T>::operator[](const uint64_t offset) const
{
    GlobalTensor result = *this;
    if constexpr (IsHalfByteDataType<PrimType>()) {
        result.address_ = result.address_ + offset / INT4_TWO;
        result.oriAddress_ = result.oriAddress_ + offset / INT4_TWO;





    } else {
        result.address_ = result.address_ + offset;
        result.oriAddress_ = result.oriAddress_ + offset;
    }
    return result;
}

template <typename T> [aicore] inline void GlobalTensor<T>::SetShapeInfo(const ShapeInfo& shapeInfo)
{
    static_assert(IsSameType<T, PrimType>::value, "only primitive type Tensor has shape info!");

    shapeInfo_ = shapeInfo;

}

template <typename T> [aicore] inline ShapeInfo GlobalTensor<T>::GetShapeInfo() const
{
    static_assert(IsSameType<T, PrimType>::value, "only primitive type Tensor has shape info!");

    return shapeInfo_;




}

template <typename T>
template<CacheRwMode rwMode>
[aicore] inline void GlobalTensor<T>::SetL2CacheHint(CacheMode mode) {
    this->cacheMode_ = mode;



    if (mode == CacheMode::CACHE_MODE_NORMAL) {
        this->address_ = this->oriAddress_;
    } else {
        this->address_ = L2CacheAlter<PrimType, rwMode>(this->oriAddress_, mode);
    }




}
# 1353 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tensor_impl.h"
template <Hardware hard>
[aicore] inline LocalMemAllocator<hard>::LocalMemAllocator()
{
    static_assert((hard != Hardware::GM) && (hard != Hardware::MAX) && "illegal hardware position GM or MAX");





}

template <Hardware hard>
[aicore] inline uint32_t LocalMemAllocator<hard>::GetCurAddr() const
{
    return head_;
}

template <Hardware hard>
template <TPosition pos, class DataType, uint32_t tileSize>
[aicore] inline LocalTensor<DataType> LocalMemAllocator<hard>::Alloc()
{
    static_assert(!is_tensorTrait_v<DataType>, "currently not support TensorTrait type!");
    static_assert(GetPhyType(pos) == hard, "logic pos and hardware pos not matched.");
    LocalTensor<DataType> output(pos, head_, tileSize);
    head_ += SizeOfBits<DataType>::value * tileSize / SizeOfBits<uint8_t>::value;
    return output;
}

template <Hardware hard>
template <class DataType, uint32_t tileSize>
[aicore] inline LocalTensor<DataType> LocalMemAllocator<hard>::Alloc()
{
    static_assert(!is_tensorTrait_v<DataType>, "currently not support TensorTrait type!");
    LocalTensor<DataType> output(GetDefaultPosition(hard), head_, tileSize);
    head_ += SizeOfBits<DataType>::value * tileSize / SizeOfBits<uint8_t>::value;
    return output;
}

template <Hardware hard>
template <TPosition pos, class DataType>
[aicore] inline LocalTensor<DataType> LocalMemAllocator<hard>::Alloc(uint32_t tileSize)
{
    static_assert(!is_tensorTrait_v<DataType>, "currently not support TensorTrait type!");
    static_assert(GetPhyType(pos) == hard, "logic pos and hardware pos not matched.");
    LocalTensor<DataType> output(pos, head_, tileSize);
    head_ += SizeOfBits<DataType>::value * tileSize / SizeOfBits<uint8_t>::value;
    return output;
}

template <Hardware hard>
template <class DataType>
[aicore] inline LocalTensor<DataType> LocalMemAllocator<hard>::Alloc(uint32_t tileSize)
{
    static_assert(!is_tensorTrait_v<DataType>, "currently not support TensorTrait type!");
    LocalTensor<DataType> output(GetDefaultPosition(hard), head_, tileSize);
    head_ += SizeOfBits<DataType>::value * tileSize / SizeOfBits<uint8_t>::value;
    return output;
}
template <Hardware hard>
template <class TensorTraitType>
[aicore] inline LocalTensor<TensorTraitType> LocalMemAllocator<hard>::Alloc()
{
    static_assert(is_tensorTrait_v<TensorTraitType>, "only support TensorTrait type!");
    static_assert(GetPhyType(TensorTraitType::tPos) == hard, "logic pos and hardware pos not matched.");
    using liteType = typename TensorTraitType::LiteType;
    static_assert(SupportBytes<liteType, B8_BYTE_SIZE, B16_BYTE_SIZE, B32_BYTE_SIZE, B64_BYTE_SIZE>(), "Only supoort B8/B16/B32/B64 datatype");
    LocalTensor<TensorTraitType> tensorOut(head_);
    head_ += SizeOfBits<liteType>::value * tensorOut.GetSize() / SizeOfBits<uint8_t>::value;
    return tensorOut;
}

template <Hardware hard>
template <class TensorTraitType, typename LayoutType>
[aicore] inline LocalTensor<TensorTraitType> LocalMemAllocator<hard>::Alloc(const LayoutType& layout)
{
    static_assert(is_tensorTrait_v<TensorTraitType>, "only support TensorTrait type!");
    static_assert(GetPhyType(TensorTraitType::tPos) == hard, "logic pos and hardware pos not matched.");
    using liteType = typename TensorTraitType::LiteType;
    static_assert(SupportBytes<liteType, B8_BYTE_SIZE, B16_BYTE_SIZE, B32_BYTE_SIZE, B64_BYTE_SIZE>(), "Only supoort B8/B16/B32/B64 datatype");
    LocalTensor<TensorTraitType> tensorOut(head_, layout);
    head_ += SizeOfBits<liteType>::value * tensorOut.GetSize() / SizeOfBits<uint8_t>::value;
    return tensorOut;
}
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h" 2





# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_common_impl.h"
namespace AscendC {
[aicore] inline int64_t GetSubBlockIdxImpl()
{






    return get_subblockid();

}

[aicore] inline int64_t GetTaskRationImpl()
{
    if constexpr(g_coreType == AscendC::AIC) {
        return 1;
    } else {



        return get_subblockdim();

    }
}

[aicore] inline int64_t GetBlockIdxImpl()
{
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_common_impl.h"
    if constexpr(g_coreType == AscendC::AIV) {
        return get_block_idx() * GetTaskRationImpl() + get_subblockid();
    } else {
        return get_block_idx();
    }

}

[[deprecated(
    "NOTICE: SetSysWorkSpace has been deprecated and will be removed in the next version.")]]
[aicore] inline void SetSysWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{




    if (g_sysWorkspaceReserved == nullptr) {
        g_sysWorkspaceReserved = workspace;
    }

}

[aicore] inline void SetSysWorkspaceForce(__attribute__((cce_global)) uint8_t* workspace)
{




    g_sysWorkspaceReserved = workspace;

}

[aicore] inline __attribute__((cce_global)) uint8_t* GetUserWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{





    (void)(workspace);
    return g_sysWorkspaceReserved + RESERVED_WORKSPACE;

}

template <atomic_type_t type, atomic_op_t op>
[aicore] inline void SetStoreAtomicConfigImpl()
{
    set_st_atomic_cfg(type, op);
}

[aicore] inline int64_t GetStoreAtomicConfigImpl()
{
    return get_st_atomic_cfg();
}

[aicore] inline void GetStoreAtomicConfigImpl(uint16_t &atomicType, uint16_t &atomicOp)
{
    int64_t stAtomic = get_st_atomic_cfg();
    constexpr uint64_t typeMask = 0x7;
    constexpr uint64_t opBit = 4;
    constexpr uint64_t opMask = 0x3;
    atomicType = (static_cast<uint64_t>(stAtomic) & typeMask);
    atomicOp = ((static_cast<uint64_t>(stAtomic) >> opBit) & opMask);
}

template <typename T>
[aicore] inline void DataCachePreloadImpl(const GlobalTensor<uint64_t> &src, const T cacheOffset)
{
    if constexpr ((IsSameType<T, int16_t>::value) || (IsSameType<T, int64_t>::value)) {
        dc_preload((__attribute__((cce_global)) uint64_t *)src.GetPhyAddr(), cacheOffset);
    } else {
                                                              ;
    }
}

[aicore] inline void PreLoadImpl(void *pc, const int64_t preFetchLen)
{
    preload(pc, preFetchLen);
}

[aicore] inline int64_t GetICachePreloadStatusImpl()
{
    return get_icache_prl_st();
}

[aicore] inline void CheckLocalMemoryIAImpl(const CheckLocalMemoryIAParam& checkParams)
{
    uint64_t config = 0;
    config = config | (static_cast<uint64_t>(checkParams.startAddr) << 48);
    config = config | (static_cast<uint64_t>(checkParams.endAddr) << 32);
    config = config | (static_cast<uint64_t>(checkParams.isScalarRead) << 31);
    config = config | (static_cast<uint64_t>(checkParams.isScalarWrite) << 30);
    config = config | (static_cast<uint64_t>(checkParams.isVectorRead) << 29);
    config = config | (static_cast<uint64_t>(checkParams.isVectorWrite) << 28);
    config = config | (static_cast<uint64_t>(checkParams.isMteRead) << 27);
    config = config | (static_cast<uint64_t>(checkParams.isMteWrite) << 26);
    config = config | (checkParams.reserved << 1);
    config = config | (static_cast<uint8_t>(checkParams.isEnable));
    if (checkParams.enableBit == SET_DATA_EXP_ZERO) {
        set_data_exp_0(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_ONE) {
        set_data_exp_1(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_TWO) {
        set_data_exp_2(config);
    } else if (checkParams.enableBit == SET_DATA_EXP_THREE) {
        set_data_exp_3(config);
    } else {
                                                      ;
    }
}

[aicore] inline void PreLoad(const int64_t preFetchLen)
{
    int64_t pc = get_pc() & 0xFFFFFFFFFFFF;
    PreLoadImpl(reinterpret_cast<void *>(pc), preFetchLen);
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_set_atomic_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_set_atomic_impl.h"
namespace AscendC {

template <typename T>
[aicore] inline void SetAtomicTypeImpl()
{


                                 ;
    if constexpr(IsSameType<T, float>::value) {
        set_atomic_f32();
    } else if constexpr(IsSameType<T, half>::value) {
        set_atomic_f16();
    } else if constexpr(IsSameType<T, int16_t>::value) {
        set_atomic_s16();
    } else if constexpr(IsSameType<T, int32_t>::value) {
        set_atomic_s32();
    } else if constexpr(IsSameType<T, int8_t>::value) {
        set_atomic_s8();
    } else if constexpr(IsSameType<T, bfloat16_t>::value) {
        set_atomic_bf16();
    }
}


[aicore] inline void SetAtomicNoneImpl()
{
    set_atomic_none();
}


template <typename T>
[aicore] inline void SetAtomicAddImpl()
{


                                 ;
    set_atomic_add();
    SetAtomicTypeImpl<T>();
}


template <typename T>
[aicore] inline void SetAtomicMaxImpl()
{


                                 ;
    set_atomic_max();
    SetAtomicTypeImpl<T>();
}


template <typename T>
[aicore] inline void SetAtomicMinImpl()
{


                                 ;
    set_atomic_min();
    SetAtomicTypeImpl<T>();
}

}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_log.h" 1
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h" 2

namespace AscendC {


[aicore] inline void Barrier()
{



    __asm__ __volatile__("");

}

enum class KFC_Enum : uint16_t {
    SERVICE_ID_MASK = 0xFF00,
    SERVICE_ID_SCM = 0x0100,
    SCMFUN_GM2L1,
    SCMFUN_GM2L1ND2NZ,
    SERVICE_ID_MATMUL = 0x0300,
    MMFUN_MASK = 0x0380,
    MMFUN_ITERATE = 0x0380,
    MMFUN_ITERATE_ALL = 0x0381,
    MMFUN_INIT = 0x0301,
    MMFUN_GET_TENSOR_C,
    MMFUN_ITERATE_ALL_RESP,
    MMFUN_GET_TENSOR_C_RESP,
    MMFUN_GET_OFFSET_C,
    MMFUN_GET_OFFSET_C_RESP,
    MMFUN_SET_ORG_SHAPE,
    MMFUN_SET_HF32,
    MMFUN_SET_USER_DEF_INFO,
    MMFUN_ITERATE_BATCH_ALL,
    MMFUN_ITERATE_BATCH_ALL_RESP,
    MMFUN_ITERATE_N_BATCH_ALL,
    MMFUN_ITERATE_N_BATCH_ALL_RESP,
    MMFUN_END,
    CONVFUNC_ITERATE,
    CONVFUNC_ITERATE_ALL,
    CONVFUNC_GET_TENSOR_C,
    CONVFUNC_END,
    SERVICE_QUIT = 0xfd00,
    SERVICE_BALANCE = 0xfe00,
    SERVICE_ID_NONE = 0xff00
};

enum class MSG_STATE : uint8_t {
    STATE_INVALID,
    STATE_SET,
};






constexpr int32_t MIX_NUM = 2;

constexpr int32_t MAX_BLOCK_AIV_NUM = 2;
constexpr int32_t MIX_COEFFICIENT = 1;
constexpr int32_t MAX_MATMUL_OBJ = 8;
constexpr int MAX_AIV_NUM = 50;
constexpr int MAX_AIC_NUM = 25;
constexpr int ALIGN_SIZE = 32;
constexpr int BIDIRECTION_NUM = 2;
constexpr bool KFC_APPLY_MSG = true;
constexpr uint64_t INC_PROCESS_CHECK = 14;
constexpr uint64_t WORKSPACE_UB_SIZE = TOTAL_UB_SIZE;
constexpr int32_t MAX_GROUP_ID = 32;
constexpr int32_t MM_CNT_MAX = 1024;
constexpr int32_t QUIT_CNT = 4;
constexpr int32_t MAX_SYNC_COUNT = 100000000;
constexpr int32_t MMCNT_L1_RESERVERD_SIZE = 64;
constexpr uint16_t KFC_SYNC_ID = 15;

struct TilingInfo {
    __attribute__((cce_global)) uint8_t* tilingAddr;
};

struct MatmulOrgShape {
    uint32_t orgM;
    uint32_t orgN;
    uint32_t orgKa;
    uint32_t orgKb;
    uint32_t orgKc;
};
# 126 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h"
struct MatmulConfigParams {

    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t isTransA : 1;
    uint32_t isTransB : 1;
    uint32_t enPartialSum : 1;
    uint32_t setTail : 1;
    uint32_t setTensorA : 1;
    uint32_t setTensorB : 1;
    uint32_t setTensorBias : 1;
    uint32_t setClearBias : 1;
    uint32_t cIsTscm : 1;
    uint32_t isFirstIter : 1;
    uint32_t sync : 1;
    uint32_t enHF32 : 1;
    uint32_t hf32TransMode : 1;
    uint32_t setQuant : 1;
    uint32_t setBatch : 1;
    uint32_t waitIterateAll : 1;
    uint32_t waitIterateBatch : 1;
    uint32_t iterateFakeMsg : 1;

    uint32_t singleM;
    uint32_t singleN;
    uint32_t singleK;
    uint32_t sizeAmatrix;
    uint32_t sizeBmatrix;

    uint64_t aAddr;
    uint64_t bAddr;
    uint64_t cAddr;
    uint64_t biasAddr;
    uint64_t quantAddr;
    uint32_t quantSize;
    uint32_t quantMode;
    uint64_t quantScalar;
    uint32_t batchA;
    uint32_t batchB;
    uint32_t matrixStrideA;
    uint32_t matrixStrideB;
    uint32_t matrixStrideC;
    uint32_t batchLoop;
    uint32_t counterId;
    uint32_t reserved0;
    uint64_t dataPtr;
};

struct Conv3DBpInputConfigParams {
    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t setTensorWeight : 1;
    uint32_t sync : 1;
    uint32_t isFirstIter : 1;
    uint32_t setTensorFmap : 1;
    uint32_t setTensorOutBackprop : 1;
    uint32_t setSingleShape : 1;
    uint32_t setStartIdx : 1;
    uint32_t enPartialSum : 1;

    uint32_t singleShapeN;
    uint32_t singleShapeD;
    uint32_t curDinStartIdx;

    uint32_t singleShapeM;
    uint32_t singleShapeK;

    uint64_t weightAddr;
    uint64_t outBackpropAddr;
    uint64_t outputAddr;

    int32_t curHoStartIdx;
    uint32_t res;
    uint32_t res1[16];
};

struct Conv3DBpFilterConfigParams {
    uint32_t enAtomic : 8;
    uint32_t enSequentialWrite : 1;
    uint32_t setTensorFmap : 1;
    uint32_t setTensorOutBackprop : 1;
    uint32_t setSingleShape : 1;
    uint32_t sync : 1;
    uint32_t isFirstIter : 1;
    uint32_t waitIterateAll : 1;
    uint32_t enPartialSum : 1;

    uint32_t singleShapeM;
    uint32_t singleShapeN;
    uint32_t singleShapeK;

    uint64_t fmapAddr;
    uint64_t outBackpropAddr;
    uint64_t outputAddr;

    uint32_t curHoStartIdx;
    uint32_t fmapSize;
    uint32_t outBackpropSize;
    uint32_t res;

    uint32_t res1[16];
};

struct Conv3DForwardConfigParams {
    uint32_t enAtomic: 8;
    uint32_t enSequentialWrite: 1;
    uint32_t enSetTensorFmap: 1;
    uint32_t enSetTensorWeight: 1;
    uint32_t enSetTensorBias: 1;
    uint32_t sync: 1;
    uint32_t enSetSingleOutputShape: 1;
    uint32_t enSetFmapStartPosition: 1;
    uint32_t waitIterateAll: 1;
    uint32_t enPartialSum: 1;
    uint32_t fmapSize;

    uint64_t fmapAddr;
    uint64_t weightAddr;
    uint64_t biasAddr;
    uint64_t outputAddr;

    uint32_t weightSize;
    uint32_t biasSize;
    uint32_t singleCoreBatch;
    uint32_t singleCo;
    uint32_t singleDo;
    uint32_t singleCoreM;
    uint32_t singleGroupOpt;
    uint32_t diStartPos;
    uint32_t mStartPos;
    uint32_t ciStartPos;
    uint32_t res[10];
};

struct MatmulUserDefInfo {
    uint64_t tilingPtr;
};

constexpr uint16_t KFC_MSG_BYTE_OFFSET = 16;

[aicore] inline uint16_t KfcMsgGetEvtCnt(uint32_t flag)
{
    return flag & 0x00007fff;
}

[aicore] inline uint16_t KfcMsgGetInstID(uint32_t flag)
{
    return flag & 0x000000ff;
}
[aicore] inline KFC_Enum KfcMsgGetFunID(uint32_t flag)
{
    return static_cast<KFC_Enum>((flag & 0xffff0000) >> KFC_MSG_BYTE_OFFSET);
}
[aicore] inline uint32_t KfcMsgGetState(uint32_t flag)
{
    return (flag & 0x00008000);
}
[aicore] inline uint32_t KfcMsgMakeFlag(KFC_Enum funID, uint16_t instID)
{
    return (((static_cast<uint16_t>(funID) << KFC_MSG_BYTE_OFFSET) + 0x8000) + (instID));
}


struct KfcMsg {
    uint32_t head = 0;
    int32_t ubAddr = -1;
    union {
        uint8_t buffer[120];
        TilingInfo tilingInfo;
        MatmulConfigParams body;
        MatmulOrgShape orgShape;
        MatmulUserDefInfo userDefInfo;
        Conv3DBpInputConfigParams convBpInputBody;
        Conv3DBpFilterConfigParams convBpFilterBody;
        Conv3DForwardConfigParams convForwardBody;
    };
};
struct MsgUBAvalied {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};
struct MsgMatmulCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct QuitCnt {
    int32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MmTaskCnt {
    uint32_t head;
    uint32_t res;
    uint8_t buffer[56];
};

struct MsgGroupSync {
    int32_t syncCount;
    uint8_t res[28];
    uint32_t allNumber;
    uint8_t buffer[28];
};

struct MsgGroupSyncAux {

    int32_t curNumber;
    uint8_t res[28];
    uint32_t idField;
    uint8_t buffer[28];
};

[aicore] inline constexpr int AlignTo32(int size)
{
    return (size + ALIGN_SIZE - 1) / ALIGN_SIZE * ALIGN_SIZE;
}

struct SysWorkspaceDesc {
    KfcMsg kfcMsg[MAX_AIV_NUM * BIDIRECTION_NUM * 64 * MIX_COEFFICIENT];
    MsgMatmulCnt cntMsg[MAX_AIV_NUM * MIX_COEFFICIENT][MAX_MATMUL_OBJ];
    MsgUBAvalied ubMsg[MAX_AIV_NUM];
    uint8_t ubMap[MAX_AIV_NUM][WORKSPACE_UB_SIZE];
    QuitCnt quitCnt[QUIT_CNT];
    MmTaskCnt mmTaskCnt[MM_CNT_MAX];
    MsgGroupSync groupSyncMsg[MAX_GROUP_ID];
    MsgGroupSyncAux groupSyncAuxMsg[MAX_GROUP_ID];
};

[aicore] inline void ClearWorkspaceImpl(__attribute__((cce_global)) uint8_t* workspace)
{
    constexpr uint32_t size = BIDIRECTION_NUM * 64 * AlignTo32(sizeof(KfcMsg)) * MIX_NUM;
    constexpr uint32_t sizeUbmsg = MIX_NUM * AlignTo32(sizeof(MsgUBAvalied));
    constexpr uint32_t offsetUbMsg = MAX_AIV_NUM * BIDIRECTION_NUM * 64 *
        MIX_COEFFICIENT * AlignTo32(sizeof(KfcMsg)) + MAX_AIV_NUM * MIX_COEFFICIENT *
        MAX_MATMUL_OBJ * AlignTo32(sizeof(MsgMatmulCnt));
    constexpr uint32_t block = size / 2048;
    uint32_t ubOffset11 = 0;
    uint32_t msgOffset11 = 0;
    if constexpr (MIX_NUM == 1) {
        msgOffset11 = BIDIRECTION_NUM * 64 * AlignTo32(sizeof(KfcMsg));
        ubOffset11 = AlignTo32(sizeof(MsgUBAvalied));
    }
    __attribute__((cce_global)) uint8_t* msgStartAddr = (__attribute__((cce_global)) uint8_t*)(workspace + (size + msgOffset11) * GetBlockIdxImpl());
    __attribute__((cce_global)) uint8_t* ubMsgStartAddr =
        (__attribute__((cce_global)) uint8_t*)(workspace + offsetUbMsg + (sizeUbmsg + ubOffset11) * GetBlockIdxImpl());
    create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)(0), 0x10040, 0);
    SetFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    WaitFlag<HardEvent::MTE2_MTE3>(EVENT_ID0);
    for (size_t i = 0; i < block; i++) {
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)(msgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, 64, 1, 1);
        msgStartAddr += 2048;
    }
    copy_cbuf_to_gm((__attribute__((cce_global)) void*)(ubMsgStartAddr), (__attribute__((cce_cube_buff)) void*)(0), 0, 1, sizeUbmsg / 32, 1, 1);
    PipeBarrier<PIPE_ALL>();
}
[aicore] inline __attribute__((cce_global)) uint8_t* GetMsgHead(__attribute__((cce_global)) uint8_t* workspace, int i)
{
# 394 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h"
                                                                                                                ;

                                                                                 ;

    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);

    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr->kfcMsg[flatBlockID * BIDIRECTION_NUM * 64]);
}

[aicore] inline __attribute__((cce_global)) uint8_t* GetUBMapAddr(__attribute__((cce_global)) uint8_t* workspace, int i = 0)
{







                                                                                 ;

    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(ptr->ubMap[flatBlockID]);
}

[aicore] inline __attribute__((cce_global)) uint8_t* GetMatmulIncAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t flatBlockID, uint32_t instID)
{


                                                                                                                 ;

                                                                                 ;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->cntMsg[flatBlockID][instID]));
}

[aicore] inline __attribute__((cce_global)) uint8_t* GetUBAvaliedAddr(__attribute__((cce_global)) uint8_t* workspace, uint32_t i = 0)
{
                                ;
    auto flatBlockID = get_block_idx() * MAX_BLOCK_AIV_NUM + i;
    auto ptr = reinterpret_cast<__attribute__((cce_global)) struct SysWorkspaceDesc *>(workspace);
    return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&(ptr->ubMsg[flatBlockID]));
}

[aicore] inline __attribute__((cce_global)) KfcMsg *AllocMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgSendHead, uint8_t &msgSendPos, __attribute__((cce_global)) KfcMsg *&msgSendStart)
{
    auto msg = msgSendHead;

                                                                                   ;

                                                                                    ;
    if constexpr (KFC_APPLY_MSG) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while (static_cast<bool>(KfcMsgGetState(msg->head))) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            Barrier();
        }
    }

                                                                                                                      ;
    msgSendPos++;
    if (msgSendPos >= 64) {
        msgSendPos = 0;
        msgSendHead = msgSendStart;
    } else {
        msgSendHead++;
    }
    return msg;
}

[aicore] inline __attribute__((cce_global)) KfcMsg *RcvMessageImpl(
    __attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos, __attribute__((cce_global)) KfcMsg *&msgRcvStart)
{



                                                                                  ;

                                                                                   ;


      ;
    __attribute__((cce_global)) KfcMsg* msg = msgRcvHead;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();

    dc_preload((__attribute__((cce_global)) uint64_t*)msg, int64_t(0));
# 496 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm.h"
    if (!(static_cast<bool>(KfcMsgGetState(msg->head)))) {
        return nullptr;
    }
    msgRcvPos++;
    if (msgRcvPos >= 64) {
        msgRcvPos = 0;
        msgRcvHead = msgRcvStart;
    } else {
        msgRcvHead++;
    }
    return msg;
}

[aicore] inline void FreeMessageImpl(__attribute__((cce_global)) KfcMsg *msg)
{

                                                                           ;
    __asm__ __volatile__("" ::: "memory");
    *(reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg)) = 0;
    Barrier();
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
    Barrier();
}

[aicore] inline void RollBackMsgImpl(__attribute__((cce_global)) KfcMsg *&msgRcvHead, uint8_t &msgRcvPos)
{
    if (msgRcvPos == 0) {
        msgRcvPos = 64;
        msgRcvHead = msgRcvHead + 64 -1;
    } else {
        msgRcvPos--;
        msgRcvHead--;
    }
}

}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h" 2
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
namespace AscendC {

template <int depth>
struct TBufHandleAux {
    using T = TBufHandle[depth];
};

template <>
struct TBufHandleAux<1> {
    using T = TBufHandle;
};
constexpr TEventID INVALID_TEVENTID = (static_cast<TEventID>(-1));
# 108 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
struct TEventPool {
    uint64_t eventOccupy;
};

struct TPipeBufPool {
    uint32_t maxAddr;
};
# 123 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
struct TShareBuf {
    enum class ShareHard : uint8_t {
        L1 = 0,
        L0C = 1,
        UB = 2,
        MAX,
    };
    int32_t start[static_cast<uint8_t>(ShareHard::MAX)];
    int32_t maxAddr[static_cast<uint8_t>(ShareHard::MAX)];
                                                                     ;
};

struct SpmInfo {
    uint64_t spmAddr;
    int32_t spmBuffSize;
    uint8_t spmBufType;
};

struct TPipeImpl {
    struct TEventPool eventPool_[EVENT_NUM];
    struct TPipeBufPool bufPool_[static_cast<uint8_t>(Hardware::MAX)];



    struct TBufType buf_[64];
    TShareBuf shareBufPool_;
    SpmInfo spmInfo_;

    uint32_t tscmBufferPtr_;






    uint8_t curBufSize_;
    bool isDestroy;
};

constexpr uint32_t defaultBufIDSize = 4;

template <uint32_t bufIDSize = defaultBufIDSize>
struct TBufPoolImpl {
    struct TBufType buf_[bufIDSize];
    uint32_t startAddr_;
    uint32_t maxAddr_;
    uint32_t maxLen_;
    uint8_t curBufSize_;




    uint8_t isReset_;
};

class TPipeBase {
public:
    [aicore] inline void InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens, uint8_t subBlockIdx);
    [aicore] inline void InitShareBufEnd();

protected:
    TPipeImpl g_tpipeImpl;
    [aicore] inline void AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                            uint8_t subBlockIdx);
};

[aicore] inline void TPipeBase::InitShareBufStart(uint32_t mode, uint32_t* shareLens, uint32_t lens,
                                                    uint8_t subBlockIdx)
{





    (void)(lens);



                                                                                                        ;
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1), Hardware::L1, subBlockIdx);
    AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C), Hardware::L0C, subBlockIdx);



    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;




    return;
}

[aicore] inline void TPipeBase::InitShareBufEnd()
{

    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];
# 234 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
    return;
}

[aicore] inline void TPipeBase::AuxShareBufStart(uint32_t mode, uint32_t* shareLens, uint8_t pos, Hardware hard,
                                                   uint8_t subBlockIdx)
{
    uint8_t hardU8 = static_cast<uint8_t>(hard);
    if (__builtin_expect(!!(g_tpipeImpl.shareBufPool_.start[pos] == -1), 0)) {

        g_tpipeImpl.shareBufPool_.start[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.shareBufPool_.maxAddr[pos] = g_tpipeImpl.shareBufPool_.start[pos] + shareLens[pos];
                                                                          ;
    } else {


                                                                              ;

        g_tpipeImpl.shareBufPool_.maxAddr[pos] = this->g_tpipeImpl.bufPool_[hardU8].maxAddr;
        g_tpipeImpl.bufPool_[hardU8].maxAddr = g_tpipeImpl.shareBufPool_.start[pos];
    }

    if (mode == 1 && subBlockIdx == 1) {
        this->g_tpipeImpl.bufPool_[hardU8].maxAddr += shareLens[pos] / HALF_FACTOR;
    }






}
# 307 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_base.h"
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h" 2
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h"
namespace AscendC {
class TPipe;
template <TPosition src, TPosition dst, int32_t depth, auto mask = 0> class TQueBind {
public:
    [aicore] inline TQueBind();
    [aicore] inline void FreeBuffer(TBufHandle buf);
    [aicore] inline TBuffAddr GetBufferAddr(TBufHandle buf);
    template <typename T> [aicore] inline __attribute__((sync_alias)) LocalTensor<T> AllocTensor();
    template <typename T> [aicore] inline __attribute__((sync_alias)) void AllocTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] inline void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] inline bool EnQue(const LocalTensor<T>& tensor);
    [aicore] inline bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos, typename T>
    [aicore] inline bool EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] inline void DeQue(LocalTensor<T>& tensor);
    template <typename T> [aicore] inline LocalTensor<T> DeQue();
    [aicore] inline TBufHandle DeQue();
    template <TPosition srcUserPos, TPosition dstUserPos, typename T> [aicore] inline LocalTensor<T> DeQue();
    [aicore] inline bool VacantInQue();
    [aicore] inline bool HasTensorInQue();
    [aicore] inline int32_t GetTensorCountInQue();
    [aicore] inline bool HasIdleBuffer();
    [aicore] inline void FreeAllEvent();
    template <typename T> [aicore] inline TBufState GetState(const LocalTensor<T>& tensor) const;
    [aicore] inline void InitStartBufHandle(TBufHandle startBufhandle, uint8_t num, uint32_t len);
    template <typename T>
    [aicore] inline void InitBufHandle(T* bufPool, uint32_t index, TBufHandle bufhandle,
        uint32_t curPoolAddr, uint32_t len);
protected:
    static constexpr TQueConfig config = GetTQueConfig(mask);
    static constexpr bool nd2nz = config.nd2nz;
    static constexpr bool nz2nd = config.nz2nd;
    static constexpr bool scmBlockGroup = config.scmBlockGroup;
    static constexpr bool enableLoopQueue = config.enableLoopQueue;
    static constexpr TPosition srcPosition = src;
    static constexpr TPosition dstPosition = dst;
    static constexpr Hardware srcHardType = GetPhyType(src);
    static constexpr Hardware dstHardType = GetPhyType(dst);
    static constexpr HardEvent enQueEvt = GetQueEvt(srcHardType, dstHardType, true, nd2nz, nz2nd);
    static constexpr HardEvent freeBufEvt = GetQueEvt(srcHardType, dstHardType, false, nd2nz, nz2nd);
    static constexpr int32_t queDepth = depth;
    union {
        uint64_t value;
        struct {
            uint8_t bufNum = 0;
            uint8_t usedCount;
            uint16_t head;
            uint16_t tail;
            uint8_t bufUsedCount;
            uint8_t bufCursor;
        };
    };
    typename TBufHandleAux<depth>::T que_;
    struct TBufType* bufStart;
                               ;
    friend class TPipe;
    template <TPosition pos, int32_t d, auto m> friend class TQue;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;



private:
    [aicore] inline void SetTBufPoolHandle(uint64_t bufPoolHandle);
    template <typename T> [aicore] inline LocalTensor<T> Buf2Tensor(TBufHandle buf);
    [aicore] inline TBufState GetState(const TBufHandle& handle) const;
    static constexpr bool isTQue = true;
    [aicore] inline TBufHandle AllocBuffer();
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] inline bool EnQue(TBufHandle buf);
    template <TPosition srcUserPos, TPosition dstUserPos> [aicore] inline TBufHandle DeQue();
};





template <TPosition pos, int32_t depth, auto mask = 0>
class TQue : public TQueBind<GetBufferLogicPos(pos, true), GetBufferLogicPos(pos, false), depth, mask> {
public:
    [aicore] inline TQue() = default;
private:
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = true;
};

template <TPosition pos = TPosition::LCM> class TBuf : public TQueBind<pos, pos, 0, 0> {
public:
    [aicore] inline TBuf() = default;
    template <typename T> [aicore] inline LocalTensor<T> Get();
    template <typename T> [aicore] inline LocalTensor<T> Get(uint32_t len);
    template <typename T> [aicore] inline LocalTensor<T> GetWithOffset(uint32_t size, uint32_t bufOffset);

    template <typename T> [aicore] inline void EnQue(const LocalTensor<T>& tensor);
    template <typename T> [aicore] inline LocalTensor<T> DeQue();
    template <typename T> [aicore] inline LocalTensor<T> AllocTensor();
    template <typename T> [aicore] inline void FreeTensor(LocalTensor<T>& tensor);
    template <typename T> [aicore] inline TBufState GetState(const LocalTensor<T>& tensor) const;
    [aicore] inline bool EnQue(TBufHandle buf);
    [aicore] inline TBufHandle DeQue();
    [aicore] inline void FreeBuffer(TBufHandle buf);
    [aicore] inline TBuffAddr GetBufferAddr(TBufHandle buf);
    [aicore] inline void InitStartBufHandle(TBufHandle startBufhandle, uint8_t num, uint32_t len);

private:
    [aicore] inline TBufHandle Get();
    [aicore] inline TBufHandle Get(uint32_t len);
    [aicore] inline uint32_t GetBufLen() const;
    [aicore] inline void SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn);
    template <TPosition posPopBuffer>
    friend [aicore] inline bool PopStackBuffer(TBuf<posPopBuffer> &popBuffer, TBufType &bufStart);
    [aicore] inline TBufHandle AllocBuffer();

private:
    struct TBufType* bufStart;
    uint32_t bufLen;
    uint32_t offset;
    friend class TPipe;
    template<TPosition bufPos, uint32_t bufIDSize> friend class TBufPool;
    static constexpr bool isTQue = false;
};

template <TPosition pos, uint32_t bufIDSize = defaultBufIDSize>
class TBufPool {
public:
    static constexpr TPosition poolPos = pos;
public:
    [aicore] inline TBufPool();
    [aicore] inline ~TBufPool();
    template <class T> [aicore] inline bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <TPosition bufPos> [aicore] inline bool InitBuffer(TBuf<bufPos>& buf, uint32_t len);
    template <class T, class U> [aicore] inline bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <class T> [aicore] inline bool InitBufPool(T& bufPool, uint32_t len);
    [aicore] inline void Reset();
protected:
    TBufPoolImpl<bufIDSize> tBufPoolImpl;
private:
    [aicore] inline void Init();
    [aicore] inline void ResetPool();
private:
    friend class TPipe;
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition bufPos, int32_t depth, auto mask> friend class TQue;
    template <TPosition bufPos> friend class TBuf;
    static constexpr bool isTbufPool = true;
};
# 234 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h"
class TPipe : public TPipeBase {
public:
    [aicore] inline TPipe();
    [aicore] inline ~TPipe();
    [aicore] inline void Init();
    template <class T> [aicore] inline bool InitBuffer(T& que, uint8_t num, uint32_t len);
    template <class T, class U, class V, class... Addrs>
    [aicore] inline bool InitBuffer(T& que, const Std::tuple<U, V>& addr0, const Addrs&... addrs);
    template <TPosition pos> [aicore] inline bool InitBuffer(TBuf<pos>& buf, uint32_t len);
    template <class T> [aicore] inline bool InitBufPool(T& bufPool, uint32_t len);
    template <class T, class U> [aicore] inline bool InitBufPool(T& bufPool, uint32_t len, U& shareBuf);
    template <HardEvent evt> [aicore] inline TEventID AllocEventID();
    template <HardEvent evt> [aicore] inline void ReleaseEventID(TEventID id);
    template <HardEvent evt> [aicore] inline TEventID FetchEventID();
    [aicore] inline TEventID FetchEventID(HardEvent evt);
    template <TPosition pos, typename T>
    [aicore] inline LocalTensor<T> GetAbsAddr(int32_t offset, int32_t size) const;
    template <TPosition pos> [aicore] inline TBuffAddr GetAbsAddr(int32_t offset, int32_t len) const;
# 264 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_tpipe.h"
    template <typename T>
    [aicore] inline void InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize);
    [aicore] inline void InitSpmBuffer(const int32_t bufferSize);
    template <typename T>
    [aicore] inline void WriteSpmBuffer(const LocalTensor<T>& writeBuffer, const DataCopyParams& copyParams,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] inline void ReadSpmBuffer(const LocalTensor<T>& readBuffer, const DataCopyParams& copyParams,
        int32_t readOffset = 0);
    template <typename T>
    [aicore] inline void WriteSpmBuffer(const LocalTensor<T>& writeBuffer, const int32_t writeSize,
        int32_t writeOffset = 0);
    template <typename T>
    [aicore] inline void ReadSpmBuffer(const LocalTensor<T>& readBuffer, const int32_t readSize,
        int32_t readOffset = 0);
    [aicore] inline void Destroy();
    [aicore] inline void Reset();





protected:
    template <TPosition src, TPosition dst, int32_t depth, auto mask> friend class TQueBind;
    template <TPosition pos, int32_t depth, auto mask> friend class TQue;
    template <TPosition pos> friend class TBuf;
    template<TPosition pos, uint32_t bufIDSize> friend class TBufPool;
    template <TPosition pos> friend [aicore] inline bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart);
    template <typename T, TPosition pos> friend [aicore] inline bool PopStackBuffer(LocalTensor<T>& popBuffer);




private:



    friend [aicore] inline void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
        uint32_t lens, uint8_t subBlockIdx);
    friend [aicore] inline void InitShareBufEnd(TPipe* tpipe);
    [aicore] inline void InitSocState() const;
    [aicore] inline void ResetPool();
    template <class T> [aicore] inline bool TscmInitBuffer(T& que, uint8_t num, uint32_t len);
    template <class T, class First, class... Rest>
    [aicore] inline void AllocAddrs(TBufType* ptr, const First& addr, const Rest&... addrs);



    template <TPosition pos> [aicore] inline uint64_t GetQueueEndAddress();
};

template <TPosition pos, int32_t depth = 1, auto mask = 0>
using TSCM = TQueBind<pos, TPosition::TSCM, depth, mask>;
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquebind_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquebind_impl.h"
namespace AscendC {

[aicore] inline constexpr bool IsAivTscm(TPosition src, TPosition dst)
{

    if (GetPosition(src, dst) == TPosition::TSCM) {
        return true;
    }




    return false;
}


template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline TQueBind<src, dst, depth, mask>::TQueBind()
{



}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline void TQueBind<src, dst, depth, mask>::InitStartBufHandle(
    TBufHandle startBufhandle, uint8_t num, uint32_t len)
{
    static_assert(isTQue, "InitTQueAddr only support TQue class");
    auto ptr = reinterpret_cast<TBufType*>(startBufhandle);
    this->value = num;
    this->bufStart = ptr;
                                        ;
    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline void TQueBind<src, dst, depth, mask>::InitBufHandle(T* bufPool,
    uint32_t index, TBufHandle bufhandle, uint32_t curPoolAddr, uint32_t len)
{
    (void)(bufPool);
    (void)(index);
                                                                                                                        ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    auto ptr = reinterpret_cast<TBufType*>(bufhandle);
    ptr->state = TBufState::FREE;
    ptr->freeBufEvt = freeBufEvt;
    ptr->enQueEvtID = INVALID_TEVENTID;
    ptr->freeBufEvtID = INVALID_TEVENTID;
    ptr->address = curPoolAddr;
    ptr->dataLen = len;
    ptr->usertag = -1;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::AllocTensor()
{
    static_assert((depth != 0), "must use AllocTensor<LocalTensor&> api while tque's depth is zero");
    auto buf = AllocBuffer();
    return Buf2Tensor<T>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline __attribute__((sync_alias)) void TQueBind<src, dst, depth, mask>::AllocTensor(LocalTensor<T>& input) {
    static_assert((depth == 0), "can not AllocTensor in place while tque's depth is non zero");
    TBufType* ret;
    do {
        ret = this->bufStart + this->bufCursor;
        if constexpr (config.bufferNumber != 1) {
            this->bufCursor += 1;
            if (this->bufCursor == this->bufNum) {
                this->bufCursor = 0;
            }
        }
        if (ret->state == TBufState::FREE) {
            ret->state = TBufState::OCCUPIED;
            break;
        }
    } while (true);
    WaitFlag<freeBufEvt>(ret->freeBufEvtID);
    TBuffAddr addr = GetBufferAddr(reinterpret_cast<TBufHandle>(ret));
    input.SetAddr(addr);
# 116 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquebind_impl.h"
}


template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline void TQueBind<src, dst, depth, mask>::FreeTensor(LocalTensor<T>& input)
{
    FreeBuffer(input.GetBufferHandle());
    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& input)
{
    auto buf = input.GetBufferHandle();
    return EnQue(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] inline __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(const LocalTensor<T>& input)
{
    auto buf = input.GetBufferHandle();
    return EnQue<srcUserPos, dstUserPos>(reinterpret_cast<TBufHandle>(buf));
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] inline __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{
    static_assert((depth != 0), "can not enque tbuf with user pos while tque's depth is zero");
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "enque only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "enque only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "enque src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent enQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);



                                                    ;
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth == 1) {
        this->que_ = buf;
    } else {
        this->que_[this->tail] = buf;
    }
    this->usedCount++;



                                                                ;


                                              ;
                                                ;
                                             ;


    if constexpr (enQueUserEvt == HardEvent::V_V) {
        SetFlag<enQueUserEvt>(0);
        ptr->enQueEvtID = 0;
    } else {
        auto enQueUserEvtID = GetTPipePtr()->AllocEventID<enQueUserEvt>();
        SetFlag<enQueUserEvt>(enQueUserEvtID);
        ptr->enQueEvtID = enQueUserEvtID;
    }
    if constexpr (depth != 1) {
        if (++this->tail >= depth) {
            this->tail = 0;
        }
    }






    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline __attribute__((sync_alias)) bool TQueBind<src, dst, depth, mask>::EnQue(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);
    if constexpr (depth != 0) {


                       ;

        if constexpr (depth == 1) {
            this->que_ = buf;
        } else {
            this->que_[this->tail] = buf;
        }
        this->usedCount++;
    }


                                           ;

                                                                                                                        ;
                                             ;
    if constexpr (depth == 0) {

        if constexpr ((GetPosition(src, dst) != TPosition::TSCM)) {
            SetFlag<enQueEvt>(ptr->enQueEvtID);
        }
    } else {





        if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
            auto enQueEvtID = GetTPipePtr()->AllocEventID<enQueEvt>();
            SetFlag<enQueEvt>(enQueEvtID);
            ptr->enQueEvtID = enQueEvtID;
        }





        if constexpr (depth != 1) {
            if (++this->tail >= depth) {
                this->tail = 0;
            }
        }
    }






    return true;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    static_assert((depth != 0), "must use DeQue<LocalTensor&> api while tque's depth is zero");
    auto buf = DeQue();
    auto ret = Buf2Tensor<T>(buf);
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T> [aicore] inline void TQueBind<src, dst, depth, mask>::DeQue(LocalTensor<T>& input) {
    static_assert((depth == 0), "can not DeQue tensor in place while tque's depth is non zero");
    auto bufHandle = input.GetBufferHandle();
    auto ptr = reinterpret_cast<TBufType*>(bufHandle);
    WaitFlag<enQueEvt>(ptr->enQueEvtID);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos, typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::DeQue()
{
    static_assert((depth != 0), "must use DeQue<LocalTensor&> api while tque's depth is zero");
    auto buf = DeQue<srcUserPos, dstUserPos>();
    auto ret = Buf2Tensor<T>(buf);
    return ret;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline __attribute__((sync_alias)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }
                                                                                              ;
    auto ptr = reinterpret_cast<TBufType*>(buf);







                                                   ;
    this->usedCount--;



                                             ;

    if (g_coreType != AIV || (GetPosition(src, dst) != TPosition::TSCM)) {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<enQueEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<enQueEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }







    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <TPosition srcUserPos, TPosition dstUserPos>
[aicore] inline __attribute__((sync_alias)) TBufHandle TQueBind<src, dst, depth, mask>::DeQue()
{
    static_assert(((srcUserPos == TPosition::GM) || (srcUserPos == TPosition::VECIN) ||
                (srcUserPos == TPosition::VECOUT) || (srcUserPos == TPosition::VECCALC)) &&
                "DeQue only support src position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(((dstUserPos == TPosition::GM) || (dstUserPos == TPosition::VECIN) ||
                (dstUserPos == TPosition::VECOUT) || (dstUserPos == TPosition::VECCALC)) &&
                "DeQue only support dst position GM/VECIN/VECOUT/VECCALC currently.");
    static_assert(!((srcUserPos == TPosition::GM) && (dstUserPos == TPosition::GM)) &&
                "DeQue src and dst position cannot be GM at the same time.");
    constexpr Hardware srcUserHardType = GetPhyType(srcUserPos);
    constexpr Hardware dstUserHardType = GetPhyType(dstUserPos);
    constexpr HardEvent deQueUserEvt = GetQueEvt(srcUserHardType, dstUserHardType, true, false, false);

    TBufHandle buf;
    if constexpr (depth == 1) {
        buf = this->que_;
    } else {
        buf = this->que_[this->head];
    }

                                                           ;
    auto ptr = reinterpret_cast<TBufType*>(buf);



                                              ;


                                                   ;
    this->usedCount--;




                                             ;

    if constexpr (deQueUserEvt == HardEvent::V_V) {
        WaitFlag<deQueUserEvt>(0);
        ptr->enQueEvtID = INVALID_TEVENTID;
    } else {
        if (ptr->enQueEvtID != INVALID_TEVENTID) {
            WaitFlag<deQueUserEvt>(ptr->enQueEvtID);
            GetTPipePtr()->ReleaseEventID<deQueUserEvt>(ptr->enQueEvtID);
            ptr->enQueEvtID = INVALID_TEVENTID;
        }
    }

    if constexpr (depth != 1) {
        if (++this->head >= depth) {
            this->head = 0;
        }
    }






    return reinterpret_cast<TBufHandle>(buf);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline void TQueBind<src, dst, depth, mask>::FreeBuffer(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);


                                           ;

                                                                                                             ;
    if constexpr (depth == 0) {
        if constexpr (!IsAivTscm(src, dst)) {

            SetFlag<freeBufEvt>(ptr->freeBufEvtID);
        }
    } else {
        if constexpr (!IsAivTscm(src, dst)) {


            ptr->freeBufEvtID = GetTPipePtr()->AllocEventID<freeBufEvt>();
            SetFlag<freeBufEvt>(ptr->freeBufEvtID);
            if constexpr (enableLoopQueue) {
                ptr->freeBufEvt = freeBufEvt;
            }
# 439 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquebind_impl.h"
        } else if constexpr (srcHardType == Hardware::GM) {
            if constexpr(g_coreType == AscendC::AIC) {
                ptr->freeBufEvtID = GetTPipePtr()->AllocEventID<freeBufEvt>();
                SetFlag<freeBufEvt>(ptr->freeBufEvtID);
                if constexpr (enableLoopQueue) {
                    ptr->freeBufEvt = freeBufEvt;
                }
            }
        }
    }
    ptr->state = TBufState::FREE;
    if constexpr (depth != 0) {
        this->bufUsedCount--;
    }






    return;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline TBufHandle TQueBind<src, dst, depth, mask>::AllocBuffer()
{
                                ;

                                                                                                            ;
    TBufType* ret;
    do {
        ret = this->bufStart + this->bufCursor;
        if constexpr (config.bufferNumber != 1) {
            this->bufCursor += 1;
            if (this->bufCursor == this->bufNum) {
                this->bufCursor = 0;
            }
        }
        if (ret->state == TBufState::FREE) {
            ret->state = TBufState::OCCUPIED;
            if constexpr (IsAivTscm(src, dst)) {
                if constexpr (srcHardType == Hardware::UB) {
                    break;
                } else if constexpr (srcHardType == Hardware::GM) {
                    if constexpr(g_coreType == AscendC::AIV) {
                        break;
                    }
                }
            }
            if (ret->freeBufEvtID != INVALID_TEVENTID) {
                if constexpr (enableLoopQueue) {
                    if (freeBufEvt == ret->freeBufEvt) {
                        WaitFlag<freeBufEvt>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<freeBufEvt>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                    } else if (freeBufEvt == HardEvent::V_MTE2 && ret->freeBufEvt == HardEvent::MTE3_V) {
                        WaitFlag<HardEvent::MTE3_V>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_V>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                        TEventID evtId = GetTPipePtr()->AllocEventID<HardEvent::MTE3_MTE2>();
                        SetFlag<HardEvent::MTE3_MTE2>(evtId);
                        WaitFlag<HardEvent::MTE3_MTE2>(evtId);
                        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_MTE2>(evtId);
                    } else if (freeBufEvt == HardEvent::MTE3_V && ret->freeBufEvt == HardEvent::V_MTE2) {
                        WaitFlag<HardEvent::V_MTE2>(ret->freeBufEvtID);
                        GetTPipePtr()->ReleaseEventID<HardEvent::V_MTE2>(ret->freeBufEvtID);
                        ret->freeBufEvtID = INVALID_TEVENTID;
                    } else {

                                                                                                     ;
                    }
                } else {
                    WaitFlag<freeBufEvt>(ret->freeBufEvtID);
                    GetTPipePtr()->ReleaseEventID<freeBufEvt>(ret->freeBufEvtID);
                    ret->freeBufEvtID = INVALID_TEVENTID;
                }
            }
            break;
        }




    } while (true);
    this->bufUsedCount++;
# 537 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquebind_impl.h"
    return reinterpret_cast<TBufHandle>(ret);
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline void TQueBind<src, dst, depth, mask>::FreeAllEvent()
{
    static_assert((depth != 0), "can not use FreeAllEvent api while depth is zero");
    auto ptr = this->bufStart;
    for (int i = 0; i < this->bufNum; i++, ptr++) {


                                                                                ;
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlag<freeBufEvt>(ptr->freeBufEvtID);
            GetTPipePtr()->ReleaseEventID<freeBufEvt>(ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline void TQueBind<src, dst, depth, mask>::SetTBufPoolHandle(uint64_t bufPoolHandle)
{



    (void)(bufPoolHandle);

}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline int32_t TQueBind<src, dst, depth, mask>::GetTensorCountInQue()
{
    static_assert((depth != 0), "GetTensorCountInQue api is not supported while depth is zero");
    return usedCount;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline TBuffAddr TQueBind<src, dst, depth, mask>::GetBufferAddr(TBufHandle buf)
{
                                                                                                                        ;
    auto ptr = reinterpret_cast<TBufType*>(buf);


                                           ;

    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(GetPosition(src, dst));
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;





    return addr;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline TBufState TQueBind<src, dst, depth, mask>::GetState(const LocalTensor<T>& input) const
{
    return GetState(input.GetBufferHandle());
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline TBufState TQueBind<src, dst, depth, mask>::GetState(const TBufHandle& handle) const
{
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);


                                           ;
    return ptr->state;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline bool TQueBind<src, dst, depth, mask>::VacantInQue()
{
    static_assert((depth != 0), "VacantInQue api is not supported while depth is zero");
    return usedCount < depth;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline bool TQueBind<src, dst, depth, mask>::HasTensorInQue()
{
    static_assert((depth != 0), "HasTensorInQue api is not supported while depth is zero");
    return usedCount > 0;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
[aicore] inline bool TQueBind<src, dst, depth, mask>::HasIdleBuffer()
{
    static_assert((depth != 0), "HasIdleBuffer api is not supported while depth is zero");
    return bufUsedCount < bufNum;
}

template <TPosition src, TPosition dst, int32_t depth, auto mask>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TQueBind<src, dst, depth, mask>::Buf2Tensor(TBufHandle buf)
{
    TBuffAddr addr = GetBufferAddr(buf);
    LocalTensor<T> output;
    output.SetAddr(addr);
    return output;
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquesync_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tquesync_impl.h"
namespace AscendC {
template <pipe_t src, pipe_t dst>
[aicore] inline void TQueSync<src, dst>::SetFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");

                                                                                                          ;
    set_flag(src, dst, id);
}

template <pipe_t src, pipe_t dst>
[aicore] inline void TQueSync<src, dst>::WaitFlag(TEventID id)
{
    static_assert((src != dst), "src/dst pipe cannot be same.");
    static_assert(IsSupportedPipe(src), "src pipe not supported");
    static_assert(IsSupportedPipe(dst), "dst pipe not supported");

                                                                                                          ;
    wait_flag(src, dst, id);
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h"
namespace AscendC {

template <TPosition pos, uint32_t bufIDSize>
[aicore] inline TBufPool<pos, bufIDSize>::TBufPool()
{
    Init();
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] inline TBufPool<pos, bufIDSize>::~TBufPool()
{
    auto ptr = this->tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
};

template <TPosition pos, uint32_t bufIDSize>
[aicore] inline void TBufPool<pos, bufIDSize>::ResetPool()
{
    tBufPoolImpl.curBufSize_ = 0;
    tBufPoolImpl.startAddr_ = 0;
    tBufPoolImpl.maxAddr_ = 0;
    tBufPoolImpl.maxLen_ = 0;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] inline void TBufPool<pos, bufIDSize>::Init()
{
    constexpr auto pool = GetPhyType(pos);
    static_assert((pool == Hardware::L1 || pool == Hardware::UB),
        "TbufPool Position should be one of A1/B1/C1/VECIN/VECOUT/VECCALC");
    ResetPool();
    tBufPoolImpl.isReset_ = true;
}

template <TPosition pos, uint32_t bufIDSize>
[aicore] inline void TBufPool<pos, bufIDSize>::Reset()
{
    auto ptr = this->tBufPoolImpl.buf_;
    for (uint8_t i = 0; i < this->tBufPoolImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    ResetPool();
    tBufPoolImpl.isReset_ = true;



}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] inline bool TBufPool<pos, bufIDSize>::InitBuffer(T &que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TBufPool::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");
                                                                                                                        ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->tBufPoolImpl.buf_ + this->tBufPoolImpl.curBufSize_;
                                      ;




                                            ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
    auto ptr = que.bufStart;
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h"
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }
    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->tBufPoolImpl.curBufSize_ += num;

                                                                                                                      ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <TPosition bufPos>
[aicore] inline bool TBufPool<pos, bufIDSize>::InitBuffer(TBuf<bufPos> &buf, uint32_t len)
{
                                                                                                                        ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->tBufPoolImpl.buf_ + this->tBufPoolImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;




                                            ;
    constexpr auto pool = GetPhyType(bufPos);

                                                                                   ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
    auto ptr = buf.bufStart;
# 151 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h"
    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }




    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    this->tBufPoolImpl.curBufSize_ += bufHandleSize;




                              ;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T>
[aicore] inline bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
                                                                                                                        ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    bufPool.tBufPoolImpl.startAddr_ = this->tBufPoolImpl.maxAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = len;




                                            ;
    auto curPoolAddr = this->tBufPoolImpl.maxAddr_;
# 205 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h"
    curPoolAddr += len;




    this->tBufPoolImpl.maxAddr_ = curPoolAddr;
    return true;
}

template <TPosition pos, uint32_t bufIDSize>
template <class T, class U>
[aicore] inline bool TBufPool<pos, bufIDSize>::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TBufPool::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
                                                                                                                        ;
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr auto pool = GetPhyType(T::poolPos);
    constexpr auto sharedPool = GetPhyType(U::poolPos);

                                                                                                       ;
    bufPool.tBufPoolImpl.startAddr_ = shareBuf.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = shareBuf.tBufPoolImpl.maxLen_;



                                           ;
# 246 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbufpool_impl.h"
    return true;
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbuf_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbuf_impl.h"
namespace AscendC {

template <TPosition pos>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::Get(uint32_t len)
{
    using PrimType = PrimT<T>;
    uint32_t dataLen;
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        dataLen = len / INT4_TWO;




    } else {
        dataLen = len * sizeof(PrimType);
    }







    auto ptr = this->bufStart;
    ptr->dataLen = dataLen;
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tbuf_impl.h"
    LocalTensor<T> output;
    output.SetAddr(addr);
    return output;
}

template <TPosition pos> template <typename T> [aicore] inline __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::Get()
{
    using PrimType = PrimT<T>;
    if constexpr (IsSameType<PrimType, int4b_t>::value) {
        return Get<T>(bufLen * INT4_TWO);




    } else {
        return Get<T>(bufLen / sizeof(PrimType));
    }
}

template <TPosition pos>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::GetWithOffset(uint32_t size, uint32_t bufOffset)
{
    auto ptr = this->bufStart;
    ptr->dataLen = size * sizeof(T);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = reinterpret_cast<TBufHandle>(ptr);
    addr.bufferAddr = ptr->address + bufOffset;
    addr.dataLen = ptr->dataLen;




    LocalTensor<T> output;
    output.SetAddr(addr);
    return output;
}

template <TPosition pos> [aicore] inline void TBuf<pos>::SetTpipeBuf(TBufType* bufStartIn, uint32_t bufLenIn)
{
    this->bufStart = bufStartIn;
    this->bufLen = bufLenIn;
    this->offset = 0;
}

template <TPosition pos> template <typename T> [aicore] inline void TBuf<pos>::EnQue(const LocalTensor<T>& input)
{
    (void)(0);
}

template <TPosition pos> template <typename T> [aicore] inline LocalTensor<T> TBuf<pos>::DeQue()
{
    return Get<T>();
}

template <TPosition pos>
template <typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TBuf<pos>::AllocTensor()
{
    return Get<T>();
}

template <TPosition pos> template <typename T> [aicore] inline void TBuf<pos>::FreeTensor(LocalTensor<T>& input)
{
    (void)(0);
}

template <TPosition pos>
template <typename T>
[aicore] inline TBufState TBuf<pos>::GetState(const LocalTensor<T>& input) const
{
    TBufHandle handle = input.GetBufferHandle();
    if (handle == nullptr) {
        return TBufState::FREE;
    }
    auto ptr = reinterpret_cast<TBufType*>(handle);
    return ptr->state;
}

template <TPosition pos> [aicore] inline bool TBuf<pos>::EnQue(TBufHandle buf)
{
    return true;
}

template <TPosition pos> [aicore] inline TBufHandle TBuf<pos>::DeQue()
{
    return Get();
}

template <TPosition pos> [aicore] inline TBufHandle TBuf<pos>::AllocBuffer()
{
    return Get();
}

template <TPosition pos> [aicore] inline void TBuf<pos>::FreeBuffer(TBufHandle buf)
{
    (void)(0);
}

template <TPosition pos> [aicore] inline TBuffAddr TBuf<pos>::GetBufferAddr(TBufHandle buf)
{
    auto ptr = reinterpret_cast<TBufType*>(buf);
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = buf;
    addr.bufferAddr = ptr->address;
    addr.dataLen = ptr->dataLen;




    return addr;
}

template <TPosition pos> [aicore] inline void TBuf<pos>::InitStartBufHandle(
    TBufHandle startBufhandle, uint8_t num, uint32_t len)
{
    static_assert(!isTQue, "InitTBufAddr only support TBuf class");
                                                                                                                        ;
    auto ptr = reinterpret_cast<TBufType*>(startBufhandle);
    this->bufStart = ptr;
    this->bufLen = len;
    this->offset = 0;
    return;
}

template <TPosition pos> [aicore] inline TBufHandle TBuf<pos>::Get(uint32_t len)
{



    this->bufStart->dataLen = len;
    return reinterpret_cast<TBufHandle>(this->bufStart);
}

template <TPosition pos> [aicore] inline TBufHandle TBuf<pos>::Get()
{
    return Get(bufLen);
}

template <TPosition pos> [aicore] inline uint32_t TBuf<pos>::GetBufLen() const
{
    return bufLen;
}

}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h" 2


namespace AscendC {
[aicore] inline void PrintTimeStamp(uint32_t descId);
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
[aicore] inline TPipe::TPipe()
{
    InitSocState();
    Init();



}

[aicore] inline TPipe::~TPipe()
{
    if (g_tpipeImpl.isDestroy) {
        return;
    }
    Destroy();
};

[aicore] inline void TPipe::Init()
{
    ResetPool();



    if constexpr(g_coreType == AscendC::AIC) {
        auto enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                   ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                   ;
        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));

        enQueEvtID = this->AllocEventID<HardEvent::M_MTE1>();
                                                                                                   ;

        SetFlag<HardEvent::M_MTE1>(static_cast<event_t>(enQueEvtID));
    }
# 130 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    g_vecTPipePtr = this;






    g_tpipeImpl.isDestroy = false;
}

template <class T, class First, class... Rest>
[aicore] inline void TPipe::AllocAddrs(TBufType* ptr, const First& addr, const Rest&... addrs)
{
    static_assert(Std::is_tuple_v<First> && Std::tuple_size_v<First> == 2,
            "input Addrs must be Std::tuple type and tuple_size must be 2");
    constexpr bool useAltBufId = T::config.consumerSize > 1;
    ptr->state = TBufState::FREE;
    ptr->freeBufEvt = T::freeBufEvt;
    if constexpr (T::queDepth == 0) {
        ptr->enQueEvtID = AllocEventID<T::enQueEvt>();
        ptr->freeBufEvtID = AllocEventID<T::freeBufEvt>();
        SetFlag<T::freeBufEvt>(ptr->freeBufEvtID);
    } else {
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
    }

    if constexpr ((Std::tuple_size_v<First>) > 1) {
        ptr->address = Std::get<0>(addr);
        ptr->dataLen = Std::get<1>(addr);
    }
# 169 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    ptr->usertag = -1;
    if constexpr (sizeof...(addrs) > 0) {
        AllocAddrs<T>(++ptr, addrs...);
    }
}

template <class T, class U, class V, class... Addrs>
[aicore] inline bool TPipe::InitBuffer(T& que, const Std::tuple<U, V>& addr0, const Addrs&... addrs)
{
    static_assert((T::isTQue), "TPipe::InitBuffer(T& que, Addrs ...addrs) not supports T as TBuf");
    constexpr uint32_t num = sizeof...(addrs) + 1;


                                      ;
    static_assert(T::dstPosition != TPosition::TSCM, "Init Buffer is not support Postion TSCM");
    Hardware pool = GetBufferPos(T::srcPosition, T::dstPosition);
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
    que.value = num;
                                                                                                                ;
                                                                                                                  ;
    auto ptr = que.bufStart;
    AllocAddrs<T>(ptr, addr0, addrs...);
    this->g_tpipeImpl.curBufSize_ += num;



                      ;



    return true;
}

template <class T> [aicore] inline bool TPipe::InitBuffer(T& que, uint8_t num, uint32_t len)
{
    static_assert((T::isTQue), "TPipe::InitBuffer(T& que, uint8_t num, uint32_t len) not supports T as TBuf");


                                      ;

                                                                                                                 ;
                                                                        ;
    if constexpr (T::dstPosition == TPosition::TSCM) {
        return TscmInitBuffer(que, num, len);
    }
    Hardware pool = GetBufferPos(T::srcPosition, T::dstPosition);
# 224 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

                                                                                                                ;
                                                                                                                  ;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = que.bufStart;







    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        if constexpr (T::queDepth == 0) {
            ptr->enQueEvtID = AllocEventID<T::enQueEvt>();
            ptr->freeBufEvtID = AllocEventID<T::freeBufEvt>();
            SetFlag<T::freeBufEvt>(ptr->freeBufEvtID);
        } else {
            ptr->enQueEvtID = INVALID_TEVENTID;
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }




    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += num;



                                                     ;




                                                  ;



    return true;
}

template <TPosition pos> [aicore] inline bool TPipe::InitBuffer(TBuf<pos>& buf, uint32_t len)
{
    constexpr auto pool = GetPhyType(pos);
# 290 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    constexpr int32_t bufHandleSize = 1;
    buf.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
    buf.bufLen = len;
    buf.offset = 0;

                                                                                                                ;

    auto curPoolAddr = g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    auto ptr = buf.bufStart;






    for (uint8_t i = 0; i < bufHandleSize; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }




    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;
    this->g_tpipeImpl.curBufSize_ += bufHandleSize;



                      ;



    return true;
}

template <class T>
[aicore] inline bool TPipe::InitBufPool(T &bufPool, uint32_t len)
{
    static_assert(
        (T::isTbufPool), "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T as TbufPool");
    constexpr auto pool = GetPhyType(T::poolPos);
# 346 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    bufPool.tBufPoolImpl.startAddr_ = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = len;
    auto curPoolAddr = this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr;
# 362 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    curPoolAddr += len;




    this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(pool)].maxAddr = curPoolAddr;





                                                                                        ;
    return true;
}

template <class T, class U>
[aicore] inline bool TPipe::InitBufPool(T &bufPool, uint32_t len, U &shareBuf)
{
    static_assert((T::isTbufPool && U::isTbufPool),
        "TPipe::InitBufPool(T& bufPool, uint32_t len, U& shareBuf) only supports T and U as TBufPool");
    constexpr auto pool = GetPhyType(T::poolPos);

                                                                                                ;
# 394 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;

    bufPool.tBufPoolImpl.startAddr_ = shareBuf.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxAddr_ = bufPool.tBufPoolImpl.startAddr_;
    bufPool.tBufPoolImpl.maxLen_ = shareBuf.tBufPoolImpl.maxLen_;



                                           ;
# 413 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    return true;
}

template <HardEvent evt> [aicore] inline TEventID TPipe::AllocEventID()
{

                                                                                           ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);


                           ;
    ptr->eventOccupy = sbitset1(ptr->eventOccupy, lastId);
    return lastId;
}

template <HardEvent evt> [aicore] inline void TPipe::ReleaseEventID(TEventID id)
{


                                                     ;
                                                                                                           ;
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    ptr->eventOccupy = sbitset0(ptr->eventOccupy, id);
    return;
}

[aicore] inline TEventID TPipe::FetchEventID(HardEvent evt)
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);


                           ;
    return lastId;
}

template <HardEvent evt> [aicore] inline TEventID TPipe::FetchEventID()
{
    auto ptr = this->g_tpipeImpl.eventPool_ + EventToIndex(evt);
    auto lastId = sff0(ptr->eventOccupy);


                           ;
    return lastId;
}

template <TPosition pos> [aicore] inline uint64_t TPipe::GetQueueEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                       ;
    return this->g_tpipeImpl.bufPool_[static_cast<uint8_t>(hardType)].maxAddr;
}

[aicore] inline void TPipe::Destroy()
{
    g_tpipeImpl.isDestroy = true;
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID && ptr->state == TBufState::FREE) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }


    if constexpr(g_coreType == AscendC::AIC) {
        WaitFlag<HardEvent::M_MTE1>(0);
        ReleaseEventID<HardEvent::M_MTE1>(0);
        WaitFlag<HardEvent::M_MTE1>(1);
        ReleaseEventID<HardEvent::M_MTE1>(1);

        WaitFlag<HardEvent::M_MTE1>(2);
        ReleaseEventID<HardEvent::M_MTE1>(2);
    }
# 497 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    pipe_barrier(PIPE_ALL);




}

[aicore] inline void TPipe::Reset()
{
    auto ptr = this->g_tpipeImpl.buf_;
    for (uint8_t i = 0; i < this->g_tpipeImpl.curBufSize_; i++, ptr++) {
        if (ptr->freeBufEvtID != INVALID_TEVENTID && ptr->state == TBufState::FREE) {
            WaitFlagImpl(ptr->freeBufEvt, ptr->freeBufEvtID);
            ptr->freeBufEvtID = INVALID_TEVENTID;
        }
    }
    InitSocState();
    ResetPool();
# 523 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
}

[aicore] inline void InitShareBufStart(TPipe* tpipe, uint32_t mode, uint32_t* shareLens,
    uint32_t lens, uint8_t subBlockIdx)
{





    (void)(lens);



                                                                                                        ;
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L1),
        Hardware::L1, subBlockIdx);
    tpipe->AuxShareBufStart(mode, shareLens, static_cast<uint8_t>(TShareBuf::ShareHard::L0C),
        Hardware::L0C, subBlockIdx);




    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0A)].maxAddr = 0;
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0B)].maxAddr = 0;

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::BIAS)].maxAddr = 0;

    return;
}

[aicore] inline void InitShareBufEnd(TPipe* tpipe)
{

    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L1)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L1)];
    tpipe->g_tpipeImpl.bufPool_[static_cast<uint8_t>(Hardware::L0C)].maxAddr =
        tpipe->g_tpipeImpl.shareBufPool_.maxAddr[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)];





    return;
}

template <typename T>
[aicore] inline void TPipe::InitSpmBuffer(const GlobalTensor<T>& workspace, const int32_t bufferSize)
{
    g_tpipeImpl.spmInfo_.spmBuffSize = bufferSize;
    g_tpipeImpl.spmInfo_.spmAddr = reinterpret_cast<uint64_t>(workspace.GetPhyAddr());
    g_tpipeImpl.spmInfo_.spmBufType = static_cast<uint8_t>(Hardware::GM);
}

[aicore] inline void TPipe::InitSpmBuffer(const int32_t bufferSize)
{

    (void)(bufferSize);

                                                                               ;
# 595 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
}

template <typename T>
[aicore] inline void TPipe::WriteSpmBuffer(const LocalTensor<T>& write, const DataCopyParams& copyParams,
    int32_t writeOffset)
{




    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(write.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                         ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(write.GetPhyAddr()), copyParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] inline void TPipe::ReadSpmBuffer(const LocalTensor<T>& read, const DataCopyParams& copyParams,
    int32_t readOffset)
{




    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(read.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);
        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(read.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, copyParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <typename T>
[aicore] inline void TPipe::WriteSpmBuffer(const LocalTensor<T>& write, const int32_t writeSize,
    int32_t writeOffset)
{




    int computeSize = writeSize != 0 ? writeSize : GetShapeSize(write.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
    event_t eventIDMTE3ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_V));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);

    SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        DataCopyUB2GMImpl(reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(write.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                         ;

                       ;
        DataCopyUB2L1Impl(reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + writeOffset,
            reinterpret_cast<__attribute__((cce_unif_buff)) T*>(write.GetPhyAddr()), repeatParams);
        event_t eventIDMTE3ToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE1));
        SetFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
        WaitFlag<HardEvent::MTE3_MTE1>(eventIDMTE3ToMTE1);
    }

    SetFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
    WaitFlag<HardEvent::MTE3_V>(eventIDMTE3ToV);
}

template <typename T>
[aicore] inline void TPipe::ReadSpmBuffer(const LocalTensor<T>& read, const int32_t readSize, int32_t readOffset)
{




    int computeSize = readSize != 0 ? readSize : GetShapeSize(read.GetShapeInfo());
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = computeSize / AscendCUtils::GetC0Count(sizeof(T));
    if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::GM)) {
        event_t eventIDVToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE2));
        event_t eventIDMTE2ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_V));
        event_t eventIDMTE2ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        WaitFlag<HardEvent::V_MTE2>(eventIDVToMTE2);
        DataCopyGM2UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(read.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_global)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);
        WaitFlag<HardEvent::MTE2_V>(eventIDMTE2ToV);

        SetFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
        WaitFlag<HardEvent::MTE2_MTE3>(eventIDMTE2ToMTE3);
    } else if (g_tpipeImpl.spmInfo_.spmBufType == static_cast<uint8_t>(Hardware::L1)) {

                        ;
                                                                                                                                                ;
        event_t eventIDVToMTE1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE1));
        event_t eventIDMTE1ToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_V));
        event_t eventIDMTE1ToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_MTE3));
        SetFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        WaitFlag<HardEvent::V_MTE1>(eventIDVToMTE1);
        DataCopyL12UBImpl(reinterpret_cast<__attribute__((cce_unif_buff)) T*>(read.GetPhyAddr()),
            reinterpret_cast<__attribute__((cce_cube_buff)) T*>(g_tpipeImpl.spmInfo_.spmAddr) + readOffset, repeatParams);

        SetFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);
        WaitFlag<HardEvent::MTE1_V>(eventIDMTE1ToV);

        SetFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
        WaitFlag<HardEvent::MTE1_MTE3>(eventIDMTE1ToMTE3);
    }
}

template <TPosition pos>
[aicore] inline TBuffAddr TPipe::GetAbsAddr(int32_t offset, int32_t len) const
{
    TBuffAddr addr;
    addr.logicPos = static_cast<uint8_t>(pos);
    addr.bufferHandle = nullptr;
    addr.bufferAddr = offset;
    addr.dataLen = len;
# 773 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
    return addr;
}

template <TPosition pos, typename T>
[aicore] inline __attribute__((sync_alias)) LocalTensor<T> TPipe::GetAbsAddr(int32_t offset, int32_t size) const
{
    TBuffAddr addr = GetAbsAddr<pos>(offset, static_cast<int32_t>((size * sizeof(T))));
    LocalTensor<T> output;
    output.SetAddr(addr);
    return output;
}
# 885 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_tpipe_impl.h"
[aicore] inline void TPipe::InitSocState() const
{
    AscendCUtils::InitSocStateImpl();
}

[aicore] inline void TPipe::ResetPool()
{
    g_tpipeImpl.tscmBufferPtr_ = TOTAL_L1_SIZE;
    g_tpipeImpl.curBufSize_ = 0;
    auto buf = g_tpipeImpl.bufPool_;
    for (int32_t i = 0; i < static_cast<int32_t>(Hardware::MAX); i++, buf++) {
        buf->maxAddr = 0;
    }
    auto evt = g_tpipeImpl.eventPool_;
    for (int32_t i = 0; i < EVENT_NUM; i++, evt++) {
        evt->eventOccupy = 0;
    }
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L1)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::UB)] = -1;
    g_tpipeImpl.shareBufPool_.start[static_cast<uint8_t>(TShareBuf::ShareHard::L0C)] = -1;
}

template <class T> [aicore] inline bool TPipe::TscmInitBuffer(T& que, uint8_t num, uint32_t len)
{
                                                                                                                        ;


                                      ;

    len = (len + ONE_BLK_SIZE - MIN_BLOCK_LEN) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    que.value = num;
    que.bufStart = this->g_tpipeImpl.buf_ + this->g_tpipeImpl.curBufSize_;
                                      ;

    constexpr Hardware pool = Hardware::L1;





    uint32_t curPoolAddr;
    if constexpr (T::scmBlockGroup) {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - num * len;
        g_tpipeImpl.tscmBufferPtr_ -= num * len;
    } else {
        curPoolAddr = g_tpipeImpl.tscmBufferPtr_ - (GetTaskRationImpl() - GetSubBlockIdxImpl()) * len * num;
        g_tpipeImpl.tscmBufferPtr_ -= GetTaskRationImpl() * num * len;
    }

    auto ptr = que.bufStart;
    for (int32_t i = 0; i < num; i++, ptr++) {
        ptr->state = TBufState::FREE;
        ptr->freeBufEvt = T::freeBufEvt;
        ptr->enQueEvtID = INVALID_TEVENTID;
        ptr->freeBufEvtID = INVALID_TEVENTID;
        ptr->address = curPoolAddr;
        ptr->dataLen = len;
        ptr->usertag = -1;
        curPoolAddr += len;
    }




                                                                                ;
    this->g_tpipeImpl.curBufSize_ += num;



                       ;
    return true;
}

template <TPosition pos>
[aicore] inline uint64_t TransUBAddr(uint64_t addr)
{




    return addr;
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_type.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_prof_trace_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_prof_trace_intf.h"
namespace AscendC {
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_prof_trace_intf.h"
[aicore] inline void MetricsProfStart();

[aicore] inline void MetricsProfStop();
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_prof_trace_intf_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_prof_trace_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_prof_trace.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_prof_trace.h"
namespace AscendC {
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_prof_trace.h"
[aicore] inline void ProfStartImpl()
{


    bisheng::cce::metrics_prof_start();




}

[aicore] inline void ProfStopImpl()
{


    bisheng::cce::metrics_prof_stop();




}
# 72 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_prof_trace.h"
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_prof_trace_intf_impl.h" 2

namespace AscendC {
[aicore] inline void MetricsProfStart()
{
    ProfStartImpl();
}

[aicore] inline void MetricsProfStop()
{
    ProfStopImpl();
}
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_prof_trace_intf_impl.h"
}
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_prof_trace_intf.h" 2
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
namespace AscendC {
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline void __attribute__((inout_pipe("MTE2")))
    DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src, const DataCopyParams& repeatParams);
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
                                                     const Nd2NzParams& intriParams);
# 91 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                     const Nd2NzParams& intriParams);
# 105 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
                                                     const DataCopyParams& repeatParams);
# 119 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                const DataCopyParams& repeatParams);
# 133 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T, typename U>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                const DataCopyParams& repeatParams);
# 150 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                              const uint64_t mask[], const uint8_t repeatTime,
                                              const CopyRepeatParams& repeatParams);


template <typename T, bool isSetMask = true>
[aicore] inline __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                              const uint64_t mask, const uint8_t repeatTime,
                                              const CopyRepeatParams& repeatParams);
# 170 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 184 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
                                                     const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[],
                                                     const uint32_t dimValue = 1);
# 196 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
                                                     const uint32_t count);
# 207 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
                                                     const uint32_t count);
# 218 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                const uint32_t count);







template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
                                                     const Nz2NdParamsFull& intriParams);
# 250 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
                                                     const DataCopyParams& intriParams,
                                                     const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);

template <typename T, typename U>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                const DataCopyCO12DstParams& intriParams);

template <typename T, typename U>
[aicore] inline void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<U>& src,
                                const DataCopyCO12DstParams& intriParams);



template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, bfloat16_t>::value && Std::is_same<PrimT<U>, float>::value,
                                  bool>::type = true>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);



template <
    typename T, typename U,
    typename Std::enable_if<Std::is_same<PrimT<T>, half>::value && Std::is_same<PrimT<U>, float>::value, bool>::type = true>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, half>::value && Std::is_same<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, int16_t>::value && Std::is_same<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, int8_t>::value && Std::is_same<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, uint8_t>::value && Std::is_same<PrimT<U>, int32_t>::value,
                                  bool>::type = true>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);


template <
    typename T, typename U,
    typename Std::enable_if<Std::is_same<PrimT<T>, float>::value && Std::is_same<PrimT<U>, half>::value, bool>::type = true>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
                                                  const DataCopyParams& intriParams,
                                                  const DataCopyEnhancedParams& enhancedParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dst,
                                                        const GlobalTensor<T>& src,
                                                        const DataCopyParams& dataCopyParams,
                                                        const DataCopyPadParams& padParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dst,
                                                        const LocalTensor<T>& src,
                                                        const DataCopyParams& dataCopyParams);

template <typename T>
[aicore] inline void DataCopyPad(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                        const DataCopyParams& dataCopyParams, const Nd2NzParams& nd2nzParams);


template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dst,
                                                        const GlobalTensor<T>& src,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<T>& padParams);



template <typename T, typename U,
          typename Std::enable_if<Std::is_same<PrimT<T>, U>::value && (!Std::is_same<T, U>::value), bool>::type = true>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T>& dst,
                                                        const GlobalTensor<T>& src,
                                                        const DataCopyExtParams& dataCopyParams,
                                                        const DataCopyPadExtParams<U>& padParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T>& dst,
                                                        const LocalTensor<T>& src,
                                                        const DataCopyExtParams& dataCopyParams);

template <typename T>
[aicore] inline void DataCopyPad(const LocalTensor<T>& dst, const LocalTensor<T>& src,
                                        const DataCopyExtParams& dataCopyParams, const Nd2NzParams& nd2nzParams);

template <typename T, TPosition pos = TPosition::MAX>
[aicore] inline void SetPadValue(T paddingValue);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_check.h" 1
# 310 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_check.h"
namespace AscendC {







template <typename T>
[aicore] inline void CheckTensorAlign(const LocalTensor<T>& input, uint32_t alignByte, __attribute__((cce_global)) const char* tensorName,
    __attribute__((cce_global)) const char* apiMsg)
{






}

template <typename T>
[aicore] inline void CheckTensorPos(const LocalTensor<T>& input, const Hardware expectPos,
    __attribute__((cce_global)) const char* tensorName, __attribute__((cce_global)) const char* tPosName, __attribute__((cce_global)) const char* apiMsg)
{





}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_data_copy_base_impl.h" 1
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_data_copy_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_scm_data_copy_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_scm_data_copy_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_client.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_client.h"
namespace AscendC {
class KfcCommClient {
public:

    __attribute__((cce_global)) KfcMsg *msgSendHead;
    __attribute__((cce_global)) KfcMsg *msgSendStart;


    __attribute__((cce_global)) KfcMsg *msgRcvHead;
    __attribute__((cce_global)) KfcMsg *msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubStart;
    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    __attribute__((cce_unif_buff)) KfcMsg *ubMsg;
    uint32_t head;
    uint32_t tail;
    uint8_t msgRcvPos;
    uint8_t msgSendPos;
    uint8_t eventID_;
    uint8_t enableHardWare;

public:
    [aicore] inline KfcCommClient(__attribute__((cce_global)) uint8_t* workspace, int subBlockID, uint8_t enableHardWare = 0)
    {
        if constexpr(g_coreType == AscendC::AIV) {
            this->enableHardWare = enableHardWare;
            if (enableHardWare) {
                return;
            }
                                                                                                                 ;
                                                                                                                     ;

                                                                                               ;

            this->msgSendStart = (__attribute__((cce_global)) KfcMsg *)GetMsgHead(workspace, subBlockID);
            this->msgRcvStart = this->msgSendStart + 64;

            this->msgSendHead = this->msgSendStart;
            this->msgSendPos = 0;
            this->msgRcvHead = this->msgRcvStart;
            this->msgRcvPos = 0;






            ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) KfcMsg *>(TOTAL_UB_SIZE - sizeof(KfcMsg));

            eventID_ = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
            SetFlag<HardEvent::MTE3_S>((event_t)eventID_);







            ubStart = GetUBMapAddr(workspace, subBlockID);
            ubAvalidTail = GetUBAvaliedAddr(workspace, subBlockID);
            head = 0;
            tail = 0;
        }
    }

    [aicore] inline ~KfcCommClient()
    {
        if constexpr(g_coreType == AscendC::AIV) {
            if (this->enableHardWare) {
                return;
            }
            if constexpr (MIX_NUM == 1) {

                if (GetSubBlockIdxImpl() == 1) {
                    return;
                }
            }
            __attribute__((cce_global)) KfcMsg *msg = AllocMessage();

                                                                                                                     ;
            uint32_t quitSignal = KfcMsgMakeFlag(KFC_Enum::SERVICE_QUIT, 0);
            *((__attribute__((cce_global)) uint32_t *)msg) = quitSignal;
            msg->ubAddr = GetTaskRationImpl();
# 114 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_client.h"
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
    }

    template <bool isAck>
    [aicore] inline void PostMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        PipeBarrier<PIPE_MTE3>();
        copy_ubuf_to_gm((__attribute__((cce_global)) void *)msg, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(KfcMsg) / ONE_BLK_SIZE, 0, 0);
# 136 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_client.h"
        SetFlag<HardEvent::MTE3_S>((event_t)this->eventID_);
    }

    [aicore] inline __attribute__((cce_global)) KfcMsg *AllocMessage()
    {
        auto ret = AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
        WaitFlag<HardEvent::MTE3_S>((event_t)this->eventID_);

                                                                                     ;
        return ret;
    }

    [aicore] inline void FreeMessage(__attribute__((cce_global)) KfcMsg *msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] inline __attribute__((cce_global)) uint8_t* AllocUB(uint32_t size, int32_t &tailInfo)
    {
# 165 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_client.h"
        __attribute__((cce_global)) uint8_t* ret;
        if (head + size >= WORKSPACE_UB_SIZE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
            tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
            while (head < tail || tail == 0) {
                Barrier();
                dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                    dcci_dst_t::CACHELINE_OUT);
                Barrier();
                tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
            }
            if (tail == head && size == tail) {
                tail = 0;
            }
            head = 0;
        }

        while (head < tail && (head + size >= tail)) {
            Barrier();
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(ubAvalidTail), cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
            Barrier();
            tail = *(reinterpret_cast<__attribute__((cce_global)) uint32_t *>(ubAvalidTail));
        }





        ret = ubStart + head;
        head += size;
        tailInfo = head;
        return ret;
    }

    [aicore] inline __attribute__((cce_global)) KfcMsg *RcvMessage()
    {
        auto ret = RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return ret;
    }
};





[[block_local]] __inline__ AscendC::KfcCommClient* g_kfcClient;





[aicore] inline AscendC::KfcCommClient* GetKfcClient()
{


    return reinterpret_cast<AscendC::KfcCommClient*>(g_kfcClient);







}
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_scm_data_copy_impl.h" 2


namespace AscendC {
struct Gm2L1Params {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    DataCopyParams intri;
};
struct Gm2L1Nd2NzParams {
    __attribute__((cce_cube_buff)) void* dst = nullptr;
    __attribute__((cce_global)) void* src = nullptr;
    uint8_t dataTypeLen = 2;
    Nd2NzParams intri;
};




[aicore] inline void ScmDataCopyMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const DataCopyParams& intriParams,
    int32_t ubAddr)
{
                             ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                             ;

    __attribute__((cce_unif_buff)) struct Gm2L1Params* p = (__attribute__((cce_unif_buff)) struct Gm2L1Params*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->intri.blockCount = intriParams.blockCount;
    p->intri.blockLen = intriParams.blockLen;
    p->intri.srcStride = intriParams.srcStride;
    p->intri.dstStride = intriParams.dstStride;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1, 0);
    GetKfcClient()->PostMessage<false>(msg);
}

[aicore] inline void ScmDataCopyND2NZMsg(__attribute__((cce_cube_buff)) void* dst, __attribute__((cce_global)) void* src, const uint8_t dataTypeSize,
    const Nd2NzParams& intriParams, int32_t ubAddr)
{
                             ;
                          ;
                          ;
                                     ;
    auto msg = GetKfcClient()->AllocMessage();
                                                                  ;

    auto p = (__attribute__((cce_unif_buff)) struct Gm2L1Nd2NzParams*)&(GetKfcClient()->ubMsg->buffer);
    p->dst = dst;
    p->src = src;
    p->dataTypeLen = dataTypeSize;
    p->intri.ndNum = intriParams.ndNum;
    p->intri.nValue = intriParams.nValue;
    p->intri.dValue = intriParams.dValue;
    p->intri.srcNdMatrixStride = intriParams.srcNdMatrixStride;
    p->intri.dstNzC0Stride = intriParams.dstNzC0Stride;
    p->intri.dstNzNStride = intriParams.dstNzNStride;
    p->intri.dstNzMatrixStride = intriParams.dstNzMatrixStride;
    p->intri.srcDValue = intriParams.srcDValue;
    GetKfcClient()->ubMsg->ubAddr = ubAddr;
    GetKfcClient()->ubMsg->head = KfcMsgMakeFlag(KFC_Enum::SCMFUN_GM2L1ND2NZ, 0);
    GetKfcClient()->PostMessage<false>(msg);
}
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h" 2


namespace AscendC {


template <typename T>
[aicore] inline void CheckDataCopyPadParams(uint16_t blockCount, uint32_t blockLen, bool isGMtoUB)
{
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
}


[aicore] inline void CheckDataCopyParams(uint16_t blockCount, uint16_t blockLen)
{
                                                                                  ;
                                                                              ;
}

[aicore] inline void ValidateUbL1Address(uint64_t absUbAddr, uint64_t absL1Addr, uint32_t tensorSize)
{


      ;



      ;


      ;



      ;
}




template <typename T>
[aicore] inline void DataCopyGM2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                                             ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_global)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] inline void DataCopyGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, false, intriParams);
        }
        copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, (int8_t)0, static_cast<uint16_t>(intriParams.blockCount),
            static_cast<uint16_t>(intriParams.blockLen), static_cast<uint16_t>(intriParams.srcStride), static_cast<uint16_t>(intriParams.dstStride), (pad_t)0);
    } else if constexpr(g_coreType == AscendC::AIV) {
# 104 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)src, intriParams, -1);


    }
}

template <typename T>
[aicore] inline void DataCopyUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                     ;
                                                                                                     ;
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                    ;

        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] inline void DataCopyUB2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);

                                                    ;

                                                    ;
        copy_ubuf_to_ubuf((__attribute__((cce_unif_buff)) void*)dst, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] inline void DataCopyUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;

                                                    ;

        uint32_t tensorSize = intriParams.blockCount * intriParams.blockLen * 32;
        int32_t ubAddr = -1;







        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.srcStride);
# 185 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
        ScmDataCopyMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, intriParams, ubAddr);


    }
}

template <typename T>
[aicore] inline void DataCopyUB2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nd2NzParams& intriParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
                                         ;

                                                    ;
        uint32_t tensorSize = intriParams.nValue * intriParams.dValue;
        int32_t ubAddr = -1;
# 215 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);

        copy_ubuf_to_gm((__attribute__((cce_global)) void*)gmAddr, (__attribute__((cce_unif_buff)) void*)src, 0, 1, tensorSize * sizeof(T) / 32, 0, 0);






        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), intriParams, ubAddr);


    }
}

template <typename T>
[aicore] inline void DataCopyL12UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
                                                                                ;
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE1"))) void DataCopyL12BTImpl(const uint64_t dst, __attribute__((cce_cube_buff)) T* src, const uint16_t isEnableConv,
    const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_bt(dst, (__attribute__((cce_cube_buff)) void*)src, isEnableConv, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("FIX"))) void DataCopyL12FBImpl(
    __attribute__((cce_fixpipe_buff)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams &intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
        copy_cbuf_to_fbuf((__attribute__((cce_fixpipe_buff)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}

template <typename T>
[aicore] inline void DataCopyGM2L1ND2NZImplBase(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNd2Nz(src, workSpace, true, intriParams);
        }
        if constexpr (sizeof(T) == B8_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
            copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) T*)dst, (__attribute__((cce_global)) T*)src, 0, intriParams.ndNum, intriParams.nValue,
                intriParams.dValue, intriParams.srcNdMatrixStride, intriParams.srcDValue, intriParams.dstNzC0Stride,
                intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
        }
    } else if constexpr(g_coreType == AscendC::AIV) {






        ScmDataCopyND2NZMsg(dst, src, sizeof(T), intriParams, -1);


    }
}

template <typename T>
[aicore] inline void DataCopyGM2L1ND2NZImpl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{

                                                       ;
    if constexpr (SupportType<T, int4b_t>()) {
        DataCopyGM2L1ND2NZImplBase((__attribute__((cce_cube_buff)) int8_t *)dst, (__attribute__((cce_global)) int8_t *)src, intriParams);
    } else if (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE || sizeof(T) == B32_BYTE_SIZE){
        DataCopyGM2L1ND2NZImplBase(dst, src, intriParams);
    } else {


                                                                ;
    }
}

template <typename T>
[aicore] inline void DataCopyL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckDataCopyParams(intriParams.blockCount, intriParams.blockLen);
                                                                                                                   ;
        if constexpr (g_gm_overflow_check) {
            __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
            AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, false, intriParams);
        }
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)dst, (__attribute__((cce_cube_buff)) void*)src, 0, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}





template <typename T>
[aicore] inline void CopyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const CopyRepeatParams& repeatParams)
{
                                                                                                 ;
                                                                                                 ;
    if constexpr(sizeof(T) == B16_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src, repeatTime, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
        vcopy((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src, repeatTime, repeatParams.dstStride,
            repeatParams.srcStride, repeatParams.dstRepeatSize, repeatParams.srcRepeatSize);
    } else {

                                                                                                                      ;
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], uint8_t repeatTime,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        CopyIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void CopyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const CopyRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        CopyIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}



template <typename T, typename U>
[aicore] inline __attribute__((inout_pipe("MTE1"))) void DataCopyL12L0CImpl(
    __attribute__((cce_cube_c)) T* dst, __attribute__((cce_cube_buff)) U* src, const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        static_assert((SupportType<Tuple<U, T>, Tuple<half, half>, Tuple<float, half>, Tuple<float, bfloat16_t>,
            Tuple<float, float>, Tuple<bfloat16_t, bfloat16_t>, Tuple<int32_t, int32_t>, Tuple<uint32_t, uint32_t>>()),
            "Failed to check dtype in DataCopy from A1 / B1 to CO1, current api support "
            "dtype combination is src: half, dst: half; src: float, dst: half / bfloat16_t / float; src: bfloat16_t, "
            "dst: bfloat16_t; src: int32_t, dst: int32_t; src: uint32_t, dst: uint32_t.");
                                                                                                                    ;
                                                                                                             ;
        copy_matrix_cbuf_to_cc((__attribute__((cce_cube_c)) T*)dst, (__attribute__((cce_cube_buff)) U*)src, intriParams.blockCount, intriParams.blockLen,
            intriParams.srcStride, intriParams.dstStride);
    }
}




template <typename T, typename U>
[aicore] inline void DataCopyL0C2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T, typename U>
[aicore] inline void DataCopyUB2L0CImpl(__attribute__((cce_cube_c)) T* dst, __attribute__((cce_unif_buff)) U* src, const DataCopyParams& intriParams,
    const DataCopyEnhancedParams& enhancedParams)
{
                                                                 ;
}

template <typename T>
[aicore] inline void DataCopySliceGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParamsIn)
{

                                                ;
    DataCopyPadExtParams<T> padParams{ false, 0, 0, 0 };
    uint16_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadGm2UBImpl(dst, src, intriParams, padParams);
}

template <typename T>
[aicore] inline void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams,
    const DataCopyPadParams& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                              ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), true);
    if (padParams.isPad) {
        set_mov_pad_val(padParams.paddingValue);
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] inline void DataCopyPadGm2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyExtParams& intriParams,
    const DataCopyPadExtParams<T>& padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                       ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, true);
    if (padParams.isPad) {
        set_mov_pad_val(GetScalarBitcodeValue(static_cast<T>(padParams.paddingValue)));
    }
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(src, workSpace, true, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, padParams.leftPadding,
            padParams.rightPadding, intriParams.srcStride, intriParams.dstStride);
    } else if constexpr (sizeof(T) == B64_BYTE_SIZE) {
        copy_gm_to_ubuf_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen,
            (padParams.leftPadding << 1), (padParams.rightPadding << 1), intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] inline void DataCopySliceUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParamsIn)
{

                                                ;
    uint32_t burstLen = intriParamsIn.blockLen * ONE_BLK_SIZE;
    DataCopyExtParams intriParams{ intriParamsIn.blockCount, burstLen, intriParamsIn.srcStride, intriParamsIn.dstStride,
        0 };
    DataCopyPadUB2GMImpl(dst, src, intriParams);
}

template <typename T>
[aicore] inline void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                       ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            static_cast<uint32_t>(intriParams.srcStride), static_cast<uint32_t>(intriParams.dstStride));
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            static_cast<uint32_t>(intriParams.srcStride), static_cast<uint32_t>(intriParams.dstStride));
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            static_cast<uint32_t>(intriParams.srcStride), static_cast<uint32_t>(intriParams.dstStride));
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] inline void DataCopyPadUB2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                       ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr (g_gm_overflow_check && (sizeof(T) == B8_BYTE_SIZE || sizeof(T) == B16_BYTE_SIZE
        || sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE)) {
        __attribute__((cce_global)) uint8_t* workSpace = GetSysWorkSpacePtr();
        AscendCUtils::CheckGmMemOverflowNormal(dst, workSpace, false, true, intriParams);
    }
    if constexpr (sizeof(T) == B8_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b8(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            static_cast<uint32_t>(intriParams.srcStride), static_cast<uint32_t>(intriParams.dstStride));
    } else if constexpr (sizeof(T) == B16_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b16(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            static_cast<uint32_t>(intriParams.srcStride), static_cast<uint32_t>(intriParams.dstStride));
    } else if constexpr (sizeof(T) == B32_BYTE_SIZE || sizeof(T) == B64_BYTE_SIZE) {
        copy_ubuf_to_gm_align_b32(dst, src, 0, intriParams.blockCount, intriParams.blockLen, static_cast<uint8_t>(0), static_cast<uint8_t>(0),
            intriParams.srcStride, intriParams.dstStride);
    } else {


                                                                                                ;
    }
}

template <typename T>
[aicore] inline void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, static_cast<uint32_t>(intriParams.blockLen), false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                          ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 592 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);





        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);


    }
}

template <typename T>
[aicore] inline void DataCopyPadUB2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyExtParams& intriParams,
    const Nd2NzParams& nd2nzParams)
{

                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
    CheckDataCopyPadParams<T>(intriParams.blockCount, intriParams.blockLen, false);
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                       ;


                          ;
        uint32_t tensorSize = nd2nzParams.nValue * nd2nzParams.dValue;
        int32_t ubAddr = -1;
# 641 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
        __attribute__((cce_global)) uint8_t* gmAddr = (GetKfcClient()->AllocUB(tensorSize, ubAddr));

        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
        SetFlag<HardEvent::V_MTE3>(eventID);
        WaitFlag<HardEvent::V_MTE3>(eventID);


                                                                                            ;
                                                                                                                       ;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)src, intriParams);





        ScmDataCopyND2NZMsg((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)gmAddr, sizeof(T), nd2nzParams, ubAddr);


    }
}



[aicore] inline void ScmDataCopy(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Params*>(kfcMsgPtr);



      ;
    auto dst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
    auto& intriParams = scmCopyParams->intri;







    copy_gm_to_cbuf((__attribute__((cce_cube_buff)) void*)dst, (__attribute__((cce_global)) void*)scmCopyParams->src, (int8_t)0, static_cast<uint16_t>(intriParams.blockCount),
        static_cast<uint16_t>(intriParams.blockLen), static_cast<uint16_t>(intriParams.srcStride), static_cast<uint16_t>(intriParams.dstStride), (pad_t)0);

    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

[aicore] inline void ScmDataCopyND2NZ(__attribute__((cce_global)) void* kfcMsgPtr)
{
                                                                                               ;
    auto scmCopyParams = reinterpret_cast<__attribute__((cce_global)) struct Gm2L1Nd2NzParams*>(kfcMsgPtr);
    auto& intriParams = scmCopyParams->intri;
    auto l1AddrDst = reinterpret_cast<__attribute__((cce_cube_buff)) void*>(scmCopyParams->dst);
# 705 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_data_copy_impl.h"
    if (scmCopyParams->dataTypeLen == 2) {
        copy_gm_to_cbuf_multi_nd2nz_b16((__attribute__((cce_cube_buff)) half*)l1AddrDst, (__attribute__((cce_global)) half*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else if (scmCopyParams->dataTypeLen == 4) {
        copy_gm_to_cbuf_multi_nd2nz_b32s((__attribute__((cce_cube_buff)) float*)l1AddrDst, (__attribute__((cce_global)) float*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    } else {



          ;
        copy_gm_to_cbuf_multi_nd2nz_b8((__attribute__((cce_cube_buff)) int8_t*)l1AddrDst, (__attribute__((cce_global)) int8_t*)scmCopyParams->src, 0,
            intriParams.ndNum, intriParams.nValue, intriParams.dValue, intriParams.srcNdMatrixStride,
            intriParams.srcDValue, intriParams.dstNzC0Stride, intriParams.dstNzNStride, intriParams.dstNzMatrixStride);
    }
    event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE1));
    SetFlag<HardEvent::MTE2_MTE1>(eventID);
    WaitFlag<HardEvent::MTE2_MTE1>(eventID);
}

template <typename T>
[aicore] inline void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                                                         ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(T);
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(T));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, static_cast<uint32_t>(computeLen),
        static_cast<uint32_t>(intriParams.srcDValue * sizeof(T) - computeLen),
        static_cast<uint32_t>((dstNzNStride - static_cast<uint16_t>(DEFAULT_C0_SIZE)) / static_cast<uint16_t>(DEFAULT_C0_SIZE)), 0 };
    DataCopyPadExtParams<T> padParams;
    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) T*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)(dst + dstOffset), (__attribute__((cce_global)) T*)(src + srcOffset), copyParams, padParams);
    }
}

template <typename T>
[aicore] inline void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_global)) T* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                                                         ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = DEFAULT_C0_SIZE / sizeof(T);
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T>
[aicore] inline void DataCopyUB2GMNZ2NDImplBase(__attribute__((cce_global)) T* dstAddr, __attribute__((cce_unif_buff)) T* srcAddr, uint16_t height,
    uint16_t width, uint16_t srcNStride, uint16_t dstDStride)
{

                                                                         ;
    const uint16_t blkCntLimit = UINT12_MAX;
    const uint16_t repeatTime = height / blkCntLimit;
    const uint16_t tailBlock = height % blkCntLimit;
    const uint16_t widthBlkNum = (width + BLOCK_CUBE - 1) / BLOCK_CUBE;

    for (uint16_t i = 0; i < widthBlkNum; ++i) {
        uint16_t num = (i != widthBlkNum -1) ? BLOCK_CUBE : (width - i * BLOCK_CUBE);
        uint32_t blockLen = static_cast<uint32_t>(num * sizeof(T));
        uint32_t dstStride = static_cast<uint32_t>((dstDStride - num) * sizeof(T));
        for (uint16_t j = 0; j < repeatTime; ++j) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + j * blkCntLimit * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + j * blkCntLimit * BLOCK_CUBE,
                {blkCntLimit, blockLen, 0, dstStride, 0});
        }
        if (tailBlock) {
            DataCopyPadUB2GMImpl(dstAddr + i * BLOCK_CUBE + repeatTime * blkCntLimit * dstDStride,
                srcAddr + i * srcNStride * BLOCK_CUBE + repeatTime * blkCntLimit * BLOCK_CUBE,
                {tailBlock, blockLen, 0, dstStride, 0});
        }
    }
}

template <typename T>
[aicore] inline void DataCopyUB2GMNZ2NDImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_unif_buff)) T* src, const Nz2NdParamsFull& intriParams)
{

                                                                         ;

                                                                                       ;
    const uint16_t ndNum = intriParams.ndNum;
    const uint16_t nValue = intriParams.nValue;
    const uint16_t dValue = intriParams.dValue;
    const uint16_t srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t srcNStride = intriParams.srcNStride;
    const uint16_t dstDStride = intriParams.dstDStride;
    const uint16_t dstNdMatrixStride = intriParams.dstNdMatrixStride;

    if (ndNum != 1 && nValue != 0) {

                                                                                                                      ;
    }
                                                                                                                ;
    for (uint16_t i = 0; i < ndNum; ++i) {
        DataCopyUB2GMNZ2NDImplBase(dst + i * dstNdMatrixStride, src + i * srcNdMatrixStride * BLOCK_CUBE * BLOCK_CUBE,
            nValue, dValue, srcNStride, dstDStride);
    }
}

template <>
[aicore] inline void DataCopyGM2UBSingleImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams,
    const int copyTime, const int computeNum)
{

                                                                         ;
    const uint16_t &nValue = intriParams.nValue;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& computeLen = computeNum * sizeof(float);
    const uint16_t& c0Count = BLOCK_CUBE;
    const uint16_t& maxC0Count = MAX_REPEAT_TIMES * c0Count;
    const uint16_t& maxdValue = MAX_REPEAT_TIMES * dValue;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& repeatCount = nValue / MAX_REPEAT_TIMES;
    const uint16_t& repeatTail = nValue % MAX_REPEAT_TIMES;
    const uint16_t& srcCopyStartOffset = copyTime * c0Count;
    const uint16_t& dstCopyStartOffset = copyTime * dstNzC0Stride * (DEFAULT_C0_SIZE / sizeof(float));
    DataCopyExtParams copyParams = { MAX_REPEAT_TIMES, static_cast<uint32_t>(computeLen),
        static_cast<uint32_t>(intriParams.srcDValue * sizeof(float) - computeLen),
        static_cast<uint32_t>((dstNzNStride * DEFAULT_C0_SIZE - static_cast<uint16_t>(c0Count) * sizeof(float)) / static_cast<uint16_t>(DEFAULT_C0_SIZE)),
        0 };
    DataCopyPadExtParams<float> padParams;
    if (computeNum < c0Count) {
        copyParams.dstStride = (c0Count - computeNum) * sizeof(float) / DEFAULT_C0_SIZE;
        padParams.paddingValue = 0;
    }

    for (int repeatTime = 0; repeatTime < repeatCount; ++repeatTime) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstCopyStartOffset + repeatTime * maxC0Count),
            (__attribute__((cce_global)) float*)(src + srcCopyStartOffset + repeatTime * maxdValue), copyParams, padParams);
    }
    copyParams.blockCount = repeatTail;
    if (repeatTail != 0) {
        int dstOffset = (dstCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * c0Count);
        int srcOffset = (srcCopyStartOffset + repeatCount * MAX_REPEAT_TIMES * dValue);
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) float*)(dst + dstOffset), (__attribute__((cce_global)) float*)(src + srcOffset), copyParams,
            padParams);
    }
}

template <>
[aicore] inline void DataCopyGM2UBND2NZImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_global)) float* src, const Nd2NzParams& intriParams)
{
    if constexpr(g_coreType != AscendC::AIV) {
        return;
    }

                                                                         ;
    const uint16_t &ndNum = intriParams.ndNum;
    const uint16_t& dValue = intriParams.dValue;
    const uint16_t& srcNdMatrixStride = intriParams.srcNdMatrixStride;
    const uint16_t& srcDValue = intriParams.srcDValue;
    const uint16_t& dstNzC0Stride = intriParams.dstNzC0Stride;
    const uint16_t& dstNzNStride = intriParams.dstNzNStride;
    const uint16_t& dstNzMatrixStride = intriParams.dstNzMatrixStride;
    const uint16_t& c0Count = BLOCK_CUBE;
    for (int index = 0; index < ndNum; ++index) {
        int16_t copyNum = (dValue + c0Count - 1) / c0Count;
        for (int copyTime = 0; copyTime < copyNum; ++copyTime) {
            int computeCount = (dValue >= (copyTime + 1) * c0Count) ? c0Count : (dValue % c0Count);
            DataCopyGM2UBSingleImpl(dst + dstNzMatrixStride, src + srcNdMatrixStride, intriParams, copyTime,
                computeCount);
        }
    }
}

template <typename T, typename U>
[aicore] inline void DataCopyL0C2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T, typename U>
[aicore] inline void DataCopyL0C2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src, const DataCopyCO12DstParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                          ;
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::NoQuant,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322F16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::F322BF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::DEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VDEQF16,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::QF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VQF322B8_PRE,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::REQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm(dst, src, intriParams.sid, intriParams.nSize, intriParams.mSize,
                    intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag, QuantMode_t::VREQ8,
                    intriParams.reluPre, intriParams.channelSplit, intriParams.nz2ndEn);
            default:


                                                      ;
        }
    }
}

template <typename T>
[aicore] inline void DataCopyL12UBIntf(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyParams &intriParams)
{
    DataCopyL12UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(),
        intriParams);
}

template <typename T>
[aicore] inline void DataCopyUB2L0CIntf(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)src.GetPhyAddr(),
        intriParams, enhancedParams);
}

#pragma begin_pipe(V)
template <typename T>
[aicore] inline void DataCopyUB2UBIntf(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyParams &intriParams)
{
    DataCopyUB2UBImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)src.GetPhyAddr(),
        intriParams);
}
#pragma end_pipe

template <typename T>
[aicore] inline void DataCopyPadL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyParams& intriParams)
{
                                                                        ;
}

template <typename T>
[aicore] inline void DataCopyPadL12GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_buff)) T* src, const DataCopyExtParams& intriParams)
{
                                                                        ;
}

template <typename T>
[aicore] inline void DataCopyPadGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyParams& intriParams,
    const DataCopyPadParams& padParams)
{
                                                                        ;
}

template <typename T>
[aicore] inline void DataCopyPadGM2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const DataCopyExtParams& intriParams,
    const DataCopyPadExtParams<T>& padParams)
{
                                                                        ;
}
}
# 31 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_data_copy_base_impl.h" 2
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_data_copy_base_impl.h"
namespace AscendC {

enum class ReduceType : uint8_t {
    NO_REDUCE,
    REDUCE_ADD,
    REDUCE_MIN,
    REDUCE_MAX,
};

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] inline void DataCopyWithReduce(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const uint32_t count)
{
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = count / AscendCUtils::GetC0Count(sizeof(T));
    DataCopyWithReduce<T, reduceType>(dst, src, repeatParams);
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] inline void DataCopyWithReduce(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const DataCopyParams& repeatParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopy(dst, src, repeatParams);
    AscendC::SetAtomicNoneImpl();
}

template <typename T, enum ReduceType reduceType = ReduceType::NO_REDUCE>
[aicore] inline void DataCopyPadWithReduce(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const DataCopyExtParams& dataCopyExtParams)
{
    AscendC::SetAtomicNoneImpl();
    if constexpr (reduceType == ReduceType::REDUCE_ADD) {
        AscendC::SetAtomicAddImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MIN) {
        AscendC::SetAtomicMinImpl<T>();
    } else if constexpr (reduceType == ReduceType::REDUCE_MAX) {
        AscendC::SetAtomicMaxImpl<T>();
    }
    DataCopyPad(dst, src, dataCopyExtParams);
    AscendC::SetAtomicNoneImpl();
}


[aicore] inline void DataCopyGetOffsetList(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue, uint32_t *count, uint32_t *offsetList)
{
    uint32_t sliceSize = 1;
    uint32_t copyCount = 1;
    uint32_t currentCount = 1;
    uint32_t preCopyCount = 0;
    uint32_t iter = 0;
    uint32_t totalSliceCount = 0;

    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            *(offsetList + totalSliceCount) = 0;
            totalSliceCount++;
            continue;
        }
        iter = 0;
        sliceSize = sliceSize * shapeInfo[i - 1];
        currentCount =
            (sliceInfo[i].endIndex - sliceInfo[i].startIndex + 1 + sliceInfo[i].stride) / (1 + sliceInfo[i].stride);
        preCopyCount = copyCount;
        copyCount = copyCount * currentCount;
        for (uint32_t j = preCopyCount; j < copyCount; j += preCopyCount) {
            iter++;
            for (uint32_t k = 0; k < preCopyCount; k++) {
                *(offsetList + totalSliceCount) =
                    (*(offsetList + k)) + (iter * (1 + sliceInfo[i].stride)) * sliceSize;
                totalSliceCount++;
            }
        }
    }
    *count = totalSliceCount;
}

[aicore] inline uint32_t DataCopyGetPhyStartIndex(
    const SliceInfo sliceInfo[], uint32_t shapeInfo[], const uint32_t dimValue)
{
    uint32_t phyStartIndex = 0;
    uint32_t sliceSize = 1;
    for (uint32_t i = 0; i < dimValue; i++) {
        if (i == 0) {
            phyStartIndex = phyStartIndex + sliceInfo[i].startIndex;
        } else {
            sliceSize = sliceSize * shapeInfo[i - 1];
            phyStartIndex = phyStartIndex + sliceSize * sliceInfo[i].startIndex;
        }
    }
    return phyStartIndex;
}
# 164 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_data_copy_base_impl.h"
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h" 2

namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline void __attribute__((inout_pipe("MTE2"))) DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
    if (dstHWPos == Hardware::UB) {





        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            repeatParams);

    } else if (dstHWPos == Hardware::L1) {





        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            repeatParams);

    } else {


                                                                                              ;
    }
}

[aicore] inline void CheckNd2NzParams(Nd2NzParams params, const __attribute__((cce_global)) char *msg)
{
    constexpr uint16_t nd2NzLimit = 16384;
                                                                        ;
                                                                          ;
                                                                                        ;
                                                                                      ;
}
# 145 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const Nd2NzParams& intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
                                                                                       ;
    if (dstHWPos == Hardware::L1) {

        DataCopyGM2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            intriParams);
    } else if (dstHWPos == Hardware::UB) {
        DataCopyGM2UBND2NZImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            intriParams);
    } else {


                                                                                              ;
    }
}
# 214 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const Nd2NzParams &intriParams)
{
    CheckNd2NzParams(intriParams, "DataCopy with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(src, Hardware::UB, "src", "VECIN / VECCALC / VECOUT",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
    CheckTensorPos<T>(dst, Hardware::L1, "dst", "TSCM",
        "DataCopy from LocalTensor to LocalTensor with Nd2NzParams");
                                                                                       ;
    DataCopyUB2L1ND2NZImpl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        intriParams);
}
# 239 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const DataCopyParams& repeatParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
# 259 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
    if (srcHWPos == Hardware::UB) {





        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
            repeatParams);

    } else if (srcHWPos == Hardware::L1) {





        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
            repeatParams);

    } else {







                                                                                              ;

    }







}
# 307 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyParams &repeatParams)
{
    using PrimType = PrimT<T>;

    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());

                                                                                        ;
    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::UB) {






            DataCopyUB2UBIntf(dst, src, repeatParams);
        } else if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
                                repeatParams);
        } else {



                                                                                                  ;





        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dst, src, repeatParams);
        } else if (dstHWPos == Hardware::BIAS) {
            CheckTensorAlign<T>(dst, 64, "dst", "DataCopy from C1 to C2");
            CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "DataCopy from C1 to C2");
            DataCopyL12BTImpl((uint64_t)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(), static_cast<uint16_t>(0),
                            repeatParams);

        } else if (dstHWPos == Hardware::FIXBUF) {
            CheckTensorAlign<T>(dst, 128, "dst", "DataCopy from A1 / B1 / C1 to C2PIPE2GM");
            CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "DataCopy from A1 / B1 / C1 to C2PIPE2GM");
            DataCopyL12FBImpl((__attribute__((cce_fixpipe_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
                            repeatParams);

        } else {


                                                                                                  ;
        }
    } else {


                                                                                              ;
    }
}
# 379 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T, typename U>
[aicore] inline void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());

                                                                                                ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::BIAS) {

            CheckTensorAlign<T>(dst, 64, "dst", "DataCopy from C1 to C2");
            CheckTensorAlign<U>(src, ONE_BLK_SIZE, "src", "DataCopy from C1 to C2");
# 412 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
            if constexpr (Std::is_same<PrimDstType, PrimSrcType>::value) {

                DataCopyL12BTImpl((uint64_t)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimSrcType*)src.GetPhyAddr(),
                    static_cast<uint16_t>(0), repeatParams);
            } else if constexpr (Std::is_same<PrimDstType, float>::value && Std::is_same<PrimSrcType, half>::value) {
                DataCopyL12BTImpl((uint64_t)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) half *)src.GetPhyAddr(), static_cast<uint16_t>(1),
                    repeatParams);
            } else {

                                                                                                  ;
            }
        } else {


                                                                                                  ;
        }
    } else {


                                                                                              ;
    }
}
# 448 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T, bool IsSetMask>
[aicore] inline __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const uint8_t repeatTime, const CopyRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}


template <typename T, bool IsSetMask>
[aicore] inline __attribute__((inout_pipe("V"))) void Copy(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask, const uint8_t repeatTime, const CopyRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    CopyImpl<PrimType, IsSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 510 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T> &dst, const GlobalTensor<T> &src,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(Std::is_same<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");






    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : src.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dst.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);
    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();





    if ((srcSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {




            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T *)dst.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)src.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);

        }
    } else {
        repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {




            DataCopySliceGm2UBImpl((__attribute__((cce_unif_buff)) T *)dst.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_global)) T *)src.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);

        }
    }
}
# 585 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T> &dst, const LocalTensor<T> &src,
    const SliceInfo dstSliceInfo[], const SliceInfo srcSliceInfo[], const uint32_t dimValue)
{
    using PrimType = PrimT<T>;
    static_assert(Std::is_same<PrimType, T>::value, "TensorTrait is not supported by DataCopy with SliceInfo!");






    uint32_t srcStartIndex = 0;
    uint32_t dstStartIndex = 0;
    uint32_t srcOffsetListSize = 0;
    uint32_t dstOffsetListSize = 0;
    uint32_t srcShapeInfo[8];
    uint32_t dstShapeInfo[8];
    bool useShapeValue = !(srcSliceInfo[0].shapeValue == 0);
    for (int i = 0; i < dimValue; i++) {
        srcShapeInfo[i] = useShapeValue ? srcSliceInfo[i].shapeValue : src.GetShapeInfo().shape[i];
        dstShapeInfo[i] = useShapeValue ? dstSliceInfo[i].shapeValue : dst.GetShapeInfo().shape[i];
    }

    srcStartIndex = DataCopyGetPhyStartIndex(srcSliceInfo, srcShapeInfo, dimValue);
    dstStartIndex = DataCopyGetPhyStartIndex(dstSliceInfo, dstShapeInfo, dimValue);
    uint32_t dstOffsetList[MAX_SLICE_SIZE];
    uint32_t srcOffsetList[MAX_SLICE_SIZE];
    DataCopyGetOffsetList(srcSliceInfo, srcShapeInfo, dimValue, &srcOffsetListSize, srcOffsetList);
    DataCopyGetOffsetList(dstSliceInfo, dstShapeInfo, dimValue, &dstOffsetListSize, dstOffsetList);

    struct DataCopyParams repeatParams;
    repeatParams.blockLen = srcSliceInfo[0].burstLen;
    uint32_t oneSliceLen = srcSliceInfo[0].burstLen * AscendCUtils::GetC0Count(sizeof(T)) + srcSliceInfo[0].stride;
    repeatParams.blockCount =
        (srcSliceInfo[0].endIndex - srcSliceInfo[0].startIndex + 1 + srcSliceInfo[0].stride) / oneSliceLen;
    repeatParams.srcStride = srcSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();




    if ((dstSliceInfo[0].stride * sizeof(T)) % AscendCUtils::GetC0Size() == 0) {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T) / AscendCUtils::GetC0Size();
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {




            DataCopyUB2GMImpl((__attribute__((cce_global)) T *)dst.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)src.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);

        }
    } else {
        repeatParams.dstStride = dstSliceInfo[0].stride * sizeof(T);
        for (uint32_t i = 0; i < srcOffsetListSize; i++) {




            DataCopySliceUB2GMImpl((__attribute__((cce_global)) T *)dst.GetPhyAddr() + dstStartIndex + dstOffsetList[i],
                (__attribute__((cce_unif_buff)) T *)src.GetPhyAddr() + srcStartIndex + srcOffsetList[i], repeatParams);

        }
    }
}
# 658 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const uint32_t count)
{
    using PrimType = PrimT<T>;
                                                                                   ;
    struct DataCopyParams repeatParams;
# 685 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
    {



                      ;
        repeatParams.blockLen = count / AscendCUtils::GetC0Count(sizeof(PrimType));
    }
    DataCopy(dst, src, repeatParams);
}
# 702 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const uint32_t count)
{
    using PrimType = PrimT<T>;
                                                                                   ;
    struct DataCopyParams repeatParams;
# 729 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
    {



                      ;
        repeatParams.blockLen = count / AscendCUtils::GetC0Count(sizeof(PrimType));
    }
    DataCopy(dst, src, repeatParams);
}
# 746 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T> &dst, const LocalTensor<T> &src, const uint32_t count)
{
    using PrimType = PrimT<T>;



                  ;
                                                                                   ;
    struct DataCopyParams repeatParams;

    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
if (srcHWPos != Hardware::L1) {
        repeatParams.blockLen = count / AscendCUtils::GetC0Count(sizeof(PrimType));
    } else {
        if (dstHWPos == Hardware::UB) {
            repeatParams.blockLen = count / AscendCUtils::GetC0Count(sizeof(PrimType));
        } else if (dstHWPos == Hardware::BIAS) {
            repeatParams.blockLen = count / (64 / sizeof(PrimType));
        } else if (dstHWPos == Hardware::FIXBUF) {
            repeatParams.blockLen = count / (128 / sizeof(PrimType));
        }
    }
    DataCopy(dst, src, repeatParams);
}

[aicore] inline void CheckNz2NdParams(const Nz2NdParamsFull& params)
{
    constexpr uint16_t nz2NdLimit = 8192;
                                                                                                    ;
                                                                                                      ;
                                                                                                      ;
                                                                                                                           ;
                                                                                                              ;
}







template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const Nz2NdParamsFull& intriParams)
{
    CheckNz2NdParams(intriParams);
    using PrimType = PrimT<T>;
# 803 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
    if (srcHWPos != Hardware::UB) {





                                                                                              ;

    }






    DataCopyUB2GMNZ2NDImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        intriParams);







}
# 848 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopy(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
                                                                                                         ;

    if (dstHWPos == Hardware::UB) {






        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            intriParams);

    } else if (dstHWPos == Hardware::L1) {






        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            intriParams);

    } else {


                                                                                              ;
    }
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<T>& src,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
                                                                                                         ;





    if (srcHWPos == Hardware::UB) {





        DataCopyUB2GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
            intriParams);

    } else if (srcHWPos == Hardware::L1) {





        DataCopyL12GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
            intriParams);

    } else {


                                                                                              ;
    }
}

template <typename T>
[aicore] inline void DataCopy(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    using PrimType = PrimT<T>;
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
                                                                                                         ;

    if (srcHWPos == Hardware::UB) {
        if (dstHWPos == Hardware::L1) {

            DataCopyUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
                                intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyUB2L0CIntf(dst, src, intriParams, enhancedParams);
        } else if (dstHWPos == Hardware::UB) {

            DataCopyUB2UBIntf(dst, src, intriParams);
        } else {


                                                                                                  ;
        }
    } else if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL12UBIntf(dst, src, intriParams);
        } else if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
                            intriParams, enhancedParams);
        } else {


                                                                                                  ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimType*)src.GetPhyAddr(),
                            intriParams, enhancedParams);
        } else {


                                                                                                  ;
        }
    } else {







                                                                                              ;

    }
}

template <typename T, typename U>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");
    CheckTensorPos<T>(dst, Hardware::L1, "dst", "A1",
        "DataCopy from LocalTensor to LocalTensor with DataCopyCO12DstParams");
                                                                                         ;

    DataCopyL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), intriParams);
}

template <typename T, typename U>
[aicore] inline void DataCopy(const GlobalTensor<T>& dst, const LocalTensor<U>& src,
    const DataCopyCO12DstParams& intriParams)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1",
        "DataCopy from LocalTensor to GlobalTensor with DataCopyCO12DstParams");
                                                                                         ;

    DataCopyL0C2GMImpl((__attribute__((cce_global)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), intriParams);
}



template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, bfloat16_t>::value &&
    Std::is_same<PrimT<U>, float>::value, bool>::type>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
                                                                                                         ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimT<U>*)src.GetPhyAddr(),
                intriParams, enhancedParams);
        } else {


                                                                                                  ;
        }
    } else {


                                                                                              ;
    }
}



template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, half>::value &&
    Std::is_same<PrimT<U>, float>::value, bool>::type>
[aicore] inline void DataCopy(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const DataCopyParams& intriParams, const DataCopyEnhancedParams& enhancedParams)
{
    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
                                                                                                         ;
    if (srcHWPos == Hardware::L1) {
        if (dstHWPos == Hardware::L0C) {

            DataCopyL12L0CImpl((__attribute__((cce_cube_c)) half*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) float*)src.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {


                                                                                                  ;
        }
    } else if (srcHWPos == Hardware::L0C) {
        if (dstHWPos == Hardware::UB) {

            DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) half*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) float*)src.GetPhyAddr(), intriParams,
                enhancedParams);
        } else {


                                                                                                  ;
        }
    } else {


                                                                                              ;
    }
}

template <typename T, typename U>
[aicore] inline void CheckTensorL0C2UB(const LocalTensor<T>& dst, const LocalTensor<U>& src)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dst, Hardware::UB, "dst", "CO2",
        "DataCopy from LocalTensor(CO1) to LocalTensor(CO2) with DataCopyEnhancedParams");
}


template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, half>::value &&
    Std::is_same<PrimT<U>, int32_t>::value, bool>::type>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dst, src);
                                                                                                         ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) half*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)src.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, int16_t>::value &&
    Std::is_same<PrimT<U>, int32_t>::value, bool>::type>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dst, src);
                                                                                                         ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int16_t*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)src.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, int8_t>::value &&
    Std::is_same<PrimT<U>, int32_t>::value, bool>::type>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dst, src);
                                                                                                         ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) int8_t*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)src.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, uint8_t>::value &&
    Std::is_same<PrimT<U>, int32_t>::value, bool>::type>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorL0C2UB(dst, src);
                                                                                                         ;
    DataCopyL0C2UBImpl((__attribute__((cce_unif_buff)) uint8_t*)dst.GetPhyAddr(), (__attribute__((cce_cube_c)) int32_t*)src.GetPhyAddr(), intriParams,
        enhancedParams);
}


template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, float>::value &&
    Std::is_same<PrimT<U>, half>::value, bool>::type>
[aicore] inline __attribute__((inout_pipe("V"))) void DataCopy(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const DataCopyParams &intriParams, const DataCopyEnhancedParams &enhancedParams)
{
    CheckTensorPos<U>(src, Hardware::UB, "src", "CO2",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
    CheckTensorPos<T>(dst, Hardware::L0C, "dst", "CO1",
        "DataCopy from LocalTensor(CO2) to LocalTensor(CO1) with DataCopyEnhancedParams");
                                                                                                         ;
    DataCopyUB2L0CImpl((__attribute__((cce_cube_c)) float*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) half*)src.GetPhyAddr(), intriParams,
        enhancedParams);
}
# 1193 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dst,
    const GlobalTensor<T> &src, const DataCopyParams &dataCopyParams, const DataCopyPadParams &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    if (dstHWPos == Hardware::UB) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            dataCopyParams, padParams);
} else if (dstHWPos == Hardware::L1) {
        DataCopyPadGM2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
            dataCopyParams, padParams);
    } else {







                                                                                              ;

    }
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dst,
    const LocalTensor<T> &src, const DataCopyParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                               ;
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
    if (srcHWPos == Hardware::UB) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
            dataCopyParams);
} else if (srcHWPos == Hardware::L1) {
        DataCopyPadL12GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
            dataCopyParams);
    } else {







                                                                                              ;

    }
}


template <typename T>
[aicore] inline void DataCopyPad(const LocalTensor<T> &dst,
    const LocalTensor<T> &src, const DataCopyParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopyPad with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dst, Hardware::L1, "dst", "TSCM", "DataCopyPad with Nd2NzParams");
    CheckTensorPos<T>(src, Hardware::UB, "src", "VECIN / VECOUT", "DataCopyPad with Nd2NzParams");
                                                                                                            ;
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}
# 1297 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dst,
    const GlobalTensor<T> &src, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<T> &padParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    const Hardware dstHWPos = GetPhyType((TPosition)dst.GetPosition());
    if (dstHWPos == Hardware::UB) {
        DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_global)) T*)src.GetPhyAddr(),
            dataCopyParams, padParams);
} else if (dstHWPos == Hardware::L1) {
        DataCopyPadGM2L1Impl((__attribute__((cce_cube_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_global)) T*)src.GetPhyAddr(),
            dataCopyParams, padParams);
    } else {







                                                                                              ;

    }
}




template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value &&
    (!Std::is_same<T, U>::value), bool>::type>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void DataCopyPad(const LocalTensor<T> &dst,
    const GlobalTensor<T> &src, const DataCopyExtParams &dataCopyParams, const DataCopyPadExtParams<U> &padParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }






    CheckTensorPos<T>(dst, Hardware::UB, "dst", "VECIN / VECOUT", "DataCopyPad from GM to VECIN / VECOUT");
    DataCopyPadGm2UBImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimType*)src.GetPhyAddr(),
        dataCopyParams, padParams);
}
# 1377 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE3"))) void DataCopyPad(const GlobalTensor<T> &dst,
    const LocalTensor<T> &src, const DataCopyExtParams &dataCopyParams)
{
    using PrimType = PrimT<T>;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                               ;
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());
    if (srcHWPos == Hardware::UB) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
            dataCopyParams);
} else if (srcHWPos == Hardware::L1) {
        DataCopyPadL12GMImpl((__attribute__((cce_global)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_cube_buff)) PrimType*)src.GetPhyAddr(),
            dataCopyParams);
    } else {







                                                                                              ;

    }
}


template <typename T>
[aicore] inline void DataCopyPad(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const DataCopyExtParams &dataCopyParams, const Nd2NzParams &nd2nzParams)
{
    CheckNd2NzParams(nd2nzParams, "DataCopyPad with Nd2NzParams");
    using PrimType = PrimT<T>;
    CheckTensorPos<T>(dst, Hardware::L1, "dst", "TSCM", "DataCopyPad with Nd2NzParams");
    CheckTensorPos<T>(src, Hardware::UB, "src", "VECIN / VECOUT", "DataCopyPad with Nd2NzParams");
                                                                                                            ;
    DataCopyPadUB2L1Impl((__attribute__((cce_cube_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        dataCopyParams, nd2nzParams);
}

template <typename T, TPosition pos>
[aicore] inline void SetPadValue(T paddingValue)
{

    if (g_coreType == AIC) {
        return;
    }
    set_mov_pad_val(GetScalarBitcodeValue((T)paddingValue));
# 1439 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
}
# 1477 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_data_copy_intf_impl.h"
}
# 373 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_data_copy_intf.h" 2
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_fixpipe.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_fixpipe.h"
namespace AscendC {
enum class CO2Layout : uint8_t {
    NZ = 0,
    ROW_MAJOR,
    COLUMN_MAJOR
};

struct FixpipeConfig {
    CO2Layout format;
};

constexpr FixpipeConfig CFG_NZ = {CO2Layout::NZ};
constexpr FixpipeConfig CFG_ROW_MAJOR = {CO2Layout::ROW_MAJOR};
constexpr FixpipeConfig CFG_COLUMN_MAJOR = {CO2Layout::COLUMN_MAJOR};

struct FixpipeParamsV220 {
    [aicore] FixpipeParamsV220() {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn)
    {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn),
          quantPre(quantPreIn),
          deqScalar(deqScalarIn),
          ndNum(ndNumIn),
          srcNdStride(srcNdStrideIn),
          dstNdStride(dstNdStrideIn),
          unitFlag(unitFlagIn)
    {}

    [aicore] FixpipeParamsV220(const uint16_t nSizeIn, const uint16_t mSizeIn, const uint16_t srcStrideIn,
        const uint32_t dstStrideIn, const bool reluEnIn, const QuantMode_t quantPreIn, const int64_t deqScalarIn,
        const uint16_t ndNumIn, const uint16_t srcNdStrideIn, const uint16_t dstNdStrideIn, const uint8_t unitFlagIn,
        const bool isChannelSplitIn)
        : nSize(nSizeIn),
          mSize(mSizeIn),
          srcStride(srcStrideIn),
          dstStride(dstStrideIn),
          reluEn(reluEnIn),
          quantPre(quantPreIn),
          deqScalar(deqScalarIn),
          ndNum(ndNumIn),
          srcNdStride(srcNdStrideIn),
          dstNdStride(dstNdStrideIn),
          unitFlag(unitFlagIn),
          isChannelSplit(isChannelSplitIn)
    {}

    uint16_t nSize = 0;
    uint16_t mSize = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;

    QuantMode_t quantPre = QuantMode_t::NoQuant;
    uint64_t deqScalar;

    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;
    bool reluEn = false;
    uint8_t unitFlag = 0;
    bool isChannelSplit = false;
};

using FixpipeParamsM300 = FixpipeParamsV220;
using FixpipeParamsM310 = FixpipeParamsV220;
}




namespace AscendC {

struct FixpipeTrait {
    QuantMode_t quantPre = QuantMode_t::NoQuant;
    bool enableRelu = false;
    bool enableChannleSplit = false;
    uint8_t unitFlag = false;
    uint8_t dualDstCtl = false;
};
constexpr FixpipeTrait DEFAULT_FIXPIPE_TRAIT;

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h" 2

namespace AscendC {
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h"
template <typename T>
[aicore] inline void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false);

template <typename T, bool setRelu = false>
[aicore] inline void SetFixPipeConfig(const LocalTensor<T> &preData, bool isUnitFlag = false);

[aicore] inline void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride);

[aicore] inline void SetFixpipePreQuantFlag(uint64_t config);

[aicore] inline void SetFixPipeClipRelu(uint64_t config);

template <typename T>
[aicore] inline void SetFixPipeAddr(const LocalTensor<T> &eleWiseData, uint16_t c0ChStride);



template <typename T, typename U, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] inline void Fixpipe(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const FixpipeParamsV220& intriParams);


template <typename T, typename U, const FixpipeConfig& config = CFG_ROW_MAJOR, typename S = uint64_t,
    typename Std::enable_if<Std::is_same<PrimT<S>, uint64_t>::value, bool>::type = true>
[aicore] inline void Fixpipe(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const LocalTensor<S>& cbufWorkspace, const FixpipeParamsV220& intriParams);


template <typename T, typename U, const FixpipeConfig& config = CFG_ROW_MAJOR>
[aicore] inline void Fixpipe(const GlobalTensor<T>& dst, const LocalTensor<U>& src,
    const FixpipeParamsV220& intriParams);


template <typename T, typename U, const FixpipeConfig& config = CFG_ROW_MAJOR, typename S = uint64_t,
    typename Std::enable_if<Std::is_same<PrimT<S>, uint64_t>::value, bool>::type = true>
[aicore] inline void Fixpipe(const GlobalTensor<T>& dst, const LocalTensor<U>& src,
    const LocalTensor<S>& cbufWorkspace, const FixpipeParamsV220& intriParams);
# 124 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h"
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_set_spr_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_set_spr_impl.h"
namespace AscendC {
[aicore] inline void SetQuantPreImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}

[aicore] inline void SetNdParaImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_nd_para(config);
    }
}

[aicore] inline void SetFpcImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_fpc(config);
    }
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_impl.h" 2

namespace AscendC {



template <typename T>
[aicore] inline void SetFixPipeConfigImpl(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(reluPre, Hardware::FIXBUF, "reluPre", "C2PIPE2GM", "SetFixPipeConfig");
        CheckTensorPos<T>(quantPre, Hardware::FIXBUF, "quantPre", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        config = config | ((uint64_t)reluPre.GetPhyAddr() >> 6);
        config = config | (((uint64_t)quantPre.GetPhyAddr() >> 7) << 8);
        config = config | (static_cast<uint64_t>(isUnitFlag) << 63);
        set_fpc(config);
    }
}

template <typename T, bool setRelu = false>
[aicore] inline void SetFixPipeConfigImpl(const LocalTensor<T> &pre, bool isUnitFlag = false)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckTensorPos<T>(pre, Hardware::FIXBUF, "pre", "C2PIPE2GM", "SetFixPipeConfig");
        uint64_t config = 0;
        if constexpr (setRelu) {
            config = config | ((uint64_t)pre.GetPhyAddr() >> 6);
        } else {
            config = config | (((uint64_t)pre.GetPhyAddr() >> 7) << 8);
        }
        config = config | (static_cast<uint64_t>(isUnitFlag) << 63);
        set_fpc(config);
    }
}

[aicore] inline void SetFixpipeNz2ndFlagImpl(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    if constexpr(g_coreType == AscendC::AIC) {
                                                                                                  ;
        uint64_t config = 0;
        config = config | (static_cast<uint64_t>(ndNum));
        config = config | (static_cast<uint64_t>(srcNdStride) << 16);
        config = config | (static_cast<uint64_t>(dstNdStride) << 32);
        set_nd_para(config);
    }
}

[aicore] inline void SetFixpipePreQuantFlagImpl(uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_quant_pre(config);
    }
}






struct FixpipeTiling {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] inline FixpipeTiling GenFixpipeTiling(uint16_t n)
{
    FixpipeTiling tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

template <typename T> struct FixpipeInfoParams {
    [aicore] inline FixpipeInfoParams() {}

    [aicore] inline FixpipeInfoParams(const FixpipeParams<T>& intriParams, const uint8_t dstByteSize)
    {
        dstTypeSize = dstByteSize;
        srcTypeSize = B32_BYTE_SIZE;
        howo = (intriParams.burstLen * ONE_BLK_SIZE / srcTypeSize) / BLOCK_CUBE;
        roundHowo = DivCeil(howo, BLOCK_CUBE) * BLOCK_CUBE;
        fracLen = BLOCK_CUBE;
        c0 = fracLen;




        n = intriParams.cburstNum * BLOCK_CUBE;
        m = howo;



        srcStride = intriParams.srcStride * BLOCK_CUBE + roundHowo;




        if (intriParams.nz2ndParams.nz2ndEn) {

            dstStride = intriParams.dstStride;



              ;
            n = intriParams.nz2ndParams.originalNSize;
        } else {


            dstStride = intriParams.dstStride + intriParams.burstLen * dstTypeSize / srcTypeSize;
        }

        sid = 0;
        quantPre = intriParams.quantParams.quantPre;
        reluEn = intriParams.reluEn;
        nz2ndEn = intriParams.nz2ndParams.nz2ndEn;
        ndNum = intriParams.nz2ndParams.ndNum;
        srcNdStride = intriParams.nz2ndParams.srcNdStride;
        dstNdStride = intriParams.nz2ndParams.dstNdStride;


        if (intriParams.quantParams.quantPre == QuantMode_t::DEQF16 ||
            intriParams.quantParams.quantPre == QuantMode_t::QF322B8_PRE ||
            intriParams.quantParams.quantPre == QuantMode_t::REQ8) {
            deqScalar = intriParams.quantParams.deqScalar;
        }

        unitFlag = intriParams.unitFlag;
    }


    uint8_t dstTypeSize = 0;
    uint8_t srcTypeSize = 0;
    uint16_t howo = 0;
    uint16_t roundHowo = 0;
    uint8_t fracLen = 0;
    uint8_t c0 = 0;
    uint16_t n = 0;
    uint16_t m = 0;
    uint16_t srcStride = 0;
    uint32_t dstStride = 0;
    uint16_t burstLen = 0;
    uint8_t sid = 0;
    bool channelSplit = false;
    uint8_t unitFlag = 0;


    QuantMode_t quantPre = QuantMode_t::NoQuant;
    __attribute__((cce_cube_buff)) uint64_t* cbufWorkspace;
    uint64_t deqScalar = 0;

    bool reluEn = false;

    bool nz2ndEn = false;
    uint16_t ndNum = 1;
    uint16_t srcNdStride = 0;
    uint16_t dstNdStride = 0;

    FixpipeTiling tiling;
};


template <typename T>
[aicore] inline void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, FixpipeInfoParams<T> &fixpipeInfo)
{
                                                                                                          ;
}

template <typename T, typename U>
[aicore] inline void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src, FixpipeInfoParams<U>& fixpipeInfo)
{
                                                                                                         ;






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }





    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }

    FixpipeL0cToL1(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename T, typename U>
[aicore] inline void FixpipeL0C2GMImpl(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src, FixpipeInfoParams<U>& fixpipeInfo)
{
    if (fixpipeInfo.nz2ndEn) {
        uint64_t ndPara = static_cast<uint64_t>(fixpipeInfo.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(fixpipeInfo.ndNum);
        SetNdParaImpl(ndPara);
    }






    if (fixpipeInfo.quantPre == QuantMode_t::VDEQF16 || fixpipeInfo.quantPre == QuantMode_t::VQF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::VREQ8) {
        fixpipeInfo.tiling = GenFixpipeTiling(fixpipeInfo.n);
        for (uint16_t i = 0; i < fixpipeInfo.tiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.nSize, i);
        }

        if (fixpipeInfo.tiling.tailNSize > 0) {
            FixpipeL0C2GMImplN(dst, src, fixpipeInfo, fixpipeInfo.tiling.tailNSize, fixpipeInfo.tiling.nIterNum);
        }
        return;
    }






    if (fixpipeInfo.quantPre == QuantMode_t::DEQF16 || fixpipeInfo.quantPre == QuantMode_t::QF322B8_PRE ||
        fixpipeInfo.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(fixpipeInfo.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, fixpipeInfo.n);
}

template <typename T, typename U>
[aicore] inline void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src,
    const FixpipeInfoParams<U>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1(dst, src, fixpipeInfo, calNSize, nIterIndex);
}

template <typename T, typename U>
[aicore] inline void FixpipeL0C2GMImplN(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src,
    const FixpipeInfoParams<U>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(fixpipeInfo, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut(dst, src, fixpipeInfo, calNSize, nIterIndex);
}



template <typename T, typename U>
[aicore] inline void FixpipeL0cToL1(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_cube_c)) U* src,
    const FixpipeInfoParams<U>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(T);
    }



    return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename T>
[aicore] inline uint64_t GetGMLen(const FixpipeInfoParams<T>& fixpipeInfo,
                                    const uint16_t& calNSize, const uint16_t& dstEleSize)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen = (cburstNum - 1) * fixpipeInfo.dstStride * dstStrideUnit +
        fixpipeInfo.m * fractalNsize * dstEleSize;
    if (fixpipeInfo.nz2ndEn) {

        gmLen = (static_cast<uint64_t>(fixpipeInfo.ndNum) - 1) * dstEleSize * fixpipeInfo.dstNdStride +
            (fixpipeInfo.m - 1) * fixpipeInfo.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename T, typename U>
[aicore] inline void FixpipeL0cToOut(__attribute__((cce_global)) T* dst, __attribute__((cce_cube_c)) U* src,
    const FixpipeInfoParams<U>& fixpipeInfo, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t cburstNum = fixpipeInfo.tiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * fixpipeInfo.srcStride * fixpipeInfo.c0;
    uint32_t dstOffset = 0;
    if (fixpipeInfo.nz2ndEn) {
        dstOffset = nIterIndex * fixpipeInfo.tiling.nSize;
    } else {
        dstOffset = cburstNum * nIterIndex * fixpipeInfo.dstStride * 32 / sizeof(T);
    }
    if constexpr (g_gm_overflow_check) {
        bool isSrc = false;
        uint16_t dstEleSize = sizeof(T);
        uint64_t gmLen = GetGMLen(fixpipeInfo, calNSize, dstEleSize);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) T*)(dst + dstOffset), isSrc, gmLen);
    }


    return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), fixpipeInfo.sid,
        calNSize, fixpipeInfo.m, fixpipeInfo.dstStride, fixpipeInfo.srcStride, fixpipeInfo.unitFlag,
        fixpipeInfo.quantPre, static_cast<uint8_t>(fixpipeInfo.reluEn), fixpipeInfo.channelSplit, fixpipeInfo.nz2ndEn);
}

template <typename T>
[aicore] inline void CopyDeqTensorToFbuf(const FixpipeInfoParams<T>& fixpipeInfo, uint16_t calNSize,
    uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t* deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeInfo.tiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, fixpipeInfo.cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = ((uint64_t)deqTensorTempBuf >> static_cast<uint64_t>(7)) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename T, typename U, typename S = PrimT<U>,
    typename std::enable_if<IsSameType<PrimT<U>, S>::value, bool>::type = true>
[aicore] inline void Fixpipe(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const FixpipeParams<S>& intriParams)
{
    FixpipeInfoParams<PrimT<U>> fixpipeInfo(intriParams, sizeof(PrimT<T>));
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), fixpipeInfo);
}

template <typename T, typename U, typename S, typename V = PrimT<U>,
    typename std::enable_if<IsSameType<PrimT<U>, V>::value, bool>::type = true>
[aicore] inline void Fixpipe(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const LocalTensor<S>& cbufWorkspace, const FixpipeParams<V>& intriParams)
{
    FixpipeInfoParams<PrimT<U>> fixpipeInfo(intriParams, sizeof(PrimT<T>));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2L1Impl((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), fixpipeInfo);
}


template <typename T, typename U, typename S = PrimT<U>,
    typename std::enable_if<IsSameType<PrimT<U>, S>::value, bool>::type = true>
[aicore] inline void Fixpipe(const GlobalTensor<T>& dst, const LocalTensor<U>& src,
    const FixpipeParams<S>& intriParams)
{







    FixpipeInfoParams<PrimT<U>> fixpipeInfo(intriParams, sizeof(PrimT<T>));

    FixpipeL0C2GMImpl((__attribute__((cce_global)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), fixpipeInfo);






}


template <typename T, typename U, typename S, typename V = PrimT<U>,
    typename std::enable_if<IsSameType<PrimT<U>, V>::value, bool>::type = true>
[aicore] inline void Fixpipe(const GlobalTensor<T> &dst, const LocalTensor<U> &src,
    const LocalTensor<S> &cbufWorkspace, const FixpipeParams<V> &intriParams)
{
    FixpipeInfoParams<PrimT<U>> fixpipeInfo(intriParams, sizeof(PrimT<T>));
    fixpipeInfo.cbufWorkspace = (__attribute__((cce_cube_buff)) uint64_t *)cbufWorkspace.GetPhyAddr();
    FixpipeL0C2GMImpl((__attribute__((cce_global)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), fixpipeInfo);
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_v2_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_v2_impl.h"
namespace AscendC {
[aicore] inline void SetFixPipeClipReluImpl(uint64_t config)
{
    (void)(config);
                                                           ;
}

template <typename T>
[aicore] inline void SetFixPipeAddrImpl(const LocalTensor<T> &eleWise, uint16_t c0ChStride)
{
                                                       ;
}




const uint32_t L0C_SRC_ALIGN = 16 * sizeof(float);

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void CheckCommonFixpipeParam(__attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &params)
{
# 89 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_fixpipe_v2_impl.h"
}

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void CheckFixpipeL0C2L1Param(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<T, U, config>(src, params);
                                                                                                                   ;

                                                                               ;

                                                                              ;





                                     ;
}

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void CheckFixpipeL0C2GMParam(__attribute__((cce_global)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &params)
{
    CheckCommonFixpipeParam<T, U, config>(src, params);





                                                                                                                ;
    if constexpr(IsSameType<U, float>::value && IsSameType<T, float>::value) {

                                                                                               ;
    } else if constexpr(IsSameType<U, int32_t>::value && IsSameType<T, int32_t>::value) {

                                                                                                   ;
    }
    if (params.isChannelSplit) {

                                                                                                                       ;

                                                                            ;
    }
}


struct FixpipeTilingV220 {
    uint16_t nIterNum = 0;
    uint16_t nSize = 0;
    bool isDb = false;
    uint16_t tailNSize = 0;
};


[aicore] inline FixpipeTilingV220 GenFixpipeTilingV220(uint16_t n)
{
    FixpipeTilingV220 tiling;

    uint16_t maxDeqNums = 256;
    if (n <= maxDeqNums) {
        tiling.nIterNum = 1;
        tiling.nSize = n;
        tiling.isDb = false;
        tiling.tailNSize = 0;
    } else {
        tiling.isDb = true;
        uint16_t dbMaxDeqNums = maxDeqNums / 2;
        tiling.nIterNum = n / dbMaxDeqNums;
        tiling.nSize = dbMaxDeqNums;
        tiling.tailNSize = n % dbMaxDeqNums;
    }
    return tiling;
}

[aicore] inline void CopyDeqTensorToFbuf(
    __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        return;
    }
    uint16_t deqDataSize = DivCeil(calNSize * sizeof(uint64_t), 128) * 128;
    __attribute__((cce_fixpipe_buff)) uint64_t *deqTensorTempBuf =
        AscendCUtils::GetTemporaryFbBufferAddr<uint64_t>(0, deqDataSize / sizeof(uint64_t));
    uint32_t deqValueOffset = nIterIndex * fixpipeTiling.nSize;

    uint16_t fbufBurstLen = deqDataSize / 128;
    copy_cbuf_to_fbuf(deqTensorTempBuf, cbufWorkspace + deqValueOffset, 1, fbufBurstLen, 0, 0);

    uint64_t deqTensorAddr = ((uint64_t)deqTensorTempBuf >> static_cast<uint64_t>(7)) << 8;
    set_fpc(deqTensorAddr);
    AscendCUtils::FreeTemporaryFbBuffer<uint64_t>(deqTensorTempBuf);
}

template <typename T, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, const FixpipeParamsV220 &intriParams)
{


                                     ;
}

template <typename T, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) T *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{


                                     ;
}

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void FixpipeL0C2UBImpl(__attribute__((cce_unif_buff)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &intriParams)
{
                                                                                                                  ;
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2UBImpl(
    __attribute__((cce_unif_buff)) T *dst, __attribute__((cce_cube_c)) U *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
                                                                                                                  ;
}

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void FixpipeL0C2L1Impl(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<T, U, config>(dst, src, intriParams);





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {

        SetQuantPreImpl(intriParams.deqScalar);
    }
    FixpipeTilingV220 fixpipeTiling;

    FixpipeL0cToL1<T, U, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2L1Impl(
    __attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) U *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2L1Param<T, U, config>(dst, src, intriParams);






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2L1ImplN<T, U, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2L1ImplN<T, U, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename T, typename U, const FixpipeConfig& config>
[aicore] inline void FixpipeL0C2GMImpl(__attribute__((cce_global)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<T, U, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }
    FixpipeTilingV220 fixpipeTiling;





    if (intriParams.quantPre == QuantMode_t::DEQF16 || intriParams.quantPre == QuantMode_t::QF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::REQ8) {
        SetQuantPreImpl(intriParams.deqScalar);
    }
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<T, U, config>(dst, src, intriParams, fixpipeTiling, intriParams.nSize);
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2GMImpl(
    __attribute__((cce_global)) T *dst, __attribute__((cce_cube_c)) U *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckFixpipeL0C2GMParam<T, U, config>(dst, src, intriParams);
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        uint64_t ndPara = static_cast<uint64_t>(intriParams.dstNdStride) << 32;
        ndPara |= static_cast<uint64_t>(intriParams.srcNdStride) << 16;
        ndPara |= static_cast<uint64_t>(intriParams.ndNum);
        SetNdParaImpl(ndPara);
    }






    FixpipeTilingV220 fixpipeTiling = GenFixpipeTilingV220(intriParams.nSize);
    if (intriParams.quantPre == QuantMode_t::VDEQF16 || intriParams.quantPre == QuantMode_t::VQF322B8_PRE ||
        intriParams.quantPre == QuantMode_t::VREQ8) {
        for (uint16_t i = 0; i < fixpipeTiling.nIterNum; ++i) {
            FixpipeL0C2GMImplN<T, U, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.nSize, i);
        }

        if (fixpipeTiling.tailNSize > 0) {
            FixpipeL0C2GMImplN<T, U, config>(
                dst, src, cbufWorkspace, intriParams, fixpipeTiling, fixpipeTiling.tailNSize, fixpipeTiling.nIterNum);
        }
        return;
    }
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2L1ImplN(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) U *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToL1<T, U, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0C2GMImplN(__attribute__((cce_global)) T *dst, __attribute__((cce_cube_c)) U *src, __attribute__((cce_cube_buff)) uint64_t *cbufWorkspace,
    const FixpipeParamsV220 &intriParams, const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize,
    uint16_t nIterIndex)
{

    CopyDeqTensorToFbuf(cbufWorkspace, fixpipeTiling, calNSize, nIterIndex);
    PipeBarrier<PIPE_FIX>();

    FixpipeL0cToOut<T, U, config>(dst, src, intriParams, fixpipeTiling, calNSize, nIterIndex);
}



template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0cToL1(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(T);


    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_cbuf((__attribute__((cce_cube_buff)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), false, false);
            default:
                                                                                                                      ;
        }
    }
}

[aicore] inline uint64_t GetGMLength(
    const FixpipeParamsV220 &intriParams, const uint16_t &calNSize, const uint16_t &dstEleSize, const bool &nz2ndEn)
{
    constexpr uint16_t dstStrideUnit = 32;
    constexpr uint16_t fractalNsize = 16;
    uint64_t cburstNum = calNSize / fractalNsize;
    uint64_t gmLen =
        (cburstNum - 1) * intriParams.dstStride * dstStrideUnit + intriParams.mSize * fractalNsize * dstEleSize;
    if (nz2ndEn) {
        gmLen = (static_cast<uint64_t>(intriParams.ndNum) - 1) * dstEleSize * intriParams.dstNdStride +
                (intriParams.mSize - 1) * intriParams.dstStride * dstEleSize + cburstNum * fractalNsize * dstEleSize;
    }
    return gmLen;
}



template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void FixpipeL0cToOut(__attribute__((cce_global)) T *dst, __attribute__((cce_cube_c)) U *src, const FixpipeParamsV220 &intriParams,
    const FixpipeTilingV220 &fixpipeTiling, uint16_t calNSize, uint16_t nIterIndex = 0)
{
    uint16_t cburstNum = fixpipeTiling.nSize / 16;
    uint32_t srcOffset = cburstNum * nIterIndex * intriParams.srcStride * BLOCK_CUBE;
    uint32_t dstOffset = 0;
    bool nz2ndEn = false;
    if constexpr (config.format == CO2Layout::ROW_MAJOR) {
        dstOffset = nIterIndex * fixpipeTiling.nSize;
        nz2ndEn = true;
    } else {
        dstOffset = cburstNum * nIterIndex * intriParams.dstStride * 32 / sizeof(T);
    }

    if constexpr (g_gm_overflow_check) {
        uint64_t gmLen = GetGMLength(intriParams, calNSize, sizeof(T), nz2ndEn);
        AscendCUtils::CheckGmMemOverflow((__attribute__((cce_global)) T *)(dst + dstOffset), false, gmLen);
    }
    if constexpr(g_coreType == AscendC::AIC) {
        switch (intriParams.quantPre) {
            case QuantMode_t::NoQuant:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::NoQuant, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322F16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322F16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::F322BF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::F322BF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::DEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::DEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VDEQF16:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VDEQF16, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::QF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::QF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VQF322B8_PRE:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VQF322B8_PRE, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::REQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::REQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            case QuantMode_t::VREQ8:
                return copy_matrix_cc_to_gm((__attribute__((cce_global)) T*)(dst + dstOffset), (__attribute__((cce_cube_c)) U*)(src + srcOffset), 0,
                    calNSize, intriParams.mSize, intriParams.dstStride, intriParams.srcStride, intriParams.unitFlag,
                    QuantMode_t::VREQ8, static_cast<uint8_t>(intriParams.reluEn), intriParams.isChannelSplit, nz2ndEn);
            default:
                                                                                                                      ;
        }
    }
}
}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h" 2
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h"
namespace AscendC {
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h"
template <typename T>
[aicore] inline void SetFixPipeConfig(const LocalTensor<T> &reluPre, const LocalTensor<T> &quantPre,
    bool isUnitFlag)
{
    SetFixPipeConfigImpl<T>(reluPre, quantPre, isUnitFlag);
}

template <typename T, bool setRelu>
[aicore] inline void SetFixPipeConfig(const LocalTensor<T> &preData, bool isUnitFlag)
{
    SetFixPipeConfigImpl<T, setRelu>(preData, isUnitFlag);
}







[aicore] inline void SetFixpipeNz2ndFlag(uint16_t ndNum, uint16_t srcNdStride, uint16_t dstNdStride)
{
    SetFixpipeNz2ndFlagImpl(ndNum, srcNdStride, dstNdStride);
}


[aicore] inline void SetFixpipePreQuantFlag(uint64_t config)
{
    SetFixpipePreQuantFlagImpl(config);
}

[aicore] inline void SetFixPipeClipRelu(uint64_t config)
{
    SetFixPipeClipReluImpl(config);
}

template <typename T>
[aicore] inline void SetFixPipeAddr(const LocalTensor<T> &eleWiseData, uint16_t c0ChStride)
{
    SetFixPipeAddrImpl(eleWiseData, c0ChStride);
}

template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void Fixpipe(const LocalTensor<T> &dst, const LocalTensor<U> &src,
    const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1", "Fixpipe");


                                                                                          ;

    if ((GetPhyType((TPosition)dst.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((TPosition)dst.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), intriParams);
    }
}


template <typename T, typename U, const FixpipeConfig& config, typename S,
    typename Std::enable_if<Std::is_same<PrimT<S>, uint64_t>::value, bool>::type>
[aicore] inline void Fixpipe(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const LocalTensor<S>& cbufWorkspace, const FixpipeParamsV220& intriParams)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1", "Fixpipe");


                                                                                          ;
    CheckTensorPos<S>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    if ((GetPhyType((TPosition)dst.GetPosition()) == Hardware::L1)) {
        FixpipeL0C2L1Impl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
    } else if ((GetPhyType((TPosition)dst.GetPosition()) == Hardware::UB)) {
        FixpipeL0C2UBImpl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(), (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
    }
}


template <typename T, typename U, const FixpipeConfig &config>
[aicore] inline void Fixpipe(const GlobalTensor<T> &dst, const LocalTensor<U> &src,
    const FixpipeParamsV220 &intriParams)
{







    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1", "Fixpipe");
    FixpipeL0C2GMImpl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_global)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(),
        intriParams);






}


template <typename T, typename U, const FixpipeConfig &config, typename S,
    typename Std::enable_if<Std::is_same<PrimT<S>, uint64_t>::value, bool>::type>
[aicore] inline void Fixpipe(const GlobalTensor<T> &dst, const LocalTensor<U> &src,
    const LocalTensor<S> &cbufWorkspace, const FixpipeParamsV220 &intriParams)
{
    CheckTensorPos<U>(src, Hardware::L0C, "src", "CO1", "Fixpipe");
    CheckTensorPos<S>(cbufWorkspace, Hardware::L1, "cbufWorkspace", "A1", "Fixpipe");


                                                                                                       ;
    FixpipeL0C2GMImpl<PrimT<T>, PrimT<U>, config>((__attribute__((cce_global)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_cube_c)) PrimT<U>*)src.GetPhyAddr(),
        (__attribute__((cce_cube_buff)) uint64_t*)cbufWorkspace.GetPhyAddr(), intriParams);
}
# 274 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_fixpipe_intf_impl.h"
}
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_fixpipe_intf.h" 2
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_dump_tensor_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_dump_tensor_intf.h"
namespace AscendC {
template <typename T>
[aicore] inline void DumpTensor(const LocalTensor<T> &tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] inline void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc, uint32_t dumpSize);
template <typename T>
[aicore] inline void DumpTensor(const LocalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] inline void DumpTensor(const GlobalTensor<T>& tensor, uint32_t desc,
    uint32_t dumpSize, const ShapeInfo& shapeInfo);
template <typename T>
[aicore] inline void DumpAccChkPoint(const LocalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);
template <typename T>
[aicore] inline void DumpAccChkPoint(const GlobalTensor<T> &tensor,
    uint32_t index, uint32_t countOff, uint32_t dumpSize);

template <class... Args>
[aicore] inline void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args);
template <class... Args>
[aicore] inline void printf(__attribute__((cce_global)) const char* fmt, Args&&... args);
# 55 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_dump_tensor_intf.h"
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
inline __attribute__((cce_global)) uint8_t* g_sysPrintFifoSpace = nullptr;






# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_pop_stack_buffer.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_pop_stack_buffer.h"
namespace AscendC {
template <TPosition pos> [aicore] inline uint64_t GetEndAddress()
{
    Hardware hardType = GetPhyType(pos);
                                                                                                      ;


    return TOTAL_UB_SIZE - sizeof(KfcMsg);





}

template <typename T, TPosition pos> [aicore] inline bool PopStackBuffer(LocalTensor<T>& pop)
{
    TBuffAddr addr;
    addr.logicPos = (int8_t)pos;
                                                                                                       ;
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;




      ;
    addr.dataLen = static_cast<uint32_t>(endAddress - queEndAddress);
    addr.bufferAddr = queEndAddress;
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_pop_stack_buffer.h"
    pop.SetAddr(addr);
    return true;
}

template <TPosition pos> [aicore] inline bool PopStackBuffer(TBuf<pos>& popBuffer, TBufType& bufStart)
{
    uint64_t endAddress = GetEndAddress<pos>();
    uint64_t queEndAddress = GetTPipePtr()->GetQueueEndAddress<pos>();

                                                                                                                   ;




      ;
    uint32_t dataLen = static_cast<uint32_t>(endAddress - queEndAddress);
    bufStart.address = queEndAddress;
    bufStart.dataLen = dataLen;
    popBuffer.SetTpipeBuf(&bufStart, dataLen);







    return true;
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h" 2





namespace AscendC {
[[block_local]] __inline__ __attribute__((cce_global)) uint8_t* g_dumpWorkspaceReserved;

template <typename T>
[aicore] constexpr inline Internal::DumpTensorDataType GetTensorDataType();

template <typename T> [aicore] inline uint32_t GetDataType(T data)
{
    return static_cast<uint32_t>(GetTensorDataType<T>());
}

[aicore] inline uint8_t GetDumpBlockIdx()
{
    if constexpr(g_coreType == AscendC::AIV) {
        return GetBlockIdxImpl();
    } else {
        return GetBlockIdxImpl() + AIV_CORE_NUM;
    }
}


[aicore] inline int64_t GetBlockNum();
[aicore] inline void InitDumpImpl(bool mixFlag, uint32_t gmLen)
{
    uint64_t firstTimeStamp = static_cast<uint64_t>(GetSystemCycle());
    uint32_t totalBlockNum;

    if (g_dumpWorkspaceReserved == nullptr) {

                                                                                        ;
        return;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    if (mixFlag == true) {
        totalBlockNum = GetBlockNum() * (1 + MIX_NUM);
    } else {
        totalBlockNum = GetBlockNum();
    }
    uint32_t blockDumpSize = DUMP_UINTSIZE;

    uint32_t blockDim = GetDumpBlockIdx();
    if (blockDim >= DUMP_CORE_COUNT) {
        return;
    }



    uint32_t blkInfoLen = sizeof(BlockInfo) + sizeof(DumpMeta);

    uint64_t blockInfoStart = dumpWorkspaceStart + blockDim * DUMP_UINTSIZE;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_LEN_POS) = blockDumpSize;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_CORE_POS) = blockDim;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_BLOCKNUM_POS) = totalBlockNum;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = blockDumpSize - blkInfoLen;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_MAGIC_POS) = 0x5aa5bccd;
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_RSV_POS) = 0;
    *((__attribute__((cce_global)) uint64_t*)((__attribute__((cce_global)) uint32_t*)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = blockInfoStart + blkInfoLen;

    blockInfoStart = blockInfoStart + sizeof(BlockInfo);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TYPE_POS) =
        static_cast<uint32_t>(DumpType::DUMP_META);
    *(__attribute__((cce_global)) uint32_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_LEN_POS) = 8;
    *(__attribute__((cce_global)) uint16_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_BLOCK_DIM_POS) =
        static_cast<uint16_t>(GetBlockNum());
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_CORE_TYPE_POS) =
        static_cast<uint8_t>(g_coreType);
    *(__attribute__((cce_global)) uint8_t*)((__attribute__((cce_global)) uint8_t*)blockInfoStart + DUMP_META_TASK_RATION) =
        static_cast<uint8_t>(mixFlag);
    *((__attribute__((cce_global)) uint32_t*)blockInfoStart + DUMP_META_RSV_POS) = 0;
# 107 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h"
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] inline DataCopyParams GetDataCopyParamImpl(uint32_t offset)
{
    DataCopyParams repeatParams;
    repeatParams.blockCount = 1;
    repeatParams.blockLen = offset / ONE_BLK_SIZE;
    repeatParams.srcStride = 0;
    repeatParams.dstStride = 0;
    return repeatParams;
}

[aicore] inline void GetMatCopyParam(
    uint32_t dumpSize, uint16_t& n, uint16_t& m, uint16_t& dstStrideDstD, uint16_t& srcStride);

template <typename T>
[aicore] inline void DumpTensorL0C2GMImpl(const LocalTensor<T>& src, __attribute__((cce_global)) BlockInfo* ptr, uint32_t dumpSize)
{
    uint16_t n, m, dstStrideDstD, srcStride;
    GetMatCopyParam(dumpSize, n, m, dstStrideDstD, srcStride);

    copy_matrix_cc_to_gm((__attribute__((cce_global)) float *)(ptr->dumpAddr), (__attribute__((cce_cube_c)) float *)(src.GetPhyAddr()),
        0, n, m, dstStrideDstD, srcStride, 0, QuantMode_t::NoQuant,
        static_cast<uint8_t>(false), false, false);
}

template <typename T>
[aicore] inline uint32_t CheckValidPosition(const LocalTensor<T>& src)
{

    uint32_t position = 0;
    if ((Hardware)GetPhyType((TPosition)src.GetPosition()) == Hardware::UB) {
        position = static_cast<uint32_t>(AscendC::Hardware::UB);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)src.GetPosition()) == Hardware::L1) {
        position = static_cast<uint32_t>(AscendC::Hardware::L1);
        return position;
    } else if ((Hardware)GetPhyType((TPosition)src.GetPosition()) == Hardware::L0C) {
        position = static_cast<uint32_t>(AscendC::Hardware::L0C);
        return position;
    } else {
        return false;
    }
}

[aicore] inline void WriteDumpShapeInfo(const ShapeInfo &shapeInfo)
{
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t valueSize = sizeof(DumpShapeMessageHead);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    uint32_t tlvSize = valueSize + DUMP_SHAPE_MESSAGE_TL_LEN;
    if (ptr->dumpOffset < tlvSize) {





          ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_SHAPE);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_LEN_POS) = valueSize;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_DIM_POS) = shapeInfo.shapeDim;
    for (uint32_t idx = 0; idx < shapeInfo.shapeDim && idx < 8; idx++) {
        *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_SHAPE_START_POS + idx) = shapeInfo.shape[idx];
    }
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_SHAPE_MESSAGE_HEAD_RSV_POS) = 0;

    ptr->dumpAddr += tlvSize;
    ptr->dumpOffset -= tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] inline void WriteRingBufShapeInfo(const ShapeInfo &shapeInfo);

[aicore] inline void DumpShapeImpl(const ShapeInfo &shapeInfo)
{
    if (g_sysPrintFifoSpace != nullptr) {
        WriteRingBufShapeInfo(shapeInfo);
    } else {
        WriteDumpShapeInfo(shapeInfo);
    }
}

template <typename T>
[aicore] inline void DumpTensorLocal2GMEntityImpl(const LocalTensor<T>& src, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = CheckValidPosition(src);

    if (position == 0) {

                                                                                                          ;
        return;
    }

    T data;
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return;
    }
    uint32_t offset = dumpSize * sizeof(T);
    uint32_t padOffset = AlignUp(offset, ONE_BLK_SIZE);

    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (padOffset + sizeof(DumpMessageHead))) {





          ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = padOffset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(src.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DUMP_SIZE_POS) = dumpSize;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    DataCopyParams repeatParams = GetDataCopyParamImpl(padOffset);
    const Hardware srcHWPos = GetPhyType((TPosition)src.GetPosition());

    PipeBarrier<PIPE_ALL>();
    if (srcHWPos == Hardware::UB) {
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L1) {
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(ptr->dumpAddr), (__attribute__((cce_cube_buff)) T*)src.GetPhyAddr(), repeatParams);
    } else if (srcHWPos == Hardware::L0C) {
        if constexpr(g_coreType != AscendC::AIC) {
            return;
        }
        DumpTensorL0C2GMImpl(src, ptr, dumpSize);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= padOffset;
    ptr->dumpAddr += padOffset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}


template <template<typename> class Tensor, typename T>
[aicore] inline void DumpTensorRingBufImpl(const Tensor<T>& src, uint32_t desc, uint32_t dumpSize);

template <typename T>
[aicore] inline void DumpTensorLocal2GMImpl(const LocalTensor<T>& src, uint32_t desc, uint32_t dumpSize)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    if (g_sysPrintFifoSpace != nullptr) {
        DumpTensorRingBufImpl(src, desc, dumpSize);
    } else {
        DumpTensorLocal2GMEntityImpl(src, desc, dumpSize);
    }
    set_ctrl(ctrlValue);
}

[aicore] inline uint32_t GetLoopCount(uint32_t offset)
{
    uint32_t loopCount = 0;
    if (offset % ONE_DUMP_BACKUP_SIZE != 0) {
        loopCount = 1 + offset / ONE_DUMP_BACKUP_SIZE;
    } else {
        loopCount = offset / ONE_DUMP_BACKUP_SIZE;
    }
    return loopCount;
}

template <typename T>
[aicore] inline void InitTmpTensor(LocalTensor<T>& tmp, uint8_t quePos)
{
    TBuffAddr tbufTmpLocal;
    tbufTmpLocal.logicPos = quePos;
    tmp.SetAddr(tbufTmpLocal);



    tmp.address_.bufferAddr = (uint64_t)(0);

    tmp.address_.dataLen = ONE_DUMP_BACKUP_SIZE;
}
[aicore] inline bool CheckDumpValid(uint32_t padOffset)
{
    uint8_t core = GetDumpBlockIdx();
    if (core >= DUMP_CORE_COUNT) {
        return false;
    }
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    if (reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) < DUMP_WORKSPACE_SIZE) {
                                                                                                                                                 ;
        return false;
    }
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    if (ptr->dumpOffset < (padOffset + sizeof(DumpMessageHead) + ONE_DUMP_BACKUP_SIZE)) {



                            ;
        *((__attribute__((cce_global)) uint32_t*)ptr + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return false;
    }

    return true;
}
template <typename T>
[aicore] inline void DumpBlockInfoImpl(const GlobalTensor<T>& src, uint32_t desc, uint32_t dumpSize, uint32_t padOffset)
{
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;

    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_TYPE_POS) = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_LEN_POS) = padOffset + DUMP_MSG_HEAD_SIZE;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_ADDR_POS) =
        static_cast<uint32_t>(reinterpret_cast<uintptr_t>(src.GetPhyAddr()));
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DATA_TYPE_POS) = GetDataType(data);
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DESC_POS) = desc;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_BUFFERID_POS) = 0;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_POSITION_POS) = position;
    *((__attribute__((cce_global)) uint32_t*)ptr->dumpAddr + DUMP_MESSAGE_HEAD_DUMP_SIZE_POS) = dumpSize;

    ptr->dumpAddr += sizeof(DumpMessageHead);
    ptr->dumpOffset -= sizeof(DumpMessageHead);
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
template <typename T>
[aicore] inline void DumpGMTailImpl(LocalTensor<T>& tmp, uint32_t alignSize, uint64_t tmpAddr,
                                      uint64_t gmAddr, uint32_t offset)
{
    DataCopyParams tailParams = GetDataCopyParamImpl((alignSize + ONE_BLK_SIZE - 1) & (~(ONE_BLK_SIZE - 1)));
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(), tailParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(),
                          (__attribute__((cce_global)) T*)(tmpAddr + offset - alignSize), tailParams);
        PipeBarrier<PIPE_ALL>();
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(), tailParams);
    }
    PipeBarrier<PIPE_ALL>();
}
template <typename T>
[aicore] inline void DumpTensorGM2GMEntityImpl(const GlobalTensor<T>& src, uint32_t desc, uint32_t dumpSize)
{
    uint32_t position = static_cast<uint32_t>(AscendC::Hardware::GM);
    T data;
    uint32_t offset = dumpSize * sizeof(T);
    uint32_t padOffset = AlignUp(offset, ONE_BLK_SIZE);
    if (!CheckDumpValid(padOffset)) {
        return;
    }
    DumpBlockInfoImpl(src, desc, dumpSize, padOffset);
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo* ptr = (__attribute__((cce_global)) BlockInfo*)(dumpWorkspaceStart + DUMP_UINTSIZE * GetDumpBlockIdx());
    DataCopyParams backupParams = GetDataCopyParamImpl(ONE_DUMP_BACKUP_SIZE);
    LocalTensor<T> tmp;
    uint64_t gmBackAddr = dumpWorkspaceStart + DUMP_UINTSIZE * (GetDumpBlockIdx() + 1) - ONE_DUMP_BACKUP_SIZE;


    PipeBarrier<PIPE_ALL>();
    if (g_coreType == AIV) {
        InitTmpTensor(tmp, static_cast<uint8_t>(TPosition::VECIN));
        DataCopyUB2GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(), backupParams);
    } else if (g_coreType == AIC) {
        InitTmpTensor(tmp, static_cast<uint8_t>(TPosition::A1));
        DataCopyL12GMImpl((__attribute__((cce_global)) T*)(gmBackAddr), (__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(), backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    dcci((__attribute__((cce_global)) uint64_t*)gmBackAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);

    uint32_t alignSize = padOffset % ONE_DUMP_BACKUP_SIZE;
    uint64_t tmpAddr = static_cast<uint64_t>(reinterpret_cast<uintptr_t>(src.GetPhyAddr()));
    uint64_t gmAddr = ptr->dumpAddr;
    for (int i = 0; i < padOffset / ONE_DUMP_BACKUP_SIZE; i++) {
        if (g_coreType == AIV) {
            DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyUB2GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        } else if (g_coreType == AIC) {
            DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(),
                              (__attribute__((cce_global)) T*)(tmpAddr + ONE_DUMP_BACKUP_SIZE * i), backupParams);
            PipeBarrier<PIPE_ALL>();
            DataCopyL12GMImpl((__attribute__((cce_global)) T*)gmAddr, (__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(), backupParams);
            gmAddr += ONE_DUMP_BACKUP_SIZE;
        }
        PipeBarrier<PIPE_ALL>();
    }
    if (alignSize != 0) {
        DumpGMTailImpl(tmp, alignSize, tmpAddr, gmAddr, padOffset);
    }
    if (g_coreType == AIV) {
        DataCopyGM2UBImpl((__attribute__((cce_unif_buff)) T*)tmp.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    } else if (g_coreType == AIC) {
        DataCopyGM2L1Impl((__attribute__((cce_cube_buff)) T*)tmp.GetPhyAddr(), (__attribute__((cce_global)) T*)gmBackAddr, backupParams);
    }
    PipeBarrier<PIPE_ALL>();
    ptr->dumpOffset -= padOffset;
    ptr->dumpAddr += padOffset;
    dcci((__attribute__((cce_global)) uint64_t*)ptr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] inline void DumpTensorGM2GMImpl(const GlobalTensor<T>& src, uint32_t desc, uint32_t dumpSize)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    if (g_sysPrintFifoSpace != nullptr) {
        DumpTensorRingBufImpl(src, desc, dumpSize);
    } else {
        DumpTensorGM2GMEntityImpl(src, desc, dumpSize);
    }
    set_ctrl(ctrlValue);
}

[aicore] inline uint32_t GetArgsNum()
{
    return 0;
}

template <typename T, typename... Args>
[aicore] inline uint32_t GetArgsNum(T scalar, Args... args)
{
    return 1 + GetArgsNum(args...);
}

[aicore] inline uint32_t GetStringLength(__attribute__((cce_global)) const char* s)
{
    uint32_t i = 0;
    while (*(s + i) != '\0') {
        i++;
    }
    return i + 1;
}

[aicore] inline uint32_t GetArgsSize()
{
    return 0;
}

template <typename... Args>
[aicore] inline uint32_t GetArgsSize(Args&&... args);

template <typename... Args>
[aicore] inline uint32_t GetArgsSizeImpl(__attribute__((cce_global)) const char* s, Args&&... args)
{
    uint32_t strLen = GetStringLength(s);
    uint32_t strParamSize = ONE_PARAM_SIZE + strLen;
    return strParamSize + GetArgsSize(args...);
}

template <typename T, typename... Args>
[aicore] inline uint32_t GetArgsSizeImpl(T scalar, Args&&... args)
{
    return ONE_PARAM_SIZE + GetArgsSize(args...);
}

template <typename... Args>
[aicore] inline uint32_t GetArgsSize(Args&&... args)
{
    return GetArgsSizeImpl(args...);
}

template <typename... Args>
[aicore] inline uint32_t GetParamSize(__attribute__((cce_global)) const char* fmt, Args&&... args)
{
    uint32_t fmtSize = GetStringLength(fmt);
    uint32_t argsSize = GetArgsSize(args...);
    return fmtSize + argsSize + ONE_PARAM_SIZE;
}

[aicore] __attribute__((cce_global)) inline BlockInfo *GetBlockInfo()
{
    uint8_t core = GetDumpBlockIdx();
    uint64_t dumpWorkspaceStart = reinterpret_cast<uint64_t>(g_dumpWorkspaceReserved) - DUMP_WORKSPACE_SIZE;
    __attribute__((cce_global)) BlockInfo *blockInfo = (__attribute__((cce_global)) BlockInfo *)(dumpWorkspaceStart + DUMP_UINTSIZE * core);
    return blockInfo;
}

[aicore] inline void WriteString(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, __attribute__((cce_global)) const char* s, uint32_t& offset)
{
    __attribute__((cce_global)) uint64_t *stringAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr) + paramIdx;
    __attribute__((cce_global)) uint64_t *dstStrAddr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(paramAddr + offset);


    *((__attribute__((cce_global)) uint64_t *)stringAddr) = static_cast<uint64_t>(offset - ONE_PARAM_SIZE * paramIdx);
    dcci((__attribute__((cce_global)) uint64_t*)stringAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);


    __attribute__((cce_global)) char *d = (__attribute__((cce_global)) char *)(dstStrAddr);
    uint32_t strLen = GetStringLength(s);

    for (uint32_t i = 0; i < strLen; i++) {
        *(d + i) = *(s + i);
        dcci((__attribute__((cce_global)) uint64_t*)d, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    }
    offset += strLen;
}

template <typename T>
[aicore] inline void WriteScalar(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, T scalar)
{
    __attribute__((cce_global)) uint64_t *scalarAddr = (__attribute__((cce_global)) uint64_t *)paramAddr + paramIdx;
    *scalarAddr = 0;

    static_assert(!SupportType<T, double>(), "printf unsupport double type");

    if constexpr (SupportType<T, half, float>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = static_cast<float>(scalar);
    } else if constexpr(SupportType<T, bfloat16_t>()) {
        *((__attribute__((cce_global)) float *)scalarAddr) = ToFloat(scalar);
    } else if constexpr (std::is_signed<T>::value) {
        *((__attribute__((cce_global)) int64_t *)scalarAddr) = static_cast<int64_t>(scalar);
    } else if constexpr(std::is_unsigned<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = static_cast<uint64_t>(scalar);
    } else if constexpr(std::is_pointer<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = (uintptr_t)scalar;
    } else if constexpr(std::is_enum<T>::value) {
        *((__attribute__((cce_global)) uint64_t *)scalarAddr) = static_cast<uint64_t>(scalar);
    }

    dcci((__attribute__((cce_global)) uint64_t*)scalarAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] inline void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset)
{
    return;
}

template <typename... Args>
[aicore] inline void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args);

template <typename... Args>
[aicore] inline void SetParamImpl(__attribute__((cce_global)) uint8_t *paramAddr, uint32_t paramIdx, uint32_t &offset,
                                    __attribute__((cce_global)) const char *s, Args&&... args)
{
    WriteString(paramAddr, paramIdx, s, offset);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename T, typename... Args>
[aicore] inline void SetParamImpl(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, T scalar,
                                    Args&&... args)
{
    WriteScalar(paramAddr, paramIdx, scalar);
    SetParam(paramAddr, paramIdx + 1, offset, args...);
}

template <typename... Args>
[aicore] inline void SetParam(__attribute__((cce_global)) uint8_t* paramAddr, uint32_t paramIdx, uint32_t& offset, Args&&... args)
{
    SetParamImpl(paramAddr, paramIdx, offset, args...);
}

[aicore] inline void WriteTLHead(DumpType printType, __attribute__((cce_global)) uint8_t *tlv, uint32_t valueSize)
{
    *((__attribute__((cce_global)) uint32_t *)tlv) = static_cast<uint32_t>(printType);
    *((__attribute__((cce_global)) uint32_t *)tlv + 1) = valueSize;
    dcci((__attribute__((cce_global)) uint64_t*)tlv, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}
[aicore] inline void UpdateBlockInfo(uint32_t tlvSize)
{
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t lastDumpAddr = blockInfo->dumpAddr;

    __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
    *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMPOFFSET_POS) = remainSize - tlvSize;
    *((__attribute__((cce_global)) uint64_t *)((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_DUMP_ADDR)) = lastDumpAddr + tlvSize;
    dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <class... Args>
[aicore] inline void PrintfEntityImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{

    uint8_t blockIdx = GetDumpBlockIdx();
    if (blockIdx >= DUMP_CORE_COUNT) {
        return;
    }
    __attribute__((cce_global)) BlockInfo *blockInfo = GetBlockInfo();
    uint32_t remainSize = blockInfo->dumpOffset;
    uint64_t dumpAddr = blockInfo->dumpAddr;

    uint32_t paramSize = GetParamSize(fmt, args...);
    uint32_t paramNum = GetArgsNum(args...) + 1;
    paramSize = (paramSize + ONE_PARAM_SIZE - 1) & (~(ONE_PARAM_SIZE - 1));

    uint32_t tlvSize = paramSize + ONE_PARAM_SIZE;
    if (tlvSize > remainSize) {
        __attribute__((cce_global)) uint8_t *blockInfoStart = (__attribute__((cce_global)) uint8_t *)blockInfo;
        *((__attribute__((cce_global)) uint32_t *)blockInfoStart + BLOCK_INFO_RSV_POS) = DUMP_EXC_FLAG;
        dcci((__attribute__((cce_global)) uint64_t*)blockInfoStart, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
        return;
    }

    __attribute__((cce_global)) uint8_t *tlvAddr = (__attribute__((cce_global)) uint8_t *)dumpAddr;
    WriteTLHead(printType, tlvAddr, paramSize);
    __attribute__((cce_global)) uint8_t *paramAddr = tlvAddr + ONE_PARAM_SIZE;
    uint32_t offset = paramNum * ONE_PARAM_SIZE;
    WriteString(paramAddr, 0, fmt, offset);
    uint32_t paramIdx = 1;
    SetParam(paramAddr, paramIdx, offset, args...);


    UpdateBlockInfo(tlvSize);

}

[aicore] inline uint32_t GetPrintArgsLen(uint32_t& argsNum)
{
    return 0;
}

template <typename... Args>
[aicore] inline uint32_t GetPrintArgsLen(uint32_t& argsNum, Args&&... args);

template <typename... Args>
[aicore] inline uint32_t GetPrintArgsLenImpl(uint32_t& argsNum, __attribute__((cce_global)) const char* s, Args&&... args)
{
    constexpr uint32_t paramSize = sizeof(uint64_t);
    const uint32_t& strLen = GetStringLength(s);
    argsNum += 1;
    return paramSize + strLen + GetPrintArgsLen(argsNum, args...);
}

template <typename T, typename... Args>
[aicore] inline uint32_t GetPrintArgsLenImpl(uint32_t& argsNum, T scalar, Args&&... args)
{
    constexpr uint32_t paramSize = sizeof(uint64_t);
    argsNum += 1;
    return paramSize + GetPrintArgsLen(argsNum, args...);
}

template <typename... Args>
[aicore] inline uint32_t GetPrintArgsLen(uint32_t& argsNum, Args&&... args)
{
    return GetPrintArgsLenImpl(argsNum, args...);
}

[aicore] constexpr uint32_t AlignTlvLen(const uint32_t& dataLen)
{
    constexpr uint32_t num = 7;
    return ((dataLen + num) & ~num) + num + 1;
}

template <typename... Args>
[aicore] inline uint32_t GetPrintTlvLen(uint32_t& argsNum, __attribute__((cce_global)) const char* fmt, Args&&... args)
{
    constexpr uint32_t printInfoLen = sizeof(PrintTlvInfoHead);
    const uint32_t& fmtLen = GetStringLength(fmt);
    const uint32_t& argsLen = GetPrintArgsLen(argsNum, args...);
    return AlignTlvLen(printInfoLen + argsLen + fmtLen);
}

[aicore] __attribute__((cce_global)) inline BlockRingBufInfo* GetBlockRingBufInfo()
{
    uint32_t blockIdx = (get_coreid() & 0x00FF) % DUMP_CORE_COUNT;
    uint32_t blockLength = reinterpret_cast<__attribute__((cce_global)) BlockRingBufInfo*>(g_sysPrintFifoSpace)->length;
    __attribute__((cce_global)) BlockRingBufInfo* ringBufInfo =
        reinterpret_cast<__attribute__((cce_global)) BlockRingBufInfo*>(g_sysPrintFifoSpace + blockLength * blockIdx);
    return ringBufInfo->magic == 0xAE86 ? ringBufInfo : nullptr;
}

[aicore] inline void SkipRingBufDirectly(__attribute__((cce_global)) RingBufWriteInfo* writeInfo)
{
    writeInfo->bufOffset = 0;
    dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(writeInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    return;
}

[aicore] inline void SkipRingBufWithInfo(
    __attribute__((cce_global)) RingBufWriteInfo* writeInfo, __attribute__((cce_global)) uint8_t* ringBufAddr, const uint32_t& ringBufLen)
{
    __attribute__((cce_global)) SkipTlvInfo* skipInfo = reinterpret_cast<__attribute__((cce_global)) SkipTlvInfo*>(ringBufAddr + writeInfo->bufOffset);
    skipInfo->type = static_cast<uint32_t>(DumpType::DUMP_SKIP);
    skipInfo->length = ringBufLen - writeInfo->bufOffset - sizeof(SkipTlvInfo);
    writeInfo->bufOffset = 0;
    writeInfo->packIdx += 1;
    dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(skipInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(writeInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    return;
}

[aicore] inline bool RingBufferWait(__attribute__((cce_global)) RingBufReadInfo* readInfo, __attribute__((cce_global)) RingBufWriteInfo* writeInfo,
                                      const uint32_t& tlvLen)
{
    const uint64_t firstTimeStamp = static_cast<uint64_t>(GetSystemCycle());
    constexpr uint64_t timeoutCycle = 50 * 1000 * 1000 * 5;
    while (writeInfo->bufOffset < readInfo->bufOffset && writeInfo->bufOffset + tlvLen >= readInfo->bufOffset) {
        uint64_t spendTime = static_cast<uint64_t>(GetSystemCycle()) - firstTimeStamp;
        if (spendTime > timeoutCycle) {
            return false;
        }
        dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(readInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    }
    return true;
}

[aicore] inline void WriteRingBufTlvHead(
    DumpType printType, __attribute__((cce_global)) PrintTlvInfoHead* printTlv, const uint32_t& tlvLen, const uint32_t& argsNum)
{
    printTlv->type = static_cast<uint32_t>(printType);
    printTlv->length = tlvLen - sizeof(uint32_t[2]);
    printTlv->resvMem[0] = static_cast<uint32_t>(0U);
    printTlv->resvMem[1] = static_cast<uint32_t>(0U);
    printTlv->fmtOffset = (argsNum + 1) * sizeof(uint64_t);
    dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(printTlv), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] inline void MemCopyGm2Gm(__attribute__((cce_global)) uint8_t* dst, __attribute__((cce_global)) const uint8_t* src, const uint32_t& len)
{
    if (dst == nullptr || src == nullptr)
    {
        return;
    }
    for (uint32_t i = 0; i < len; i++) {
        *(dst + i) = *(src + i);
    }
    dcci((__attribute__((cce_global)) uint64_t*)(dst), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename... Args>
[aicore] inline void WriteRingBufTlvData(__attribute__((cce_global)) PrintTlvInfoHead* printTlv, __attribute__((cce_global)) const char* fmt, Args&&... args)
{
    const uint32_t& strLen = GetStringLength(fmt);
    __attribute__((cce_global)) uint8_t* paramAddr =
        reinterpret_cast<__attribute__((cce_global)) uint8_t*>(printTlv + 1);
    __attribute__((cce_global)) uint8_t* fmtAddr = paramAddr + printTlv->fmtOffset - sizeof(uint64_t);
    __attribute__((cce_global)) uint8_t* strParamAddr = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(fmtAddr) + strLen;
    MemCopyGm2Gm(fmtAddr, reinterpret_cast<__attribute__((cce_global)) const uint8_t*>(fmt), strLen);
    uint32_t strParamOffset = printTlv->fmtOffset + strLen;
    SetParam(paramAddr, 0, strParamOffset, args...);
}

[aicore] inline void UpdateWriteInfo(__attribute__((cce_global)) RingBufWriteInfo* writeInfo, const uint32_t& tlvLen)
{
    writeInfo->bufOffset += tlvLen;
    writeInfo->packIdx += 1;
    dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(writeInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

[aicore] __attribute__((cce_global)) inline RingBufReadInfo* GetRingBufReadInfo(__attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo)
{
    __attribute__((cce_global)) uint8_t* blockHead = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(blockRingBufInfo);

    return reinterpret_cast<__attribute__((cce_global)) RingBufReadInfo*>(blockHead + sizeof(BlockRingBufInfo));
}

[aicore] __attribute__((cce_global)) inline RingBufWriteInfo* GetRingBufWriteInfo(__attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo)
{
    __attribute__((cce_global)) uint8_t* ringBufAddr = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(blockRingBufInfo->ringBufAddr);

    return reinterpret_cast<__attribute__((cce_global)) RingBufWriteInfo*>(ringBufAddr + blockRingBufInfo->ringBufLen);
}

[aicore] inline bool WaitRingBufBeginRead(__attribute__((cce_global)) RingBufReadInfo* readInfo)
{
    const uint64_t firstTimeStamp = static_cast<uint64_t>(GetSystemCycle());
    constexpr uint64_t timeoutCycle = 50 * 1000 * 1000 * 5;
    while (readInfo->bufOffset == 0) {
        uint64_t spendTime = static_cast<uint64_t>(GetSystemCycle()) - firstTimeStamp;
        if (spendTime > timeoutCycle) {
            return false;
        }
        dcci(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(readInfo), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
    }
    return true;
}

[aicore] inline bool CheckAndWaitRingBufSpace(__attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo, const uint32_t& tlvLen)
{
    constexpr uint32_t minTlvLen = sizeof(SkipTlvInfo);

    __attribute__((cce_global)) uint8_t* ringBufAddr = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(blockRingBufInfo->ringBufAddr);
    uint32_t ringBufLen = blockRingBufInfo->ringBufLen;

    __attribute__((cce_global)) RingBufReadInfo* readInfo = GetRingBufReadInfo(blockRingBufInfo);
    __attribute__((cce_global)) RingBufWriteInfo* writeInfo = GetRingBufWriteInfo(blockRingBufInfo);

    if (minTlvLen >= ringBufLen || tlvLen > ringBufLen) {
        return false;
    } else if (writeInfo->bufOffset + minTlvLen >= ringBufLen) {
        if (!WaitRingBufBeginRead(readInfo)) {
            return false;
        }
        SkipRingBufDirectly(writeInfo);
    } else if (writeInfo->bufOffset + tlvLen > ringBufLen) {
        if (!WaitRingBufBeginRead(readInfo)) {
            return false;
        }
        SkipRingBufWithInfo(writeInfo, ringBufAddr, ringBufLen);
    }
    if (writeInfo->packIdx > 0 &&
        writeInfo->bufOffset < readInfo->bufOffset &&
        writeInfo->bufOffset + tlvLen >= readInfo->bufOffset) {
        return RingBufferWait(readInfo, writeInfo, tlvLen);
    }
    return true;
}

[aicore] __attribute__((cce_global)) inline uint8_t* GetRingBufTlv(__attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo)
{
    __attribute__((cce_global)) RingBufWriteInfo* writeInfo = GetRingBufWriteInfo(blockRingBufInfo);
    __attribute__((cce_global)) uint8_t* ringBufAddr = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(blockRingBufInfo->ringBufAddr);
    return ringBufAddr + writeInfo->bufOffset;
}

template <class... Args>
[aicore] inline void PrintfRingBufImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{

    __attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo = GetBlockRingBufInfo();
    if (blockRingBufInfo == nullptr) {
        return;
    }
    uint32_t argsNum = 0;
    const uint32_t& tlvLen = GetPrintTlvLen(argsNum, fmt, args...);
    if (!CheckAndWaitRingBufSpace(blockRingBufInfo, tlvLen)) {
        return;
    }

    __attribute__((cce_global)) PrintTlvInfoHead* printTlv = reinterpret_cast<__attribute__((cce_global)) PrintTlvInfoHead*>(GetRingBufTlv(blockRingBufInfo));

    WriteRingBufTlvHead(printType, printTlv, tlvLen, argsNum);
    WriteRingBufTlvData(printTlv, fmt, args...);

    __attribute__((cce_global)) RingBufWriteInfo* writeInfo = GetRingBufWriteInfo(blockRingBufInfo);

    UpdateWriteInfo(writeInfo, tlvLen);

}

template <typename T>
[aicore] inline Hardware CheckDumpTensorPosition(const LocalTensor<T>& src)
{
    Hardware position = GetPhyType(static_cast<TPosition>(src.GetPosition()));
    if (position != Hardware::UB && position != Hardware::L1 && position != Hardware::L0C) {
        return Hardware::MAX;
    }
    return position;
}

template <typename T>
[aicore] constexpr inline Internal::DumpTensorDataType GetTensorDataType()
{
    if constexpr (IsSameType<T, bool>::value) {
        return Internal::DumpTensorDataType::ACL_BOOL;
    } else if (IsSameType<T, uint8_t>::value) {
        return Internal::DumpTensorDataType::ACL_UINT8;
    } else if (IsSameType<T, int8_t>::value) {
        return Internal::DumpTensorDataType::ACL_INT8;
    } else if (IsSameType<T, int16_t>::value) {
        return Internal::DumpTensorDataType::ACL_INT16;
    } else if (IsSameType<T, uint16_t>::value) {
        return Internal::DumpTensorDataType::ACL_UINT16;
    } else if (IsSameType<T, int32_t>::value) {
        return Internal::DumpTensorDataType::ACL_INT32;
    } else if (IsSameType<T, uint32_t>::value) {
        return Internal::DumpTensorDataType::ACL_UINT32;
    } else if (IsSameType<T, uint64_t>::value) {
        return Internal::DumpTensorDataType::ACL_UINT64;
    } else if (IsSameType<T, int64_t>::value) {
        return Internal::DumpTensorDataType::ACL_INT64;
    } else if (IsSameType<T, float>::value) {
        return Internal::DumpTensorDataType::ACL_FLOAT;
    } else if (IsSameType<T, half>::value) {
        return Internal::DumpTensorDataType::ACL_FLOAT16;
    } else if (IsSameType<T, bfloat16_t>::value) {
        return Internal::DumpTensorDataType::ACL_BF16;
    } else {
        return Internal::DumpTensorDataType::ACL_MAX;
    }
}

[aicore] inline void GetMatCopyParam(
    uint32_t dumpSize, uint16_t& n, uint16_t& m, uint16_t& dstStrideDstD, uint16_t& srcStride)
{

    uint16_t align = (dumpSize % DEFAULT_BLOCK_SIZE == 0) ? 0 : 1;
    uint16_t countBlks = align + dumpSize / DEFAULT_BLOCK_SIZE;

    uint16_t burstLen = static_cast<uint16_t>(SRC_BURST_LEN_SIZE_ELE * SRC_BURST_LEN_SIZE_ELE
        * sizeof(float) / ONE_BLK_SIZE);
    n = countBlks * BLOCK_CUBE;
    m = (burstLen * ONE_BLK_SIZE / B32_BYTE_SIZE) / BLOCK_CUBE;
    uint16_t howo = (burstLen * ONE_BLK_SIZE / sizeof(float)) / BLOCK_CUBE;
    srcStride = DivCeil(howo, BLOCK_CUBE) * BLOCK_CUBE;
    dstStrideDstD = burstLen;
}

template <typename T>
[aicore] inline void SetDumpDataL0C2GM(__attribute__((cce_global)) uint8_t* dst, const LocalTensor<T>& src, uint32_t dumpSize)
{
    if constexpr(g_coreType != AscendC::AIC) {
        return;
    }
    uint16_t n, m, dstStrideDstD, srcStride;
    GetMatCopyParam(dumpSize, n, m, dstStrideDstD, srcStride);

    copy_matrix_cc_to_gm(reinterpret_cast<__attribute__((cce_global)) float*>(dst), reinterpret_cast<__attribute__((cce_cube_c)) float*>(src.GetPhyAddr()),
        0, n, m, dstStrideDstD, srcStride, 0, QuantMode_t::NoQuant,
        static_cast<uint8_t>(false), false, false);
}

template <template<typename> class Tensor, typename T>
[aicore] inline void WriteRingBufTlvHead(const Tensor<T>& src, __attribute__((cce_global)) DumpTensorTlvInfoHead* dumpTensorTlv,
    const uint32_t& alignDumpDataLen, const uint32_t& desc, const uint32_t& dumpSize)
{
    Hardware position;
    if constexpr (IsSameType<Tensor<T>, LocalTensor<T>>::value) {
        position = GetPhyType(static_cast<TPosition>(src.GetPosition()));
    } else if (IsSameType<Tensor<T>, GlobalTensor<T>>::value) {
        position = Hardware::GM;
    }
    dumpTensorTlv->type = static_cast<uint32_t>(DumpType::DUMP_TENSOR);
    dumpTensorTlv->length = sizeof(DumpTensorTlvInfoHead) - sizeof(uint32_t[2]) + alignDumpDataLen;
    dumpTensorTlv->tensorAddr = static_cast<uint32_t>(reinterpret_cast<uintptr_t>(src.GetPhyAddr()));
    dumpTensorTlv->dataType = static_cast<uint32_t>(GetTensorDataType<T>());
    dumpTensorTlv->desc = desc;
    dumpTensorTlv->bufferId = static_cast<uint32_t>(0U);
    dumpTensorTlv->position = static_cast<uint16_t>(position);
    dumpTensorTlv->resv0 = static_cast<uint16_t>(0U);
    dumpTensorTlv->dim = static_cast<uint32_t>(0U);
    for (uint32_t i = 0; i < 8; ++i) {
        dumpTensorTlv->shape[i] = static_cast<uint32_t>(0U);
    }
    dumpTensorTlv->resv1 = static_cast<uint32_t>(0U);
    dumpTensorTlv->dumpSize = dumpSize * sizeof(T);
    dcci((__attribute__((cce_global)) uint64_t*)(dumpTensorTlv), cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] inline void WriteRingBufTlvData(const LocalTensor<T>& src, __attribute__((cce_global)) DumpTensorTlvInfoHead* dumpTensorTlv,
    const uint32_t& alignDumpDataLen, const uint32_t& dumpSize)
{
    __attribute__((cce_global)) T* dumpDataAddr = reinterpret_cast<__attribute__((cce_global)) T*>(dumpTensorTlv + 1);
    DataCopyParams copyParams = {1, static_cast<uint16_t>(alignDumpDataLen / ONE_BLK_SIZE), 0, 0};

    PipeBarrier<PIPE_ALL>();

    if (dumpTensorTlv->position == static_cast<uint16_t>(Hardware::UB)) {
        DataCopyUB2GMImpl(dumpDataAddr, reinterpret_cast<__attribute__((cce_unif_buff)) T*>(src.GetPhyAddr()), copyParams);
    } else if (dumpTensorTlv->position == static_cast<uint16_t>(Hardware::L1)) {
        DataCopyL12GMImpl(dumpDataAddr, reinterpret_cast<__attribute__((cce_cube_buff)) T*>(src.GetPhyAddr()), copyParams);
    } else if (dumpTensorTlv->position == static_cast<uint16_t>(Hardware::L0C)) {
        SetDumpDataL0C2GM(reinterpret_cast<__attribute__((cce_global)) uint8_t*>(dumpDataAddr), src, dumpSize);
    }

    PipeBarrier<PIPE_ALL>();

    dcci((__attribute__((cce_global)) uint64_t*)dumpDataAddr, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] inline void WriteRingBufTlvData(
    const GlobalTensor<T>& src, __attribute__((cce_global)) DumpTensorTlvInfoHead* dumpTensorTlv, const uint32_t& dumpSize)
{
    __attribute__((cce_global)) uint8_t* dst = reinterpret_cast<__attribute__((cce_global)) uint8_t*>(dumpTensorTlv + 1);
    MemCopyGm2Gm(dst, reinterpret_cast<__attribute__((cce_global)) const uint8_t*>(src.GetPhyAddr()), dumpSize * sizeof(T));
}

template <template<typename> class Tensor, typename T>
[aicore] inline void DumpTensorRingBufImpl(const Tensor<T>& src, uint32_t desc, uint32_t dumpSize)
{
    if constexpr (GetTensorDataType<T>() == Internal::DumpTensorDataType::ACL_MAX) {

                                                                                           ;
        return;
    }

    if (dumpSize == 0) {
        return;
    }

    if constexpr (IsSameType<Tensor<T>, LocalTensor<T>>::value) {
        Hardware position = CheckDumpTensorPosition(src);

        if (position == Hardware::MAX) {

                                                                                                           ;
            return;
        } else if (position == Hardware::L0C) {
            if constexpr(g_coreType != AscendC::AIC) {
                return;
            }
        }
    }

    __attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo = GetBlockRingBufInfo();
    if (blockRingBufInfo == nullptr) {
        return;
    }
    uint32_t alignDumpDataLen = AlignUp(dumpSize * sizeof(T), ONE_BLK_SIZE);
    uint32_t tlvLen = sizeof(DumpTensorTlvInfoHead) + alignDumpDataLen;
    if (!CheckAndWaitRingBufSpace(blockRingBufInfo, tlvLen)) {
        return;
    }

    __attribute__((cce_global)) DumpTensorTlvInfoHead* dumpTensorTlv =
        reinterpret_cast<__attribute__((cce_global)) DumpTensorTlvInfoHead*>(GetRingBufTlv(blockRingBufInfo));

    WriteRingBufTlvHead(src, dumpTensorTlv, alignDumpDataLen, desc, dumpSize);
    if constexpr (IsSameType<Tensor<T>, LocalTensor<T>>::value) {
        WriteRingBufTlvData(src, dumpTensorTlv, alignDumpDataLen, dumpSize);
    } else if (IsSameType<Tensor<T>, GlobalTensor<T>>::value) {
        WriteRingBufTlvData(src, dumpTensorTlv, dumpSize);
    }

    __attribute__((cce_global)) RingBufWriteInfo* writeInfo = GetRingBufWriteInfo(blockRingBufInfo);

    UpdateWriteInfo(writeInfo, tlvLen);
}

[aicore] inline void WriteRingBufShapeInfo(const ShapeInfo &shapeInfo)
{
    __attribute__((cce_global)) BlockRingBufInfo* blockRingBufInfo = GetBlockRingBufInfo();
    if (blockRingBufInfo == nullptr) {
        return;
    }
    uint32_t tlvLen = sizeof(DumpShapeTlvInfo);
    if (!CheckAndWaitRingBufSpace(blockRingBufInfo, tlvLen)) {
        return;
    }
    __attribute__((cce_global)) DumpShapeTlvInfo* shapeTlv =
        reinterpret_cast<__attribute__((cce_global)) DumpShapeTlvInfo*>(GetRingBufTlv(blockRingBufInfo));
    shapeTlv->type = static_cast<uint32_t>(DumpType::DUMP_SHAPE);
    shapeTlv->length = tlvLen - sizeof(uint32_t[2]);
    shapeTlv->dim = shapeInfo.shapeDim;
    for (uint32_t i = 0; i < 8; ++i) {
        shapeTlv->shape[i] = shapeInfo.shape[i];
    }
    shapeTlv->resv = static_cast<uint32_t>(0U);;
    dcci((__attribute__((cce_global)) uint64_t*)shapeTlv, cache_line_t::ENTIRE_DATA_CACHE, dcci_dst_t::CACHELINE_OUT);

    __attribute__((cce_global)) RingBufWriteInfo* writeInfo = GetRingBufWriteInfo(blockRingBufInfo);

    UpdateWriteInfo(writeInfo, tlvLen);
}

template <class... Args>
[aicore] inline void PrintfImpl(DumpType printType, __attribute__((cce_global)) const char* fmt, Args&&... args)
{
    uint64_t ctrlValue = get_ctrl();
    set_atomic_none();
    if (g_sysPrintFifoSpace != nullptr) {
        PrintfRingBufImpl(printType, fmt, args...);
    } else {
        PrintfEntityImpl(printType, fmt, args...);
    }
    set_ctrl(ctrlValue);
}

[aicore] inline void WriteTimeStampInfo(uint32_t descId)
{
# 1099 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h"
}

[aicore] inline void WriteRingBufTimeStampInfo(uint32_t descId)
{
# 1130 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_dump_tensor_impl.h"
}

[aicore] inline void DumpTimeStampImpl(uint32_t descId)
{
    if (g_sysPrintFifoSpace != nullptr) {
        WriteRingBufTimeStampInfo(descId);
    } else {
        WriteTimeStampInfo(descId);
    }
}

[aicore] inline void AscendCTimeStamp(uint32_t descId, uint64_t pcPtr = 0)
{



}

[aicore] inline void InitDump(bool mixFlag, uint32_t gmLen)
{

    if (g_sysPrintFifoSpace != nullptr) {
        return;
    }
    g_dumpWorkspaceReserved = GetSysWorkSpacePtr();
    InitDumpImpl(mixFlag, gmLen);



}
[aicore] inline void InitDump(bool mixFlag, __attribute__((cce_global)) uint8_t* dumpStartAddr, uint32_t gmLen)
{

    if (g_sysPrintFifoSpace != nullptr) {
        return;
    }
    g_dumpWorkspaceReserved = dumpStartAddr + DUMP_WORKSPACE_SIZE;
    InitDumpImpl(mixFlag, gmLen);



}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h" 2
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void DumpTensor(const LocalTensor<T> &input, uint32_t desc, uint32_t dumpSize)
{
                                                                                                ;

    DumpTensorLocal2GMImpl(input, desc, dumpSize);

    return;
}
template <typename T>
[aicore] inline void DumpTensor(const GlobalTensor<T>& input, uint32_t desc, uint32_t dumpSize)
{
                                                                                                ;

    DumpTensorGM2GMImpl(input, desc, dumpSize);

    return;
}
template <typename T>
[aicore] inline void DumpTensor(const GlobalTensor<T>& input, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{
                                                                                                ;

    DumpShapeImpl(shapeInfo);
    DumpTensorGM2GMImpl(input, desc, dumpSize);

    return;
}
template <typename T>
[aicore] inline void DumpTensor(const LocalTensor<T>& input, uint32_t desc, uint32_t dumpSize, const ShapeInfo& shapeInfo)
{
                                                                                                ;

    DumpShapeImpl(shapeInfo);
    DumpTensorLocal2GMImpl(input, desc, dumpSize);

    return;
}

template <typename T>
[aicore] inline void DumpAccChkPoint(const LocalTensor<T> &input, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{
                                                                                                     ;

    if (countOff > input.GetSize()) {


                                                      ;
        return;
    }
    LocalTensor<T> tmp = input[countOff];
    DumpTensorLocal2GMImpl(tmp, index, dumpSize);

    return;
}
template <typename T>
[aicore] inline void DumpAccChkPoint(const GlobalTensor<T> &input, uint32_t index, uint32_t countOff, uint32_t dumpSize)
{
                                                                                                     ;

    if (countOff > input.GetSize()) {


                                                      ;
        return;
    }
    GlobalTensor<T> tmp = input[countOff];
    DumpTensorGM2GMImpl(tmp, index, dumpSize);

    return;
}
# 155 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
template <class... Args>
[aicore] inline void PRINTF(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}

template <class... Args>
[aicore] inline void printf(__attribute__((cce_global)) const char* fmt, Args&&... args)
{

    PrintfImpl(DumpType::DUMP_SCALAR, fmt, args...);

}
# 183 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
template <class... Args>
[aicore] inline void AssertImpl(__attribute__((cce_global)) const char* fmt, Args&&... args)
{
# 194 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
    PrintfImpl(DumpType::DUMP_ASSERT, fmt, args...);



}
# 229 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_dump_tensor_intf_impl.h"
[aicore] inline void PrintTimeStamp(uint32_t descId)
{



}

}
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_dump_tensor_intf.h" 2
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_mm_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_mm_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] inline void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, loadDataParam.sid, 0, inc);
        } else {
            if (loadDataParam.ifTranspose) {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 1, inc);
            } else {
                load_cbuf_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                    loadDataParam.dstGap, loadDataParam.sid, 0, inc);
            }
        }
    }
}

template <typename T>
[aicore] inline void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_ca(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] inline void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        load_gm_to_cb(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
            loadDataParam.dstGap, loadDataParam.sid, (addr_cal_mode_t)0);
    }
}

template <typename T>
[aicore] inline void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T* dst, __attribute__((cce_global)) T* src, const LoadData2DParams& loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (loadDataParam.addrMode == 0) {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, inc);
        } else {
            load_gm_to_cbuf(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes, loadDataParam.srcStride,
                loadDataParam.dstGap, loadDataParam.sid, dec);
        }
    }
}

template <typename T>
[aicore] inline void LoadData2DL12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData2DL12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData2DGM2L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData2DGM2L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData2DGM2L1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src, const LoadData2DParamsV2 &loadDataParam)
{
                                                                                            ;
}

template <typename T>
[aicore] inline void LoadData2DL12L0ATransposeCal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {


                                                                                                                   ;
        if constexpr (!IsSameType<T, int4b_t>::value) {
            load_cbuf_to_ca_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] inline void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {



                         ;
        if constexpr (IsSameType<T, int4b_t>::value) {
            load_cbuf_to_cb_transpose_s4((__attribute__((cce_cube_b)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParam.startIndex,
                loadDataParam.repeatTimes, loadDataParam.srcStride, loadDataParam.dstGap, inc,
                loadDataParam.dstFracGap);
        } else {
            load_cbuf_to_cb_transpose(dst, src, loadDataParam.startIndex, loadDataParam.repeatTimes,
                loadDataParam.srcStride, loadDataParam.dstGap, inc, loadDataParam.dstFracGap);
        }
    }
}

template <typename T>
[aicore] inline void LoadData2DL12L0BTransposeCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData2dTransposeParamsV2 &loadDataParam)
{
                                                                                                             ;
}




template <typename T>
[aicore] inline void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.kExtension,
                loadDataParams.mExtension, loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW,
                loadDataParams.strideH, loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <typename T>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2<T> &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.kExtension, loadDataParams.mExtension,
                loadDataParams.kStartPt, loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH,
                loadDataParams.filterW, loadDataParams.filterH, loadDataParams.dilationFilterW,
                loadDataParams.dilationFilterH, loadDataParams.filterSizeW, loadDataParams.filterSizeH,
                loadDataParams.enTranspose, loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] inline void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_ca(reinterpret_cast<__attribute__((cce_cube_a)) half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_cb(reinterpret_cast<__attribute__((cce_cube_b)) half*>(dst),
            reinterpret_cast<__attribute__((cce_cube_buff)) half*>(src),
            loadDataParams.kExtension, loadDataParams.mExtension, loadDataParams.kStartPt,
            loadDataParams.mStartPt, loadDataParams.strideW, loadDataParams.strideH, loadDataParams.filterW,
            loadDataParams.filterH, loadDataParams.dilationFilterW, loadDataParams.dilationFilterH,
            loadDataParams.filterSizeW, loadDataParams.filterSizeH, loadDataParams.enTranspose,
            loadDataParams.fMatrixCtrl, loadDataParams.channelSize);
    }
}




template <typename T>
[aicore] inline void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_ca_s4((__attribute__((cce_cube_a)) void *)dst, (__attribute__((cce_cube_buff)) void *)src, loadDataParams.extConfig,
                loadDataParams.extConfig >> LOAD_M_EXTENSION, loadDataParams.extConfig >> LOAD_K_START_POSITION,
                loadDataParams.extConfig >> LOAD_M_START_POSITION, loadDataParams.filterConfig,
                loadDataParams.filterConfig >> LOAD_STRIDE_H, loadDataParams.filterConfig >> LOAD_FILTER_W,
                loadDataParams.filterConfig >> LOAD_FILTER_H, loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        } else {
            img2colv2_cbuf_to_ca(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] inline void LoadData3DV2L12L0ACal(__attribute__((cce_cube_a)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_ca((__attribute__((cce_cube_a)) half*)dst, (__attribute__((cce_cube_buff)) half*)src,
            loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
            loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
            loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
            loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
            loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
            loadDataParams.channelSize);
    }
}

template <typename T>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_cube_buff)) T *src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr (!IsSameType<T, int4b_t>::value) {
            img2colv2_cbuf_to_cb(dst, src, loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
                loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
                loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
                loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
                loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
                loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
                loadDataParams.channelSize);
        }
    }
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) bfloat16_t* dst, __attribute__((cce_cube_buff)) bfloat16_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        img2colv2_cbuf_to_cb((__attribute__((cce_cube_b)) half*)dst, (__attribute__((cce_cube_buff)) half*)src,
            loadDataParams.extConfig, loadDataParams.extConfig >> LOAD_M_EXTENSION,
            loadDataParams.extConfig >> LOAD_K_START_POSITION, loadDataParams.extConfig >> LOAD_M_START_POSITION,
            loadDataParams.filterConfig, loadDataParams.filterConfig >> LOAD_STRIDE_H,
            loadDataParams.filterConfig >> LOAD_FILTER_W, loadDataParams.filterConfig >> LOAD_FILTER_H,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_W,
            loadDataParams.filterConfig >> LOAD_DILATION_FILTER_H, loadDataParams.filterSizeW,
            loadDataParams.filterSizeH, loadDataParams.enTranspose, loadDataParams.fMatrixCtrl,
            loadDataParams.channelSize);
    }
}



template <typename T, typename U, typename S>
[aicore] inline void MmadCal(__attribute__((cce_cube_c)) T* c, __attribute__((cce_cube_a)) U* a, __attribute__((cce_cube_b)) S* b, const MmadParams& mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {





                                                            ;
        bool cmatrixInitVal = mmadParams.cmatrixInitVal && (!mmadParams.isBias);
        if constexpr ((IsSameType<U, int4b_t>::value) && (IsSameType<S, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, mmadParams.cmatrixSource, cmatrixInitVal);
        } else {
            mad(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, mmadParams.kDirectionAlign,
                mmadParams.cmatrixSource, cmatrixInitVal);
        }
    }
}

template <typename T, typename U, typename S>
[aicore] inline void MmadCal(__attribute__((cce_cube_c)) T* c, __attribute__((cce_cube_a)) U* a, __attribute__((cce_cube_b)) S* b, uint64_t bias,
    const MmadParams& mmadParams, bool cmatrixSource)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if constexpr ((IsSameType<U, int4b_t>::value) && (IsSameType<S, int4b_t>::value)) {
            mad_s4(c, (__attribute__((cce_cube_a)) void *)a, (__attribute__((cce_cube_b)) void *)b, mmadParams.m, mmadParams.k, mmadParams.n,
                mmadParams.unitFlag, mmadParams.kDirectionAlign, cmatrixSource,
                mmadParams.cmatrixInitVal);
        } else {
            mad(c, a, b, bias, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag,
                mmadParams.kDirectionAlign, cmatrixSource, mmadParams.cmatrixInitVal);
        }
    }
}

[aicore] inline void MmadSpCal(__attribute__((cce_cube_c)) int32_t *c, __attribute__((cce_cube_a)) int8_t *a, __attribute__((cce_cube_b)) int8_t *b, const MmadParams &mmadParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        mad_sp(c, a, b, mmadParams.m, mmadParams.k, mmadParams.n, mmadParams.unitFlag, mmadParams.cmatrixSource,
            mmadParams.cmatrixInitVal);
    }
}

template <typename T = int8_t, typename U = uint8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] inline void LoadDataWithSparseCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<U> &idx, const LoadData2dParams &loadDataParam)
{
    if constexpr(g_coreType == AscendC::AIC) {




        uint64_t src0tmp = reinterpret_cast<uint64_t>(src.GetPhyAddr());
        uint64_t src1tmp = reinterpret_cast<uint64_t>(idx.GetPhyAddr());


        uint64_t srctmp = (src0tmp & 0xffffffff) | ((src1tmp & 0xffffffff) << 32);
        __attribute__((cce_cube_buff)) int8_t *srcAddr = reinterpret_cast<__attribute__((cce_cube_buff)) int8_t *>(srctmp);

        load_cbuf_to_cb_sp((__attribute__((cce_cube_b)) int8_t *)dst.GetPhyAddr(), srcAddr, loadDataParam.startIndex,
            loadDataParam.repeatTimes);
    }
}

template <typename T = int8_t, typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true>
[aicore] inline void LoadUnzipIndexCal(const GlobalTensor<T>& src, uint32_t numOfIndexTabEntry)
{
                                                       ;
}




[aicore] inline void Load3DSetFMatrixCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= uint64_t(l1W & 0xFFFF);

        uint32_t l1HShiftBit = 16;
        regFMatrix |= uint64_t(l1H & 0xFFFF) << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix(regFMatrix);
    }
}

[aicore] inline void Load3DSetFMatrixBCal(uint16_t l1H, uint16_t l1W, const uint8_t padList[4])
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t regFMatrix = 0;
        regFMatrix |= static_cast<uint64_t>(l1W);

        uint32_t l1HShiftBit = 16;
        regFMatrix |= static_cast<uint64_t>(l1H) << l1HShiftBit;

        uint32_t padNumber = 4;
        uint32_t padListShiftBit = 8;
        uint32_t padListShiftBase = 32;
        for (uint32_t i = 0; i < padNumber; i++) {
            regFMatrix |= uint64_t(padList[i] & 0xFF) << (padListShiftBase + i * padListShiftBit);
        }
        set_fmatrix_b(regFMatrix);
    }
}

template <typename T> [aicore] inline void Load3DSetPaddingCal(const T padValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t paddingValue = 0;
        uint64_t padValueShiftBit = 8;
        if constexpr (sizeof(T) == B16_BYTE_SIZE || sizeof(T) == B32_BYTE_SIZE) {
            paddingValue = static_cast<uint64_t>(GetScalarBitcodeValue((T)padValue));
        } else {
            paddingValue = ((static_cast<uint64_t>(padValue)) << padValueShiftBit) | (static_cast<uint64_t>(padValue) & 0xFF);
        }
        set_padding(paddingValue);
    }
}




template <typename T>
[aicore] inline void LoadData3DV1L12L0ACal(__attribute__((cce_cube_a)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData3DV1L12L0BCal(__attribute__((cce_cube_b)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}

template <typename T>
[aicore] inline void LoadData3DV1L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV1<T>& loadDataParams)
{
                                                                                       ;
}




template <typename T>
[aicore] inline void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2<T>& loadDataParams)
{
                                                                                       ;
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2<int8_t>& loadDataParams)
{
                                                                                                        ;
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2<uint8_t>& loadDataParams)
{
                                                                                                         ;
}




template <typename T>
[aicore] inline void LoadData3DV2L12UBCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_cube_buff)) T* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                          ;
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) int8_t* dst, __attribute__((cce_cube_buff)) int8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                           ;
}

template <>
[aicore] inline void LoadData3DV2L12L0BCal(__attribute__((cce_cube_b)) uint8_t* dst, __attribute__((cce_cube_buff)) uint8_t* src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
                                                                                                            ;
}




template <typename T>
[aicore] inline void BroadCastVecToMMCal(__attribute__((cce_cube_c)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t blockCount,
    const uint8_t blockLen, const uint8_t srcGap, const uint8_t dstGap)
{
                                                         ;
}




[aicore] inline void CheckInitConstValueParams(uint16_t repeatTime, uint16_t blockNum, uint16_t dstGap)
{
                                                                                        ;
                                                                                    ;
                                                                                ;
}

template <typename T>
[aicore] inline void InitL1BufferCal(__attribute__((cce_cube_buff)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = (static_cast<uint64_t>(initConstValueParams.blockNum) << 16) |
            (static_cast<uint64_t>(initConstValueParams.dstGap) << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cbuf_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_cbuf_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cbuf_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cbuf_matrix(dst, repeatBit, static_cast<uint32_t>(GetScalarBitcodeValue(initConstValueParams.initValue)));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] inline void InitL0ANzMatrixCal(__attribute__((cce_cube_a)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = (static_cast<uint64_t>(initConstValueParams.blockNum) << 16) |
            (static_cast<uint64_t>(initConstValueParams.dstGap) << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_ca_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_ca_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_ca_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_ca_matrix(dst, repeatBit, static_cast<uint32_t>(GetScalarBitcodeValue(initConstValueParams.initValue)));
        } else {

                                                                                                                     ;
        }
    }
}




template <typename T>
[aicore] inline void InitL0BNzMatrixCal(__attribute__((cce_cube_b)) T *dst, const InitConstValueParams<T> &initConstValueParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        CheckInitConstValueParams(initConstValueParams.repeatTimes, initConstValueParams.blockNum,
            initConstValueParams.dstGap);
        int64_t repeatBit = (static_cast<uint64_t>(initConstValueParams.blockNum) << 16) |
            (static_cast<uint64_t>(initConstValueParams.dstGap) << 32) | initConstValueParams.repeatTimes;
        if constexpr (IsSameType<T, bfloat16_t>::value) {
            create_cb_matrix_bf16(dst, repeatBit, initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, uint32_t>::value || IsSameType<T, half>::value) {
            create_cb_matrix(dst, repeatBit, (T)initConstValueParams.initValue);
        } else if constexpr (IsSameType<T, int16_t>::value || IsSameType<T, uint16_t>::value) {
            create_cb_matrix(dst, repeatBit, GetScalarBitcodeToHalf(initConstValueParams.initValue));
        } else if constexpr (IsSameType<T, float>::value || IsSameType<T, int32_t>::value) {
            create_cb_matrix(dst, repeatBit, static_cast<uint32_t>(GetScalarBitcodeValue(initConstValueParams.initValue)));
        } else {

                                                                                                                     ;
        }
    }
}



[aicore] inline void SetLoadDataRepeatCal(const LoadDataRepeatParam& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t rptConfig = static_cast<uint64_t>(repeatParams.repeatStride) | (static_cast<uint64_t>(repeatParams.repeatTime) << 16) |
            (static_cast<uint64_t>(repeatParams.repeatMode) << 24);
        set_l3d_rpt(rptConfig);
    }
}




[aicore] inline void SetLoadDataBoundaryCal(uint32_t boundaryValue)
{
    if constexpr(g_coreType == AscendC::AIC) {
        set_l1_3d_size(static_cast<uint64_t>(boundaryValue));
    }
}




template <typename T>
[aicore] inline void LoadImageToLocalCal(__attribute__((cce_cube_buff)) T *dst, const LoadImageToLocalParams &loadDataParams)
{
    if constexpr(g_coreType == AscendC::AIC) {

                                                                                                ;
        load_image_to_cbuf(dst, static_cast<uint16_t>(loadDataParams.horizSize - 1),
            static_cast<uint16_t>(loadDataParams.vertSize - 1), loadDataParams.horizStartPos,
            loadDataParams.vertStartPos, static_cast<uint16_t>(loadDataParams.srcHorizSize - 1),
            loadDataParams.topPadSize, loadDataParams.botPadSize, loadDataParams.leftPadSize,
            loadDataParams.rightPadSize, loadDataParams.sid);
    }
}




template <typename T>
[aicore] inline void LoadDataUnzipToL1Cal(__attribute__((cce_cube_buff)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] inline void LoadDataUnzipToL0BCal(__attribute__((cce_cube_b)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}

template <typename T>
[aicore] inline void LoadDataUnzipToL0ACal(__attribute__((cce_cube_a)) T *dst, __attribute__((cce_global)) T *src)
{
                                                      ;
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h" 2
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_check.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_check.h"
namespace AscendC {

template <typename T>
[aicore] static inline bool ChannelSizeRemainder(const uint16_t channelSize, uint16_t remainder[], uint16_t size)
{
    uint16_t oneBlkNum = ONE_BLK_SIZE / sizeof(T);
    if constexpr (IsSameType<T, int4b_t>::value) {
        oneBlkNum = 64;
    }
    for (uint16_t i = 0; i < size; i++) {
        if (channelSize % oneBlkNum == remainder[i]) {
            return true;
        }
    }
    return false;
}

template <typename T, typename U, typename S>
[aicore] static inline void CheckMmadAlign(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter) {
    constexpr uint64_t align1024B = 1024;
    if constexpr ((IsSameType<PrimT<U>, half>::value) && (IsSameType<PrimT<S>, half>::value) &&
        (IsSameType<PrimT<T>, half>::value)) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "Mmad");
    } else {
        CheckTensorAlign<T>(dst, align1024B, "dst", "Mmad");
    }
    CheckTensorAlign<U>(fm, VALUE_512, "fm", "Mmad");
    CheckTensorAlign<S>(filter, VALUE_512, "filter", "Mmad");
}


template <typename T>
[aicore] static inline void CheckLoadData2dDatatype()
{
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_check.h"
                                                   ;






}


[aicore] static inline void CheckLoadData3dParams(const uint16_t srcHeight, const uint16_t srcWeight,
    const uint8_t srcWStride, const uint8_t srcHStride)
{
                                                                                                               ;
                                                                                                               ;

                                         ;

                                         ;
}


template <typename T>
[aicore] static inline void CheckLoadData3dv2ChannelSize(const uint16_t channelSize)
{
# 121 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_check.h"
    if constexpr (SupportType<PrimT<T>, half, bfloat16_t>()) {
        uint16_t remainderList[] = {0, 4, 8};



                                                                                    ;
    }

    if constexpr (SupportType<PrimT<T>, float, int32_t, uint32_t>()) {
        uint16_t remainderList[] = {0, 4};



                                                                                         ;
    } else if constexpr (SupportType<PrimT<T>, int8_t, uint8_t>()) {
        uint16_t remainderList[] = {0, 4, 8, 16};



                                                                                         ;
    } else if constexpr (IsSameType<PrimT<T>, int4b_t>::value) {
        uint16_t remainderList[] = {0, 8, 16, 32};



                                                                                      ;
    }

}


template <typename T>
[aicore] static inline void CheckLoadData3dv2MatrixParams(const uint16_t kExtension, const uint16_t mExtension,
    const uint16_t kStartPt, const uint16_t mStartPt) {
    constexpr uint16_t base16 = 16;
    if constexpr (SupportType<PrimT<T>, half, int8_t, int4b_t>()) {


                                                 ;
    }
    uint16_t kExtBase = (SupportType<PrimT<T>, int4b_t>()) ? 64 : ONE_BLK_SIZE / sizeof(PrimT<T>);
    if constexpr (SupportType<PrimT<T>, half, int8_t, int4b_t, int32_t, uint32_t, float>()) {


                                                                                      ;


                                                                                    ;
    }







                                                                                                      ;

}
}
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
namespace AscendC {
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] inline void LoadDataImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParams& loadDataParams)
{





    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "LoadData with LoadData2DParams");
    CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData2DParams");
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {


                                                                                              ;
    }
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dst,
    const GlobalTensor<T>& src, const LoadData2DParams& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData2DParams");
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData2DParams");
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dst, ONE_BLK_SIZE, "dst",
            "LoadData with LoadData2DParams");
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
                           (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {


                                                                                              ;
    }
}
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] inline void LoadDataWithTransposeImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParams& loadDataParams)
{





    CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadDataWithTranspose");

    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ATransposeCal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(),
            loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(),
            loadDataParams);
    } else {

                                                                                              ;
    }
}
# 148 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] inline void LoadDataWithTransposeImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParamsV2& loadDataParams)
{





    CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "LoadDataWithTranspose");

    CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadDataWithTranspose");

    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1", "LoadDataWithTranspose");
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BTransposeCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(),
            loadDataParams);
    } else {

                                                                                              ;
    }
}
# 190 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] inline void LoadDataImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParamsV2& loadDataParams)
{





    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1",
        "LoadData with LoadData2DParamsV2");
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData2DL12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DL12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {


                                                                                              ;
    }
}
# 236 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadDataImpl(const LocalTensor<T>& dst,
    const GlobalTensor<T>& src, const LoadData2DParamsV2& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
# 263 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_load2d_impl.h"
    if (dstScope == Hardware::L0A) {
        LoadData2DGM2L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData2DGM2L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                            (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L1) {
        LoadData2DGM2L1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(),
                           (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {


                                                                                              ;
    }

}
}
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h" 2

namespace AscendC {
struct IsResetLoad3dConfig {
    [aicore] constexpr IsResetLoad3dConfig(const bool isSetFMatrixIn, const bool isSetPaddingIn)
    {
        isSetFMatrix = isSetFMatrixIn;
        isSetPadding = isSetPaddingIn;
    }
    bool isSetFMatrix = true;
    bool isSetPadding = true;
};

constexpr IsResetLoad3dConfig IS_RESER_LOAD3D_DEFAULT_CONFIG = {true, true};
# 79 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LoadDataImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV1<U>& loadDataParams)
{
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
                     ;

    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1", "LoadData with LoadData3DParamsV1");
    CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "LoadData with LoadData3DParamsV1");
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dst, ONE_BLK_SIZE, "dst", "LoadData with LoadData3DParamsV1");
        LoadData3DV1L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {

                                                                                              ;
    }
}
# 149 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LoadDataImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2<U>& loadDataParams)
{


      ;
    if constexpr (defaultConfig.isSetFMatrix) {
        Load3DSetFMatrixCal(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.padList);
    }
    if constexpr (defaultConfig.isSetPadding) {
        Load3DSetPaddingCal(loadDataParams.padValue);
    }

    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());






    if (dstScope == Hardware::L0A) {



                                                       ;
    } else if (dstScope == Hardware::L0B) {


                                                                                                        ;
    }
# 204 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
    CheckTensorPos<T>(src, Hardware::L1, "src", "A1 / B1", "LoadData with LoadData3DParamsV2");
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        CheckTensorAlign<T>(dst, ONE_BLK_SIZE, "dst", "LoadData with LoadData3DParamsV2");
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {

                                                                                              ;
    }
}




template <const IsResetLoad3dConfig& defaultConfig>
[[deprecated("NOTICE: LoadData<IsResetLoad3dConfig> has been deprecated and will be removed in the next version."
             " Please do not use it!")]]
[aicore] inline void LoadData(const LocalTensor<bfloat16_t>& dst, const LocalTensor<bfloat16_t>& src,
    const LoadData3DParamsV2<bfloat16_t>& loadDataParams)
{
    LoadDataImpl<bfloat16_t, defaultConfig>(dst, src, loadDataParams);
}
# 273 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] inline void LoadDataImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2Pro& loadDataParams)
{





    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        LoadData3DV2L12L0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::L0B) {
        LoadData3DV2L12L0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(),
                              (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else if (dstScope == Hardware::UB) {
        LoadData3DV2L12UBCal((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
                             (__attribute__((cce_cube_buff)) PrimT<T>*)src.GetPhyAddr(), loadDataParams);
    } else {

                                                                                              ;
    }
}
# 318 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T, typename U, typename S>
[aicore] inline void MmadImpl(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const MmadParams& mmadParams)
{






    MmadCal((__attribute__((cce_cube_c)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_a)) PrimT<U>*)fm.GetPhyAddr(),
        (__attribute__((cce_cube_b)) PrimT<S>*)filter.GetPhyAddr(), mmadParams);
}

template <typename T, typename U, typename S, typename V>
[aicore] inline void MmadImpl(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const LocalTensor<V>& bias, const MmadParams& mmadParams)
{
# 353 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
    const Hardware biasScope = GetPhyType((TPosition)bias.GetPosition());
    bool cmatrixSource = false;
    if (biasScope == Hardware::BIAS) {
        cmatrixSource = true;
    } else if (biasScope == Hardware::L0C) {
        cmatrixSource = false;
    } else {

                                                                                                  ;
    }
    MmadCal((__attribute__((cce_cube_c)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_cube_a)) PrimT<U>*)fm.GetPhyAddr(),
        (__attribute__((cce_cube_b)) PrimT<S>*)filter.GetPhyAddr(), (uint64_t)bias.GetPhyAddr(), mmadParams, cmatrixSource);
}


template <typename T = int32_t, typename U = int8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int32_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, int8_t>::value, bool>::type = true>
[aicore] inline void MmadSpImpl(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<U>& filter, const MmadParams& mmadParams)
{
    CheckTensorPos<T>(dst, Hardware::L0C, "dst", "CO1", "MmadWithSparse");
    CheckTensorPos<U>(fm, Hardware::L0A, "fm", "A2", "MmadWithSparse");
    CheckTensorPos<U>(filter, Hardware::L0B, "filter", "B2", "MmadWithSparse");
    CheckTensorAlign<T>(dst, 1024, "dst", "MmadWithSparse");
    CheckTensorAlign<U>(fm, VALUE_512, "fm", "MmadWithSparse");
    CheckTensorAlign<U>(filter, VALUE_512, "filter", "MmadWithSparse");
                                                                                 ;
                                                                                 ;
                                                                                 ;
    MmadSpCal((__attribute__((cce_cube_c)) int32_t*)dst.GetPhyAddr(), (__attribute__((cce_cube_a)) int8_t*)fm.GetPhyAddr(),
        (__attribute__((cce_cube_b)) int8_t*)filter.GetPhyAddr(), mmadParams);
}

template <typename T = int8_t, typename U = uint8_t,
    typename std::enable_if<IsSameType<PrimT<T>, int8_t>::value, bool>::type = true,
    typename std::enable_if<IsSameType<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] inline void LoadDataWithSparseImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<U> &idx, const LoadData2dParams &loadDataParam)
{
    CheckTensorPos<T>(dst, Hardware::L0B, "dst", "B2", "LoadDataWithSparse");
    CheckTensorPos<T>(src, Hardware::L1, "src", "B1", "LoadDataWithSparse");
    CheckTensorPos<U>(idx, Hardware::L1, "idx", "B1", "LoadDataWithSparse");
    CheckTensorAlign<T>(dst, VALUE_512, "dst", "LoadDataWithSparse");
    CheckTensorAlign<T>(src, ONE_BLK_SIZE, "src", "LoadDataWithSparse");
    CheckTensorAlign<U>(idx, ONE_BLK_SIZE, "idx", "LoadDataWithSparse");
    LoadDataWithSparseCal(dst, src, idx, loadDataParam);
}
# 414 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T, typename U>
[aicore] inline __attribute__((inout_pipe("V"))) void BroadCastVecToMMImpl(const LocalTensor<T> &dst,
    const LocalTensor<U> &src, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{





    BroadCastVecToMMCal((__attribute__((cce_cube_c)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<U>*)src.GetPhyAddr(),
        blockCount, blockLen, srcGap, dstGap);
}
# 437 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] inline void Load3DSetPaddingImpl(const T padValue)
{
    Load3DSetPaddingCal(padValue);
}
# 455 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T, typename U = PrimT<T>,
    typename std::enable_if<IsSameType<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void InitConstValueImpl(const LocalTensor<T> &dst,
    const InitConstValueParams<U> &initConstValueParams)
{
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    if (dstScope == Hardware::L0A) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "InitConstValue when TPosition is A2");
        InitL0ANzMatrixCal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L0B) {
        CheckTensorAlign<T>(dst, VALUE_512, "dst", "InitConstValue when TPosition is B2");
        InitL0BNzMatrixCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(), initConstValueParams);
    } else if (dstScope == Hardware::L1) {
        CheckTensorAlign<T>(dst, ONE_BLK_SIZE, "dst", "InitConstValue when TPosition is A1 / B1");
        InitL1BufferCal((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(), initConstValueParams);
    } else {

                                                                                              ;
    }
}
# 487 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
[aicore] inline void SetFmatrixImpl(uint16_t l1H, uint16_t l1W, const uint8_t padList[4],
    const FmatrixMode &fmatrixMode)
{
    if (fmatrixMode == FmatrixMode::FMATRIX_LEFT) {
        Load3DSetFMatrixCal(l1H, l1W, padList);
    } else if (fmatrixMode == FmatrixMode::FMATRIX_RIGHT) {
        Load3DSetFMatrixBCal(l1H, l1W, padList);
    }
}
# 505 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
[aicore] inline void SetLoadDataBoundaryImpl(uint32_t boundaryValue)
{
    SetLoadDataBoundaryCal(boundaryValue);
}




[aicore] inline void SetLoadDataRepeatImpl(const LoadDataRepeatParam& repeatParams)
{
    SetLoadDataRepeatCal(repeatParams);
}
# 527 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
template <typename T>
[aicore] inline void LoadDataUnzipImpl(const LocalTensor<T>& dst, const GlobalTensor<T>& src)
{
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
# 542 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_mm_base_impl.h"
    if (dstScope == Hardware::L1) {
        LoadDataUnzipToL1Cal((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr());
    } else if (dstScope == Hardware::L0A) {
        LoadDataUnzipToL0ACal((__attribute__((cce_cube_a)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr());
    } else if (dstScope == Hardware::L0B) {
        LoadDataUnzipToL0BCal((__attribute__((cce_cube_b)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_global)) PrimT<T>*)src.GetPhyAddr());
    } else {

                                                       ;
    }
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h" 2

namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParams& loadDataParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const LoadData2DParams& loadDataParams);
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParamsV2& loadDataParams);

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const LoadData2DParamsV2& loadDataParams);
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV1<U>& loadDataParams);
# 131 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig = IS_RESER_LOAD3D_DEFAULT_CONFIG,
    typename U = PrimT<T>, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2<U>& loadDataParams);
# 163 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2Pro& loadDataParams);
# 182 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadDataWithTranspose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParams& loadDataParams);
# 199 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadDataWithTranspose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParamsV2& loadDataParams);
# 223 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T, typename U, typename S>
[aicore] inline void Mmad(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const MmadParams& mmadParams);

template <typename T, typename U, typename S, typename V>
[aicore] inline void Mmad(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const LocalTensor<V>& bias, const MmadParams& mmadParams);


template <typename T = int32_t, typename U = int8_t,
    typename Std::enable_if<Std::is_same<PrimT<T>, int32_t>::value, bool>::type = true,
    typename Std::enable_if<Std::is_same<PrimT<U>, int8_t>::value, bool>::type = true>
[aicore] inline void MmadWithSparse(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<U>& filter, const MmadParams& mmadParams);

template <typename T = int8_t, typename U = uint8_t,
    typename Std::enable_if<Std::is_same<PrimT<T>, int8_t>::value, bool>::type = true,
    typename Std::enable_if<Std::is_same<PrimT<U>, uint8_t>::value, bool>::type = true>
[aicore] inline void LoadDataWithSparse(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<U> &idx, const LoadData2dParams &loadDataParam);
# 253 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T, typename U>
[aicore] inline __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dst,
    const LocalTensor<U> &src, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap);
# 270 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T, typename U = PrimT<T>,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void InitConstValue(const LocalTensor<T> &dst,
    const InitConstValueParams<U> &initConstValueParams);
# 282 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void SetLoadDataPaddingValue(const T padValue);
# 295 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
[aicore] inline void SetFmatrix(uint16_t l1H, uint16_t l1W,
    const uint8_t padList[4], const FmatrixMode &fmatrixMode);
# 306 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
[aicore] inline void SetLoadDataBoundary(uint32_t boundaryValue);

[aicore] inline void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams);
# 327 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadImageToLocal(const LocalTensor<T>& dst, const LoadImageToLocalParams& loadDataParams);
# 339 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h"
template <typename T>
[aicore] inline void LoadDataUnzip(const LocalTensor<T>& dst, const GlobalTensor<T>& src);

}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dst, src, loadDataParams);
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const LoadData2DParams& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dst, src, loadDataParams);
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dst, src, loadDataParams);
}
# 89 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadData(const LocalTensor<T>& dst, const GlobalTensor<T>& src,
    const LoadData2DParamsV2& loadDataParams)
{
    CheckLoadData2dDatatype<T>();
    LoadDataImpl(dst, src, loadDataParams);
}
# 125 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig,
    typename U, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV1<U>& loadDataParams)
{
    CheckLoadData3dParams(loadDataParams.l1H, loadDataParams.l1W, loadDataParams.strideW, loadDataParams.strideH);

                                           ;

                                                           ;

                                                           ;

                                           ;

                                           ;

                                           ;
                                                                                                                ;
                                                                                                      ;
    LoadDataImpl<T, defaultConfig>(dst, src, loadDataParams);
}
# 175 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T, const IsResetLoad3dConfig &defaultConfig,
    typename U, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2<U>& loadDataParams)
{






    LoadDataImpl<T, defaultConfig>(dst, src, loadDataParams);
}
# 216 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadData(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData3DParamsV2Pro& loadDataParams)
{
    LoadDataImpl<T>(dst, src, loadDataParams);
}
# 238 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadDataWithTranspose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParams& loadDataParams)
{
    LoadDataWithTransposeImpl(dst, src, loadDataParams);
}
# 258 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadDataWithTranspose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LoadData2dTransposeParamsV2& loadDataParams)
{
    LoadDataWithTransposeImpl(dst, src, loadDataParams);
}
# 285 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T, typename U, typename S>
[aicore] inline void Mmad(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const MmadParams& mmadParams)
{
    MmadImpl(dst, fm, filter, mmadParams);
}

template <typename T, typename U, typename S, typename V>
[aicore] inline void Mmad(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<S>& filter, const LocalTensor<V>& bias, const MmadParams& mmadParams)
{
    MmadImpl(dst, fm, filter, bias, mmadParams);
}


template <typename T, typename U,
    typename Std::enable_if<Std::is_same<PrimT<T>, int32_t>::value, bool>::type,
    typename Std::enable_if<Std::is_same<PrimT<U>, int8_t>::value, bool>::type>
[aicore] inline void MmadWithSparse(const LocalTensor<T>& dst, const LocalTensor<U>& fm,
    const LocalTensor<U>& filter, const MmadParams& mmadParams)
{
    MmadSpImpl(dst, fm, filter, mmadParams);
}

template <typename T, typename U,
    typename Std::enable_if<Std::is_same<PrimT<T>, int8_t>::value, bool>::type,
    typename Std::enable_if<Std::is_same<PrimT<U>, uint8_t>::value, bool>::type>
[aicore] inline void LoadDataWithSparse(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<U> &idx, const LoadData2dParams &loadDataParam)
{
    LoadDataWithSparseImpl(dst, src, idx, loadDataParam);
}
# 330 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T, typename U>
[aicore] inline __attribute__((inout_pipe("V"))) void BroadCastVecToMM(const LocalTensor<T> &dst,
    const LocalTensor<U> &src, const int32_t blockCount, const uint8_t blockLen, const uint8_t srcGap,
    const uint8_t dstGap)
{
    BroadCastVecToMMImpl(dst, src, blockCount, blockLen, srcGap, dstGap);
}
# 350 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T, typename U, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void InitConstValue(const LocalTensor<T> &dst,
    const InitConstValueParams<U>& initConstValueParams)
{
# 367 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
    InitConstValueImpl(dst, initConstValueParams);
}
# 378 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void SetLoadDataPaddingValue(const T padValue)
{
    Load3DSetPaddingImpl(padValue);
}
# 394 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
[aicore] inline void SetFmatrix(uint16_t l1H, uint16_t l1W, const uint8_t padList[4], const FmatrixMode& fmatrixMode)
{
                                                                                     ;
                                                                                     ;
    SetFmatrixImpl(l1H, l1W, padList, fmatrixMode);
}
# 409 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
[aicore] inline void SetLoadDataBoundary(uint32_t boundaryValue)
{
    SetLoadDataBoundaryImpl(boundaryValue);
}

[aicore] inline void SetLoadDataRepeat(const LoadDataRepeatParam& repeatParams)
{
                                                                                               ;
    SetLoadDataRepeatImpl(repeatParams);
}
# 437 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline __attribute__((inout_pipe("MTE2"))) void LoadImageToLocal(const LocalTensor<T>& dst,
    const LoadImageToLocalParams& loadDataParams)
{





                                                                                                       ;
                                                                                                     ;
                                                                                                               ;
                                                                                                             ;
                                                                                                             ;
                                                                                                 ;
                                                                                                 ;
                                                                                                   ;
                                                                                                     ;
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
# 464 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
    if (dstScope == Hardware::L1) {
        LoadImageToLocalCal((__attribute__((cce_cube_buff)) PrimT<T>*)dst.GetPhyAddr(), loadDataParams);
    } else {

                                                                                          ;
    }

}
# 482 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_mm_intf_impl.h"
template <typename T>
[aicore] inline void LoadDataUnzip(const LocalTensor<T>& dst, const GlobalTensor<T>& src)
{
    LoadDataUnzipImpl(dst, src);
}
}
# 345 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_mm_intf.h" 2
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_gemm_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_gemm_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_conv2d.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_conv2d.h"
namespace AscendC {
enum class LoopMode : uint8_t {
    MODE_NM = 0,
    MODE_MN = 1,
    MODE_KM = 2,
    MODE_KN = 3
};

struct Conv2dTilling {
    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;

    uint32_t c0Size = 32;
    uint32_t dTypeSize = 1;

    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t hi = 0;
    uint32_t wi = 0;
    uint32_t ho = 0;
    uint32_t wo = 0;

    uint32_t height = 0;
    uint32_t width = 0;

    uint32_t howo = 0;

    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;

    uint32_t mBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t nBlockNum = 0;

    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;

    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;

    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;

    uint32_t mTileNums = 0;

    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTail = false;

    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;

    uint32_t mTailNums = 0;
};

struct Conv2dParams {
    [aicore] Conv2dParams() {}

    [aicore] Conv2dParams(const uint32_t imgShapeIn[CONV2D_IMG_SIZE],
        const uint32_t kernelShapeIn[CONV2D_KERNEL_SIZE], const uint32_t strideIn[CONV2D_STRIDE], const uint32_t cinIn,
        const uint32_t coutIn, const uint32_t padListIn[CONV2D_PAD], const uint32_t dilationIn[CONV2D_DILATION],
        const uint32_t initYIn, const bool partialSumIn)
    {
        for (int32_t i = 0; i < CONV2D_IMG_SIZE; ++i) {
            imgShape[i] = imgShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_KERNEL_SIZE; ++i) {
            kernelShape[i] = kernelShapeIn[i];
        }
        for (int32_t i = 0; i < CONV2D_STRIDE; ++i) {
            stride[i] = strideIn[i];
        }
        cin = cinIn;
        cout = coutIn;
        for (int32_t i = 0; i < CONV2D_PAD; ++i) {
            padList[i] = padListIn[i];
        }
        for (int32_t i = 0; i < CONV2D_DILATION; ++i) {
            dilation[i] = dilationIn[i];
        }
        initY = initYIn;
        partialSum = partialSumIn;
    }

    uint32_t imgShape[CONV2D_IMG_SIZE] = { 0 };
    uint32_t kernelShape[CONV2D_KERNEL_SIZE] = { 0 };
    uint32_t stride[CONV2D_STRIDE] = { 0 };
    uint32_t cin = 0;
    uint32_t cout = 0;
    uint32_t padList[CONV2D_PAD] = { 0 };
    uint32_t dilation[CONV2D_DILATION] = { 0 };
    uint32_t initY = 0;
    bool partialSum = false;
};

struct GemmTiling {
    [aicore] GemmTiling()
    {
        mIterNum = 1;
        nIterNum = 1;
        kIterNum = 1;
        loopMode = LoopMode::MODE_NM;
    }

    const uint32_t blockSize = 16;
    LoopMode loopMode = LoopMode::MODE_NM;
    uint32_t mNum = 0;
    uint32_t nNum = 0;
    uint32_t kNum = 0;
    uint32_t roundM = 0;
    uint32_t roundN = 0;
    uint32_t roundK = 0;
    uint32_t c0Size = 32;
    uint32_t dtypeSize = 1;
    uint32_t mBlockNum = 0;
    uint32_t nBlockNum = 0;
    uint32_t kBlockNum = 0;
    uint32_t mIterNum = 0;
    uint32_t nIterNum = 0;
    uint32_t kIterNum = 0;
    uint32_t mTileBlock = 0;
    uint32_t nTileBlock = 0;
    uint32_t kTileBlock = 0;
    uint32_t kTailBlock = 0;
    uint32_t mTailBlock = 0;
    uint32_t nTailBlock = 0;
    bool kHasTail = false;
    bool mHasTail = false;
    bool nHasTail = false;
    bool kHasTailEle = false;
    uint32_t kTailEle = 0;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_gemm_intf.h" 2
namespace AscendC {

template <typename T> [aicore] inline GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n);
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_gemm_intf.h"
template <typename T, typename U, typename S>
[aicore] inline __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum = true, int32_t initValue = 0);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_gemm_base_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_gemm_base_impl.h"
namespace AscendC {
# 130 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_gemm_base_impl.h"
[aicore] inline void CalculateGemmTiling(GemmTiling& tilling)
{
    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;

    tilling.kHasTail = tilling.kTailBlock != tilling.kTileBlock;
    tilling.kHasTailEle = tilling.roundK != tilling.kNum;
    tilling.kTailEle = tilling.kNum % (tilling.kTileBlock * tilling.c0Size);

    if (tilling.mNum != tilling.mTileBlock * tilling.blockSize) {
        tilling.mHasTail = true;
    } else {
        tilling.mHasTail = false;
    }
    tilling.nHasTail = tilling.nTileBlock != tilling.nTailBlock;
}

template <typename T>
[aicore] inline void LoadL0B(uint32_t kBlocks, uint32_t nBlocks, GemmTiling tilling, uint32_t i, uint32_t j,
    const LocalTensor<T>& src1, const LocalTensor<T>& l0b)
{
    if (tilling.nIterNum == 1) {
        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (i * tilling.kTileBlock * tilling.nBlockNum + j * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(l0b, src1[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; ++index) {
            uint32_t wSize = j * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (i * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(l0b[l0bIdx], src1[wIdx], params);
        }
    }
}

template <typename T>
[aicore] inline void LoadL0A(uint32_t kBlocks, uint32_t mBlocks, GemmTiling tilling, uint32_t i, uint32_t t,
    const LocalTensor<T>& src0, const LocalTensor<T>& l0a)
{
    if (kBlocks == 1) {
        uint32_t l1aSize = i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
        uint32_t l1aOffset = t * tilling.mTileBlock * tilling.blockSize * tilling.c0Size + l1aSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = mBlocks;
        params.srcStride = 1;
        LoadDataImpl(l0a, src0[l1aOffset], params);
    } else {

        for (size_t index = 0; index < mBlocks; index++) {
            uint32_t l0aOffset = index * kBlocks * tilling.blockSize * tilling.c0Size;
            uint32_t l1aOffset = (t * tilling.mTileBlock + index) * tilling.blockSize * tilling.c0Size +
                i * tilling.kTileBlock * tilling.mBlockNum * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = kBlocks;
            params.srcStride = tilling.mBlockNum;
            LoadDataImpl(l0a[l0aOffset], src0[l1aOffset], params);
        }
    }
}

template <typename T, typename U, typename S>
[aicore] inline void MmadFunc(const LocalTensor<U>& l0a, const LocalTensor<S>& l0b,
    const LocalTensor<T>& l0c, int32_t initValue, GemmTiling tilling, size_t i)
{
    MmadParams mmadParams;
    mmadParams.m = tilling.mTileBlock * tilling.blockSize;
    mmadParams.n = tilling.nTileBlock * tilling.blockSize;
    mmadParams.isBias = 1;

    if (tilling.kIterNum == 1) {
        mmadParams.k = tilling.kNum;
        mmadParams.isBias = initValue;
    } else if (initValue == 1 && tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue != 1 && tilling.kHasTailEle) {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailEle;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else if (initValue == 1 && !tilling.kHasTailEle) {
        if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    } else {
        if (i == 0) {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
            mmadParams.isBias = 0;
        } else if (i == tilling.kIterNum - 1) {
            mmadParams.k = tilling.kTailBlock * tilling.c0Size;
        } else {
            mmadParams.k = tilling.kTileBlock * tilling.c0Size;
        }
    }
    MmadImpl(l0c, l0a, l0b, mmadParams);
}

template <typename T, typename U>
[aicore] inline void GetPingPongBuffer(LocalTensor<T>& l0aPing, LocalTensor<T>& l0aPong,
    LocalTensor<U>& l0bPing, LocalTensor<U>& l0bPong)
{

    TBuffAddr tbufaPing;
    tbufaPing.logicPos = static_cast<uint8_t>(TPosition::A2);
    l0aPing.SetAddr(tbufaPing);
    l0aPing.InitBuffer(0, TOTAL_L0A_SIZE / 2 / sizeof(PrimT<T>));

    TBuffAddr tbufaPong;
    tbufaPong.logicPos = static_cast<uint8_t>(TPosition::A2);
    l0aPong.SetAddr(tbufaPong);
    l0aPong.InitBuffer(TOTAL_L0A_SIZE / 2, TOTAL_L0A_SIZE / 2 / sizeof(PrimT<T>));


    TBuffAddr tbufbPing;
    tbufbPing.logicPos = static_cast<uint8_t>(TPosition::B2);
    l0bPing.SetAddr(tbufbPing);
    l0bPing.InitBuffer(0, TOTAL_L0B_SIZE / 2 / sizeof(PrimT<U>));

    TBuffAddr tbufbPong;
    tbufbPong.logicPos = static_cast<uint8_t>(TPosition::B2);
    l0bPong.SetAddr(tbufbPong);
    l0bPong.InitBuffer(TOTAL_L0B_SIZE / 2, TOTAL_L0B_SIZE / 2 / sizeof(PrimT<U>));
    return;
}

template <typename T, typename U>
[aicore] inline void GetSingleThreadBuffer(LocalTensor<T>& l0a, LocalTensor<U>& l0b)
{

    TBuffAddr tbufa;
    tbufa.logicPos = static_cast<uint8_t>(TPosition::A2);
    l0a.SetAddr(tbufa);
    l0a.InitBuffer(0, TOTAL_L0A_SIZE / sizeof(PrimT<T>));


    TBuffAddr tbufb;
    tbufb.logicPos = static_cast<uint8_t>(TPosition::B2);
    l0b.SetAddr(tbufb);
    l0b.InitBuffer(0, TOTAL_L0B_SIZE / sizeof(PrimT<U>));
    return;
}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecNmNopingpong(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<U> l0a;
    LocalTensor<S> l0b;
    GetSingleThreadBuffer(l0a, l0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1, l0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0, l0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(l0a, l0b, l0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecNmPingPong(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<U> l0aPing;
    LocalTensor<U> l0aPong;
    LocalTensor<S> l0bPing;
    LocalTensor<S> l0bPong;
    GetPingPongBuffer(l0aPing, l0aPong, l0bPing, l0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1, l0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0, l0aPing);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(l0aPing, l0bPing, l0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1, l0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0, l0aPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(l0aPong, l0bPong, l0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }


    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);




}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecNm(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecNmNopingpong(l0c, src0, src1, tilling, initValue);
        return;
    }
    GemmExecNmPingPong(l0c, src0, src1, tilling, initValue);
}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecMnNopingpong(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    LocalTensor<S> l0b;
    LocalTensor<U> l0a;
    GetSingleThreadBuffer(l0a, l0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0A(kBlocks, tilling.mTileBlock, tilling, indexK, indexM, src0, l0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0B(kBlocks, tilling.nTileBlock, tilling, indexK, indexN, src1, l0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFunc(l0a, l0b, l0c, initValue, tilling, indexK);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecMnPingPong(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    uint32_t ping = 1;
    LocalTensor<U> l0aPing;
    LocalTensor<U> l0aPong;
    LocalTensor<S> l0bPing;
    LocalTensor<S> l0bPong;
    GetPingPongBuffer(l0aPing, l0aPong, l0bPing, l0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t i = 0; i < tilling.kIterNum; i++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (i == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0, l0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1, l0bPing);

                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(l0aPing, l0bPing, l0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0A(kBlocks, tilling.mTileBlock, tilling, i, indexM, src0, l0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0B(kBlocks, tilling.nTileBlock, tilling, i, indexN, src1, l0bPong);
                    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                    PipeBarrier<PIPE_M>();
                    MmadFunc(l0aPong, l0bPong, l0c, initValue, tilling, i);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename T, typename U, typename S>
[aicore] inline void GemmExecMn(const LocalTensor<T>& l0c, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, GemmTiling tilling, const int32_t initValue)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dtypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        GemmExecMnNopingpong(l0c, src0, src1, tilling, initValue);
        return;
    }
    GemmExecMnPingPong(l0c, src0, src1, tilling, initValue);
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h" 2



namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetGemmTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline GemmTiling GetGemmTiling(uint32_t m, uint32_t k, uint32_t n)
{
    uint32_t c0 = 0;
    uint32_t dSize = 1;
    if (Std::is_same<T, uint8_t>::value || Std::is_same<T, int8_t>::value) {
        c0 = 32;
        dSize = 1;
    } else {
        c0 = 16;
        dSize = 2;
    }
    GemmTiling tilling;
    tilling.c0Size = c0;
    tilling.dtypeSize = dSize;
    tilling.mNum = m;
    tilling.nNum = n;
    tilling.kNum = k;
    tilling.roundM = DivCeil(m, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(n, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(k, tilling.c0Size) * tilling.c0Size;
    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * dSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * dSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > k ? k : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }
    tilling.loopMode = LoopMode::MODE_NM;

    tilling.mBlockNum = DivCeil(m, tilling.blockSize);
    tilling.nBlockNum = DivCeil(n, tilling.blockSize);
    tilling.kBlockNum = DivCeil(k, tilling.c0Size);

    CalculateGemmTiling(tilling);

    return tilling;
}
# 93 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h"
template <typename T, typename U, typename S>
[[deprecated("NOTICE: Gemm has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline __attribute__((inout_pipe("V"))) void Gemm(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<S>& src1, const uint32_t m, const uint32_t k, const uint32_t n, GemmTiling tilling,
    bool partialsum, int32_t initValue)
{
# 114 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h"
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    LocalTensor<T> l0c;
    if (dstScope == Hardware::L0C) {
        l0c = dst[0];
    } else {
# 128 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        GemmExecNm(l0c, src0, src1, tilling, initValue);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        GemmExecMn(l0c, src0, src1, tilling, initValue);
    } else {

    }
# 145 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_gemm_intf_impl.h"
}
}
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_gemm_intf.h" 2
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_conv2d_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_conv2d_intf.h"
namespace AscendC {

template <typename T> [aicore] inline Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams);
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_conv2d_intf.h"
template <typename T, typename U>
[aicore] inline __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<T> &dst, const LocalTensor<U> &featureMap,
    const LocalTensor<U> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling);

template <typename T, typename U>
[aicore] inline __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<T> &dst,
    const LocalTensor<T> &bias, const LocalTensor<U> &featureMap, const LocalTensor<U> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_conv2d_base_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_conv2d_base_impl.h"
namespace AscendC {

template <typename T> [aicore] inline void GetTypeforC0(Conv2dParams& conv2dParams, Conv2dTilling& tilling)
{
    if (IsSameType<PrimT<T>, int8_t>::value) {
        tilling.c0Size = 32;
        tilling.dTypeSize = 1;
    } else if (IsSameType<PrimT<T>, half>::value) {
        tilling.c0Size = 16;
        tilling.dTypeSize = 2;
    } else {
        tilling.c0Size = 0;
        tilling.dTypeSize = 0;
    }
}

[aicore] inline void CalculateConv2dTiling(Conv2dTilling& tilling)
{
    tilling.mBlockNum = DivCeil(tilling.mNum, tilling.blockSize);
    tilling.nBlockNum = DivCeil(tilling.nNum, tilling.blockSize);
    tilling.kBlockNum = DivCeil(tilling.kNum, tilling.c0Size);

    tilling.roundM = DivCeil(tilling.mNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundN = DivCeil(tilling.nNum, tilling.blockSize) * tilling.blockSize;
    tilling.roundK = DivCeil(tilling.kNum, tilling.c0Size) * tilling.c0Size;

    uint32_t k0a = TOTAL_L0A_SIZE / 2 / (tilling.roundM * tilling.dTypeSize);
    uint32_t k0b = TOTAL_L0B_SIZE / 2 / (tilling.roundN * tilling.dTypeSize);
    uint32_t k0 = k0a > k0b ? k0b : k0a;
    k0 = k0 > tilling.kNum ? tilling.kNum : k0;

    tilling.kTileBlock = k0 / tilling.c0Size;
    if (tilling.kTileBlock == 0) {
        tilling.kTileBlock = 1;
    }

    tilling.mIterNum = 1;
    tilling.nIterNum = 1;
    tilling.kIterNum = DivCeil(tilling.kBlockNum, tilling.kTileBlock);

    tilling.mTileBlock = DivCeil(tilling.mBlockNum, tilling.mIterNum);
    tilling.nTileBlock = DivCeil(tilling.nBlockNum, tilling.nIterNum);

    tilling.mTileNums = tilling.mTileBlock * tilling.blockSize;

    tilling.mHasTail = (tilling.howo != tilling.mIterNum * tilling.mTileBlock * tilling.blockSize) ? true : false;
    tilling.kHasTail = (tilling.kBlockNum < tilling.kIterNum * tilling.kTileBlock) ? true : false;
    tilling.nHasTail = (tilling.nBlockNum < tilling.nIterNum * tilling.nTileBlock) ? true : false;

    tilling.mTailBlock = tilling.mBlockNum - (tilling.mIterNum - 1) * tilling.mTileBlock;
    tilling.mTailNums = tilling.howo - (tilling.mIterNum - 1) * tilling.mTileBlock * tilling.blockSize;

    tilling.kTailBlock = tilling.kBlockNum - (tilling.kIterNum - 1) * tilling.kTileBlock;
    tilling.nTailBlock = tilling.nBlockNum - (tilling.nIterNum - 1) * tilling.nTileBlock;
}

template <typename T>
[aicore] inline void LoadL0AForConv2DV1(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0, const LocalTensor<T>& l0a)
{
    uint32_t cinPos = indexK * tilling.kTileBlock;

    for (size_t index = 0; index < tilling.mTileBlock; index++) {
        uint32_t hoWoPos = (indexM * tilling.mTileBlock + index) * tilling.blockSize;
        uint32_t hoIdx = hoWoPos / tilling.wo;
        uint32_t woIdx = hoWoPos % tilling.wo;
        uint32_t hiIdx = hoIdx * tilling.strideH;
        uint32_t wiIdx = woIdx * tilling.strideW;

        uint32_t c1Idx = cinPos / (tilling.height * tilling.width);
        uint32_t kHwIdx = cinPos % (tilling.height * tilling.width);
        uint32_t l0aIdx = index * kBlocks * tilling.blockSize * tilling.c0Size;
        uint32_t disableC1 = 0;
        uint32_t c1Offset = c1Idx * tilling.c0Size * tilling.hi * tilling.wi;

        LoadData3DParamsV1<PrimT<T>> params;

        for (size_t i = 0; i < PAD_SIZE; i++) {
            params.padList[i] = conv2dParams.padList[i];
        }

        params.l1H = tilling.hi;
        params.l1W = tilling.wi;
        params.c1Index = disableC1;
        params.fetchFilterW = kHwIdx % tilling.width;
        params.fetchFilterH = kHwIdx / tilling.width;
        params.leftTopW = wiIdx - params.padList[0];
        params.leftTopH = hiIdx - params.padList[2];
        params.strideW = tilling.strideW;
        params.strideH = tilling.strideH;
        params.filterW = tilling.width;
        params.filterH = tilling.height;
        params.dilationFilterW = tilling.dilationW;
        params.dilationFilterH = tilling.dilationH;
        params.jumpStride = 1;
        params.repeatMode = 0;
        params.repeatTime = kBlocks;
        params.cSize = 0;
        params.padValue = 0;

        LoadDataImpl(l0a[l0aIdx], src0[c1Offset], params);
    }
}

template <typename T>
[aicore] inline void LoadL0AForConv2DV2(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0, const LocalTensor<T>& l0a)
{


    uint32_t kStartPt = indexK * kBlocks * tilling.c0Size;
    uint32_t mStartPt = indexM * mBlocks;
    uint32_t channelSize = conv2dParams.cin;

    LoadData3DParamsV2<PrimT<T>> params;

    for (size_t i = 0; i < PAD_SIZE; i++) {
        params.padList[i] = conv2dParams.padList[i];
    }

    params.l1H = tilling.hi;
    params.l1W = tilling.wi;
    params.channelSize = channelSize;
    params.kExtension = kBlocks * tilling.c0Size;
    params.mExtension = mBlocks;
    params.kStartPt = kStartPt;
    params.mStartPt = mStartPt;
    params.strideW = tilling.strideW;
    params.strideH = tilling.strideH;
    params.filterW = tilling.width;
    params.filterH = tilling.height;
    params.dilationFilterW = tilling.dilationW;
    params.dilationFilterH = tilling.dilationH;
    params.enTranspose = false;
    params.enSmallK = false;
    params.padValue = 0;
    params.filterSizeW = false;
    params.filterSizeH = false;
    params.fMatrixCtrl = false;

    LoadDataImpl(l0a, src0, params);
}

template <typename T>
[aicore] inline void LoadL0AForConv2D(uint32_t kBlocks, uint32_t indexK, uint32_t mBlocks, uint32_t indexM,
    Conv2dParams& conv2dParams, Conv2dTilling& tilling, const LocalTensor<T>& src0, const LocalTensor<T>& l0a)
{

    LoadL0AForConv2DV2(kBlocks, indexK, mBlocks, indexM, conv2dParams, tilling, src0, l0a);



}

template <typename T>
[aicore] inline void LoadL0BForConv2D(uint32_t kBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexN,
    Conv2dTilling& tilling, const LocalTensor<T>& src1, const LocalTensor<T>& l0b)
{
    if (tilling.nIterNum == 1) {

        uint32_t wSize = tilling.blockSize * tilling.c0Size;
        uint32_t wIdx = (indexK * tilling.kTileBlock * tilling.nBlockNum + indexN * tilling.nTileBlock) * wSize;
        LoadData2DParams params;
        params.startIndex = 0;
        params.repeatTimes = kBlocks * nBlocks;
        params.srcStride = 1;
        LoadDataImpl(l0b, src1[wIdx], params);
    } else {

        for (size_t index = 0; index < kBlocks; index++) {
            uint32_t wSize = indexN * tilling.nTileBlock * tilling.blockSize * tilling.c0Size;
            uint32_t wIdx =
                (indexK * tilling.kTileBlock + index) * tilling.nBlockNum * tilling.blockSize * tilling.c0Size + wSize;
            uint32_t l0bIdx = index * nBlocks * tilling.blockSize * tilling.c0Size;
            LoadData2DParams params;
            params.startIndex = 0;
            params.repeatTimes = nBlocks;
            params.srcStride = 1;
            LoadDataImpl(l0b[l0bIdx], src1[wIdx], params);
        }
    }
}

template <typename T, typename U>
[aicore] inline void MmadFuncForConv2D(const LocalTensor<U>& l0a, const LocalTensor<U>& l0b,
    const LocalTensor<T>& l0c, const LocalTensor<T>& bias, Conv2dParams& conv2dParams, Conv2dTilling tilling,
    uint32_t kBlocks, uint32_t mBlocks, uint32_t nBlocks, uint32_t indexK, uint32_t indexM, uint32_t indexN)
{

    uint32_t bSize = tilling.blockSize * tilling.blockSize;
    uint32_t dstFlattenIdx = (indexN * tilling.mBlockNum * tilling.nTileBlock + indexM * tilling.mTileBlock) * bSize;
    uint32_t hwActualSize = mBlocks;




    if (hwActualSize == 1) {
        hwActualSize = 2;
    }

    MmadParams mmadParams;

    mmadParams.m = hwActualSize;
    mmadParams.k = kBlocks * tilling.c0Size;
    mmadParams.n = nBlocks * tilling.blockSize;
    mmadParams.isBias = 1;

    if ((indexK == 0) && (conv2dParams.initY == 0)) {
        mmadParams.isBias = 0;
    }

    if ((indexK == 0) && (conv2dParams.initY == 2)) {
        mmadParams.isBias = 0;
        uint32_t biasOffset = nBlocks * indexN * 16;

        uint32_t burstLenUnit = 64;
        uint32_t extent = sizeof(PrimT<T>) * nBlocks * 16;
        uint32_t burstLen = extent / burstLenUnit;
        BroadCastVecToMM(l0c[dstFlattenIdx], bias[biasOffset], 1, burstLen, 0, 0);
        event_t eventIdVToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_M));
        SetFlag<HardEvent::V_M>(eventIdVToM);
        WaitFlag<HardEvent::V_M>(eventIdVToM);
    }

    MmadImpl(l0c[dstFlattenIdx], l0a, l0b, mmadParams);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecNmNopingpong(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<U> l0b;
    LocalTensor<U> l0a;
    GetSingleThreadBuffer(l0a, l0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

            LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0b);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0, l0a);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(l0a, l0b, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

[aicore] inline void SetWaitFlagMte1ToM()
{
    event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
    SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
    WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
    PipeBarrier<PIPE_M>();
}

[aicore] inline void PingPongRealeaseEvent(event_t eventId0, event_t eventId1)
{
    WaitFlag<HardEvent::M_MTE1>(eventId0);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId0);
    WaitFlag<HardEvent::M_MTE1>(eventId1);
    GetTPipePtr()->ReleaseEventID<HardEvent::M_MTE1>(eventId1);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecNmPingPong(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<U> l0aPing;
    LocalTensor<U> l0bPing;
    LocalTensor<U> l0aPong;
    LocalTensor<U> l0bPong;
    GetPingPongBuffer(l0aPing, l0aPong, l0bPing, l0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());

    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0bPing);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0,
                        l0aPing);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(l0aPing, l0bPing, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0bPong);
                for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                    LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0,
                        l0aPong);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(l0aPong, l0bPong, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    PingPongRealeaseEvent(eventId0, eventId1);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecNm(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecNmNopingpong(l0c, bias, src0, src1, conv2dParams, tilling);
        return;
    }
    Conv2DExecNmPingPong(l0c, bias, src0, src1, conv2dParams, tilling);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecMnNopingpong(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    LocalTensor<U> l0a;
    LocalTensor<U> l0b;
    GetSingleThreadBuffer(l0a, l0b);
    event_t eventIdMToMte1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::M_MTE1));
    SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
        for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

            LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0, l0a);
            for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0b);
                event_t eventIdMte1ToM = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE1_M));
                SetFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                WaitFlag<HardEvent::MTE1_M>(eventIdMte1ToM);
                PipeBarrier<PIPE_M>();
                MmadFuncForConv2D(l0a, l0b, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                    tilling.nTileBlock, indexK, indexM, indexN);
            }
        }
        SetFlag<HardEvent::M_MTE1>(eventIdMToMte1);
    }
    WaitFlag<HardEvent::M_MTE1>(eventIdMToMte1);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecMnPingPong(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t ping = 1;
    LocalTensor<U> l0aPing;
    LocalTensor<U> l0aPong;
    LocalTensor<U> l0bPing;
    LocalTensor<U> l0bPong;
    GetPingPongBuffer(l0aPing, l0aPong, l0bPing, l0bPong);

    event_t eventId0 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    event_t eventId1 = static_cast<event_t>(GetTPipePtr()->AllocEventID<HardEvent::M_MTE1>());
    SetFlag<HardEvent::M_MTE1>(eventId0);
    SetFlag<HardEvent::M_MTE1>(eventId1);

    for (size_t indexK = 0; indexK < tilling.kIterNum; indexK++) {
        uint32_t kBlocks = tilling.kTileBlock;
        if (indexK == tilling.kIterNum - 1) {
            kBlocks = tilling.kTailBlock;
        }
        if (ping == 1) {
            WaitFlag<HardEvent::M_MTE1>(eventId0);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0, l0aPing);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0bPing);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(l0aPing, l0bPing, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId0);
        } else {
            WaitFlag<HardEvent::M_MTE1>(eventId1);
            for (size_t indexM = 0; indexM < tilling.mIterNum; indexM++) {

                LoadL0AForConv2D(kBlocks, indexK, tilling.mTileNums, indexM, conv2dParams, tilling, src0, l0aPong);
                for (size_t indexN = 0; indexN < tilling.nIterNum; indexN++) {

                    LoadL0BForConv2D(kBlocks, tilling.nTileBlock, indexK, indexN, tilling, src1, l0bPong);
                    SetWaitFlagMte1ToM();
                    MmadFuncForConv2D(l0aPong, l0bPong, l0c, bias, conv2dParams, tilling, kBlocks, tilling.mTileNums,
                        tilling.nTileBlock, indexK, indexM, indexN);
                }
            }
            SetFlag<HardEvent::M_MTE1>(eventId1);
        }
        ping = 1 - ping;
    }

    PingPongRealeaseEvent(eventId0, eventId1);
}

template <typename T, typename U>
[aicore] inline void Conv2DExecMn(const LocalTensor<T>& l0c, const LocalTensor<T>& bias,
    const LocalTensor<U>& src0, const LocalTensor<U>& src1, Conv2dParams& conv2dParams,
    Conv2dTilling& tilling)
{
    uint32_t needL0Bsize = tilling.roundN * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    uint32_t needL0Asize = tilling.roundM * tilling.dTypeSize * tilling.c0Size * tilling.kTileBlock * 2;
    if (needL0Asize > TOTAL_L0A_SIZE || needL0Bsize > TOTAL_L0B_SIZE) {
        Conv2DExecMnNopingpong(l0c, bias, src0, src1, conv2dParams, tilling);
        return;
    }
    Conv2DExecMnPingPong(l0c, bias, src0, src1, conv2dParams, tilling);
}

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h" 2



namespace AscendC {

template <typename T>
[[deprecated("NOTICE: GetConv2dTiling has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline Conv2dTilling GetConv2dTiling(Conv2dParams& conv2dParams)
{
    Conv2dTilling tilling;
    GetTypeforC0<T>(conv2dParams, tilling);

    tilling.loopMode = LoopMode::MODE_NM;

    tilling.strideH = conv2dParams.stride[0];
    tilling.strideW = conv2dParams.stride[1];
    tilling.dilationH = conv2dParams.dilation[0];
    tilling.dilationW = conv2dParams.dilation[1];
    tilling.hi = conv2dParams.imgShape[0];
    tilling.wi = conv2dParams.imgShape[1];
    tilling.ho = (tilling.hi + conv2dParams.padList[2] + conv2dParams.padList[3] -
        tilling.dilationH * (conv2dParams.kernelShape[0] - 1) - 1) /
        tilling.strideH +
        1;
    tilling.wo = (tilling.wi + conv2dParams.padList[0] + conv2dParams.padList[1] -
        tilling.dilationW * (conv2dParams.kernelShape[1] - 1) - 1) /
        tilling.strideW +
        1;

    tilling.height = conv2dParams.kernelShape[0];
    tilling.width = conv2dParams.kernelShape[1];

    tilling.howo = tilling.ho * tilling.wo;

    tilling.mNum = tilling.howo;
    tilling.nNum = conv2dParams.cout;
    tilling.kNum = conv2dParams.cin * tilling.height * tilling.width;

    CalculateConv2dTiling(tilling);

    return tilling;
}
# 81 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h"
template <typename T, typename U>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline __attribute__((in_pipe("MTE2")))
    __attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<T> &dst, const LocalTensor<U> &featureMap,
    const LocalTensor<U> &weight, Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
    if (conv2dParams.initY == 2) {
        return;
    }

    Conv2D(dst, dst, featureMap, weight, conv2dParams, tilling);
}

template <typename T, typename U>
[[deprecated("NOTICE: Conv2D has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline __attribute__((in_pipe("MTE2")))__attribute__((out_pipe("MTE3"))) void Conv2D(const LocalTensor<T> &dst,
    const LocalTensor<T> &bias, const LocalTensor<U> &featureMap, const LocalTensor<U> &weight,
    Conv2dParams &conv2dParams, Conv2dTilling &tilling)
{
# 115 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h"
    const Hardware dstScope = GetPhyType((TPosition)dst.GetPosition());
    LocalTensor<T> l0c;
    if (dstScope == Hardware::L0C) {
        l0c = dst[0];
    } else {
# 129 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h"
    }

    if (tilling.loopMode == LoopMode::MODE_NM) {
        Conv2DExecNm(l0c, bias, featureMap, weight, conv2dParams, tilling);
    } else if (tilling.loopMode == LoopMode::MODE_MN) {
        Conv2DExecMn(l0c, bias, featureMap, weight, conv2dParams, tilling);
    } else {

    }
# 146 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_conv2d_intf_impl.h"
}
}
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_conv2d_intf.h" 2
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_common_intf.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_common_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_utils_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_utils_intf.h"
namespace AscendC {
enum class EngineType : int32_t {
    AIC = 1,
    AIV = 2
};

template <EngineType engine, auto funPtr, class... Args>
[aicore] void Async(Args... args)
{
    if constexpr (engine == EngineType::AIV) {
        if constexpr(g_coreType == AscendC::AIV) {
            funPtr(args...);
        }
    } else if constexpr (engine == EngineType::AIC) {
        if constexpr(g_coreType == AscendC::AIC) {
            funPtr(args...);
        }
    }
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_common_intf.h" 2






namespace AscendC {





template<pipe_t AIV_PIPE = PIPE_MTE3, pipe_t AIC_PIPE = PIPE_FIX>



[aicore] inline void SetNextTaskStart();

[aicore] inline void WaitPreTaskEnd();

[aicore] inline void InitSocState();

template <typename T>
[aicore] inline __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value = 0);

enum class AtomicDtype { ATOMIC_NONE = 0, ATOMIC_F32, ATOMIC_F16, ATOMIC_S16, ATOMIC_S32, ATOMIC_S8, ATOMIC_BF16 };

enum class AtomicOp { ATOMIC_SUM = 0 };

template <AtomicDtype type, AtomicOp op>
[aicore] inline void SetStoreAtomicConfig();

[aicore] inline void GetStoreAtomicConfig(uint16_t& atomicType, uint16_t& atomicOp);

[aicore] inline void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 1
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h"
namespace AscendC {



[aicore] inline void ClcSyncCount(__attribute__((cce_global)) int32_t* localSyncGM, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int32_t blockIdx, const int32_t totalBlocks, bool isFirst, int32_t& count)
{
    if (isFirst) {
        __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
        *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = 1;
        event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
        copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0,
            0);
        event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
        count = 1;
        for (int32_t i = 0; i < totalBlocks; i++) {
            if (i != blockIdx) {
                count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
            }
        }
    } else {
        for (int32_t i = 0; i < totalBlocks; i++) {
            count += *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(ubWorkspaceAddr) + i * ONE_BLK_FLOAT_NUM);
        }
    }
}

[aicore] inline int64_t GetBlockNum();
template <bool isAIVOnly = true>
[aicore] inline void SoftSyncAllImpl(__attribute__((cce_global)) int32_t* gmWorkspaceAddr, __attribute__((cce_unif_buff)) int32_t* ubWorkspaceAddr,
    const int usedCores)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    __sync_all_stub(usedCores, isAIVOnly);
    PipeBarrier<PIPE_ALL>();

    int32_t totalBlocks = isAIVOnly ? GetBlockNum() : (GetTaskRationImpl() * GetBlockNum());
    totalBlocks = usedCores != 0 ? usedCores : totalBlocks;



    int32_t blockIdx = isAIVOnly ? get_block_idx() : GetBlockIdxImpl();


    __attribute__((cce_global)) int32_t* localSyncGM = gmWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);
    __attribute__((cce_unif_buff)) int32_t* localUbAddr = ubWorkspaceAddr + (blockIdx * DEFAULT_BLK_NUM);

    copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), static_cast<__attribute__((cce_global)) void*>(localSyncGM), 0, 1, 1, 0, 0);
    event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
    SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
    int32_t curValue = *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) + 1;
    bool isFirst = curValue == 1 ? true : false;
    *(reinterpret_cast<__attribute__((cce_unif_buff)) int32_t*>(localUbAddr)) = curValue;
    event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
    copy_ubuf_to_gm(static_cast<__attribute__((cce_global)) void*>(localSyncGM), static_cast<__attribute__((cce_unif_buff)) void*>(localUbAddr), 0, 1, 1, 0, 0);
    event_t eventIDMTE3ToMTE2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
    SetFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    WaitFlag<HardEvent::MTE3_MTE2>(eventIDMTE3ToMTE2);
    int32_t totalBlockCount = ONE_BLK_FLOAT_NUM * totalBlocks;
    uint16_t blockLen = totalBlockCount / AscendCUtils::GetC0Count(sizeof(int32_t));
    while (true) {
        copy_gm_to_ubuf(static_cast<__attribute__((cce_unif_buff)) void*>(ubWorkspaceAddr), static_cast<__attribute__((cce_global)) void*>(gmWorkspaceAddr), 0, 1,
            blockLen, 0, 0);
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        int32_t count = 0;
        ClcSyncCount(localSyncGM, ubWorkspaceAddr, blockIdx, totalBlocks, isFirst, count);
        event_t eventIdSToMte2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
        SetFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        WaitFlag<HardEvent::S_MTE2>(eventIdSToMte2);
        if (count >= (totalBlocks * curValue)) {
            break;
        }
    }
    __sync_all_stub(usedCores, isAIVOnly);
}

constexpr uint16_t SYNC_AIV_FLAG = 12;
constexpr uint16_t SYNC_AIC_FLAG = 11;
constexpr uint16_t SYNC_AIC_AIV_FLAG = 13;
constexpr uint16_t SYNC_AIV_ONLY_ALL = 14;
constexpr uint16_t SYNC_MODE_SHIFT_VALUE = 4;
constexpr uint16_t SYNC_FLAG_SHIFT_VALUE = 8;


[aicore] inline uint16_t GetffstMsg(uint16_t mode, uint16_t flagId)
{
    return (0x1 + ((mode & 0x3) << SYNC_MODE_SHIFT_VALUE) + ((flagId & 0xf) << SYNC_FLAG_SHIFT_VALUE));
}

template<pipe_t AIV_PIPE = PIPE_MTE3, pipe_t AIC_PIPE = PIPE_FIX>
[aicore] inline void SetNextTaskStartImpl()
{
    if constexpr(g_coreType == AscendC::AIC) {
        ffts_cross_core_sync(AIC_PIPE, AscendC::GetffstMsg(0x0, AscendC::SYNC_AIC_FLAG));
    }
    if constexpr(g_coreType == AscendC::AIV) {
        ffts_cross_core_sync(AIV_PIPE, AscendC::GetffstMsg(0x0, AscendC::SYNC_AIV_ONLY_ALL));
    }
    return;
}

[aicore] inline void WaitPreTaskEndV2Impl()
{
# 203 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h"
}


template<int8_t earlyStartConfig = -1>
[aicore] inline void WaitPreTaskEndImpl()
{
# 296 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h"
}

template <bool isAIVOnly = true> [aicore] inline void SyncAllImpl()
{
# 316 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h"
    PipeBarrier<PIPE_ALL>();
    if constexpr (isAIVOnly) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x0, SYNC_AIV_ONLY_ALL));
        wait_flag_dev(SYNC_AIV_ONLY_ALL);
        return;
    }

    if constexpr(g_coreType == AscendC::AIC) {
        wait_flag_dev(SYNC_AIV_FLAG);
        ffts_cross_core_sync(PIPE_FIX, GetffstMsg(0x0, SYNC_AIC_FLAG));
        wait_flag_dev(SYNC_AIC_FLAG);
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIC_AIV_FLAG));
        return;
    }
    if constexpr(g_coreType == AscendC::AIV) {
        ffts_cross_core_sync(PIPE_MTE3, GetffstMsg(0x02, SYNC_AIV_FLAG));
        wait_flag_dev(SYNC_AIC_AIV_FLAG);
        return;
    }
}
# 418 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_sync_impl.h"
template <uint8_t modeId, pipe_t pipe>
[aicore] inline void NotifyEventImpl(uint16_t flagId)
{
    ffts_cross_core_sync(pipe, GetffstMsg(modeId, flagId));
}

template <uint8_t modeId, pipe_t pipe>
[aicore] inline void WaitEventImpl(uint16_t flagId)
{
    (void)modeId;
    wait_flag_dev(flagId);
}

[aicore] inline void SetSyncBaseAddrImpl(uint64_t config)
{
    set_ffts_base_addr(config);
}

[aicore] inline void SetSyncBaseAddr(uint64_t config)
{
    SetSyncBaseAddrImpl(config);
}
}
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_duplicate_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_duplicate_impl.h"
namespace AscendC {
template <typename T> constexpr [aicore] inline void CheckDuplicateSupportedType()
{
    static_assert(SupportType<T, half, bfloat16_t, int16_t, uint16_t, int32_t, uint32_t, float>(), "Failed to check "
        "dtype in Duplicate, current api support dtype combination is src and dst both: half / bfloat16_t / int16_t / "
        "uint16_t / int32_t / uint32_t / float.");
}

template <typename T>
[aicore] inline void DuplicateIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, T scalarValue, const uint8_t repeatTime,
    const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    vector_dup(dstLocal, scalarValue, repeatTime, dstBlockStride, 1, dstRepeatStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] inline void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTime, dstBlockStride, dstRepeatStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, repeatTime, dstBlockStride, dstRepeatStride);
    }
}

template <typename T>
[aicore] inline void DuplicateImpl(__attribute__((cce_unif_buff)) T* dstLocal, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        DuplicateIntrinsicsImpl(dstLocal, scalarValue, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        ResetMask();
        SetMaskNorm();
    }
}
}
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_server.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_server.h"
namespace AscendC {
class KfcCommServer {
public:
    __attribute__((cce_global)) KfcMsg* msgSendHead;
    __attribute__((cce_global)) KfcMsg* msgSendStart;


    __attribute__((cce_global)) KfcMsg* msgRcvHead;
    __attribute__((cce_global)) KfcMsg* msgRcvStart;

    __attribute__((cce_global)) uint8_t* ubAvalidTail;

    uint8_t msgSendPos;
    uint8_t msgRcvPos;
    uint8_t subBlockID;

public:
    [aicore] inline void Init(__attribute__((cce_global)) uint8_t* workspace, int i)
    {

        this->msgRcvStart = (__attribute__((cce_global)) KfcMsg*)GetMsgHead(workspace, i);
        this->msgSendStart = this->msgRcvStart + 64;

        this->msgSendHead = this->msgSendStart;
        this->msgSendPos = 0;
        this->msgRcvHead = this->msgRcvStart;
        this->msgRcvPos = 0;
        this->subBlockID = i;

                                                                             ;

                                                                            ;
        ubAvalidTail = GetUBAvaliedAddr(workspace, i);
    }

    [aicore] inline __attribute__((cce_global)) KfcMsg* AllocMessage()
    {
        return AllocMessageImpl(this->msgSendHead, this->msgSendPos, this->msgSendStart);
    }

    [aicore] inline void FreeMessage(__attribute__((cce_global)) KfcMsg* msg)
    {
        FreeMessageImpl(msg);
    }

    [aicore] inline void FreeUB(int32_t addr)
    {
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE3_MTE2));
        SetFlag<HardEvent::MTE3_MTE2>(eventID);
        WaitFlag<HardEvent::MTE3_MTE2>(eventID);
        __attribute__((cce_cube_buff)) uint32_t* dst = (__attribute__((cce_cube_buff)) uint32_t*)(TOTAL_L1_SIZE);




        create_cbuf_matrix((__attribute__((cce_cube_buff)) uint32_t*)dst, 0x10001, static_cast<uint32_t>(addr));

        eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_MTE3));
        SetFlag<HardEvent::MTE2_MTE3>(eventID);
        WaitFlag<HardEvent::MTE2_MTE3>(eventID);
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kfc/kfc_comm_server.h"
        copy_cbuf_to_gm((__attribute__((cce_global)) void*)ubAvalidTail, (__attribute__((cce_cube_buff)) void*)dst, 0, 1, 1, 1, 1);
    }

    [aicore] inline __attribute__((cce_global)) KfcMsg* RcvMessage()
    {
        auto msg = (__attribute__((cce_global)) KfcMsg*)RcvMessageImpl(this->msgRcvHead, this->msgRcvPos, this->msgRcvStart);
        return msg;
    }

    [aicore] inline void RollBackMsg()
    {
        RollBackMsgImpl(this->msgRcvHead, this->msgRcvPos);
        return;
    }
};

typedef KfcCommServer* KFC_COMM_SERVER_PTR;

}
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/core_mng/roc/kernel_operator_cube_group_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/core_mng/roc/kernel_operator_cube_group_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_cube_group_info.h"
namespace AscendC {

constexpr uint32_t MAX_MSG_PER_AIV = 4;
constexpr uint32_t BARRIER_SIZE = 64;
constexpr uint32_t BARRIER_MAX_AIV = 50;
constexpr uint16_t CACHE_LINE_LEN = 512;
constexpr uint32_t UB_START_ADDR = TOTAL_UB_SIZE - ONE_BLK_SIZE * BARRIER_MAX_AIV;
constexpr uint16_t CACHELINE_BLKNUM = CACHE_LINE_LEN / ONE_BLK_SIZE;

enum class CubeMsgState : uint8_t {
    FREE = 0,
    VALID,
    QUIT,
    FAKE

};

struct CubeGroupMsgHead {
    volatile CubeMsgState msgState = CubeMsgState::FREE;
    volatile uint8_t aivID;
};

struct BarrierInfo {
    volatile uint32_t head;
    uint32_t buffer[15];
};


enum class PipeMode : uint8_t { SCALAR_MODE = 0, MTE3_MODE = 1, MAX };

template <int32_t ActualFuncId, int32_t ExpectFuncId>
struct IsEqual {};

template <int32_t FuncId>
struct IsEqual<FuncId, FuncId> {
    using Type = void;
};
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/core_mng/roc/kernel_operator_cube_group_intf.h" 2
namespace AscendC {
class KfcWorkspace {
public:
    [aicore] inline KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace);
    [aicore] inline void UpdateKfcWorkspace(uint32_t offset);
    [aicore] inline __attribute__((cce_global)) uint8_t* GetKfcWorkspace();
    [aicore] inline ~KfcWorkspace();

private:
    friend [aicore] inline uint8_t GetEventId(KfcWorkspace &desc);
    [aicore] inline KfcWorkspace() = delete;
    __attribute__((cce_global)) uint8_t* msgStart;
    uint8_t evtID;
};

template <typename CubeMsgType>
class CubeResGroupHandle {
public:
    [aicore] inline CubeResGroupHandle() = default;

    [aicore] inline CubeResGroupHandle(
        __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn);




    [aicore] inline void AssignQueue(uint8_t queueIdIn);



    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] inline __attribute__((cce_global)) CubeMsgType *AllocMessage();


    template <PipeMode pipeMode = PipeMode::SCALAR_MODE>
    [aicore] inline uint16_t PostMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);


    [aicore] inline uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] inline uint16_t FreeMessage(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState waitState);


    [aicore] inline uint16_t PostFakeMsg(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] inline void SetQuit(__attribute__((cce_global)) CubeMsgType *msg);



    [aicore] inline void SetSkipMsg(uint8_t skipCnt);



    template <bool sync = true>
    [aicore] inline bool Wait(uint16_t offset);

private:
    template <typename T>
    friend [aicore] inline bool __IsRun(T handle);

    template <typename T>
    friend [aicore] inline uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct);


    template <typename T>
    friend [aicore] inline void __SetAivQuit(T *handle, uint8_t aivID);


    template <typename T>
    friend [aicore] inline __attribute__((cce_global)) T *__RcvMessage(CubeResGroupHandle<T> handle);


    template <typename T>
    friend [aicore] inline void __ReleaseMessage(T *handle);


    [aicore] inline __attribute__((cce_global)) uint8_t* __GetAicMsgHead(__attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic);


    [aicore] inline void __AivUpdateMsgCurrent();


    [aicore] inline void __AicUpdateMsgCurrent();


    [aicore] inline bool __AivIsRun(uint8_t aivID);


    [aicore] inline void __WriteGmCubeMsgByScalar(__attribute__((cce_global)) CubeMsgType *msg);


    [aicore] inline void __WriteGmStateByScalar(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgState newState);

    [aicore] inline void __WriteGmCubeMsgByDatacopy(__attribute__((cce_global)) CubeMsgType *msgPtr, CubeMsgType &cubeMsgInput);


    [aicore] inline void __CopyCubeMsg(__attribute__((cce_global)) CubeMsgType *msg, CubeMsgType &msgInput);

    uint8_t aicSize = 0;
    uint8_t aivSize = 0;
    uint8_t aivPerAic = 0;
    uint8_t aivNumForCurAic = 0;
    uint8_t queueId = 0;

    __attribute__((cce_global)) CubeMsgType *msgHead;

    __attribute__((cce_global)) CubeMsgType *msgCurrent;

    __attribute__((cce_unif_buff)) CubeMsgType *ubMsg;

    uint16_t msgPos = 0;

    uint16_t msgSize = 0;


    uint64_t aivWorkState = 0;
    uint8_t aivWork = 0;
    uint8_t eventID;
};

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename CubeMsgType>
[aicore] inline CubeResGroupHandle<CubeMsgType> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling);
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_cube_group_handle_impl.h" 2
namespace AscendC {
[aicore] inline KfcWorkspace::KfcWorkspace(__attribute__((cce_global)) uint8_t* workspace)
{
    msgStart = workspace;
    if constexpr(g_coreType == AscendC::AIV) {
        evtID = GetTPipePtr()->AllocEventID<HardEvent::MTE3_S>();
        SetFlag<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] inline void KfcWorkspace::UpdateKfcWorkspace(uint32_t offset)
{
    msgStart += offset;
}

[aicore] inline __attribute__((cce_global)) uint8_t* KfcWorkspace::GetKfcWorkspace()
{
    return msgStart;
}

[aicore] inline KfcWorkspace::~KfcWorkspace()
{
    if constexpr(g_coreType == AscendC::AIV) {
        WaitFlag<HardEvent::MTE3_S>(evtID);
        GetTPipePtr()->ReleaseEventID<HardEvent::MTE3_S>(evtID);
    }
}

[aicore] inline uint8_t GetEventId(KfcWorkspace &kfcWorkspace)
{
    return kfcWorkspace.evtID;
}

template <typename T>
[aicore] inline CubeResGroupHandle<T>::CubeResGroupHandle(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, uint8_t evtIDIn)
{
                                                                                                                                       ;
                                                                                                                                    ;
    aicSize = blockSize / MIX_NUM;
    aivSize = msgQueueSize;

    aivPerAic = Ceil(aivSize, aicSize);
    int8_t aivInLastAic = aivSize - (aicSize - 1) * aivPerAic;
                                                                                                              ;

    aivNumForCurAic = (GetBlockIdxImpl() == blockStart / MIX_NUM + aicSize - 1) ? aivInLastAic : aivPerAic;
    aivWorkState = (static_cast<uint64_t>(1) << aivNumForCurAic) - 1;

    msgSize = aivPerAic * aicSize * MAX_MSG_PER_AIV;
    msgHead = (__attribute__((cce_global)) T *)__GetAicMsgHead(workspace, blockStart / MIX_NUM, aivPerAic * MAX_MSG_PER_AIV);
    msgCurrent = msgHead;
    if constexpr(g_coreType == AscendC::AIV) {
        eventID = evtIDIn;
        uint32_t ubMsgAddr = TOTAL_UB_SIZE - ONE_BLK_SIZE * AIV_CORE_NUM - sizeof(T);





        ubMsg = reinterpret_cast<__attribute__((cce_unif_buff)) T *>(ubMsgAddr);

    }
}




template <typename T>
[aicore] inline void CubeResGroupHandle<T>::AssignQueue(uint8_t queueIdIn)
{
                                                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        uint8_t aicSubgroupID = queueIdIn / aivPerAic;
        uint8_t aivSubgroupID = queueIdIn % aivPerAic;
        queueId = queueIdIn;

                                                                                                                                             ;
        if (aicSubgroupID != aicSize - 1) {
            aivNumForCurAic = aivPerAic;
        } else {
            aivNumForCurAic = aivSize - (aicSize - 1) * aivPerAic;
        }



                             ;
        msgHead += aicSubgroupID * aivPerAic * MAX_MSG_PER_AIV + aivSubgroupID;
        msgCurrent = msgHead;
    }
}



template <typename T>
template <PipeMode pipeMode>
[aicore] inline __attribute__((cce_global)) T *CubeResGroupHandle<T>::AllocMessage()
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            WaitFlag<HardEvent::MTE3_S>((event_t)eventID);
        }
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msgCurrent->head).msgState != CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
        __attribute__((cce_global)) T *msgReturn = msgCurrent;
        __AivUpdateMsgCurrent();
        return msgReturn;
    }
    return msgCurrent;
}


template <typename T>
template <PipeMode pipeMode>
[aicore] inline uint16_t CubeResGroupHandle<T>::PostMessage(__attribute__((cce_global)) T *msg, T &msgInput)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (pipeMode == PipeMode::SCALAR_MODE) {
            __CopyCubeMsg(msg, msgInput);
            __WriteGmCubeMsgByScalar(msg);
        } else if constexpr (pipeMode == PipeMode::MTE3_MODE) {
            __WriteGmCubeMsgByDatacopy(msg, msgInput);
            SetFlag<HardEvent::MTE3_S>((event_t)eventID);
        } else {
                                                                                                                           ;
        }
    }
    return msg - msgHead;
}


template <typename T>
[aicore] inline uint16_t CubeResGroupHandle<T>::FreeMessage(__attribute__((cce_global)) T *msg)
{
    if constexpr(g_coreType == AscendC::AIC) {
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename T>
[aicore] inline uint16_t CubeResGroupHandle<T>::FreeMessage(__attribute__((cce_global)) T *msg, CubeMsgState waitState)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while ((msg->head).msgState != waitState) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteGmStateByScalar(msg, CubeMsgState::FREE);
    }
    return msg - msgHead;
}


template <typename T>
[aicore] inline uint16_t CubeResGroupHandle<T>::PostFakeMsg(__attribute__((cce_global)) T *msg)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __WriteGmStateByScalar(msg, CubeMsgState::FAKE);
    }
    return msg - msgHead;
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::SetQuit(__attribute__((cce_global)) T *msg)
{
    uint8_t aivID = queueId % aivPerAic;
    if constexpr(g_coreType == AscendC::AIV) {



                             ;
        (msg->head).aivID = aivID;
        __WriteGmStateByScalar(msg, CubeMsgState::QUIT);
    }
}



template <typename T>
[aicore] inline void CubeResGroupHandle<T>::SetSkipMsg(uint8_t skipCnt)
{
    if constexpr(g_coreType == AscendC::AIC) {
        aivWork += skipCnt;




                             ;
        msgPos += skipCnt;
    }
}



template <typename T>
template <bool sync>
[aicore] inline bool CubeResGroupHandle<T>::Wait(uint16_t offset)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_global)) T *cubeMsgCur = msgHead + offset;
        dcci(
            reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        if constexpr (sync) {
            while ((cubeMsgCur->head).msgState != CubeMsgState::FREE) {
                dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(cubeMsgCur),
                    cache_line_t::SINGLE_CACHE_LINE,
                    dcci_dst_t::CACHELINE_OUT);
            }
            return true;
        } else {
            return (cubeMsgCur->head).msgState == CubeMsgState::FREE;
        }
    }
    return true;
}

template <typename T>
[aicore] inline bool __IsRun(T handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return handle.aivWorkState;
    }
    return false;
};

template <typename T>
[aicore] inline uint32_t __GetMsgAreaLen(T handle, uint32_t sizeOfCubeMsgStruct)
{
    return handle.msgSize * sizeOfCubeMsgStruct;
}


template <typename T>
[aicore] inline void __SetAivQuit(T *handle, uint8_t aivID)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint64_t mask = ~(static_cast<uint64_t>(1) << aivID);
        handle->aivWorkState &= mask;
    }
}


template <typename T>
[aicore] inline void __ReleaseMessage(T *handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
            uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
            handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                 ? 0
                                 : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
            handle->aivWork = 0;
        } else {
            handle->msgPos += 1;
            handle->aivWork += 1;
        }
        handle->msgCurrent = handle->msgHead + handle->msgPos;
        if (handle->aivWorkState == 0) {
            return;
        }
        while (!(handle->aivWorkState & (static_cast<uint64_t>(1) << handle->aivWork))) {
            if (handle->aivWork + 1 >= handle->aivNumForCurAic) {
                uint8_t lineCnt = handle->msgPos / handle->aivPerAic;
                handle->msgPos = (lineCnt == MAX_MSG_PER_AIV - 1)
                                     ? 0
                                     : handle->msgPos + (handle->aivPerAic - handle->aivNumForCurAic) + 1;
                handle->aivWork = 0;
            } else {
                handle->msgPos += 1;
                handle->aivWork += 1;
            }
            handle->msgCurrent = handle->msgHead + handle->msgPos;
        }
    }
}

template <typename T>
[aicore] inline __attribute__((cce_global)) T *__RcvMessage(CubeResGroupHandle<T> handle)
{
    if constexpr(g_coreType == AscendC::AIC) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
        while ((handle.msgCurrent->head).msgState == CubeMsgState::FREE) {
            dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(handle.msgCurrent),
                cache_line_t::SINGLE_CACHE_LINE,
                dcci_dst_t::CACHELINE_OUT);
        }
    }
    return handle.msgCurrent;
}


template <typename T>
[aicore] inline __attribute__((cce_global)) uint8_t* CubeResGroupHandle<T>::__GetAicMsgHead(
    __attribute__((cce_global)) uint8_t* workspace, uint8_t aicStart, uint16_t msgSizePerAic)
{
    if constexpr(g_coreType == AscendC::AIC) {
        uint8_t aicIndex = GetBlockIdxImpl() - aicStart;
        auto ptr = reinterpret_cast<__attribute__((cce_global)) T *>(workspace);
        return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(&ptr[aicIndex * msgSizePerAic]);
    }
    return workspace;
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__AivUpdateMsgCurrent()
{
    msgPos = (msgPos == 3) ? 0 : msgPos + 1;
    msgCurrent = msgHead + msgPos * aivPerAic;
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__AicUpdateMsgCurrent()
{
    if (aivWork + 1 >= aivNumForCurAic) {
        uint8_t lineCnt = msgPos / aivPerAic;
        msgPos = (lineCnt == MAX_MSG_PER_AIV - 1) ? 0 : msgPos + (aivPerAic - aivNumForCurAic) + 1;
        aivWork = 0;
    } else {
        msgPos += 1;
        aivWork += 1;
    }
    msgCurrent = msgHead + msgPos;
}


template <typename T>
[aicore] inline bool CubeResGroupHandle<T>::__AivIsRun(uint8_t aivID)
{
    return aivWorkState & (static_cast<uint64_t>(1) << aivID);
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__WriteGmCubeMsgByScalar(__attribute__((cce_global)) T *msg)
{
                                                                                                                         ;

    for (uint32_t i = 1; i < sizeof(T) / sizeof(int64_t); i++) {
        dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg) + sizeof(int64_t) * i,
            cache_line_t::SINGLE_CACHE_LINE,
            dcci_dst_t::CACHELINE_OUT);
    }
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__WriteGmStateByScalar(
    __attribute__((cce_global)) T *msg, CubeMsgState newState)
{
                                                                                                                         ;
    (msg->head).msgState = newState;
    dcci(reinterpret_cast<__attribute__((cce_global)) int64_t *>(msg), cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
}

template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__WriteGmCubeMsgByDatacopy(
    __attribute__((cce_global)) T *msgPtr, T &cubeMsgInput)
{
                                                                                                                                ;
    auto ubData = reinterpret_cast<__attribute__((cce_unif_buff)) uint64_t *>(ubMsg);
    auto msgData = reinterpret_cast<uint64_t *>(&cubeMsgInput);
    for (uint32_t i = 0; i < sizeof(T) / sizeof(uint64_t); i++, ubData++, msgData++) {
        *ubData = *msgData;
    }
    event_t evtID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_MTE3>(evtID);
    WaitFlag<HardEvent::S_MTE3>(evtID);
    PipeBarrier<PIPE_MTE3>();
    copy_ubuf_to_gm((__attribute__((cce_global)) void *)msgPtr, (__attribute__((cce_unif_buff)) void *)ubMsg, 0, 1, sizeof(T) / ONE_BLK_SIZE, 0, 0);
}


template <typename T>
[aicore] inline void CubeResGroupHandle<T>::__CopyCubeMsg(__attribute__((cce_global)) T *msg, T &msgInput)
{
    auto gmPtr = reinterpret_cast<__attribute__((cce_global)) uint64_t *>(msg);
    auto msgDataPtr = reinterpret_cast<uint64_t *>(&msgInput);
    for (uint32_t i = 0; i < sizeof(T) / sizeof(int64_t); i++, gmPtr++, msgDataPtr++) {
        *gmPtr = *msgDataPtr;
    }
}

template <int groupID, class MatmulApiType, template <class, class> class CallBack, typename T>
[aicore] inline CubeResGroupHandle<T> CreateCubeResGroup(
    KfcWorkspace &desc, uint8_t blockStart, uint8_t blockSize, uint8_t msgQueueSize, __attribute__((cce_global)) uint8_t* tiling)
{

                                                                                                                     ;
    CubeResGroupHandle handle =
        CubeResGroupHandle<T>(desc.GetKfcWorkspace(), blockStart, blockSize, msgQueueSize, GetEventId(desc));
    desc.UpdateKfcWorkspace(__GetMsgAreaLen(handle, sizeof(T)));

    if constexpr(g_coreType == AscendC::AIV) {
        return handle;
    }


    auto aicId = GetBlockIdxImpl();
    if ((aicId < blockStart / MIX_NUM) || (aicId >= (blockStart / MIX_NUM + blockSize / MIX_NUM))) {
        return handle;
    }
    MatmulApiType mm;
    CallBack<MatmulApiType, T> obj;
    obj.Init(obj, mm, tiling);
    while (__IsRun(handle)) {
        auto rcvMsg = __RcvMessage(handle);
        if ((rcvMsg->head).msgState == CubeMsgState::QUIT) {
            __SetAivQuit(&handle, (rcvMsg->head).aivID);
        } else {
            obj.Call(mm, rcvMsg, handle);
        }
        __ReleaseMessage(&handle);
    }
    return handle;
}

}
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/core_mng/roc/kernel_operator_group_barrier_intf.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/core_mng/roc/kernel_operator_group_barrier_intf.h"
namespace AscendC {
template <PipeMode pipeMode>
class GroupBarrier {
public:
    [aicore] inline GroupBarrier(__attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn);
    [aicore] inline void Arrive(uint32_t arriveIndex);

    [aicore] inline void Wait(uint32_t waitIndex);
    [aicore] inline uint64_t GetWorkspaceLen();

private:
    [aicore] inline void __WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *barrierInfoAddr);
    [aicore] inline GroupBarrier() = delete;
    __attribute__((cce_global)) BarrierInfo *barrierInfoArrive;
    __attribute__((cce_global)) BarrierInfo *barrierInfoWait;
    uint32_t arriveSize;
    uint32_t waitSize;
    uint32_t counter;
    bool hasArrive;
};
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/core_mng/roc/kernel_operator_group_barrier_impl.h" 2
namespace AscendC {
[aicore] inline int64_t GetBlockNum();
template <PipeMode pipeMode>
[aicore] inline GroupBarrier<pipeMode>::GroupBarrier(
    __attribute__((cce_global)) uint8_t* groupWorkspace, uint32_t arriveSizeIn, uint32_t waitSizeIn)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                                                   ;
                                                                                                                                             ;
                                                                                                                                       ;



                           ;


                           ;
        this->barrierInfoArrive = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace);
        this->barrierInfoWait = reinterpret_cast<__attribute__((cce_global)) BarrierInfo *>(groupWorkspace + BARRIER_SIZE);
        this->arriveSize = arriveSizeIn;
        this->waitSize = waitSizeIn;
        this->counter = 1;
        this->hasArrive = false;






        __attribute__((cce_unif_buff)) int32_t *dst = reinterpret_cast<__attribute__((cce_unif_buff)) int32_t *>(UB_START_ADDR);

        for (uint32_t i = 0; i < BARRIER_MAX_AIV; i++) {
            *(__attribute__((cce_unif_buff)) uint32_t *)(dst + DEFAULT_BLK_NUM * i) = 1;
        }
    }
}

template <PipeMode pipeMode>
[aicore] inline void GroupBarrier<pipeMode>::Arrive(uint32_t arriveIndex)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (counter > 1) {
            uint32_t expectedWaitNum = (counter - 1) * waitSize;
            GlobalTensor<uint32_t> barrierInfoWaitGlobal;
            __attribute__((cce_global)) BarrierInfo *barrierInfoAddr =
                barrierInfoWait + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * arriveIndex;
            dcci((__attribute__((cce_global)) uint64_t *)barrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            while (barrierInfoAddr->head != expectedWaitNum) {
                dcci((__attribute__((cce_global)) uint64_t *)barrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
            }
        }
        __WriteCurrentValue(barrierInfoArrive);
        counter += 1;
        hasArrive = true;
    }
}


template <PipeMode pipeMode>
[aicore] inline void GroupBarrier<pipeMode>::Wait(uint32_t waitIndex)
{



    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t waitCounter = (hasArrive) ? counter - 1 : counter;
        uint32_t expectedArriveNum = waitCounter * arriveSize;
        __attribute__((cce_global)) BarrierInfo *barrierInfoAddr = barrierInfoArrive + (CACHE_LINE_LEN / sizeof(BarrierInfo)) * waitIndex;
        dcci((__attribute__((cce_global)) uint64_t *)barrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        while (barrierInfoAddr->head < expectedArriveNum) {
            dcci((__attribute__((cce_global)) uint64_t *)barrierInfoAddr, cache_line_t::SINGLE_CACHE_LINE, dcci_dst_t::CACHELINE_OUT);
        }
        __WriteCurrentValue(barrierInfoWait);
        counter =
            (hasArrive) ? counter : counter + 1;
        hasArrive = false;
    }
}

template <PipeMode pipeMode>
[aicore] inline uint64_t GroupBarrier<pipeMode>::GetWorkspaceLen()
{
    if constexpr(g_coreType == AscendC::AIV) {
                                                                                                                                  ;
                                                                                                                            ;

                                                                                                                                      ;

                                                                                                                                          ;
        return (arriveSize > waitSize) ? arriveSize * CACHE_LINE_LEN : waitSize * CACHE_LINE_LEN;
    }
}

template <PipeMode pipeMode>
[aicore] inline void GroupBarrier<pipeMode>::__WriteCurrentValue(__attribute__((cce_global)) BarrierInfo *barrierInfoAddr)
{
    if constexpr(g_coreType == AscendC::AIV) {
        uint32_t num = (arriveSize >= waitSize) ? arriveSize : waitSize;
        event_t eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_MTE3>(eventID);
        WaitFlag<HardEvent::S_MTE3>(eventID);
        SetAtomicAddImpl<int32_t>();







        __attribute__((cce_unif_buff)) int32_t *dst = (__attribute__((cce_unif_buff)) int32_t *)(UB_START_ADDR);

        copy_ubuf_to_gm((__attribute__((cce_global)) void *)barrierInfoAddr, (__attribute__((cce_unif_buff)) void *)dst, 0, num, 1, 0, CACHELINE_BLKNUM - 1);

        SetAtomicNoneImpl();
    }
}
}
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h" 2
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
namespace AscendC {
# 79 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
[aicore] inline int64_t GetBlockNum();
template <bool isAIVOnly>
[aicore] inline void IBSet(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = GetBlockNum();


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if (!isAIVOnly) {
        blockNum = GetBlockNum() * 2;
    }


    __ib_set_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 0) {
            ubWorkspace.SetValue(0, 1);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_set_stub(blockIdx, eventID, isAIVOnly);

}

template <bool isAIVOnly>
[aicore] inline void IBWait(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, int32_t blockIdx, int32_t eventID)
{
    int32_t blockNum = GetBlockNum();


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if (!isAIVOnly) {
        blockNum = GetBlockNum() * 2;
    }


    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

    auto localSyncGM = gmWorkspace[blockNum * 8 * eventID + blockIdx * 8];
    pipe_barrier(PIPE_ALL);

    while (true) {
        DataCopy(ubWorkspace, localSyncGM, ONE_BLK_SIZE / sizeof(int32_t));
        event_t eventIdMte2ToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
        SetFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        WaitFlag<HardEvent::MTE2_S>(eventIdMte2ToS);
        if (ubWorkspace.GetValue(0) == 1) {
            ubWorkspace.SetValue(0, 0);
            event_t eventIdSToMte3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMte3);
            DataCopy(localSyncGM, ubWorkspace, ONE_BLK_SIZE / sizeof(int32_t));
            break;
        }
    }
    pipe_barrier(PIPE_ALL);

    __ib_wait_stub(blockIdx, eventID, isAIVOnly);

}





template<pipe_t AIV_PIPE, pipe_t AIC_PIPE>
[aicore] inline void SetNextTaskStart()
{



}

[aicore] inline void WaitPreTaskEnd()
{



}







template <bool isAIVOnly>
[aicore] inline void SyncAll(const GlobalTensor<int32_t> &gmWorkspace,
    const LocalTensor<int32_t> &ubWorkspace, const int usedCores)
{




    SoftSyncAllImpl<isAIVOnly>((__attribute__((cce_global)) int32_t*)gmWorkspace.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)ubWorkspace.GetPhyAddr(), usedCores);

}

[aicore] inline void InitSocState()
{
    AscendCUtils::InitSocStateImpl();
}

[aicore] inline int64_t GetBlockIdx()
{
    return GetBlockIdxImpl();
}

[aicore] inline int64_t GetBlockNum()
{





    return get_block_num();

}

[aicore] inline int64_t GetSubBlockIdx()
{
    return GetSubBlockIdxImpl();
}

[aicore] inline int64_t GetTaskRatio()
{
    return GetTaskRationImpl();
}

[aicore] inline int64_t GetTaskRation()
{
    return GetTaskRatio();
}

template <typename T>
[aicore] inline __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitOutput(GlobalTensor<T> gmWorkspaceAddr, uint32_t size, T value)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<T> popBuffer;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
    uint32_t maxBurstSize = (MAX_REPEAT_TIMES * ONE_BLK_SIZE) / sizeof(T);
    uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    uint32_t round = size / popSize;
    uint32_t tail = size % popSize;
    uint32_t roundSize = round != 0 ? popSize : 0;
    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(), value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    uint32_t comOffset = 0;

    repeatParams = { 1, static_cast<uint32_t>(roundSize * sizeof(T)), 0, 0, 0 };
    for (int index = 0; index < round; ++index) {
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
        comOffset += roundSize;
    }

    repeatParams = {1, static_cast<uint32_t>(tail * sizeof(T)), 0, 0, 0};
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPadUB2GMImpl((__attribute__((cce_global)) T*)gmWorkspaceAddr.GetPhyAddr() + comOffset,
            (__attribute__((cce_unif_buff)) T*)popBuffer.GetPhyAddr(),
            repeatParams);
    }

}
# 285 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
template<bool isAIVOnly>
[aicore] inline void SyncAll()
{
    SyncAllImpl<isAIVOnly>();
}


template <AtomicDtype type, AtomicOp op>
[aicore] inline void SetStoreAtomicConfig()
{
    SetStoreAtomicConfigImpl<static_cast<atomic_type_t>(type), static_cast<atomic_op_t>(op)>();
}

[aicore] inline int64_t GetStoreAtomicConfig()
{
    return GetStoreAtomicConfigImpl();
}

[aicore] inline void GetStoreAtomicConfig(uint16_t &atomicType, uint16_t &atomicOp)
{
    GetStoreAtomicConfigImpl(atomicType, atomicOp);
}
# 321 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
template <pipe_t pipe>
[aicore] inline void NotifyEvent(uint16_t flagId)
{
    constexpr uint8_t subBlockSyncMode = 0x02;
    NotifyEventImpl<subBlockSyncMode, pipe>(flagId);
}

template <pipe_t pipe=PIPE_S>
[aicore] inline void WaitEvent(uint16_t flagId)
{
    constexpr uint8_t mode = 0;
    WaitEventImpl<mode, pipe>(flagId);
}
# 352 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
template<uint8_t modeId, pipe_t pipe>
[aicore] inline void CrossCoreSetFlag(uint16_t flagId)
{
    NotifyEventImpl<modeId, pipe>(flagId);
}

template <uint8_t modeId, pipe_t pipe>
[aicore] inline void CrossCoreWaitFlag(uint16_t flagId)
{
    WaitEventImpl<modeId, pipe>(flagId);
}

template <typename T>
[aicore] inline void DataCachePreload(const GlobalTensor<uint64_t> &src, const T cacheOffset)
{



    DataCachePreloadImpl(src, cacheOffset);

}

[aicore] inline void ICachePreLoad(const int64_t preFetchLen)
{
    PreLoad(preFetchLen);
}

[aicore] inline int64_t GetICachePreloadStatus()
{
    return GetICachePreloadStatusImpl();
}

[aicore] inline void CheckLocalMemoryIA(const CheckLocalMemoryIAParam& checkParams)
{
    CheckLocalMemoryIAImpl(checkParams);
}


template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] inline void HSetFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HSetFlagImpl<event, memT, isVirtual>(eventID);
}

template <HardEvent event, MemoryT memT, bool isVirtual> [aicore] inline void HWaitFlag(int32_t eventID)
{
    if (g_coreType == AIV) {
        return;
    }
    HWaitFlagImpl<event, memT, isVirtual>(eventID);
}
# 434 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_common_intf_impl.h"
}
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_common_intf.h" 2
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 124 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 142 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 164 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 182 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 205 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                 const LocalTensor<U>& src1, const uint64_t mask[], const uint8_t repeatTime,
                                 const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                 const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime,
                                 const BinaryRepeatParams& repeatParams);
# 223 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, typename U>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                 const LocalTensor<U>& src1, const int32_t& count);
# 245 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 263 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 285 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 303 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 325 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                           const BinaryRepeatParams& repeatParams);
# 343 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                           const LocalTensor<T>& src1, const int32_t& count);
# 365 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                          const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                          const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                          const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                          const BinaryRepeatParams& repeatParams);
# 383 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                          const LocalTensor<T>& src1, const int32_t& count);
# 405 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                               const BinaryRepeatParams& repeatParams);
# 423 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, const int32_t& count);
# 445 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <bool isSetMask = true>
[aicore] inline void AddDeqRelu(const LocalTensor<half>& dst, const LocalTensor<int32_t>& src0,
                                  const LocalTensor<int32_t>& src1, uint64_t mask[], const uint8_t repeatTime,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddDeqRelu(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                  const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
                                  const BinaryRepeatParams& repeatParams);

template <bool isSetMask = true>
[aicore] inline void AddDeqRelu(const LocalTensor<half>& dst, const LocalTensor<int32_t>& src0,
                                  const LocalTensor<int32_t>& src1, uint64_t mask, const uint8_t repeatTime,
                                  const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddDeqRelu(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                  const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime,
                                  const BinaryRepeatParams& repeatParams);
# 472 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
[aicore] inline void AddDeqRelu(const LocalTensor<half>& dst, const LocalTensor<int32_t>& src0,
                                  const LocalTensor<int32_t>& src1, const int32_t& count);

template <typename T, typename U>
[aicore] inline void AddDeqRelu(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
                                  const LocalTensor<U>& src1, const int32_t& count);
# 497 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                   const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                                   const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                   const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                                   const BinaryRepeatParams& repeatParams);
# 515 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                   const LocalTensor<T>& src1, const int32_t& count);
# 537 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                       const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                                       const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                       const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                                       const BinaryRepeatParams& repeatParams);
# 555 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                                       const LocalTensor<T>& src1, const int32_t& count);
# 577 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
                               const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime,
                               const BinaryRepeatParams& repeatParams);

template <typename T>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
                               const LocalTensor<T>& src1, const int32_t& count);
}
#pragma end_pipe
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h" 1
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 95 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Add(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    AddImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    SubImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 166 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Sub(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    SubImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 198 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 237 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Mul(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    MulImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 304 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    DivImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 359 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Div(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    DivImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 392 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;






    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;






    MulAddDstImpl<PrimDstType, PrimSrcType, isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}
# 436 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, typename U>
[aicore] inline void MulAddDst(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const int32_t& count)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;





    MulAddDstImpl((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), count);
}
# 469 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 508 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Max(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    MaxImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 540 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 579 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Min(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    MinImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 611 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AndImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 650 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void And(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    AndImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 682 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    OrImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 721 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void Or(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;





    OrImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 811 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    AddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T>
[aicore] inline void AddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    AddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 891 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <bool isSetMask>
[aicore] inline void AddDeqRelu(const LocalTensor<half>& dst, const LocalTensor<int32_t>& src0,
    const LocalTensor<int32_t>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) half*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void AddDeqRelu(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((Std::is_same<PrimDstType, half>::value && Std::is_same<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <bool isSetMask>
[aicore] inline void AddDeqRelu(const LocalTensor<half> &dst, const LocalTensor<int32_t> &src0,
    const LocalTensor<int32_t> &src1, uint64_t mask, const uint8_t repeatTime,
    const BinaryRepeatParams &repeatParams)
{






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) half*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void AddDeqRelu(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask, const uint8_t repeatTime,
    const BinaryRepeatParams &repeatParams)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((Std::is_same<PrimDstType, half>::value && Std::is_same<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");






    AddDeqReluImpl<isSetMask>((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}
# 969 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
[aicore] inline void AddDeqRelu(const LocalTensor<half>& dst, const LocalTensor<int32_t>& src0,
    const LocalTensor<int32_t>& src1, const int32_t& count)
{





    AddDeqReluImpl((__attribute__((cce_unif_buff)) half *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) int32_t *)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) int32_t *)src1.GetPhyAddr(), count);
}

template <typename T, typename U>
[aicore] inline void AddDeqRelu(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const int32_t& count)
{
    using PrimDstType = PrimT<T>;
    using PrimSrcType = PrimT<U>;
    static_assert((Std::is_same<PrimDstType, half>::value && Std::is_same<PrimSrcType, int32_t>::value) &&
        "Failed to check dtype in AddDeqRelu, current api support dtype combination is src: int32_t, dst: half.");





    AddDeqReluImpl((__attribute__((cce_unif_buff)) PrimDstType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimSrcType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimSrcType*)src1.GetPhyAddr(), count);
}
# 1016 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}
# 1067 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void FusedMulAdd(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 1104 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    FusedMulAddReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}
# 1155 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T>
[aicore] inline void FusedMulAddRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    FusedMulAddReluImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 1192 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), mask, repeatTime,
        repeatParams);
}

template <typename T>
[aicore] inline void SubRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const int32_t& count)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    SubReluImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), count);
}
# 1369 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_intf_impl.h"
}
#pragma end_pipe
# 593 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_intf.h" 2
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 118 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 143 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 169 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 194 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 220 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 245 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 271 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 296 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn);

template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn = false);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn);
# 322 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
# 347 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 373 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count);

template <typename T, typename U, bool isSetMask = true,
    typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type = true>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_binary_scalar_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_binary_scalar_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void AddsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Adds, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vadds(dst, src, scalarValue, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}


template <typename T, bool isSetMask = true>
[aicore] inline void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AddsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void AddsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            AddsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        AddsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void MulsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Muls, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmuls(dst, src, scalarValue, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}


template <typename T, bool isSetMask = true>
[aicore] inline void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MulsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void MulsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MulsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        MulsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void MaxsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Maxs, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmaxs(dst, src, scalarValue, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride), false, false);
}


template <typename T, bool isSetMask = true>
[aicore] inline void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MaxsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MaxsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        MaxsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void MinsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int16_t, int32_t>(), "Failed to check dtype in Mins, current api support "
        "dtype combination is src and dst both: half / float / int16_t / int32_t.");
    vmins(dst, src, scalarValue, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride), false, false);
}


template <typename T, bool isSetMask = true>
[aicore] inline void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        MinsIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void MinsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            MinsIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        MinsIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void ShiftLeftIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, uint16_t, uint32_t, int16_t, int32_t>(), "Failed to check dtype in ShiftLeft, current "
        "api support dtype combination is src and dst both: uint16_t / uint32_t / int16_t / int32_t.");

                                                                                        ;
    vshl(dst, src, static_cast<uint32_t>(scalarValue), repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride),
        static_cast<uint16_t>(repeatParams.srcBlkStride), static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}


template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void ShiftLeftImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        ShiftLeftIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void ShiftRightIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams, bool roundEn)
{


                                ;

                                                                                         ;

    if (roundEn) {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride), true);
        } else {
            vshr(dst, src, static_cast<uint32_t>(scalarValue), repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride), true);
        }
    } else {
        if constexpr (SupportType<T, int16_t, int32_t>()) {
            vshr(dst, src, (int32_t)scalarValue, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride), false);
        } else {
            vshr(dst, src, static_cast<uint32_t>(scalarValue), repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride), false);
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams, roundEn);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn = false)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams, roundEn);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void ShiftRightImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        ShiftRightIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE }, false);
        ResetMask();
        SetMaskNorm();
    }
}




template <typename T>
[aicore] inline void LeakyReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, T scalarValue, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in LeakyRelu, current api support dtype "
        "combination is src and dst both: half / float.");
    vlrelu(dst, src, scalarValue, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint16_t>(repeatParams.dstRepStride), static_cast<uint16_t>(repeatParams.srcRepStride));
}


template <typename T, bool isSetMask = true>
[aicore] inline void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask[],
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const uint64_t mask,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void LeakyReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const T& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (!isSetMask) {
            LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            return;
        }
        SetMaskCount();
        AscendCUtils::SetMask<T>(0, count);
        LeakyReluIntrinsicsImpl(dst, src, scalarValue, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        ResetMask();
        SetMaskNorm();
    }
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 54 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same< PrimT<T>, U>::value, bool>::type>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same< PrimT<T>, U>::value, bool>::type>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}
# 120 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    AddsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same< PrimT<T>, U>::value, bool>::type>
[aicore] inline void Adds(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    AddsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, count);
}
# 165 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same< PrimT<T>, U>::value, bool>::type>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}
# 231 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    MulsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same< PrimT<T>, U>::value, bool>::type>
[aicore] inline void Muls(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    MulsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, count);
}
# 276 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}
# 342 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    MaxsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Maxs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    MaxsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, count);
}
# 387 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, mask, repeatTime, repeatParams);
}
# 453 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    MinsImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void Mins(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    MinsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        scalarValue, count);
}
# 498 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type >
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams);
}
# 564 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    ShiftLeftImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void ShiftLeft(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    ShiftLeftImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, count);
}
# 609 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams, roundEn);
}

template <typename T, bool isSetMask>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams, roundEn);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams, bool roundEn)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams, roundEn);
}
# 675 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    ShiftRightImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void ShiftRight(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    ShiftRightImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, count);
}
# 720 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams);
}

template < typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type >
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, mask, repeatTime, repeatParams);
}
# 786 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const T& scalarValue,
    const int32_t& count)
{






    LeakyReluImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src.GetPhyAddr(), scalarValue,
        count);
}

template < typename T, typename U, bool isSetMask, typename Std::enable_if<Std::is_same<PrimT<T>, U>::value, bool>::type >
[aicore] inline void LeakyRelu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const U& scalarValue,
    const int32_t& count)
{
    using PrimType = PrimT<T>;






    LeakyReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), scalarValue, count);
}
# 1663 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_binary_scalar_intf_impl.h"
}
#pragma end_pipe
# 385 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_binary_scalar_intf.h" 2
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_duplicate_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_duplicate_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_duplicate_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);

template <typename T, bool isSetMask = true>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride);
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_duplicate_intf.h"
template <typename T>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, const int32_t& count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_duplicate_intf_impl.h" 1
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_duplicate_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_duplicate_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, uint64_t mask,
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), scalarValue, mask, repeatTime, dstBlockStride,
        dstRepeatStride);
}

template <typename T, bool isSetMask>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, uint64_t mask[],
    const uint8_t repeatTime, const uint16_t dstBlockStride, const uint8_t dstRepeatStride)
{
    CheckDuplicateSupportedType<T>();






    DuplicateImpl<T, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), scalarValue, mask, repeatTime, dstBlockStride,
        dstRepeatStride);
}
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_duplicate_intf_impl.h"
template <typename T>
[aicore] inline void Duplicate(const LocalTensor<T>& dst, const T& scalarValue, const int32_t& count)
{
    CheckDuplicateSupportedType<T>();





    DuplicateImpl<T>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), scalarValue, count);
}
# 191 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_duplicate_intf_impl.h"
}
#pragma end_pipe
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_duplicate_intf.h" 2
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_mask_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_mask_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_gather.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_gather.h"
namespace AscendC {
struct GatherRepeatParams {
    [aicore] GatherRepeatParams() {}

    [aicore] GatherRepeatParams(const uint8_t dstBlkStrideIn, const uint8_t dstRepStrideIn)
        : dstBlkStride(dstBlkStrideIn),
          dstRepStride(dstRepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};

enum class GatherMaskMode : uint8_t {
    VERSION_V1 = 0,
    VERSION_V2 = 1
};


const GatherMaskMode defaultGahterMaskMode = GatherMaskMode::VERSION_V2;
const GatherMaskMode defaultGatherMaskMode = GatherMaskMode::VERSION_V2;





struct GatherMaskParams {
    [aicore] GatherMaskParams() {}

    [aicore] GatherMaskParams(const uint8_t src0BlockStrideIn, const uint16_t repeatTimesIn,
        const uint16_t src0RepeatStrideIn, const uint8_t src1RepeatStrideIn)
        : src0BlockStride(src0BlockStrideIn),
          repeatTimes(repeatTimesIn),
          src0RepeatStride(src0RepeatStrideIn),
          src1RepeatStride(src1RepeatStrideIn)
    {}

    uint8_t src0BlockStride = DEFAULT_BLK_STRIDE;
    uint16_t repeatTimes = 0;
    uint16_t src0RepeatStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepeatStride = DEFAULT_REPEAT_STRIDE;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_mask_intf.h" 2





namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt);

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt);
#pragma end_pipe
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_gather_mask_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_gather_mask_impl.h"
namespace AscendC {
[aicore] inline void GatherMaskImpl(__attribute__((cce_unif_buff)) uint16_t* dst, __attribute__((cce_unif_buff)) uint16_t* src0, __attribute__((cce_unif_buff)) uint16_t* src1,
    const uint8_t patternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, gatherMaskParams.repeatTimes, gatherMaskParams.src0BlockStride, patternMode,
            gatherMaskParams.src0RepeatStride, gatherMaskParams.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

[aicore] inline void GatherMaskImpl(__attribute__((cce_unif_buff)) uint32_t* dst, __attribute__((cce_unif_buff)) uint32_t* src0, __attribute__((cce_unif_buff)) uint32_t* src1,
    const uint8_t patternMode, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (reduceMode) {
            SetMaskCount();
        } else {
            SetMaskNorm();
        }







        set_vector_mask(0, mask);

        vreducev2(dst, src0, src1, gatherMaskParams.repeatTimes, gatherMaskParams.src0BlockStride, patternMode,
            gatherMaskParams.src0RepeatStride, gatherMaskParams.src1RepeatStride);
        rsvdCnt = AscendCUtils::GetRsvdCnt();
        SetMaskNorm();
    }
}

template <typename T>
[aicore] inline void GatherMaskImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, const uint8_t patternMode,
    const GatherMaskParams& gatherMaskParams)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                                                        ;
        if (sizeof(T) == sizeof(uint16_t)) {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src1), gatherMaskParams.repeatTimes, gatherMaskParams.src0BlockStride,
                patternMode, gatherMaskParams.src0RepeatStride, gatherMaskParams.src1RepeatStride);
        } else {
            vreducev2(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
                reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src1), gatherMaskParams.repeatTimes, gatherMaskParams.src0BlockStride,
                patternMode, gatherMaskParams.src0RepeatStride, gatherMaskParams.src1RepeatStride);
        }
    }
}

template <typename T>
[aicore] inline void GatherMaskImpl(
    __attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
                                                                             ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    GatherMaskImpl(dst, src0, nullsrc1, patternMode, gatherMaskParams);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] inline void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint16_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                                           ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0), src1, 0,
        reduceMode, mask, gatherMaskParams, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] inline void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* src1, const bool reduceMode,
    const uint32_t mask, const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
                                                                                                               ;


                                         ;
    GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0), src1, 0,
        reduceMode, mask, gatherMaskParams, rsvdCnt);
}

template <typename T, GatherMaskMode mode = defaultGatherMaskMode>
[aicore] inline void GatherMaskCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t src1Pattern,
    const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
                                                                                                               ;
                                                                             ;


                                                                                                    ;
    __attribute__((cce_unif_buff)) T* nullsrc1 = ONE_REPEAT_BYTE_SIZE * sizeof(T) + src0;
    if (sizeof(T) == sizeof(uint16_t)) {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(nullsrc1), src1Pattern, reduceMode, mask, gatherMaskParams, rsvdCnt);
    } else {
        GatherMaskImpl(reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(dst), reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(src0),
            reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(nullsrc1), src1Pattern, reduceMode, mask, gatherMaskParams, rsvdCnt);
    }
}

[aicore] inline int64_t GetGatherMaskRemainCountImpl()
{
    return get_rsvd_cnt();
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h"
namespace AscendC {
#pragma begin_pipe(V)
template <typename T, typename U, GatherMaskMode mode>
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<U>& src1Pattern, const bool reduceMode, const uint32_t mask,
    const GatherMaskParams& gatherMaskParams, uint64_t& rsvdCnt)
{
    using DstPrimType = PrimT<T>;
    using Src1PrimType = PrimT<U>;
# 57 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h"
    GatherMaskCal<DstPrimType, mode>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) DstPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) Src1PrimType*)src1Pattern.GetPhyAddr(), reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T, GatherMaskMode mode>
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const uint8_t src1Pattern, const bool reduceMode, const uint32_t mask, const GatherMaskParams& gatherMaskParams,
    uint64_t& rsvdCnt)
{
    using PrimType = PrimT<T>;
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_mask_intf_impl.h"
    GatherMaskCal<PrimType, mode>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), src1Pattern,
        reduceMode, mask, gatherMaskParams, rsvdCnt);

}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    using PrimType = PrimT<T>;
    GatherMaskImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), patternMode, gatherMaskParams);
}

template <typename T>
[[deprecated("NOTICE: This GatheMask in this form has been deprecated and will be removed in the next version. "
        "Please do not use it!")]]
[aicore] inline void GatherMask(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const uint8_t patternMode, const GatherMaskParams& gatherMaskParams)
{
    using PrimType = PrimT<T>;
    GatherMaskImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), patternMode,
        gatherMaskParams);
}
#pragma end_pipe

[aicore] inline __attribute__((inout_pipe("S"))) int64_t GetGatherMaskRemainCount()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return GetGatherMaskRemainCountImpl();
}
}
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_mask_intf.h" 2
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_vdeq.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_vdeq.h"
namespace AscendC {
struct VdeqInfo {
    [aicore] VdeqInfo() {}

    [aicore] VdeqInfo(const float vdeqScaleIn[VDEQ_TENSOR_SIZE], const int16_t vdeqOffsetIn[VDEQ_TENSOR_SIZE],
        const bool vdeqSignModeIn[VDEQ_TENSOR_SIZE])
    {
        for (int32_t i = 0; i < VDEQ_TENSOR_SIZE; ++i) {
            vdeqScale[i] = vdeqScaleIn[i];
            vdeqOffset[i] = vdeqOffsetIn[i];
            vdeqSignMode[i] = vdeqSignModeIn[i];
        }
    }

    float vdeqScale[VDEQ_TENSOR_SIZE] = { 0 };
    int16_t vdeqOffset[VDEQ_TENSOR_SIZE] = { 0 };
    bool vdeqSignMode[VDEQ_TENSOR_SIZE] = { 0 };
};
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint64_t mask[], const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint32_t count);
# 74 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true, bool isVecDeq = true, bool halfBlock = true>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const int32_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 89 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U, bool isVecDeq = true, bool halfBlock = true>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const uint32_t count);
# 112 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);
# 130 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const uint32_t count);
# 153 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);
# 171 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h"
template <typename T, typename U>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const uint32_t count);

#pragma end_pipe
[aicore] inline void SetDeqScale(half scale);

[aicore] inline void SetDeqScale(float scale, int16_t offset, bool signMode);

template <typename T>
[aicore] inline void SetDeqScale(const LocalTensor<T>& vdeq, const VdeqInfo& vdeqInfo);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h" 1
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_vconv_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_vconv_impl.h"
namespace AscendC {
[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    vconv_deq(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride, repeatParams.dstRepStride,
        repeatParams.srcRepStride);
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int8_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s82f16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) uint8_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_u82f16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s322f32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s322f32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s322f32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s322f32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s322f32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s322f32(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_f162f32(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s8r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s8f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s8c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s8a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s8z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s8(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162u8r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162u8f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162u8c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162u8a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162u8z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162u8(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f16r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f16f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f16c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f16a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f16z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:
            vconv_f322f16o(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f322f16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s16r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s16f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s16c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s16a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s16z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                         ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
                                                                          ;
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
                                                                         ;
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s162f16r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s162f16f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s162f16c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s162f16a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s162f16z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_s162f16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322f32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322f32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322f32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322f32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322f32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) bfloat16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322bf16r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322bf16f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322bf16c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322bf16a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322bf16z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s64r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s64f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s64c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s64a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s64z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_bf162f32(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) bfloat16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_bf162s32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_bf162s32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_bf162s32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_bf162s32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_bf162s32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:


              ;
            break;
        case RoundMode::CAST_NONE:


              ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) float* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f322s16r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f322s16f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f322s16c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f322s16a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f322s16z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int16_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s162f32(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int64_t* dst, __attribute__((cce_unif_buff)) int32_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s322s64(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_s642f32r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_s642f32f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_s642f32c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_s642f32a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_s642f32z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                         ;
            break;
        case RoundMode::CAST_NONE:

                                                                                                                          ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dst, __attribute__((cce_unif_buff)) int64_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s642s32(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) int4b_t* dst, __attribute__((cce_unif_buff)) half* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_RINT:
            vconv_f162s4r(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_FLOOR:
            vconv_f162s4f(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_CEIL:
            vconv_f162s4c(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ROUND:
            vconv_f162s4a(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_TRUNC:
            vconv_f162s4z(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_NONE:
            vconv_f162s4(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        case RoundMode::CAST_ODD:

                                                                                                                        ;
            break;
        default:

                                                                                                                      ;
            break;
    }
}

[aicore] inline void CastIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dst, __attribute__((cce_unif_buff)) int4b_t* src, const RoundMode& roundMode,
    uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    switch (roundMode) {
        case RoundMode::CAST_NONE:
            vconv_s42f16(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
                repeatParams.dstRepStride, repeatParams.srcRepStride);
            break;
        default:




              ;
            break;
    }
}


template <typename T, typename U>
[aicore] static inline void CheckCastDatatype() {
# 807 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_vconv_impl.h"
                                                       ;
}


template <typename T, typename U>
[aicore] inline void CastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const RoundMode& roundMode,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<T, U>();
        set_mask_count();
        set_vector_mask(0, count);
        if constexpr (sizeof(T) > sizeof(U)) {
            if constexpr (IsSameType<U, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, ONE_FOURTH_DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2});
            }
        } else if constexpr (sizeof(T) < sizeof(U)) {
            if constexpr (IsSameType<T, int4b_t>::value) {
                CastIntrinsicsImpl(
                    dst, src, roundMode, 1, {1, 1, ONE_FOURTH_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            } else {
                CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE});
            }
        } else {
            CastIntrinsicsImpl(dst, src, roundMode, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void CastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const RoundMode& roundMode,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<T, U>();
        if constexpr (isSetMask) {
            if (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<T>(mask[1], mask[0]);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTime, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void CastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const RoundMode& roundMode,
    const uint64_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        CheckCastDatatype<T, U>();
        if constexpr (isSetMask) {
            if (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask);
            } else {
                AscendCUtils::SetMask<T>(mask);
            }
        }
        CastIntrinsicsImpl(dst, src, roundMode, repeatTime, repeatParams);
    }
}

template <typename T, bool halfBlock>
[aicore] inline void CastDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_deqs162b8h(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_deqs162b8l(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}

template <typename T, bool halfBlock>
[aicore] inline void CastVDeqIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) int16_t* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr (halfBlock) {
        vconv_vdeqs162b8h(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    } else {
        vconv_vdeqs162b8l(dst, src, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
            repeatParams.dstRepStride, repeatParams.srcRepStride);
    }
}


template <typename T, typename U, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeqImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src,
    const uint32_t count)
{


                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        struct UnaryRepeatParams repeatParams;
        if constexpr (sizeof(U) == sizeof(int32_t)) {
            CastImpl(dst, src, RoundMode::CAST_RINT, count);
        } else{
            if constexpr (isVecDeq) {
                CastVDeqIntrinsicsImpl<T, halfBlock>(dst, src, 1, repeatParams);
            } else {
                CastDeqIntrinsicsImpl<T, halfBlock>(dst, src, 1, repeatParams);
            }
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, typename U, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeqImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{


                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<U, isSetMask>(mask[1], mask[0]);
        if constexpr (sizeof(U) == sizeof(int32_t)) {
            CastImpl<T, U, isSetMask>(dst, src, RoundMode::CAST_RINT, mask, repeatTime, repeatParams);
        } else {
            if constexpr (isVecDeq) {
                CastVDeqIntrinsicsImpl<T, halfBlock>(dst, src, repeatTime, repeatParams);
            } else {
                CastDeqIntrinsicsImpl<T, halfBlock>(dst, src, repeatTime, repeatParams);
            }
        }
    }
}


template <typename T, typename U, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeqImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src,
    const int32_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{


                                                                 ;
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<U, isSetMask>(mask);
        if constexpr (sizeof(U) == sizeof(int32_t)) {
            CastImpl<T, U, isSetMask>(dst, src, RoundMode::CAST_RINT, mask, repeatTime, repeatParams);
        } else {
            if constexpr (isVecDeq) {
                CastVDeqIntrinsicsImpl<T, halfBlock>(dst, src, repeatTime, repeatParams);
            } else {
                CastDeqIntrinsicsImpl<T, halfBlock>(dst, src, repeatTime, repeatParams);
            }
        }
    }
}

template <typename T, typename U>
[aicore] inline void AddReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<half, int8_t>, Tuple<float, half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in AddReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<half, int8_t>>()) {
        vaddreluconv_f162s8(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, half>>()) {
        vaddreluconv_f322f16(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vaddreluconv_s162s8(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint64_t mask, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask);
            } else {
                AscendCUtils::SetMask<T>(mask);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AddReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint64_t mask[], uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<T>(mask[1], mask[0]);
            }
        }
        AddReluCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void AddReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        if constexpr (sizeof(T) > sizeof(U)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(T) < sizeof(U)) {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            AddReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, typename U>
[aicore] inline void SubReluCastIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    static_assert(SupportType<Tuple<U, T>, Tuple<half, int8_t>, Tuple<float, half>, Tuple<int16_t, int8_t>>(),
        "Failed to check dtype in SubReluCast, current api support dtype combination is src: half, dst: int8_t; src: "
        "float, dst: half; src: int16_t, dst: int8_t.");
    if constexpr (SupportType<Tuple<U, T>, Tuple<half, int8_t>>()) {
        vsubreluconv_f162s8(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else if constexpr (SupportType<Tuple<U, T>, Tuple<float, half>>()) {
        vsubreluconv_f322f16(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    } else {
        vsubreluconv_s162s8(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride, false);
    }
}


template <typename T, typename U>
[aicore] inline void SubReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        if constexpr (sizeof(T) > sizeof(U)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE / HALF_FACTOR, DEFAULT_REPEAT_STRIDE / HALF_FACTOR});
        } else if constexpr (sizeof(T) < sizeof(U)) {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE / HALF_FACTOR,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        } else {
            SubReluCastIntrinsicsImpl(dst, src0, src1, 1, {1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE,
                DEFAULT_REPEAT_STRIDE});
        }
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void SubReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint64_t mask, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask);
            } else {
                AscendCUtils::SetMask<T>(mask);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void SubReluCastImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src0, __attribute__((cce_unif_buff)) U* src1,
    const uint64_t mask[], uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if constexpr (sizeof(T) >= sizeof(U)) {
                AscendCUtils::SetMask<U>(mask[1], mask[0]);
            } else {
                AscendCUtils::SetMask<T>(mask[1], mask[0]);
            }
        }
        SubReluCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

[aicore] inline uint64_t MakeDeqScaleConfig(float scale, int16_t offset, bool signMode)
{
    constexpr uint64_t signModeBit = 46;
    constexpr uint64_t offsetMask = 0x1ff;
    constexpr uint64_t offsetBit = 37;
    uint64_t config = ((static_cast<uint64_t>(signMode) << signModeBit) | ((offset & offsetMask) << offsetBit) |
                       *(reinterpret_cast<uint32_t *>(&scale)));
    return config;
}

[aicore] inline void SetDeqScaleImpl(float scale, int16_t offset, bool signMode)
{
    set_deqscale(MakeDeqScaleConfig(scale, offset, signMode));
}

template <typename T>
[aicore] inline void SetDeqScaleImpl(const LocalTensor<T>& vdeq, const VdeqInfo& vdeqInfo)
{
    for (uint8_t i = 0; i < VDEQ_TENSOR_SIZE; i++) {
        float scale = vdeqInfo.vdeqScale[i];
        int16_t offset = vdeqInfo.vdeqOffset[i];
        bool signMode = vdeqInfo.vdeqSignMode[i];
        vdeq.SetValue(i, static_cast<T>(MakeDeqScaleConfig(scale, offset, signMode)));
    }



    constexpr uint64_t deqAddr = 5;
    set_deqscale(((uint64_t)vdeq.GetPhyAddr()) >> deqAddr);

}

template<typename T>
[aicore] inline void SetDeqScaleImpl(T config)
{
    set_deqscale(config);
    g_deqValue = config;
}
}
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h" 2
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
namespace AscendC {
#pragma begin_pipe(V)
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint64_t mask[], const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
    CastImpl<DstPrimType, SrcPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(), roundMode,
        mask, repeatTime, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
    CastImpl<DstPrimType, SrcPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(), roundMode,
        mask, repeatTime, repeatParams);
}
# 118 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U>
[aicore] inline void Cast(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const RoundMode& roundMode, const uint32_t count)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;
# 137 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
    if constexpr (!(Std::is_same<DstPrimType, int4b_t>::value) && !(Std::is_same<SrcPrimType, int4b_t>::value)) {



                                                                                      ;
    }
    CastImpl((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(), roundMode, count);
}
# 157 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;







    CastDeqImpl<DstPrimType, SrcPrimType, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(), mask, repeatTime, repeatParams);
}
template <typename T, typename U, bool isSetMask, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const int32_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;







    CastDeqImpl<DstPrimType, SrcPrimType, isSetMask, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 201 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U, bool isVecDeq, bool halfBlock>
[aicore] inline void CastDeq(const LocalTensor<T>& dst, const LocalTensor<U>& src,
    const uint32_t count)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;






    CastDeqImpl<DstPrimType, SrcPrimType, isVecDeq, halfBlock>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src.GetPhyAddr(),
        count);
}
# 236 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<PrimT<T>, PrimT<U>, isSetMask>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<U>*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimT<U>*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{

    if (g_coreType == AIC) {
        return;
    }







    AddReluCastImpl<PrimT<T>, PrimT<U>, isSetMask>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<U>*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimT<U>*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 284 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U>
[aicore] inline void AddReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const uint32_t count)
{

    if (g_coreType == AIC) {
        return;
    }






    AddReluCastImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<U>*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimT<U>*)src1.GetPhyAddr(), count);
}
# 321 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<DstPrimType, SrcPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;

    if (g_coreType == AIC) {
        return;
    }







    SubReluCastImpl<DstPrimType, SrcPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 373 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
template <typename T, typename U>
[aicore] inline void SubReluCast(const LocalTensor<T>& dst, const LocalTensor<U>& src0,
    const LocalTensor<U>& src1, const uint32_t count)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;

    if (g_coreType == AIC) {
        return;
    }






    SubReluCastImpl((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), count);
}

#pragma end_pipe
[aicore] inline void SetDeqScale(half scale)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale);
}

[aicore] inline void SetDeqScale(float scale, int16_t offset, bool signMode)
{

    if (g_coreType == AIC) {
        return;
    }




    SetDeqScaleImpl(scale, offset, signMode);
}

template <typename T>
[aicore] inline void SetDeqScale(const LocalTensor<T>& vdeq, const VdeqInfo& vdeqInfo)
{

    if (g_coreType == AIC) {
        return;
    }
# 435 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
    SetDeqScaleImpl<T>(vdeq, vdeqInfo);
}

template <bool castMode>
[aicore] inline void SetCastOverflowMode()
{
    SetCastOverflowModeImpl<castMode>();
}
# 466 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vconv_intf_impl.h"
}
# 185 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vconv_intf.h" 2
# 31 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_scalar_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_scalar_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar.h"
namespace AscendC {
template <int countValue>
[aicore] inline int64_t ScalarGetCountOfValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return bcnt1(valueIn);
    } else if constexpr (countValue == 0) {
        return bcnt0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}

[aicore] inline int64_t ScalarCountLeadingZeroImpl(uint64_t valueIn)
{
    return clz(valueIn);
}
# 95 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_scalar.h"
[aicore] inline int64_t CountBitsCntSameAsSignBitImpl(int64_t valueIn)
{
    return sflbits(valueIn);
}

template <int countValue>
[aicore] inline int64_t ScalarGetSFFValueImpl(uint64_t valueIn)
{
    if constexpr (countValue == 1) {
        return sff1(valueIn);
    } else if constexpr (countValue == 0) {
        return sff0(valueIn);
    } else {
        static_assert(((countValue == 0) || (countValue == 1)) && "countValue must be 1 or 0");
        return 0;
    }
}


template <typename T>
[aicore] inline void WriteGmByPassDCacheImpl(__attribute__((cce_global)) T* addr, T value)
{
    static_assert(SupportType<T, int8_t, uint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t>(),
                  "WriteGmByPassDCache only support int8_t/uint8_t/int16_t/uint16_t/int32_t/uint32_t/int64_t/uint64_t "
                  "data type on current device!");

    if constexpr (SupportBytes<T, 8>()) {
        st_dev(*(reinterpret_cast<uint64_t*>(&value)), reinterpret_cast<__attribute__((cce_global)) uint64_t*>(addr), 0);
    } else if constexpr (SupportBytes<T, 4>()) {
        st_dev(*(reinterpret_cast<uint32_t*>(&value)), reinterpret_cast<__attribute__((cce_global)) uint32_t*>(addr), 0);
    } else if constexpr (SupportBytes<T, 2>()) {
        st_dev(*(reinterpret_cast<uint16_t*>(&value)), reinterpret_cast<__attribute__((cce_global)) uint16_t*>(addr), 0);
    } else {
        st_dev(*(reinterpret_cast<uint8_t*>(&value)), reinterpret_cast<__attribute__((cce_global)) uint8_t*>(addr), 0);
    }
}

template <typename T>
[aicore] inline T ReadGmByPassDCacheImpl(__attribute__((cce_global)) T* addr)
{
    static_assert(SupportType<T, int8_t, uint8_t, int16_t, uint16_t, int32_t, uint32_t, int64_t, uint64_t>(),
                  "ReadGmByPassDCache only support int8_t/uint8_t/int16_t/uint16_t/int32_t/uint32_t/int64_t/uint64_t "
                  "data type on current device!");

    if constexpr (SupportBytes<T, 8>()) {
        return ld_dev(reinterpret_cast<__attribute__((cce_global)) uint64_t*>(addr), 0);
    } else if constexpr (SupportBytes<T, 4>()) {
        return ld_dev(reinterpret_cast<__attribute__((cce_global)) uint32_t*>(addr), 0);
    } else if constexpr (SupportBytes<T, 2>()) {
        return ld_dev(reinterpret_cast<__attribute__((cce_global)) uint16_t*>(addr), 0);
    } else {
        return ld_dev(reinterpret_cast<__attribute__((cce_global)) uint8_t*>(addr), 0);
    }
}


template <RoundMode roundMode>
[aicore] inline half ScalarCastF322F16Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ODD:
            return conv_f322f16o(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <RoundMode roundMode>
[aicore] inline int32_t ScalarCastF322S32Impl(float valueIn)
{
    switch (roundMode) {
        case RoundMode::CAST_ROUND:
            return conv_f322s32a(valueIn);
        case RoundMode::CAST_CEIL:
            return conv_f322s32c(valueIn);
        case RoundMode::CAST_FLOOR:
            return conv_f322s32f(valueIn);
        case RoundMode::CAST_RINT:
            return conv_f322s32r(valueIn);
        default:

                                                                                                                      ;
            return 0;
    }
}

template <typename T, typename U, RoundMode roundMode>
[aicore] inline U ScalarCastImpl(T valueIn)
{



    if constexpr (std::is_same<U, half>::value) {
        return ScalarCastF322F16Impl<roundMode>(valueIn);
    } else if constexpr (std::is_same<U, int32_t>::value) {
        return ScalarCastF322S32Impl<roundMode>(valueIn);
    } else {
        static_assert(((sizeof(U) == sizeof(half)) || (sizeof(U) == sizeof(int32_t))),
            "U only support half or int32_t");
        return 0;
    }




}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_scalar_intf.h" 2

namespace AscendC {
template <int countValue>
[aicore] inline int64_t ScalarGetCountOfValue(uint64_t valueIn);

[aicore] inline int64_t ScalarCountLeadingZero(uint64_t valueIn);

[aicore] inline int64_t CountBitsCntSameAsSignBit(int64_t valueIn);

template <int countValue>
[aicore] inline int64_t ScalarGetSFFValue(uint64_t valueIn);

template <typename T, typename U, RoundMode roundMode>
[aicore] inline U ScalarCast(T valueIn);


template <typename T>
[aicore] inline void WriteGmByPassDCache(__attribute__((cce_global)) T* addr, T value);

template <typename T>
[aicore] inline T ReadGmByPassDCache(__attribute__((cce_global)) T* addr);

}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_scalar_intf_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_scalar_intf_impl.h"
namespace AscendC {
template <int countValue>
[aicore] inline int64_t ScalarGetCountOfValue(uint64_t valueIn)
{
    return ScalarGetCountOfValueImpl<countValue>(valueIn);
}

[aicore] inline int64_t ScalarCountLeadingZero(uint64_t valueIn)
{
    return ScalarCountLeadingZeroImpl(valueIn);
}
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_scalar_intf_impl.h"
[aicore] inline int64_t CountBitsCntSameAsSignBit(int64_t valueIn)
{
    return CountBitsCntSameAsSignBitImpl(valueIn);
}

template <int countValue>
[aicore] inline int64_t ScalarGetSFFValue(uint64_t valueIn)
{
    return ScalarGetSFFValueImpl<countValue>(valueIn);
}

template <typename T, typename U, RoundMode roundMode>
[aicore] inline U ScalarCast(T valueIn)
{
    return ScalarCastImpl<T, U, roundMode>(valueIn);
}


template <typename T>
[aicore] inline void WriteGmByPassDCache(__attribute__((cce_global)) T* addr, T value)
{
    return WriteGmByPassDCacheImpl(addr, value);
}

template <typename T>
[aicore] inline T ReadGmByPassDCache(__attribute__((cce_global)) T* addr)
{
    return ReadGmByPassDCacheImpl(addr);
}

}
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_scalar_intf.h" 2
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
namespace AscendC {
#pragma begin_pipe(V)
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 85 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void PairReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void PairReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void RepeatReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride);
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);
# 143 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 159 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride);

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order = ReduceOrder::ORDER_VALUE_INDEX);
# 190 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex = 0);
# 206 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex = 0);
# 221 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride);

template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex = 0);
template <typename T>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride);
# 246 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex = 0);
# 259 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex = 0);
# 271 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count);
#pragma end_pipe
template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex);

template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue);

template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) T GetAccVal();
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_reduce_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_reduce_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void BlockReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgadd(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] inline void BlockReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmax(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] inline void BlockReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcgmin(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] inline void PairReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    vcpadd(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T>
[aicore] inline void RepeatReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t srcBlkStride, const int32_t dstRepStride, const int32_t srcRepStride)
{
    vcadd(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride, 0);
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMaxIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void BlockReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        BlockReduceMinIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void PairReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        PairReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void RepeatReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t elemsInOneRepeat, const int32_t dstBlkStride, const int32_t srcBlkStride, const int32_t dstRepStride,
    const int32_t srcRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(elemsInOneRepeat);
        RepeatReduceSumIntrinsicsImpl(dstLocal, srcLocal, repeatTime, srcBlkStride, dstRepStride, srcRepStride);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint32_t count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            set_mask_count();
            set_vector_mask(0, count);
        }
        vcadd(dst, src, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, 1);
        auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        int64_t accVal = get_acc_val();
        *(dst) = *(reinterpret_cast<T*>(&accVal));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        if constexpr (isSetMask) {
            set_mask_norm();
            set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
        }
    }
}


template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmax(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMaxImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMaxImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params,
    const ReduceOrder order)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        if (order == ReduceOrder::ORDER_VALUE_INDEX) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::VALUE_INDEX);
        } else if (order == ReduceOrder::ORDER_INDEX_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::INDEX_VALUE);
        } else if (order == ReduceOrder::ORDER_ONLY_VALUE) {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_VALUE);
        } else {
            vcmin(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride,
                Order_t::ONLY_INDEX);
        }
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceMinImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride,
    const ReduceOrder order)
{
    struct ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceMinImpl<T, isSetMask>(dstLocal, srcLocal, params, order);
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, struct ReduceRepeatParams& params)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(params.highMask, params.lowMask);
        vcadd(dstLocal, srcLocal, params.repeatTimes, params.dstRepStride, params.srcBlkStride, params.srcRepStride, 0);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint64_t mask[],
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}

template <typename T, bool isSetMask = true>
[aicore] inline void WholeReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const uint32_t mask,
    const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride, const int32_t srcRepStride)
{
    struct ReduceRepeatParams params(mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
    WholeReduceSumImpl<T, isSetMask>(dstLocal, srcLocal, params);
}


template <typename T>
[aicore] inline void ReduceMaxIntrinsicsImpl(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t srcRepStride)
{
    vcmax(sharedTmpBuffer, srcLocal, repeatTime, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] inline void ReduceMinIntrinsicsImpl(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t srcRepStride)
{
    vcmin(sharedTmpBuffer, srcLocal, repeatTime, 1, 1, srcRepStride, Order_t::VALUE_INDEX);
}

template <typename T>
[aicore] inline void ReduceSumIntrinsicsImpl(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t srcRepStride)
{
    vcadd(sharedTmpBuffer, srcLocal, repeatTime, 1, 1, srcRepStride, 0);
}

template <typename T>
[aicore] inline void ReduceSumSecondStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = params.repeatTimes / elementNumPerRep;
    int32_t leftData = params.repeatTimes % elementNumPerRep;

    uint64_t highMask = 0;
    uint64_t lowMask = 0;


    lowMask = params.repeatTimes;



    SetMaskCount();

    AscendCUtils::SetMask<T>(highMask, lowMask);
    ReduceSumIntrinsicsImpl<T>(sharedTmpBuffer, sharedTmpBuffer, 1, DEFAULT_REPEAT_STRIDE);

    SetMaskNorm();
# 354 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_reduce_impl.h"
}


template <typename T>
[aicore] inline void CreateSpecialFormatMask(const int32_t& maskLen, uint64_t& highMask, uint64_t& lowMask)
{

    int32_t halfLen = HLAF_MASK_LEN / 2;
    for (int32_t i = 0; i < maskLen - halfLen; i++) {
        highMask = highMask << 2;
        highMask = highMask | 1;
    }
    int32_t lowMaskRange = maskLen >= halfLen ? halfLen : maskLen;
    for (int32_t i = 0; i < lowMaskRange; i++) {
        lowMask = lowMask << 2;
        lowMask = lowMask | 1;
    }
}

template <typename T>
[aicore] inline void ReduceOperation(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal, const int32_t repeatTime,
    const int32_t srcRepStride, const uint64_t& highMask, const uint64_t& lowMask, const ReduceMode& mode)
{
    AscendCUtils::SetMask<T>(highMask, lowMask);
    switch (mode) {
        case ReduceMode::REDUCE_MAX:
            ReduceMaxIntrinsicsImpl(sharedTmpBuffer, srcLocal, repeatTime, srcRepStride);
            break;
        case ReduceMode::REDUCE_MIN:
            ReduceMinIntrinsicsImpl(sharedTmpBuffer, srcLocal, repeatTime, srcRepStride);
            break;
        case ReduceMode::REDUCE_SUM:
            ReduceSumIntrinsicsImpl(sharedTmpBuffer, srcLocal, repeatTime, srcRepStride);
            break;
        default:
            break;
    }
}

template <typename T>
[aicore] inline void ReduceImplFirstStep(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params, const ReduceMode& mode, int32_t& curData)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = index * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES * VREDUCE_PER_REP_OUTPUT;
        srcOffset = range * MAX_REPEAT_TIMES * params.srcRepStride * ONE_BLK_SIZE / sizeof(T);
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, mode);
    }
    curData = VREDUCE_PER_REP_OUTPUT * params.repeatTimes;
}

template <typename T>
[aicore] inline void ReduceImplSecondStep(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, const ReduceMode& mode, int32_t& curData,
    int32_t preStartPos, int32_t secondStartPos)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t newMaskLen = 0;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    int32_t bodyOutputCount = 0;
    int32_t tailOutputCount = 0;

    if (newRepeatTimes >= 1) {
        highMask = (sizeof(T) == sizeof(half)) ? 0x5555555555555555 : 0;
        lowMask = 0x5555555555555555;

        ReduceOperation<T>(sharedTmpBuffer + secondStartPos, sharedTmpBuffer + preStartPos, newRepeatTimes, DEFAULT_REPEAT_STRIDE,
            highMask, lowMask, mode);
        bodyOutputCount = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
    }
    highMask = 0;
    lowMask = 0;

    if (leftData > 0) {
        newMaskLen = leftData / VREDUCE_PER_REP_OUTPUT;

        CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);

        dstOffset = secondStartPos + bodyOutputCount;
        srcOffset = preStartPos + newRepeatTimes * elementNumPerRep;
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        tailOutputCount = VREDUCE_PER_REP_OUTPUT;
    }

    curData = bodyOutputCount + tailOutputCount;
}

template <typename T>
[aicore] inline void GetIndex(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, int32_t secondStartPos, int32_t& secondIndex,
    int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer + thirdIndex + 1);
                                                                                                  ;
    } else {
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer + secondStartPos + 1);
                                                                                                ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer + thirdIndex + 1);
                                                                                                  ;
    }
}

template <typename T>
[aicore] inline void GetIndex(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    using U = typename Conditional<sizeof(T) == B16_BYTE_SIZE, uint16_t, uint32_t>::type;
    thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(sharedTmpBuffer + thirdStartPos + 1);
                                                                                                            ;
    secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(sharedTmpBuffer + secondStartPos + thirdIndex + 1);
                                                                                                              ;
    firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) U*>(sharedTmpBuffer +
        elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex + 1);
                                                                                                            ;
}

template <typename T>
[aicore] inline void GetIndex(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, int32_t secondStartPos, int32_t thirdStartPos,
    int32_t fourthStartPos, int32_t& firstIndex, int32_t& secondIndex, int32_t& thirdIndex, int32_t& fourthIndex)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (sizeof(T) == sizeof(half)) {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint16_t*>(sharedTmpBuffer +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    } else {
        fourthIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer + fourthStartPos + 1);



          ;
        thirdIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer + thirdStartPos + fourthIndex + 1);



          ;
        secondIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer + secondStartPos +
            elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex + 1);



          ;
        firstIndex = *reinterpret_cast<__attribute__((cce_unif_buff)) uint32_t*>(sharedTmpBuffer +
            elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT +
            secondIndex + 1);



          ;
    }
}

template <typename T>
[aicore] inline void ReduceImplThirdStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer, const int32_t srcRepStride,
    const ReduceMode& mode, int32_t& curData, int32_t& secondStartPos, int32_t& thirdStartPos)
{
    int32_t preNum = 0;
    int32_t firstIndex = 0;
    int32_t secondIndex = 0;
    int32_t thirdIndex = 0;
    int32_t fourthIndex = 0;
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    int32_t offsetNumPerRep = ONE_BLK_SIZE / sizeof(T) * srcRepStride;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (curData == VREDUCE_PER_REP_OUTPUT) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        GetIndex<T>(sharedTmpBuffer, secondStartPos, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT);
        int32_t redultIndex = secondIndex + preNum;
        *dstLocal = *(sharedTmpBuffer + secondStartPos);
        *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        return;
    }

    int32_t newMaskLen = curData / VREDUCE_PER_REP_OUTPUT;
    CreateSpecialFormatMask<T>(newMaskLen, highMask, lowMask);
    if (curData > elementNumPerRep) {
        ReduceImplSecondStep<T>(sharedTmpBuffer, mode, curData, secondStartPos, thirdStartPos);

        int32_t fourthStartPos =
            (((thirdStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
        dstOffset = fourthStartPos;
        srcOffset = thirdStartPos;
        PipeBarrier<PIPE_V>();
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(sharedTmpBuffer + dstOffset);

        GetIndex<T>(sharedTmpBuffer, secondStartPos, thirdStartPos, fourthStartPos, firstIndex, secondIndex, thirdIndex,
            fourthIndex);
        preNum = offsetNumPerRep *
            (elementNumPerRep * (elementNumPerRep * (fourthIndex / VREDUCE_PER_REP_OUTPUT) + thirdIndex) /
            VREDUCE_PER_REP_OUTPUT + secondIndex) / VREDUCE_PER_REP_OUTPUT;
    } else {
        dstOffset = thirdStartPos;
        srcOffset = secondStartPos;

        ReduceOperation<T>(sharedTmpBuffer + dstOffset, sharedTmpBuffer + srcOffset, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask,
            mode);
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);

        *dstLocal = *(sharedTmpBuffer + thirdStartPos);

        GetIndex<T>(sharedTmpBuffer, secondStartPos, thirdStartPos, firstIndex, secondIndex, thirdIndex);
        preNum = offsetNumPerRep * (elementNumPerRep * (thirdIndex / VREDUCE_PER_REP_OUTPUT) + secondIndex) /
            VREDUCE_PER_REP_OUTPUT;
    }

    int32_t redultIndex = firstIndex + preNum;
    *(dstLocal + 1) = *reinterpret_cast<T*>(&redultIndex);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] inline void ReduceSumFirstStep(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, __attribute__((cce_unif_buff)) T* srcLocal,
    struct ReduceRepeatParams& params)
{
    int32_t dstOffset = 0;
    int32_t srcOffset = 0;
    int32_t range = params.repeatTimes / MAX_REPEAT_TIMES;

    for (int32_t index = 0; index < range; index++) {
        dstOffset = index * MAX_REPEAT_TIMES;
        srcOffset = index * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, MAX_REPEAT_TIMES, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }

    int32_t leftRepeatTimes = params.repeatTimes % MAX_REPEAT_TIMES;
    if (leftRepeatTimes > 0) {
        dstOffset = range * MAX_REPEAT_TIMES;
        srcOffset = range * MAX_REPEAT_TIMES * (params.srcRepStride * ONE_BLK_SIZE / sizeof(T));
        ReduceOperation<T>(sharedTmpBuffer + dstOffset, srcLocal + srcOffset, leftRepeatTimes, params.srcRepStride,
            params.highMask, params.lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] inline void ReduceSumFinalStep(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer, int32_t& secondResultNum)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;
    if (secondResultNum == 1) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        *(dstLocal) = *(sharedTmpBuffer);
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    } else {
        highMask = (secondResultNum > HLAF_MASK_LEN) ? (((static_cast<uint64_t>(1)) << (secondResultNum - HLAF_MASK_LEN)) - 1) : 0;
        lowMask = (secondResultNum > HLAF_MASK_LEN) ? FULL_MASK : (((static_cast<uint64_t>(1)) << secondResultNum) - 1);
        ReduceOperation<T>(dstLocal, sharedTmpBuffer, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, ReduceMode::REDUCE_SUM);
    }
}

template <typename T>
[aicore] inline void ReduceSumImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer,
    struct ReduceRepeatParams& params)
{
    ReduceSumFirstStep<T>(sharedTmpBuffer, srcLocal, params);
    PipeBarrier<PIPE_V>();
    ReduceSumSecondStep<T>(dstLocal, sharedTmpBuffer, params);
    PipeBarrier<PIPE_V>();
    int32_t secondResultNum = DivCeil(params.repeatTimes, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    ReduceSumFinalStep<T>(dstLocal, sharedTmpBuffer, secondResultNum);
}

template <typename T>
[aicore] inline void ReduceImplSecondStepNoIndex(__attribute__((cce_unif_buff)) T* sharedTmpBuffer, const ReduceMode& mode, int32_t& curData)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    int32_t newRepeatTimes = curData / elementNumPerRep;
    int32_t leftData = curData % elementNumPerRep;
    uint64_t highMask = 0, lowMask = 0;
    if (newRepeatTimes != 0) {
        CreateSpecialFormatMask<T>(elementNumPerRep / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(sharedTmpBuffer, sharedTmpBuffer, newRepeatTimes, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    }
    highMask = 0;
    lowMask = 0;
    if (leftData > 0) {
        CreateSpecialFormatMask<T>(leftData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
        ReduceOperation<T>(sharedTmpBuffer + newRepeatTimes * VREDUCE_PER_REP_OUTPUT,
            sharedTmpBuffer + newRepeatTimes * elementNumPerRep, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
        newRepeatTimes += 1;
    }
    curData = newRepeatTimes * VREDUCE_PER_REP_OUTPUT;
}

template <typename T>
[aicore] inline void ReduceImplThirdStepNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer, const ReduceMode& mode,
    int32_t& curData)
{
    uint64_t highMask = 0;
    uint64_t lowMask = 0;

    CreateSpecialFormatMask<T>(curData / VREDUCE_PER_REP_OUTPUT, highMask, lowMask);
    ReduceOperation<T>(sharedTmpBuffer, sharedTmpBuffer, 1, DEFAULT_REPEAT_STRIDE, highMask, lowMask, mode);
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    *dstLocal = *sharedTmpBuffer;
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
}

template <typename T>
[aicore] inline void ReduceImplWithIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(dstLocal, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
        } else {
            int32_t curData = 0;
            ReduceImplFirstStep<T>(sharedTmpBuffer, srcLocal, params, mode, curData);
            PipeBarrier<PIPE_V>();
            int32_t secondStartPos =
                ((curData * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplSecondStep<T>(sharedTmpBuffer, mode, curData, 0, secondStartPos);
            PipeBarrier<PIPE_V>();
            int32_t thirdStartPos =
                (((secondStartPos + curData) * sizeof(T) + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE) * ONE_BLK_SIZE / sizeof(T);
            ReduceImplThirdStep<T>(dstLocal, sharedTmpBuffer, params.srcRepStride, mode, curData, secondStartPos,
                thirdStartPos);
        }
    }
}

template <typename T>
[aicore] inline void ReduceImplNoIndex(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, const ReduceMode& mode)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if (params.repeatTimes == 1) {
            ReduceOperation<T>(sharedTmpBuffer, srcLocal, 1, params.srcRepStride, params.highMask, params.lowMask, mode);
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);
            *dstLocal = *sharedTmpBuffer;
            event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
            SetFlag<HardEvent::S_V>(eventIdSToV);
            WaitFlag<HardEvent::S_V>(eventIdSToV);
            SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
            WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        } else {
            if (mode == ReduceMode::REDUCE_SUM) {
                ReduceSumImpl<T>(dstLocal, srcLocal, sharedTmpBuffer, params);
            } else {
                int32_t curData = 0;
                ReduceImplFirstStep<T>(sharedTmpBuffer, srcLocal, params, mode, curData);
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(sharedTmpBuffer, mode, curData);

                int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, sharedTmpBuffer, mode, curData);
                    return;
                }
                PipeBarrier<PIPE_V>();
                ReduceImplSecondStepNoIndex<T>(sharedTmpBuffer, mode, curData);
                if (curData <= elementNumPerRep) {
                    PipeBarrier<PIPE_V>();
                    ReduceImplThirdStepNoIndex<T>(dstLocal, sharedTmpBuffer, mode, curData);
                }
            }
        }
    }
}
template <typename T>
[aicore] inline void ReduceImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) T* sharedTmpBuffer,
    struct ReduceRepeatParams& params, bool calIndex, const ReduceMode& mode)
{
    if (calIndex) {
        ReduceImplWithIndex<T>(dstLocal, srcLocal, sharedTmpBuffer, params, mode);
    } else {
        ReduceImplNoIndex<T>(dstLocal, srcLocal, sharedTmpBuffer, params, mode);
    }
}


template <typename T>
[aicore] inline void ReduceTailCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex, const ReduceMode& mode)
{
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(PrimT<T>);
    int32_t repeatTime = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    PrimT<T> bodyValue = dst.GetValue(0);
    PrimT<T> bodyIndex = dst.GetValue(1);

    struct ReduceRepeatParams tailParams(tailCount, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);

    ReduceImpl<PrimT<T>>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimT<T>*)src.GetPhyAddr(elementNumPerRep * repeatTime), (__attribute__((cce_unif_buff)) PrimT<T>*)sharedTmpBuffer.GetPhyAddr(),
        tailParams, calIndex, mode);
    eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    PrimT<T> tailValue = dst.GetValue(0);
    PrimT<T> tailIndex = dst.GetValue(1);


    struct ReduceRepeatParams lastParams(2, 1, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    sharedTmpBuffer.SetValue(0, bodyValue);
    sharedTmpBuffer.SetValue(1, tailValue);
    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);

    ReduceImpl<PrimT<T>>((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)sharedTmpBuffer.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimT<T>*)sharedTmpBuffer.GetPhyAddr(), lastParams, calIndex, mode);
    if (calIndex) {
        eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        PrimT<T> lastIndexVal = dst.GetValue(1);
        uint32_t newIndex = 0;
        uint32_t lastIndex = 0;
        if (sizeof(PrimT<T>) == sizeof(half)) {
            lastIndex = *reinterpret_cast<uint16_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTime + *reinterpret_cast<uint16_t*>(&tailIndex);
        } else {
            lastIndex = *reinterpret_cast<uint32_t*>(&lastIndexVal);
            newIndex = elementNumPerRep * repeatTime + *reinterpret_cast<uint32_t*>(&tailIndex);
        }
        if (lastIndex == 1) {
            dst.SetValue(1, *reinterpret_cast<PrimT<T>*>(&newIndex));
        } else {
            dst.SetValue(1, bodyIndex);
        }
        eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        event_t eventIdSToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE3));
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        SetFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
        WaitFlag<HardEvent::S_MTE3>(eventIdSToMTE3);
    }
}

template <typename T>
[aicore] inline void GetReduceMaxMinCountImpl(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    if constexpr (IsSameType<T, half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxMinValue = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    constexpr uint64_t indexBit = 32;
    maxMinIndex = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
}

template <typename T>
[aicore] inline void GetReduceMaxMinCountImpl(uint32_t &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] inline void GetReduceMaxMinCountImpl(T &maxMinValue, T &maxMinIndex)
{
    int64_t maxMinCnt = get_max_min_cnt();
    uint32_t maxVal = 0;
    uint32_t maxIdx = 0;
    if constexpr (IsSameType<T, half>::value) {
        constexpr uint64_t valueMask = 0xffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    } else {
        constexpr uint64_t valueMask = 0xffffffff;
        maxVal = (static_cast<uint64_t>(maxMinCnt) & valueMask);
    }
    maxMinValue = *(reinterpret_cast<T*>(&maxVal));

    constexpr uint64_t indexBit = 32;
    maxIdx = (static_cast<uint64_t>(maxMinCnt) >> indexBit);
    maxMinIndex = *(reinterpret_cast<T*>(&maxIdx));
}

template <typename T>
[aicore] inline void GetReduceMaxMinCountImpl(T &maxMinValue)
{
                                                                                   ;
}

template <typename T>
[aicore] inline T GetAccValImpl()
{
    int64_t accVal = get_acc_val();
    return *(reinterpret_cast<T*>(&accVal));
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
namespace AscendC {
#pragma begin_pipe(V)
# 51 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void BlockReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 82 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void BlockReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceMaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 113 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void BlockReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceMinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 144 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void PairReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                    ;
                                                                                ;







    PairReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] inline void BlockReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] inline void BlockReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceMaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] inline void BlockReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    BlockReduceMinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}

template <typename T, bool isSetMask>
[aicore] inline void PairReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const uint64_t mask[], const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                    ;
                                                                                ;







    PairReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstRepStride, srcBlkStride, srcRepStride);
}
# 264 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void RepeatReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t mask, const int32_t dstBlkStride, const int32_t srcBlkStride,
    const int32_t dstRepStride, const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                      ;
                                                                                  ;







    RepeatReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatTime,
        mask, dstBlkStride, srcBlkStride, dstRepStride, srcRepStride);
}
# 317 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void WholeReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    WholeReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), mask,
        repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}
# 349 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void WholeReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{
    using PrimType = PrimT<T>;


                                                                                                     ;

                                                                                 ;




                                                                                       ;
# 382 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
    WholeReduceMaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), mask,
        repeatTime, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 397 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void WholeReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint64_t mask[], const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{
    using PrimType = PrimT<T>;


                                                                                                     ;

                                                                                 ;




                                                                                       ;
# 430 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
    WholeReduceMinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 454 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void WholeReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;

                                                                                                     ;
                                                                                 ;







    WholeReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride);
}


template <typename T, bool isSetMask>
[aicore] inline void WholeReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{
    using PrimType = PrimT<T>;


                                                                                                     ;

                                                                                 ;

                                                                                       ;
# 505 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
    WholeReduceMaxImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride, order);
}
template <typename T, bool isSetMask>
[aicore] inline void WholeReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t mask, const int32_t repeatTime, const int32_t dstRepStride, const int32_t srcBlkStride,
    const int32_t srcRepStride, ReduceOrder order)
{
    using PrimType = PrimT<T>;


                                                                                                     ;

                                                                                 ;




                                                                                       ;
# 541 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
    WholeReduceMinImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, dstRepStride, srcBlkStride, srcRepStride, order);
}
# 557 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex)
{
    using PrimType = PrimT<T>;
# 573 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask == 0) {
        return;
    }
    ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);

}
# 595 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex)
{
    using PrimType = PrimT<T>;
# 611 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask == 0) {
        return;
    }
    struct ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);

}
# 632 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t mask, const int32_t repeatTime, const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;
# 647 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask == 0) {
        return;
    }
    ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);

}

template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex)
{
    using PrimType = PrimT<T>;
# 674 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask[0] == 0 && mask[1] == 0) {
        return;
    }
    struct ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);

}

template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride,
    bool calIndex)
{
    using PrimType = PrimT<T>;
# 701 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask[0] == 0 && mask[1] == 0) {
        return;
    }
    struct ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);

}

template <typename T>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const uint64_t mask[], const int32_t repeatTime, const int32_t srcRepStride)
{
    using PrimType = PrimT<T>;
# 727 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
    if (mask[0] == 0 && mask[1] == 0) {
        return;
    }
    struct ReduceRepeatParams params(mask, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE, srcRepStride);

    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, 0, ReduceMode::REDUCE_SUM);

}
# 747 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T>
[aicore] inline void ReduceMin(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex)
{
    using PrimType = PrimT<T>;
# 764 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
                                                                                               ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(PrimType);
    int32_t repeatTime = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTime == 0) {
        repeatTime = 1;
        bodyCount = count;
        tailCount = 0;
    }





    if (count == 0) {
        return;
    }
    struct ReduceRepeatParams params(bodyCount, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MIN);

    if (tailCount != 0) {
        ReduceTailCompute(dst, src, sharedTmpBuffer, count, calIndex, ReduceMode::REDUCE_MIN);
    }

}
# 804 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T>
[aicore] inline void ReduceMax(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count, bool calIndex)
{
    using PrimType = PrimT<T>;
# 821 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
                                                                                     ;
                                                                                               ;
    int32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(PrimType);
    int32_t repeatTime = count / elementNumPerRep;
    int32_t tailCount = count % elementNumPerRep;
    int32_t bodyCount = elementNumPerRep;

    if (repeatTime == 0) {
        repeatTime = 1;
        bodyCount = count;
        tailCount = 0;
    }





    if (count == 0) {
        return;
    }

    struct ReduceRepeatParams params(bodyCount, repeatTime, DEFAULT_REDUCE_DST_REP_SRIDE, DEFAULT_BLK_STRIDE,
        DEFAULT_REPEAT_STRIDE);
    ReduceImpl<PrimType>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)sharedTmpBuffer.GetPhyAddr(), params, calIndex, ReduceMode::REDUCE_MAX);

    if (tailCount != 0) {
        ReduceTailCompute(dst, src, sharedTmpBuffer, count, calIndex, ReduceMode::REDUCE_MAX);
    }

}
# 861 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void ReduceSum(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& sharedTmpBuffer, const int32_t count)
{
    using PrimType = PrimT<T>;
                                                                                               ;


                                                                                     ;





    if (count == 0) {
        return;
    }
    ReduceSumImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
# 942 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_reduce_intf_impl.h"
}
#pragma end_pipe
template <typename T>
[aicore] inline void GetReduceMaxMinCount(uint32_t &maxMinValue, uint32_t &maxMinIndex)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<PrimType>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] inline void GetReduceMaxMinCount(uint32_t &maxMinValue)
{
    using PrimType = PrimT<T>;
    GetReduceMaxMinCountImpl<PrimType>(maxMinValue);
}

[aicore] inline int64_t GetAccVal()
{

    if (g_coreType == AIC) {
        return 0;
    }

    return get_acc_val();
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue, T &maxMinIndex)
{


                   ;

    if (g_coreType == AIC) {
        return;
    }

    GetReduceMaxMinCountImpl<T>(maxMinValue, maxMinIndex);
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) void GetReduceMaxMinCount(T &maxMinValue)
{

                                                                                                      ;
    GetReduceMaxMinCountImpl<T>(maxMinValue);
}

template <typename T>
[aicore] inline __attribute__((inout_pipe("S"))) T GetAccVal()
{

                                                                   ;

    if (g_coreType == AIC) {
        return 0;
    }

    return GetAccValImpl<T>();
}
}
# 286 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_reduce_intf.h" 2
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_proposal.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_proposal.h"
namespace AscendC {
struct MrgSort4Info {
    [aicore] MrgSort4Info() {}

    [aicore] MrgSort4Info(const uint16_t elementLengthsIn[MRG_SORT_ELEMENT_LEN], const bool ifExhaustedSuspensionIn,
        const uint16_t validBitIn, const uint16_t repeatTimesIn)
        : ifExhaustedSuspension(ifExhaustedSuspensionIn),
          validBit(validBitIn),
          repeatTimes(repeatTimesIn)
    {
        for (int32_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
            elementLengths[i] = elementLengthsIn[i];
        }
    }

    uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = { 0 };
    bool ifExhaustedSuspension = false;
    uint16_t validBit = 0;
    uint8_t repeatTimes = 1;
};

template <typename T> struct MrgSortSrcList {
    [aicore] MrgSortSrcList() {}

    [aicore] MrgSortSrcList(const LocalTensor<T>& src1In, const LocalTensor<T>& src2In, const LocalTensor<T>& src3In,
        const LocalTensor<T>& src4In)
    {
        src1 = src1In[0];
        src2 = src2In[0];
        src3 = src3In[0];
        src4 = src4In[0];
    }

    LocalTensor<T> src1;
    LocalTensor<T> src2;
    LocalTensor<T> src3;
    LocalTensor<T> src4;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h" 2




namespace AscendC {
#pragma begin_pipe(V)
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void MrgSort4(const LocalTensor<T>& dst, const MrgSortSrcList<T>& src,
    const MrgSort4Info& params);
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void RpSort16(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime);
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void MrgSort(const LocalTensor<T>& dst, const MrgSortSrcList<T>& src,
    const MrgSort4Info& params);
# 78 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void Sort32(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<uint32_t>& src1, const int32_t repeatTime);
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void ProposalConcat(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t modeNumber);
# 105 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void ProposalExtract(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t modeNumber);
# 118 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void Concat(LocalTensor<T> &concat, const LocalTensor<T> &src,
    const LocalTensor<T> &tmp, const int32_t repeatTime);
# 131 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T>
[aicore] inline void Extract(const LocalTensor<T> &dstValue, const LocalTensor<uint32_t> &dstIndex,
    const LocalTensor<T> &sorted, const int32_t repeatTime);
# 146 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T, bool isExhaustedSuspension = false>
[aicore] inline void MrgSort(const LocalTensor<T> &dst, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTime);
# 160 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h"
template <typename T, bool isFullSort>
[aicore] inline void Sort(const LocalTensor<T> &dst, const LocalTensor<T> &concat,
    const LocalTensor<uint32_t> &index, LocalTensor<T> &tmp, const int32_t repeatTime);







template <typename T>
[aicore] inline uint32_t GetSortOffset(const uint32_t elemOffset);







template <typename T>
[aicore] inline uint32_t GetSortLen(const uint32_t elemCount);
#pragma end_pipe
[aicore] inline __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h" 1
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_proposal_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_proposal_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_proposal_base_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/kernel_operator_proposal_base_impl.h"
namespace AscendC {
constexpr uint32_t SORT_LEN = 4;
constexpr uint32_t SORT_NUM_TWO = 2;
constexpr uint32_t SORT_NUM_THREE = 3;

[aicore] inline void ComSortInnerLoopTail(uint32_t& offset0Tail, uint32_t& offset1Tail, uint32_t& offset2Tail,
    uint32_t& offset3Tail, uint16_t& validBitTail, uint16_t (&elementCountListTail)[SORT_LEN],
    const uint32_t baseOffset, const uint32_t elementCountTail, int32_t mergeTmpTailQueNum)
{
    if (mergeTmpTailQueNum == SORT_NUM_TWO) {
        offset1Tail = offset0Tail + baseOffset;
        elementCountListTail[1] = elementCountTail;
        offset2Tail = 0;
        elementCountListTail[2] = 0;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0011;
    } else if (mergeTmpTailQueNum == SORT_NUM_THREE) {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + SORT_NUM_TWO * baseOffset;
        elementCountListTail[2] = elementCountTail;
        offset3Tail = 0;
        elementCountListTail[3] = 0;
        validBitTail = 0b0111;
    } else {
        offset1Tail = offset0Tail + baseOffset;
        offset2Tail = offset0Tail + SORT_NUM_TWO * baseOffset;
        offset3Tail = offset0Tail + SORT_NUM_THREE * baseOffset;
        elementCountListTail[3] = elementCountTail;
        validBitTail = 0b1111;
    }
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_proposal_impl.h" 2


namespace AscendC {
constexpr uint32_t singleSortElementCountV220 = 32;
constexpr uint32_t singleSortElementCountV200 = 16;
constexpr uint32_t regionProposalDataSize = 8;

template <typename T>
[[deprecated("NOTICE: MrgSort4 is not deprecated. Currently, MrgSort4 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] inline void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t config)
{
                                                 ;
}

template <typename T>
[[deprecated("NOTICE: RpSort16 is not deprecated. Currently, RpSort16 is an unsupported API on current device."
             "Please check your code!")]]
[aicore] inline void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                 ;
}

template <typename T>
[aicore] inline void VbitsortCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) uint32_t* src1Local,
    const ProposalIntriParams& intriParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vbitsort(dstLocal, src0Local, src1Local, intriParams.repeat);
    }
}

template <typename T>
[aicore] inline void Vmrgsort4Cal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* addrArray[MRG_SORT_ELEMENT_LEN], uint64_t src1,
    uint64_t config)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vmrgsort4(dstLocal, addrArray, src1, config);
    }
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalConcat is not deprecated. Currently, ProposalConcat is an unsupported API on current device."
    "Please check your code!")]]
[aicore] inline void VconcatCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                       ;
}

template <typename T>
[[deprecated(
    "NOTICE: ProposalExtract is not deprecated. Currently, ProposalExtract is an unsupported API on current device."
    "Please check your code!")]]
[aicore] inline void VextractCal(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, const ProposalIntriParams& intriParams)
{
                                                        ;
}

template <typename T>
[aicore] inline void MrgSortCal(const LocalTensor<T> &dst, const MrgSortSrcList<T> &sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTime)
{
    MrgSort4Info mrgSortInfo(elementCountList, false, validBit, static_cast<uint16_t>(repeatTime));





    uint64_t config = 0;
    config |= (mrgSortInfo.repeatTimes & 0xFF);
    config |= (uint64_t(mrgSortInfo.validBit & 0xF) << 8);
    config |= (uint64_t(mrgSortInfo.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(mrgSortInfo.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(mrgSortInfo.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(mrgSortInfo.elementLengths[3] & 0xFFFF) << 48);

    __attribute__((cce_unif_buff)) T *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) T *)sortList.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) T *)sortList.src4.GetPhyAddr()};

    Vmrgsort4Cal((__attribute__((cce_unif_buff)) T *)dst.GetPhyAddr(), addrArray, src1, config);
}

[aicore] inline void GetMrgSortResultImpl(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{
    int64_t mrgSortResult = get_vms4_sr();
    constexpr uint64_t resMask = 0xFFFF;

    mrgSortList1 = static_cast<uint64_t>(mrgSortResult) & resMask;
    constexpr uint64_t sortList2Bit = 16;

    mrgSortList2 = (static_cast<uint64_t>(mrgSortResult) >> sortList2Bit) & resMask;
    constexpr uint64_t sortList3Bit = 32;

    mrgSortList3 = (static_cast<uint64_t>(mrgSortResult) >> sortList3Bit) & resMask;
    constexpr uint64_t sortList4Bit = 48;

    mrgSortList4 = (static_cast<uint64_t>(mrgSortResult) >> sortList4Bit) & resMask;
}

template <typename T>
[aicore] inline void FullSortInnerLoop(const LocalTensor<T> &dst, const LocalTensor<T> &tmp,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const int32_t mergeTmpRepeatTimes)
{
    if (mergeTmpRepeatTimes <= 0) {
        return;
    }
    MrgSortSrcList sortList =
        MrgSortSrcList(tmp[0], tmp[baseOffset], tmp[2 * baseOffset], tmp[3 * baseOffset]);
    const uint16_t elementCountList[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
        singleMergeTmpElementCount, singleMergeTmpElementCount};
    uint32_t sortedNum[MRG_SORT_ELEMENT_LEN];
    MrgSortCal<T>(dst, sortList, elementCountList, sortedNum, 0b1111, mergeTmpRepeatTimes);
}

template <typename T>
[aicore] inline void FullSortInnerLoopTail(const LocalTensor<T> &dst, const LocalTensor<T> &tmp,
    const uint32_t baseOffset, const uint16_t singleMergeTmpElementCount, const uint32_t elementCountTail,
    const int32_t mergeTmpRepeatTimes, int32_t mergeTmpTailQueNum)
{
    if (mergeTmpTailQueNum <= 0) {
        return;
    }
    uint16_t validBitTail;
    uint16_t elementCountListTail[MRG_SORT_ELEMENT_LEN] = {singleMergeTmpElementCount, singleMergeTmpElementCount,
        singleMergeTmpElementCount, singleMergeTmpElementCount};
    uint32_t offset1Tail, offset2Tail, offset3Tail;
    uint32_t offset0Tail = MRG_SORT_ELEMENT_LEN * baseOffset * mergeTmpRepeatTimes;

    ComSortInnerLoopTail(offset0Tail, offset1Tail, offset2Tail, offset3Tail, validBitTail, elementCountListTail,
        baseOffset, elementCountTail, mergeTmpTailQueNum);
    if (mergeTmpTailQueNum > 1) {
        MrgSortSrcList sortListTail = MrgSortSrcList(tmp[offset0Tail], tmp[offset1Tail],
            tmp[offset2Tail], tmp[offset3Tail]);
        uint32_t sortedNumTail[MRG_SORT_ELEMENT_LEN];
        MrgSortCal<T>(dst[offset0Tail], sortListTail, elementCountListTail, sortedNumTail,
            validBitTail, 1);
    } else {
        if constexpr (IsSameType<T, half>::value) {
            DataCopy(dst[offset0Tail], tmp[offset0Tail], elementCountTail * 4);
        } else {
            DataCopy(dst[offset0Tail], tmp[offset0Tail], elementCountTail * 2);
        }
    }
}

[aicore] inline uint32_t GetFullSortInnerLoopTimes(const int32_t repeatTime)
{
    uint32_t loop = 0;
    int32_t queNum = repeatTime;
    while (queNum > 1) {
        queNum = Ceil(queNum, MRG_SORT_ELEMENT_LEN);
        loop++;
    }
    return loop;
}

template <typename T>
[aicore] inline void DoFullSort(const LocalTensor<T> &dst, const LocalTensor<T> &concat,
    const LocalTensor<uint32_t> &index, LocalTensor<T> &tmp, const int32_t repeatTime)
{
    uint32_t elementCount = concat.GetSize();
    uint32_t singleMergeElementCount = singleSortElementCountV220;
    uint32_t loop = GetFullSortInnerLoopTimes(repeatTime);
    uint16_t singleMergeTmpElementCount = singleMergeElementCount;
    uint32_t srcElementCount = repeatTime * singleMergeElementCount;
    uint32_t dstElementCount = srcElementCount * regionProposalDataSize / sizeof(T);
    int32_t mergeTmpTotalQueNum = repeatTime;
    int32_t mergeTmpTailQueNum = repeatTime % MRG_SORT_ELEMENT_LEN;
    int32_t mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
    int32_t mergeTmpRepeatTimes = repeatTime / MRG_SORT_ELEMENT_LEN;
    DataCopy(tmp, dst, dstElementCount);
    PipeBarrier<PIPE_V>();
    for (int i = 0; i < loop; i++) {
        uint32_t baseOffset;
        baseOffset = singleMergeTmpElementCount * regionProposalDataSize / sizeof(T);
        FullSortInnerLoop(dst, tmp, baseOffset, singleMergeTmpElementCount, mergeTmpRepeatTimes);
        PipeBarrier<PIPE_V>();
        uint16_t elementCountTail = srcElementCount % singleMergeTmpElementCount ?
            srcElementCount % singleMergeTmpElementCount :
            singleMergeTmpElementCount;
        FullSortInnerLoopTail(dst, tmp, baseOffset, singleMergeTmpElementCount, elementCountTail,
            mergeTmpRepeatTimes, mergeTmpTailQueNum);
        PipeBarrier<PIPE_V>();
        DataCopy(tmp, dst, dstElementCount);
        PipeBarrier<PIPE_V>();
        singleMergeTmpElementCount *= MRG_SORT_ELEMENT_LEN;
        mergeTmpTotalQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN ?
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN + 1 :
            mergeTmpTotalQueNum / MRG_SORT_ELEMENT_LEN;
        mergeTmpTailQueNum = mergeTmpTotalQueNum % MRG_SORT_ELEMENT_LEN;
        if (mergeTmpTailQueNum == 0 && elementCountTail != singleMergeTmpElementCount) {
            mergeTmpTailQueNum = MRG_SORT_ELEMENT_LEN;
        }
        mergeTmpQueNum = mergeTmpTotalQueNum - mergeTmpTailQueNum;
        mergeTmpRepeatTimes = mergeTmpQueNum / MRG_SORT_ELEMENT_LEN;
    }
}
}
# 31 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h" 2
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
namespace AscendC {


constexpr int32_t REGION_PROPOSAL_LABEL_POSITION = 5;
constexpr int32_t REGION_PROPOSAL_Y1_POSITION = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_EVEN = 1;
constexpr uint8_t GATHER_MASK_MODE_FOR_INDEX_ODD = 2;

constexpr uint8_t GATHER_MASK_MODE_FOR_EXTRACT_INDEX = 4;
constexpr int32_t REGION_PROPOSAL_SCORE_POSITION = 4;

#pragma begin_pipe(V)
# 73 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void MrgSort4(const LocalTensor<T>& dst, const MrgSortSrcList<T>& src,
    const MrgSort4Info& params)
{
    using PrimType = PrimT<T>;


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                  ;
    }

                                                                                                                   ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.elementLengths[0] & 0xFFF) << 8);
    config |= (uint64_t(params.elementLengths[1] & 0xFFF) << 20);
    config |= (uint64_t(params.elementLengths[2] & 0xFFF) << 32);
    config |= (uint64_t(params.elementLengths[3] & 0xFFF) << 44);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 59);
    config |= (uint64_t(params.validBit & 0xF) << 60);

    __attribute__((cce_unif_buff)) PrimType *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) PrimType *)src.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src4.GetPhyAddr()};
    Vmrgsort4Cal((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), addrArray, config);
}
# 115 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void RpSort16(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime)
{
    using PrimType = PrimT<T>;


                                            ;
                                                                           ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTime;
    VbitsortCal((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), repeatParams);
}
# 146 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void MrgSort(const LocalTensor<T>& dst, const MrgSortSrcList<T>& src,
    const MrgSort4Info& params)
{
    using PrimType = PrimT<T>;


                                            ;
    for (int8_t i = 0; i < MRG_SORT_ELEMENT_LEN; ++i) {
                                                                                                 ;
    }

                                                                                                                  ;





    uint64_t config = 0;
    config |= (params.repeatTimes & 0xFF);
    config |= (uint64_t(params.validBit & 0xF) << 8);
    config |= (uint64_t(params.ifExhaustedSuspension & 0x1) << 12);

    uint64_t src1 = 0;
    src1 |= (uint64_t(params.elementLengths[0] & 0xFFFF));
    src1 |= (uint64_t(params.elementLengths[1] & 0xFFFF) << 16);
    src1 |= (uint64_t(params.elementLengths[2] & 0xFFFF) << 32);
    src1 |= (uint64_t(params.elementLengths[3] & 0xFFFF) << 48);


    __attribute__((cce_unif_buff)) PrimType *addrArray[MRG_SORT_ELEMENT_LEN] = {(__attribute__((cce_unif_buff)) PrimType *)src.src1.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src2.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src3.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType *)src.src4.GetPhyAddr()};







    Vmrgsort4Cal((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), addrArray, src1, config);
}
# 199 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void Sort32(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<uint32_t>& src1, const int32_t repeatTime)
{
    using PrimType = PrimT<T>;


                                            ;
                                                                         ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTime;
    VbitsortCal((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src1.GetPhyAddr(), repeatParams);
}
# 228 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void ProposalConcat(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t modeNumber)
{
    using PrimType = PrimT<T>;

                                                                                      ;
                                                                                 ;
                                                                               ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTime;
    repeatParams.modeNumber = modeNumber;
    VconcatCal((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(), repeatParams);
}
# 257 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void ProposalExtract(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t repeatTime, const int32_t modeNumber)
{
    using PrimType = PrimT<T>;

                                                                                                      ;
                                                                                  ;
                                                                                ;





    struct ProposalIntriParams repeatParams;
    repeatParams.repeat = repeatTime;
    repeatParams.modeNumber = modeNumber;
    VextractCal((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(), repeatParams);
}
# 286 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void Concat(LocalTensor<T>& concat, const LocalTensor<T>& src,
    const LocalTensor<T>& tmp, const int32_t repeatTime)
{
    using PrimType = PrimT<T>;




                                                                                     ;
                                                                         ;



    concat = src;
# 310 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
}
# 321 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T>
[aicore] inline void Extract(const LocalTensor<T>& dstValue, const LocalTensor<uint32_t>& dstIndex,
    const LocalTensor<T>& sorted, const int32_t repeatTime)
{
    using PrimType = PrimT<T>;




                                                                                     ;
                                                                          ;
# 341 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
    uint64_t rsvdCnt;
    if constexpr (Std::is_same<PrimType, half>::value) {
        constexpr uint8_t gatherMaskPattern3 = 3;
        constexpr uint8_t gatherMaskPattern2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) PrimType *)dstValue.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)sorted.GetPhyAddr(),
            gatherMaskPattern3, false, static_cast<uint32_t>(0), { 1, static_cast<uint16_t>(repeatTime), DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndex.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sorted.GetPhyAddr(),
            gatherMaskPattern2, false, static_cast<uint32_t>(0), { 1, static_cast<uint16_t>(repeatTime * 2), 8, 0 }, rsvdCnt);
    } else {
        constexpr uint8_t gatherMaskPattern1 = 1;
        constexpr uint8_t gatherMaskPattern2 = 2;
        GatherMaskCal((__attribute__((cce_unif_buff)) PrimType *)dstValue.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)sorted.GetPhyAddr(),
            gatherMaskPattern1, false, static_cast<uint32_t>(0), { 1, static_cast<uint16_t>(repeatTime), DEFAULT_REPEAT_STRIDE, 0 }, rsvdCnt);
        PipeBarrier<PIPE_V>();
        GatherMaskCal((__attribute__((cce_unif_buff)) uint32_t *)dstIndex.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint32_t *)sorted.GetPhyAddr(),
            gatherMaskPattern2, false, static_cast<uint32_t>(0), { 1, static_cast<uint16_t>(repeatTime), 8, 0 }, rsvdCnt);
    }
# 375 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
}
# 388 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T, bool isExhaustedSuspension>
[aicore] inline void MrgSort(const LocalTensor<T>& dst, const MrgSortSrcList<T>& sortList,
    const uint16_t elementCountList[4], uint32_t sortedNum[4], uint16_t validBit, const int32_t repeatTime)
{
    using PrimType = PrimT<T>;

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }







                                            ;
    MrgSort4Info mrgSortInfo(elementCountList, isExhaustedSuspension, validBit, (uint16_t)repeatTime);



    MrgSort(dst, sortList, mrgSortInfo);



    if (isExhaustedSuspension) {

        constexpr uint32_t validBitMask = 0xFFFF;
        constexpr uint32_t shiftBase = 16;







        auto res = get_vms4_sr();
        sortedNum[0] = res & validBitMask;
        sortedNum[1] = (res >> shiftBase) & validBitMask;
        sortedNum[2] = (res >> (2 * shiftBase)) & validBitMask;
        sortedNum[3] = (res >> (3 * shiftBase)) & validBitMask;
    }
}
# 442 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
template <typename T, bool isFullSort>
[aicore] inline void Sort(const LocalTensor<T>& dst, const LocalTensor<T>& concat,
    const LocalTensor<uint32_t>& index, LocalTensor<T>& tmp, const int32_t repeatTime)
{
    using PrimType = PrimT<T>;




                                                                             ;
                                                                       ;
# 461 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
    Sort32(dst, concat, index, repeatTime);
# 492 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_proposal_intf_impl.h"
    if constexpr (isFullSort) {
        PipeBarrier<PIPE_V>();
        DoFullSort(dst, concat, index, tmp, repeatTime);
    }
}

constexpr uint32_t halfSortedDataSize = 4;
constexpr uint32_t floatSortedDataSize = 2;






template <typename T>
[aicore] inline uint32_t GetSortOffset(const uint32_t elemOffset)
{
    using PrimType = PrimT<T>;


                          ;



    if constexpr (Std::is_same<PrimType, half>::value) {
        return elemOffset * halfSortedDataSize;
    } else {
        return elemOffset * floatSortedDataSize;
    }



}







template <typename T>
[aicore] inline uint32_t GetSortLen(const uint32_t elemCount)
{
    using PrimType = PrimT<T>;


                          ;



    if constexpr (Std::is_same<PrimType, half>::value) {
        return elemCount * halfSortedDataSize;
    } else {
        return elemCount * floatSortedDataSize;
    }



}
#pragma end_pipe
[aicore] inline __attribute__((inout_pipe("S"))) void GetMrgSortResult(
    uint16_t &mrgSortList1, uint16_t &mrgSortList2, uint16_t &mrgSortList3, uint16_t &mrgSortList4)
{

    if (g_coreType == AIC) {
        return;
    }

    GetMrgSortResultImpl(mrgSortList1, mrgSortList2, mrgSortList3, mrgSortList4);
}
}
# 187 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_proposal_intf.h" 2
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_determine_compute_sync_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_determine_compute_sync_intf.h"
namespace AscendC {





[aicore] inline void InitDetermineComputeWorkspace(GlobalTensor<int32_t>& gmWorkspace,
    LocalTensor<int32_t>& ubWorkspace);





[aicore] inline void WaitPreBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);






[aicore] inline void NotifyNextBlock(GlobalTensor<int32_t>& gmWorkspace, LocalTensor<int32_t>& ubWorkspace);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_determine_compute_sync_intf_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_determine_compute_sync_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_determine_compute_sync_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_determine_compute_sync_impl.h"
namespace AscendC {
[aicore] inline void InitDetermineComputeWorkspaceCalc(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto tmpBlockNum = GetBlockNum();
        auto blockIdx = GetBlockIdx();
        if (GetBlockIdx() == 0) {
            Duplicate(ubWorkspace, 0, B32_DATA_NUM_PER_BLOCK * tmpBlockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, B32_DATA_NUM_PER_BLOCK * tmpBlockNum);
        }
        ubWorkspace.SetValue(tmpBlockNum * B32_DATA_NUM_PER_BLOCK, 1);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] inline bool CheckUBWorkspace(LocalTensor<int32_t> &ubWorkspace, int64_t blockIdx, int64_t tmpBlockNum)
{
    int32_t repeatTime = ubWorkspace.GetValue(tmpBlockNum * B32_DATA_NUM_PER_BLOCK);
    int64_t offset = 0;


    for (; offset < blockIdx * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != repeatTime) {
            return false;
        }
    }
    for (; offset < tmpBlockNum * B32_DATA_NUM_PER_BLOCK; offset += B32_DATA_NUM_PER_BLOCK) {
        if (ubWorkspace.GetValue(offset) != 0) {
            return false;
        }
    }
    return true;
}

[aicore] inline void WaitPreBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto tmpBlockNum = GetBlockNum();
        bool matchFlag;
        do {
            DataCopy(ubWorkspace, gmWorkspace, tmpBlockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::MTE2_S));
            SetFlag<HardEvent::MTE2_S>(eventID);
            WaitFlag<HardEvent::MTE2_S>(eventID);
            matchFlag = CheckUBWorkspace(ubWorkspace, blockIdx, tmpBlockNum);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_MTE2));
            SetFlag<HardEvent::S_MTE2>(eventID);
            WaitFlag<HardEvent::S_MTE2>(eventID);
        } while (!matchFlag);
        PipeBarrier<PIPE_ALL>();
    }
}

[aicore] inline void NotifyNextBlockCalc(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    if constexpr(g_coreType == AscendC::AIV) {
        PipeBarrier<PIPE_ALL>();
        event_t eventID;
        auto blockIdx = GetBlockIdx();
        auto tmpBlockNum = GetBlockNum();
        int32_t repeatTime = ubWorkspace.GetValue(tmpBlockNum * B32_DATA_NUM_PER_BLOCK);
        if (blockIdx + 1 == tmpBlockNum) {
            Duplicate(ubWorkspace, 0, tmpBlockNum * B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace, ubWorkspace, tmpBlockNum * B32_DATA_NUM_PER_BLOCK);
        } else {
            auto offset = blockIdx * B32_DATA_NUM_PER_BLOCK;
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);
            Duplicate(ubWorkspace[offset], repeatTime, B32_DATA_NUM_PER_BLOCK);
            eventID = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
            SetFlag<HardEvent::V_MTE3>(eventID);
            WaitFlag<HardEvent::V_MTE3>(eventID);
            DataCopy(gmWorkspace[offset], ubWorkspace[offset], B32_DATA_NUM_PER_BLOCK);
        }
        ubWorkspace.SetValue(tmpBlockNum * B32_DATA_NUM_PER_BLOCK, repeatTime + 1);
        PipeBarrier<PIPE_ALL>();
    }
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_determine_compute_sync_intf_impl.h" 2
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_determine_compute_sync_intf_impl.h"
namespace AscendC {





[aicore] inline void InitDetermineComputeWorkspace(GlobalTensor<int32_t> &gmWorkspace,
    LocalTensor<int32_t> &ubWorkspace)
{
    InitDetermineComputeWorkspaceCalc(gmWorkspace, ubWorkspace);
}





[aicore] inline void WaitPreBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    WaitPreBlockCalc(gmWorkspace, ubWorkspace);
}






[aicore] inline void NotifyNextBlock(GlobalTensor<int32_t> &gmWorkspace, LocalTensor<int32_t> &ubWorkspace)
{
    NotifyNextBlockCalc(gmWorkspace, ubWorkspace);
}
}
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_determine_compute_sync_intf.h" 2
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_transpose.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_transpose.h"
namespace AscendC {
enum class TransposeType : uint8_t {

    TRANSPOSE_TYPE_NONE,


    TRANSPOSE_NZ2ND_0213,


    TRANSPOSE_NZ2NZ_0213,


    TRANSPOSE_NZ2NZ_012_WITH_N,


    TRANSPOSE_NZ2ND_012_WITH_N,

    TRANSPOSE_NZ2ND_012_WITHOUT_N,


    TRANSPOSE_NZ2NZ_012_WITHOUT_N,
    TRANSPOSE_ND2ND_ONLY,
    TRANSPOSE_ND_UB_GM,
    TRANSPOSE_GRAD_ND_UB_GM,
    TRANSPOSE_ND2ND_B16,
    TRANSPOSE_NCHW2NHWC,
    TRANSPOSE_NHWC2NCHW,
};

struct TransDataTo5HDParams {
    constexpr [aicore] TransDataTo5HDParams() = default;

    constexpr [aicore] TransDataTo5HDParams(const bool dstHighHalfIn, const bool srcHighHalfIn, const uint8_t repeatTimesIn,
        const uint16_t dstRepStrideIn, const uint16_t srcRepStrideIn)
        : dstHighHalf(dstHighHalfIn),
          srcHighHalf(srcHighHalfIn),
          repeatTimes(repeatTimesIn),
          dstRepStride(dstRepStrideIn),
          srcRepStride(srcRepStrideIn)
    {}

    bool dstHighHalf = false;
    bool srcHighHalf = false;
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
};

struct TransposeParamsExt {
    constexpr [aicore] TransposeParamsExt() = default;

    constexpr [aicore] TransposeParamsExt(const uint16_t nSizeIn, const uint16_t cSizeIn, const uint16_t hSizeIn,
        const uint16_t wSizeIn, const TransposeType transposeTypeIn)
        : nSize(nSizeIn),
          cSize(cSizeIn),
          hSize(hSizeIn),
          wSize(wSizeIn),
          transposeType(transposeTypeIn)
    {}

    uint16_t nSize = 0;
    uint16_t cSize = 0;
    uint16_t hSize = 0;
    uint16_t wSize = 0;
    TransposeType transposeType = TransposeType::TRANSPOSE_ND2ND_B16;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h" 2





namespace AscendC {
#pragma begin_pipe(V)
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h"
template <typename T> [aicore] inline void Transpose(const LocalTensor<T>& dst, const LocalTensor<T>& src);
# 51 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h"
template <typename T>
[aicore] inline __attribute__((check_sync_alias)) void TransDataTo5HD(const LocalTensor<T> (&dstList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] inline __attribute__((check_sync_alias)) void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams);

template <typename T>
[aicore] inline void Transpose(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams);
#pragma end_pipe
template <typename T>
[aicore] inline __attribute__((check_sync_alias)) __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t>& dst, const LocalTensor<uint64_t>& src,
    const TransDataTo5HDParams& nchwconvParams);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_transpose_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_transpose_impl.h"
namespace AscendC {
constexpr int8_t TWO_NUM = 2;
[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) float* dstList[16], __attribute__((cce_unif_buff)) float* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int32_t* dstList[16], __attribute__((cce_unif_buff)) int32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint32_t* dstList[16], __attribute__((cce_unif_buff)) uint32_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int16_t* dstList[16], __attribute__((cce_unif_buff)) int16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint16_t* dstList[16], __attribute__((cce_unif_buff)) uint16_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) half* dstList[16], __attribute__((cce_unif_buff)) half* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] inline void TransDataTo5HDB8IntrinsicsImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) int8_t* dstList[16], __attribute__((cce_unif_buff)) int8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

[aicore] inline void TransDataTo5HDIntrinsicsImpl(__attribute__((cce_unif_buff)) uint8_t* dstList[16], __attribute__((cce_unif_buff)) uint8_t* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
}

template<typename T>
[aicore] inline void TransDataTo5HDIntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
                                                                                            ;
}

template<>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<float>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<int32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<uint32_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b32(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<int16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<uint16_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<half>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    scatter_vnchwconv_b16(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
        transDataTo5HDParams.srcRepStride);
}

template <typename T>
[aicore] inline void TransDataTo5HDB8IntrinsicsImpl(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == false)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, false);
    } else if ((transDataTo5HDParams.dstHighHalf == false) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, false, true);
    } else if ((transDataTo5HDParams.dstHighHalf == true) && (transDataTo5HDParams.srcHighHalf == true)) {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, true);
    } else {
        scatter_vnchwconv_b8(VA0, VA2, transDataTo5HDParams.repeatTimes, transDataTo5HDParams.dstRepStride,
            transDataTo5HDParams.srcRepStride, true, false);
    }
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<int8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<int8_t>(dstList, srcList, transDataTo5HDParams);
}

template <>
[aicore] inline void TransDataTo5HDIntrinsicsImpl<uint8_t>(uint64_t dstList[16], uint64_t srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{
    TransDataTo5HDB8IntrinsicsImpl<uint8_t>(dstList, srcList, transDataTo5HDParams);
}

template <typename T> [aicore] inline void SetVaReg(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16])
{
    uint64_t vaRegArray1[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray2[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray3[VA_REG_ARRAY_LEN];
    uint64_t vaRegArray4[VA_REG_ARRAY_LEN];

    for (int32_t i = 0; i < VA_REG_ARRAY_LEN; i++) {
        vaRegArray1[i] = (uint64_t)dstList[i];
        vaRegArray2[i] = (uint64_t)dstList[VA_REG_ARRAY_LEN + i];
        vaRegArray3[i] = (uint64_t)srcList[i];
        vaRegArray4[i] = (uint64_t)srcList[VA_REG_ARRAY_LEN + i];
    }

    set_va_reg_sb(VA0, vaRegArray1);
    set_va_reg_sb(VA1, vaRegArray2);
    set_va_reg_sb(VA2, vaRegArray3);
    set_va_reg_sb(VA3, vaRegArray4);
}

[aicore] inline void SetVaReg(uint64_t dst[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t src[NCHW_CONV_ADDR_LIST_SIZE])
{
    set_va_reg_sb(VA0, dst);
    set_va_reg_sb(VA1, dst + VA_REG_ARRAY_LEN);
    set_va_reg_sb(VA2, src);
    set_va_reg_sb(VA3, src + VA_REG_ARRAY_LEN);
}

[aicore] inline void VldVaReg(__attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src)
{
    vld_va_reg(VA0, dst, L128);
    vld_va_reg(VA1, dst, H128);
    vld_va_reg(VA2, src, L128);
    vld_va_reg(VA3, src, H128);
}

template <typename T>
[aicore] inline void TransDataTo5HDImpl(__attribute__((cce_unif_buff)) T* dstList[16], __attribute__((cce_unif_buff)) T* srcList[16],
    const TransDataTo5HDParams& transDataTo5HDParams)
{


                                                                                                  ;
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] inline void TransDataTo5HDImpl(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& transDataTo5HDParams)
{


                                                                                                  ;
    if constexpr(g_coreType == AscendC::AIV) {
        SetVaReg(dstList, srcList);
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}

template <typename T>
[aicore] inline void TransDataTo5HDVldVaRegImpl(
    __attribute__((cce_unif_buff)) uint64_t* dst, __attribute__((cce_unif_buff)) uint64_t* src, const TransDataTo5HDParams& transDataTo5HDParams)
{


                                                                                                  ;
    if constexpr(g_coreType == AscendC::AIV) {
        VldVaReg(dst, src);
        uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE] = { 0 };
        TransDataTo5HDIntrinsicsImpl<T>(dstList, srcList, transDataTo5HDParams);
    }
}


template <typename T> [aicore] inline void TransposeIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    vtranspose((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
}


template <typename T> [aicore] inline void TransposeImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src)
{
    if constexpr(g_coreType == AscendC::AIV) {
        TransposeIntrinsicsImpl((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src);
    }
}

template <typename T> struct Transpose4dParams {
    [aicore] Transpose4dParams(){};

    uint8_t blockSize = 1;
    uint16_t tmp1RemainRowCount = 0;
    uint16_t tmp1CopyCount = 0;
    uint32_t tmp1NeedRowCount = 0;
    uint16_t tmp2Count = 0;
    uint16_t tmp2NeedRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t tmp3RemainRowCount = 0;
    uint16_t copyCIndex = 0;
    uint16_t srcBlockIndex = 0;
    uint32_t preCinnerOffset = 0;
    uint32_t preCoffset = 0;
    uint16_t dstBlockNum = 0;
    uint16_t dstNeedBlockNum = 0;
    uint16_t imageSize = 0;
    uint32_t oneChwSize = 0;
    uint16_t transRowCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t copyColCount = NCHW_CONV_ADDR_LIST_SIZE;
    uint16_t transLen = 0;
    uint32_t preTmpLen = B16_TMP_ELE_LEN;
    uint32_t dstAllBlockNum = 0;
    uint32_t imageBlockNum = 0;


    TransDataTo5HDParams transDataParams1;

    __attribute__((cce_unif_buff)) T* dstList1[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList1[NCHW_CONV_ADDR_LIST_SIZE];

    DataCopyParams dataCopyParams1;
    DataCopyParams dataCopyParams2;


    TransDataTo5HDParams transDataParams2;

    __attribute__((cce_unif_buff)) T* dstList2[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) T* srcList2[NCHW_CONV_ADDR_LIST_SIZE];
};


template <typename T>
[aicore] inline void TransBroadCastForB8Cal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dst[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)src[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dst[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)src[params.srcBlockIndex * params.blockSize].GetPhyAddr();
    }
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? ONE_BLK_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
    params.transDataParams1.dstHighHalf = false;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    PipeBarrier<PIPE_V>();
    params.transDataParams1.dstHighHalf = true;
    params.transDataParams1.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
}

template <typename T>
[aicore] inline void TransFracForB8Cal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = false;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = false;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    PipeBarrier<PIPE_V>();

    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[B8_TRANS_FRACTAL + m * params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[B8_TRANS_FRACTAL + n * params.blockSize].GetPhyAddr();
    }
    params.transDataParams2.dstHighHalf = true;
    params.transDataParams2.srcHighHalf = true;
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] inline void TransBroadCastCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dst[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)src[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransBroadCastForB8Cal(dst, src, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList1[m] = (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList1[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList1[n] = (__attribute__((cce_unif_buff)) T *)src[params.srcBlockIndex * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList1, params.srcList1, params.transDataParams1);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TransFracCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    Transpose4dParams<T> &params)
{
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[m * params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    } else if constexpr (sizeof(T) == sizeof(uint8_t)) {
        TransFracForB8Cal(dst, src, params);
    } else {
        for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
            params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
            params.dstList2[m + 1] =
                (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
        }
        TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TransLastFracCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    Transpose4dParams<T> &params)
{
    for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + TWO_NUM) {
        params.dstList2[m] = (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM)].GetPhyAddr();
        params.dstList2[m + 1] =
            (__attribute__((cce_unif_buff)) T *)dst[NCHW_CONV_ADDR_LIST_SIZE * (m / TWO_NUM) + params.blockSize].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM; n++) {
        params.srcList2[n] = (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
        params.srcList2[n + NCHW_CONV_ADDR_LIST_SIZE / TWO_NUM] =
            (__attribute__((cce_unif_buff)) T *)src[n * params.blockSize].GetPhyAddr();
    }
    TransDataTo5HDImpl<T>(params.dstList2, params.srcList2, params.transDataParams2);
}

template <typename T>
[aicore] inline void CopyFirstBlockCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params, DataCopyParams &dataCopyParams)
{
    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dst[params.dstBlockNum * (params.blockSize)], src, dataCopyParams);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.tmp3RemainRowCount = 0;
    }
}

template <typename T>
[aicore] inline void UpdataCopyToTmp2ParamCal(Transpose4dParams<T> &params, const TransposeParamsExt &transposeParams)
{
    params.copyCIndex += 1;
    params.tmp2NeedRowCount -= 1;
    params.tmp1CopyCount += 1;
    params.tmp2Count += 1;
    params.tmp1RemainRowCount -= 1;
    if (params.copyCIndex == transposeParams.cSize) {
        params.copyCIndex = 0;
    }
}

template <typename T>
[aicore] inline void UpdataTransToTmp3ParamCal(Transpose4dParams<T> &params, const uint16_t cSize,
    const TransposeType transposeType)
{
    if (transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        if (params.dstNeedBlockNum == 1) {
            params.tmp2NeedRowCount = params.imageSize % params.transRowCount == 0 ?
                params.transRowCount :
                (params.imageSize % params.transRowCount);
        } else {
            params.tmp2NeedRowCount = params.transRowCount;
        }
        if ((params.dstNeedBlockNum > 1) && (sizeof(T) == sizeof(float))) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    } else if (transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        params.tmp3RemainRowCount = 1;
        params.tmp2Count = 0;
        params.tmp2NeedRowCount = cSize * params.transRowCount;

        if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum != 1)) {
            params.tmp3RemainRowCount = TWO_NUM;
        }
    }
}

template <typename T>
[aicore] inline void UpdataTransToTmp1ParamCal(Transpose4dParams<T> &params)
{

    params.srcBlockIndex += 1;

    if (params.dstNeedBlockNum == 1) {

        params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
            params.tmp1NeedRowCount :
            (params.oneChwSize % params.tmp1NeedRowCount);
    } else {
        params.tmp1RemainRowCount = params.tmp1NeedRowCount;
    }
}

template <typename T>
[aicore] inline void CopyTodstForChwCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const uint16_t cSize, Transpose4dParams<T> &params)
{
    params.dataCopyParams2.blockCount = cSize;
    params.dataCopyParams2.blockLen = params.tmp3RemainRowCount;
    params.dataCopyParams2.srcStride =
        params.preTmpLen * sizeof(T) / ONE_BLK_SIZE - params.tmp3RemainRowCount;
    params.dataCopyParams2.dstStride = params.imageBlockNum - params.tmp3RemainRowCount;

    if ((params.dstNeedBlockNum != 0) && (params.tmp3RemainRowCount != 0)) {
        DataCopy(dst[params.dstBlockNum * (params.blockSize)], src, params.dataCopyParams2);
        params.dstNeedBlockNum -= params.tmp3RemainRowCount;
        params.dstBlockNum += params.tmp3RemainRowCount;
        params.dstAllBlockNum = params.tmp3RemainRowCount * cSize;
        params.tmp3RemainRowCount = 0;
    }
    if (sizeof(T) == sizeof(float) && (params.dstNeedBlockNum == 1) && (params.dstAllBlockNum != 0) &&
        (params.srcBlockIndex % params.dstAllBlockNum == 0)) {
        params.tmp2NeedRowCount = cSize * (params.imageSize % NCHW_CONV_ADDR_LIST_SIZE);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void Transpose2HwcCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempA = sharedTmpBuffer;
    LocalTensor<T> tempB = sharedTmpBuffer[transposeParams.cSize * params.preTmpLen];
    LocalTensor<T> tempC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];

    while (params.dstNeedBlockNum != 0) {

        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempA, src, params);

            params.srcBlockIndex += 1;

            if (params.dstNeedBlockNum == 1) {

                params.tmp1RemainRowCount = params.oneChwSize % params.tmp1NeedRowCount == 0 ?
                    transposeParams.cSize * params.tmp1NeedRowCount :
                    (params.oneChwSize % params.tmp1NeedRowCount);
            } else {
                params.tmp1RemainRowCount = transposeParams.cSize * params.tmp1NeedRowCount;
            }
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp2NeedRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;

            params.preCinnerOffset = (params.copyColCount * (params.tmp1CopyCount / transposeParams.cSize));
            params.preCoffset = (params.transLen) * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempB[(params.tmp2Count % params.transRowCount) * (params.blockSize)],
                tempA[params.preCoffset + params.preCinnerOffset], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            TransFracCal(tempC, tempB, params);

            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NCHW2NHWC);
        }
        PipeBarrier<PIPE_V>();

        params.dataCopyParams1.blockCount = 1;
        params.dataCopyParams1.blockLen = params.tmp3RemainRowCount;
        CopyFirstBlockCal(dst, tempC, transposeParams, params, params.dataCopyParams1);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] inline void Transpose2ChwCal(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &sharedTmpBuffer, const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    LocalTensor<T> tempA = sharedTmpBuffer;
    LocalTensor<T> tempB = sharedTmpBuffer[params.preTmpLen];

    LocalTensor<T> tempC = sharedTmpBuffer[(transposeParams.cSize + 1) * params.preTmpLen];
    while (params.dstNeedBlockNum != 0) {
        if (params.tmp1RemainRowCount == 0) {
            TransBroadCastCal(tempA, src, params);

            UpdataTransToTmp1ParamCal(params);
        }
        PipeBarrier<PIPE_V>();

        while (params.tmp1RemainRowCount != 0) {
            params.dataCopyParams1.blockCount = 1;
            params.dataCopyParams1.blockLen = 1;
            params.preCinnerOffset = (params.copyColCount * (params.tmp2Count / transposeParams.cSize));
            params.preCoffset = params.transLen * (params.copyCIndex % transposeParams.cSize);
            DataCopy(tempB[params.preCoffset + params.preCinnerOffset],
                tempA[(params.tmp2Count % params.blockSize) * params.transRowCount], params.dataCopyParams1);

            UpdataCopyToTmp2ParamCal(params, transposeParams);

            if (params.tmp1RemainRowCount == 0) {
                params.tmp1CopyCount = 0;
                break;
            }
            PipeBarrier<PIPE_V>();
        }

        if (params.tmp2NeedRowCount == 0) {
            if ((params.imageSize % NCHW_CONV_ADDR_LIST_SIZE != 0) && (params.dstNeedBlockNum == 1)) {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransLastFracCal(tempC[k * params.preTmpLen], tempB[k * params.preTmpLen], params);
                }
            } else {
                for (int16_t k = 0; k < transposeParams.cSize; k++) {
                    TransFracCal(tempC[k * params.preTmpLen], tempB[k * params.preTmpLen], params);
                }
            }
            UpdataTransToTmp3ParamCal(params, transposeParams.cSize, TransposeType::TRANSPOSE_NHWC2NCHW);
        }
        PipeBarrier<PIPE_V>();

        CopyTodstForChwCal(dst, tempC, transposeParams.cSize, params);
    }
}

template <typename T>
[aicore] inline void GetTransposeParamCal(const TransposeParamsExt &transposeParams, Transpose4dParams<T> &params)
{
    params.imageSize = transposeParams.hSize * transposeParams.wSize;
    params.imageBlockNum = params.imageSize * sizeof(T) / ONE_BLK_SIZE;
    params.oneChwSize = params.imageSize * transposeParams.cSize;
    params.blockSize = ONE_BLK_SIZE / sizeof(T);
    params.tmp1NeedRowCount = sizeof(T) == 1 ? ONE_BLK_SIZE : params.blockSize;

    params.transDataParams1.repeatTimes = transposeParams.cSize;
    params.transDataParams1.dstRepStride = params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
    params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;

    params.transLen = BYTE_PER_FRACTAL / sizeof(T);
    if constexpr (sizeof(T) == sizeof(uint8_t)) {
        params.transRowCount = B8_TRANS_ROW;
        params.copyColCount = B8_COPY_COL;
        params.preTmpLen = B8_TMP_ELE_LEN;
        params.transLen = B8_TRANS_LEN / sizeof(T);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        params.preTmpLen = B32_TMP_ELE_LEN;
        if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
            params.copyColCount = ONE_BLK_SIZE / sizeof(T);
        }
    }
}

template <typename T>
[aicore] inline void TransposeUB2UBImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const DataCopyParams& intriParams)
{



                                                         ;

    DataCopyUB2UBImpl(dst, src, intriParams);
}

template <typename T>
[aicore] inline void Transpose4DImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{



                                                         ;

    LocalTensor<T> stackBuffer = sharedTmpBuffer.ReinterpretCast<T>();
                                                                                                                    ;

    Transpose4dParams<T> params;
    GetTransposeParamCal(transposeParams, params);

    if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.dstBlockNum = 0;
            params.srcBlockIndex = 0;
            params.copyCIndex = 0;
            params.tmp1RemainRowCount = 0;
            params.tmp1CopyCount = 0;
            params.dstNeedBlockNum = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE;
            params.tmp2NeedRowCount =
                params.dstNeedBlockNum == 1 ? (params.imageSize % params.transRowCount) : params.transRowCount;
            Transpose2HwcCal(dst[i * params.oneChwSize], src[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        for (int16_t i = 0; i < transposeParams.nSize; i++) {
            params.transDataParams1.repeatTimes = 1;
            params.transDataParams1.dstRepStride =
                params.transDataParams1.repeatTimes > 1 ? NCHW_CONV_ADDR_LIST_SIZE : 0;
            params.transDataParams1.srcRepStride = params.transDataParams1.repeatTimes > 1 ? (params.imageBlockNum) : 0;
            params.dstBlockNum = 0;
            params.dstAllBlockNum = 0;
            params.srcBlockIndex = 0;
            params.dstNeedBlockNum =
                params.imageBlockNum;

            params.tmp2NeedRowCount = params.oneChwSize * sizeof(T) / ONE_BLK_SIZE == 1 ?
                (params.imageSize % params.transRowCount) :
                transposeParams.cSize * params.transRowCount;


            if ((sizeof(T) == sizeof(float)) && (params.dstNeedBlockNum == 1)) {
                params.tmp2NeedRowCount = transposeParams.cSize * params.blockSize;
            }
            Transpose2ChwCal(dst[i * params.oneChwSize], src[i * params.oneChwSize], stackBuffer,
                transposeParams, params);
        }
    }
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
namespace AscendC {
#pragma begin_pipe(V)
# 48 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
template <typename T> [aicore] inline void Transpose(const LocalTensor<T>& dst, const LocalTensor<T>& src)
{


                                                       ;





    TransposeImpl((__attribute__((cce_unif_buff)) PrimT<T>*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T>*)src.GetPhyAddr());
}
# 75 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
template <typename T>
[aicore] inline void TransDataTo5HD(const LocalTensor<T> (&dstList)[NCHW_CONV_ADDR_LIST_SIZE],
    const LocalTensor<T> (&srcList)[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{





    __attribute__((cce_unif_buff)) PrimT<T>* dstAddrList[NCHW_CONV_ADDR_LIST_SIZE];
    __attribute__((cce_unif_buff)) PrimT<T>* srcAddrList[NCHW_CONV_ADDR_LIST_SIZE];

    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstAddrList[i] = (__attribute__((cce_unif_buff)) PrimT<T>*)dstList[i].GetPhyAddr();
        srcAddrList[i] = (__attribute__((cce_unif_buff)) PrimT<T>*)srcList[i].GetPhyAddr();
    }

    TransDataTo5HDImpl(dstAddrList, srcAddrList, nchwconvParams);
}

template <typename T>
[aicore] inline void TransDataTo5HD(uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE],
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE], const TransDataTo5HDParams& nchwconvParams)
{
# 113 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
    TransDataTo5HDImpl<T>(dstList, srcList, nchwconvParams);
}

template <typename T>
[aicore] inline void Transpose(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const TransposeParamsExt &transposeParams)
{





    if ((transposeParams.transposeType == TransposeType::TRANSPOSE_ND2ND_B16) &&
        (transposeParams.hSize == NCHW_CONV_ADDR_LIST_SIZE) && (transposeParams.wSize == NCHW_CONV_ADDR_LIST_SIZE)) {







                                                                                     ;

        TransposeImpl((__attribute__((cce_unif_buff)) PrimT<T> *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T> *)src.GetPhyAddr());
    } else if (transposeParams.transposeType == TransposeType::TRANSPOSE_NCHW2NHWC ||
        transposeParams.transposeType == TransposeType::TRANSPOSE_NHWC2NCHW) {
        if (transposeParams.cSize == 1) {
            struct DataCopyParams repeatParams;
            repeatParams.blockLen = transposeParams.nSize * transposeParams.cSize * transposeParams.hSize *
                transposeParams.wSize / AscendCUtils::GetC0Count(sizeof(PrimT<T>));
            TransposeUB2UBImpl((__attribute__((cce_unif_buff)) PrimT<T> *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimT<T> *)src.GetPhyAddr(), repeatParams);
        } else {
# 153 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_transpose_intf_impl.h"
            Transpose4DImpl(dst, src, sharedTmpBuffer, transposeParams);
        }
    }
}
#pragma end_pipe
template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void TransDataTo5HD(const LocalTensor<uint64_t> &dst,
    const LocalTensor<uint64_t> &src, const TransDataTo5HDParams &nchwconvParams)
{







    constexpr uint32_t vaRegSize = VA_REG_ARRAY_LEN / HALF_FACTOR;
    constexpr uint32_t vaOne = 1;
    constexpr uint32_t vaTwo = 2;
    constexpr uint32_t vaThree = 3;
    constexpr uint64_t vaAddr = 5;
    constexpr uint64_t vaMask = 0x1fff;
    constexpr uint64_t vaBit1 = 16;
    constexpr uint64_t vaBit2 = 32;
    constexpr uint64_t vaBit3 = 48;

    for (uint32_t i = 0; i < vaRegSize; i++)
    {
        uint64_t dstAddrConfig = (((dst.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((dst.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((dst.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((dst.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        dst.SetValue(i, dstAddrConfig);

        uint64_t srcAddrConfig = (((src.GetValue(vaRegSize * i) >> vaAddr) & vaMask) |
                                  (((src.GetValue(vaRegSize * i + vaOne) >> vaAddr) & vaMask) << vaBit1) |
                                  (((src.GetValue(vaRegSize * i + vaTwo) >> vaAddr) & vaMask) << vaBit2) |
                                  (((src.GetValue(vaRegSize * i + vaThree) >> vaAddr) & vaMask) << vaBit3));
        src.SetValue(i, srcAddrConfig);
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
    TransDataTo5HDVldVaRegImpl<T>(
        (__attribute__((cce_unif_buff)) uint64_t*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint64_t*)src.GetPhyAddr(), nchwconvParams);

}
}
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_transpose_intf.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] inline void Gatherb(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<uint32_t>& offset, const uint8_t repeatTime, const GatherRepeatParams& repeatParams);
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTime, const uint16_t dstRepStride);
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTime, const uint16_t dstRepStride);
# 83 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint32_t count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_gather_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_gather_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void GatherbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) uint32_t* offset,
    const uint32_t srcLength, uint8_t repeatTime, const GatherRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {

                                                                                                        ;
        ResetMask();
        uint16_t dstRptStd = repeatParams.dstRepStride;
        uint8_t dstBlkStd = repeatParams.dstBlkStride;
        uint32_t offsetAddr = (uint64_t)src0;




        vgatherb(dst, offset, offsetAddr, dstRptStd, dstBlkStd, repeatTime);
    }
}

template <typename T>
[aicore] inline void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask, const uint8_t repeatTime,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                                               ;
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTime);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, srcOffsetLocal, offsetAddr, dstRepStride, repeatTime);
        } else {
                                                                                                            ;
        }
    }
}

template <typename T>
[aicore] inline void GatherImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* srcOffsetLocal,
    const uint32_t srcLength, const uint32_t srcBaseAddr, const uint64_t mask[], const uint8_t repeatTime,
    const uint16_t dstRepStride)
{
    if constexpr(g_coreType == AscendC::AIV) {


                                                                                               ;
        uint32_t offsetAddr = (uint64_t)srcLocal + srcBaseAddr;




        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        if constexpr (sizeof(T) == sizeof(uint16_t)) {
            vgather((__attribute__((cce_unif_buff)) uint16_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTime);
        } else if constexpr (sizeof(T) == sizeof(uint32_t)) {
            vgather((__attribute__((cce_unif_buff)) uint32_t *)dstLocal, (__attribute__((cce_unif_buff)) uint32_t *)srcOffsetLocal, offsetAddr, dstRepStride,
                repeatTime);
        } else {
                                                                                                            ;
        }
    }
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h" 2








#pragma begin_pipe(V)
namespace AscendC {
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
template <typename T>
[aicore] inline void Gatherb(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<uint32_t>& offset, const uint8_t repeatTime, const GatherRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;





    uint32_t srcLength = src0.GetSize();
GatherbImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t*)offset.GetPhyAddr(), srcLength, repeatTime, repeatParams);
}
# 75 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint64_t mask,
    const uint8_t repeatTime, const uint16_t dstRepStride)
{
    using PrimType = PrimT<T>;





    const uint32_t srcLength = src.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffset.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTime, dstRepStride);
}
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint64_t mask[],
    const uint8_t repeatTime, const uint16_t dstRepStride)
{
    using PrimType = PrimT<T>;





    const uint32_t srcLength = src.GetSize();
    GatherImpl((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)srcOffset.GetPhyAddr(), srcLength, srcBaseAddr, mask, repeatTime, dstRepStride);
}
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
template <typename T>
[aicore] inline void Gather(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& srcOffset, const uint32_t srcBaseAddr, const uint32_t count)
{
    using PrimType = PrimT<T>;
# 145 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_gather_intf_impl.h"
    uint32_t vectorRegWidth = 256;
    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(PrimType) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }
    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTime = count / elementCountSingleRepeat;
    if (repeatTime > 0) {
        Gather(dst, src, srcOffset, srcBaseAddr, static_cast<uint64_t>(elementCountSingleRepeat), repeatTime,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Gather(dst[offset], src, srcOffset[offset], srcBaseAddr, static_cast<uint64_t>(elementCountTail), 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_gather_intf.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTime, const uint8_t srcRepStride);
# 50 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTime, const uint8_t srcRepStride);
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint32_t count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_scatter_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_scatter_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask, const uint8_t repeatTime,
    const uint8_t srcRepStride)
{
                                                ;
}

template <typename T>
[aicore] inline void ScatterImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) T* srcLocal, __attribute__((cce_unif_buff)) uint32_t* dstOffsetLocal,
    const uint32_t dstLength, const uint32_t dstBaseAddr, const uint64_t mask[], const uint8_t repeatTime,
    const uint8_t srcRepStride)
{
                                                ;
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint64_t mask,
    const uint8_t repeatTime, const uint8_t srcRepStride)
{
    using PrimType = PrimT<T>;
# 66 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
                                                                              ;






    const uint32_t dstLength = dst.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffset.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTime, srcRepStride);
}
# 89 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint64_t mask[],
    const uint8_t repeatTime, const uint8_t srcRepStride)
{
    using PrimType = PrimT<T>;
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
                                                                              ;






    const uint32_t dstLength = dst.GetSize();
    ScatterImpl((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)dstOffset.GetPhyAddr(), dstLength, dstBaseAddr, mask, repeatTime, srcRepStride);
}
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
template <typename T>
[aicore] inline void Scatter(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint32_t>& dstOffset, const uint32_t dstBaseAddr, const uint32_t count)
{
    using PrimType = PrimT<T>;
# 143 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_scatter_intf_impl.h"
                                                                              ;






    uint32_t vectorRegWidth = 256;




    uint32_t elementCountSingleRepeat;
    if constexpr (sizeof(PrimType) == sizeof(uint16_t)) {
        elementCountSingleRepeat = 128;
    } else {
        elementCountSingleRepeat = 64;
    }

    const uint32_t elementCountTail = count % elementCountSingleRepeat;
    const uint8_t repeatTime = count / elementCountSingleRepeat;
if (repeatTime > 0) {
        Scatter(dst, src, dstOffset, dstBaseAddr, static_cast<uint64_t>(elementCountSingleRepeat), repeatTime,
            DEFAULT_REPEAT_STRIDE);
    }
    if (elementCountTail > 0) {
        const uint32_t offset = count - elementCountTail;
        Scatter(dst, src[offset], dstOffset[offset], dstBaseAddr, static_cast<uint64_t>(elementCountTail), 1,
            DEFAULT_REPEAT_STRIDE);
    }

}
}
#pragma end_pipe
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_scatter_intf.h" 2
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_brcb_intf.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_brcb_intf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_brcb.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_struct_brcb.h"
namespace AscendC {
struct BrcbRepeatParams {
    [aicore] BrcbRepeatParams() {}

    [aicore] BrcbRepeatParams(const uint16_t dstBlkStrideIn, const uint16_t dstRepStrideIn)
        : dstBlkStride(dstBlkStrideIn), dstRepStride(dstRepStrideIn)
    {}

    uint32_t blockNumber = DEFAULT_BLK_NUM;
    uint16_t dstRepStride = DEFAULT_REPEAT_STRIDE;
    uint16_t dstBlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src1BlkStride = DEFAULT_BLK_STRIDE;
    uint8_t src0RepStride = DEFAULT_REPEAT_STRIDE;
    uint8_t src1RepStride = DEFAULT_REPEAT_STRIDE;
    bool repeatStrideMode = false;
    bool strideSizeMode = false;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_brcb_intf.h" 2





#pragma begin_pipe(V)
namespace AscendC {
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_brcb_intf.h"
template <typename T>
[aicore] inline void Brcb(const LocalTensor<T>& dst, const LocalTensor<T>& src0, const uint8_t repeatTime,
    const BrcbRepeatParams& repeatParams);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_brcb_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_brcb_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_brcb_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_brcb_impl.h"
namespace AscendC {



template <typename T>
[aicore] inline void BrcbImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, const uint8_t repeatTime,
    const BrcbRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        ResetMask();
        if constexpr(sizeof(T) == B16_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint16_t*)dst, (__attribute__((cce_unif_buff)) uint16_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTime);
        } else if constexpr(sizeof(T) == B32_BYTE_SIZE) {
            vbrcb((__attribute__((cce_unif_buff)) uint32_t*)dst, (__attribute__((cce_unif_buff)) uint32_t*)src0, repeatParams.dstBlkStride,
                repeatParams.dstRepStride, repeatTime);
        } else {


                               ;
        }
    }
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_brcb_intf_impl.h" 2







#pragma begin_pipe(V)
namespace AscendC {
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_brcb_intf_impl.h"
template <typename T>
[aicore] inline void Brcb(const LocalTensor<T>& dst, const LocalTensor<T>& src0, const uint8_t repeatTime,
    const BrcbRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;





    BrcbImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), repeatTime, repeatParams);
}
}
#pragma end_pipe
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_brcb_intf.h" 2
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Compare(const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask[],
    const BinaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void Compare(const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask,
    const BinaryRepeatParams& repeatParams);
# 75 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, uint32_t count);

template <typename T>
[aicore] inline void GetCmpMask(const LocalTensor<T>& dst);

template <typename T>
[aicore] inline void SetCmpMask(const LocalTensor<T>& src);
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams);
# 121 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, uint32_t count);
# 150 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint64_t mask,
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams);

template <typename T, SELMODE selMode>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint8_t repeatTime, const BinaryRepeatParams& repeatParams);

template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, uint8_t repeatTime, const BinaryRepeatParams& repeatParams);
# 180 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint32_t count);
# 203 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint64_t mask[], uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint64_t mask, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams);
# 225 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h"
template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint32_t count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmpsel_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_cmpsel_impl.h"
namespace AscendC {




template <typename T> [aicore] inline void VselIntrinsicsImplPre(__attribute__((cce_unif_buff)) T* sel, SELMODE selMode)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<uint32_t>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

    }
}

template <typename T, typename U>
[aicore] inline void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1,
    SELMODE selMode, int32_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    (void)sel;
    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}


template <typename T> [aicore] inline void VselIntrinsicsImplPre(T src1)
{
    if constexpr(g_coreType == AscendC::AIV) {
        __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<T>(ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);
    }
}

template <typename T, typename U>
[aicore] inline void VselIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    int32_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        (void)src1;
        vsel(dst, src0, sel, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, SELMODE selMode = SELMODE::VSEL_CMPMASK_SPR>
[aicore] inline void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, int32_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    if constexpr (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        vsel(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
    } else if constexpr (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {
        vsel(dst, src0, src1, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
    }
}

template <typename T, typename U>
[aicore] inline void SelectCal(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, int32_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        vsel(dst, src0, sel, repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(SELMODE::VSEL_TENSOR_SCALAR_MODE));
    }
}






template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, __attribute__((cce_unif_buff)) T* src1, SELMODE selMode,
    uint32_t count)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    BinaryRepeatParams repeatParams;
    SetMaskCount();

    if (selMode == SELMODE::VSEL_CMPMASK_SPR) {
        set_cmpmask(sel);
        PipeBarrier<PIPE_V>();

        AscendCUtils::SetMask<U>(0, count);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride);
        PipeBarrier<PIPE_V>();
    } else if (selMode == SELMODE::VSEL_TENSOR_TENSOR_MODE) {







        uint32_t selAddr = static_cast<uint32_t>(reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel)));
        __attribute__((cce_unif_buff)) uint32_t* tempBuf = AscendCUtils::GetTemporaryBufferAddr<uint32_t>(TMP_UB_OFFSET, ONE_BLK_SIZE);

        AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
        DuplicateIntrinsicsImpl(tempBuf, selAddr, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        set_cmpmask(tempBuf);
        PipeBarrier<PIPE_V>();

        AscendCUtils::FreeTemporaryBuffer<uint32_t>(tempBuf);

        AscendCUtils::SetMask<U>(0, count);
        vsel(dst, src0, src1, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
            repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
            static_cast<uint8_t>(selMode));
        PipeBarrier<PIPE_V>();
    }

    ResetMask();
    SetMaskNorm();
}

template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* sel, __attribute__((cce_unif_buff)) T* src0, T src1, SELMODE selMode,
    uint32_t count)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SetMaskCount();
    __attribute__((cce_unif_buff)) T* tempBuf = AscendCUtils::GetTemporaryBufferAddr<T>(TMP_UB_OFFSET, ONE_BLK_SIZE);

    AscendCUtils::SetMask<U>(0, ONE_BLK_SIZE);
    DuplicateIntrinsicsImpl(tempBuf, src1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    set_cmpmask(tempBuf);
    PipeBarrier<PIPE_V>();

    AscendCUtils::FreeTemporaryBuffer<T>(tempBuf);

    AscendCUtils::SetMask<U>(0, count);
    BinaryRepeatParams repeatParams;
    vsel(dst, src0, sel, 1, repeatParams.dstBlkStride, repeatParams.src0BlkStride, repeatParams.src1BlkStride,
        repeatParams.dstRepStride, repeatParams.src0RepStride, repeatParams.src1RepStride,
        static_cast<uint8_t>(selMode));

    ResetMask();
    SetMaskNorm();
}


template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, __attribute__((cce_unif_buff)) T* src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(selMask, selMode);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask[], const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask[1], mask[0]);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void VselImpl(__attribute__((cce_unif_buff)) T* dstLocal, __attribute__((cce_unif_buff)) U* selMask, __attribute__((cce_unif_buff)) T* src0Local, T src1Local,
    SELMODE selMode, const uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{


                                                                                                                ;
    if constexpr(g_coreType == AscendC::AIV) {
        VselIntrinsicsImplPre(src1Local);

        AscendCUtils::SetMask<T>(mask);
        VselIntrinsicsImpl(dstLocal, selMask, src0Local, src1Local, selMode, repeatTime, repeatParams);
    }
}
}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h" 2
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using SrcPrimType = PrimT<T>;
    using DstPrimType = PrimT<U>;







                                                            ;







    VcmpvImpl<SrcPrimType, DstPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), cmpMode, mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{
    using SrcPrimType = PrimT<T>;
    using DstPrimType = PrimT<U>;







                                                            ;







    VcmpvImpl<SrcPrimType, DstPrimType, isSetMask>((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), cmpMode, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Compare(const LocalTensor<T>& src0, const LocalTensor<T>& src1, CMPMODE cmpMode,
    const uint64_t mask[], const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;


                                                                         ;







    VcmpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), cmpMode, mask, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void Compare(const LocalTensor<T>& src0, const LocalTensor<T>& src1, CMPMODE cmpMode,
    const uint64_t mask, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;


                                                                         ;







    VcmpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), cmpMode,
        mask, repeatParams);
}
# 159 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U>
[aicore] inline void Compare(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, CMPMODE cmpMode, uint32_t count)
{
    using SrcPrimType = PrimT<T>;
    using DstPrimType = PrimT<U>;







                                                            ;
# 184 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
    VcmpvImpl((__attribute__((cce_unif_buff)) DstPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) SrcPrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) SrcPrimType*)src1.GetPhyAddr(), cmpMode, count);
}

template <typename T>
[aicore] inline void GetCmpMask(const LocalTensor<T>& dst)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    GetCmpMaskImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr());
}

template <typename T>
[aicore] inline void SetCmpMask(const LocalTensor<T>& src)
{
    using PrimType = PrimT<T>;

    if (g_coreType == AIC) {
        return;
    }






    SetCmpMaskImpl((__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr());
}
# 239 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{







                                                         ;







    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, const uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{







                                                         ;







    VcmpvsImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) U*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0.GetPhyAddr(), src1Scalar,
        cmpMode, mask, repeatTime, repeatParams);
}
# 296 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U>
[aicore] inline void CompareScalar(const LocalTensor<U>& dst, const LocalTensor<T>& src0,
    const T src1Scalar, CMPMODE cmpMode, uint32_t count)
{







                                                         ;



                                                                             ;





    VcmpvsImpl((__attribute__((cce_unif_buff)) U*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0.GetPhyAddr(), src1Scalar, cmpMode, count);
}
# 345 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint64_t mask[],
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using DataPrimType = PrimT<T>;
    using MaskPrimType = PrimT<U>;






    VselImpl((__attribute__((cce_unif_buff)) DataPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) MaskPrimType*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) DataPrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) DataPrimType*)src1.GetPhyAddr(), selMode, mask, repeatTime,
        repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint64_t mask,
    uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using DataPrimType = PrimT<T>;
    using MaskPrimType = PrimT<U>;






    VselImpl((__attribute__((cce_unif_buff)) DataPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) MaskPrimType*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) DataPrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) DataPrimType*)src1.GetPhyAddr(), selMode, mask, repeatTime,
        repeatParams);
}

template <typename T, SELMODE selMode>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;
    SelectCal<PrimType, selMode>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src1.GetPhyAddr(), repeatTime, repeatParams);
}

template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, uint8_t repeatTime, const BinaryRepeatParams& repeatParams)
{
    using DataPrimType = PrimT<T>;
    using MaskPrimType = PrimT<U>;
    SelectCal<DataPrimType, MaskPrimType>((__attribute__((cce_unif_buff)) DataPrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) MaskPrimType*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) DataPrimType*)src0.GetPhyAddr(), repeatTime,
        repeatParams);
}
# 413 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, const LocalTensor<T>& src1, SELMODE selMode, uint32_t count)
{
    using DataPrimType = PrimT<T>;
    using MaskPrimType = PrimT<U>;





    VselImpl((__attribute__((cce_unif_buff)) DataPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) MaskPrimType*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) DataPrimType*)src0.GetPhyAddr(), (__attribute__((cce_unif_buff)) DataPrimType*)src1.GetPhyAddr(), selMode, count);
}
# 447 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint64_t mask[], uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0.GetPhyAddr(),
        src1, selMode, mask, repeatTime, repeatParams);
}


template <typename T, typename U, bool isSetMask>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint64_t mask, uint8_t repeatTime,
    const BinaryRepeatParams& repeatParams)
{






    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)selMask.GetPhyAddr(), (__attribute__((cce_unif_buff)) T*)src0.GetPhyAddr(),
        src1, selMode, mask, repeatTime, repeatParams);
}
# 491 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
template <typename T, typename U>
[aicore] inline void Select(const LocalTensor<T>& dst, const LocalTensor<U>& selMask,
    const LocalTensor<T>& src0, T src1, SELMODE selMode, uint32_t count)
{
    using DataPrimType = PrimT<T>;
    using MaskPrimType = PrimT<U>;





    CheckTensorPos<U>(selMask, Hardware::UB, "selMask", "VECIN / VECCALC / VECOUT", "Select");
    VselImpl((__attribute__((cce_unif_buff)) DataPrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) MaskPrimType*)selMask.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) DataPrimType*)src0.GetPhyAddr(), src1, selMode, count);
}
# 759 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_cmpsel_intf_impl.h"
}
#pragma end_pipe
# 232 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_cmpsel_intf.h" 2
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_mulcast_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_mulcast_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams &repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams &repeatParams);

template <typename T, typename U>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint32_t count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_mulcast_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_mulcast_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_mulcast_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_mulcast_impl.h"
#pragma begin_pipe(V)
namespace AscendC {

template <typename T, typename U>
[aicore] inline void MulCastIntrinsicsImpl(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, const uint8_t repeatTime, const BinaryRepeatParams &repeatParams)
{
    if constexpr (IsSameType<PrimT<T>, int8_t>::value) {
        vmulconv_f162s8((__attribute__((cce_unif_buff)) int8_t *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) half *)src1.GetPhyAddr(), repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride);
    } else {
        vmulconv_f162u8((__attribute__((cce_unif_buff)) uint8_t *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) half *)src0.GetPhyAddr(),
            (__attribute__((cce_unif_buff)) half *)src1.GetPhyAddr(), repeatTime, repeatParams.dstBlkStride, repeatParams.src0BlkStride,
            repeatParams.src1BlkStride, repeatParams.dstRepStride, repeatParams.src0RepStride,
            repeatParams.src1RepStride);
    }
}

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulCastCalc(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<PrimT<U>, isSetMask>(mask);
        MulCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void MulCastCalc(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams &repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<PrimT<T>, isSetMask>(mask[1], mask[0]);
        MulCastIntrinsicsImpl(dst, src0, src1, repeatTime, repeatParams);
    }
}

template <typename T, typename U>
[aicore] inline void MulCastCalc(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint32_t count)
{

                                                                                                                  ;
    if constexpr(g_coreType == AscendC::AIV) {
        BinaryRepeatParams repeatParams;
        repeatParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        set_mask_count();
        set_vector_mask(0, count);
        MulCastIntrinsicsImpl(dst, src0, src1, 1, repeatParams);
        set_mask_norm();
        ResetMask();
    }
}
}
#pragma end_pipe
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_mulcast_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_mulcast_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, typename U, bool isSetMask>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask, const uint8_t repeatTime, const BinaryRepeatParams &repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;


                                                                                                                  ;





    MulCastCalc<DstPrimType, SrcPrimType, isSetMask>(dst, src0, src1, mask, repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint64_t mask[], const uint8_t repeatTime,
    const BinaryRepeatParams &repeatParams)
{
    using DstPrimType = PrimT<T>;
    using SrcPrimType = PrimT<U>;


                                                                                                                  ;





    MulCastCalc<DstPrimType, SrcPrimType, isSetMask>(dst, src0, src1, mask, repeatTime, repeatParams);
}

template <typename T, typename U>
[aicore] inline void MulCast(const LocalTensor<T> &dst, const LocalTensor<U> &src0,
    const LocalTensor<U> &src1, uint32_t count)
{





    MulCastCalc(dst, src0, src1, count);
}
}
#pragma end_pipe
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_mulcast_intf.h" 2
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_bilinearinterpolation_intf.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_bilinearinterpolation_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] inline void BilinearInterpolation(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);

template <typename T>
[aicore] inline void BilinearInterpolation(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_bilinearinterpolation_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_bilinearinterpolation_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t brcbEleNum = 8;

template <typename T>
[aicore] inline void BilinearInterpolationCalc(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    using PrimType = PrimT<T>;
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(PrimType);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0Offset.GetPhyAddr(), src0.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(PrimType);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1, src1.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<PrimType, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(PrimType) / ONE_BLK_SIZE;
    Adds(dst, sharedTmpBufferT, (PrimType)0, mask, vRepeat, addsRepeatParams);
}

template <typename T>
[aicore] inline void BilinearInterpolationCalc(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    using PrimType = PrimT<T>;
    auto sharedTmpBufferT = sharedTmpBuffer.ReinterpretCast<T>();
    GatherRepeatParams gatherbRepeatParams;
    uint8_t innerRepeatTimes = hRepeat * vRepeat;
    constexpr uint32_t eleCntOfOneRep = DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(PrimType);

    GatherbImpl((__attribute__((cce_unif_buff)) uint16_t *)sharedTmpBufferT.GetPhyAddr(), (__attribute__((cce_unif_buff)) uint16_t *)src0.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) uint32_t *)src0Offset.GetPhyAddr(), src0.GetSize(), innerRepeatTimes, gatherbRepeatParams);
    uint32_t posSrc1Brcb = hRepeat * vRepeat * DEFAULT_REPEAT_STRIDE * ONE_BLK_SIZE / sizeof(PrimType);
    BrcbRepeatParams brcbRepeatParams;
    Brcb(sharedTmpBufferT[posSrc1Brcb], src1, src1.GetSize() / brcbEleNum, brcbRepeatParams);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams mulRepeatParams;
    if (repeatMode == false) {
        mulRepeatParams.src0RepStride = 1;
        mulRepeatParams.src0BlkStride = 0;
    }

    SetMaskCount();
    SetVectorMask<PrimType, MaskMode::COUNTER>(0, innerRepeatTimes * eleCntOfOneRep);
    Mul<T, false>(sharedTmpBufferT, sharedTmpBufferT[posSrc1Brcb], sharedTmpBufferT, mask, innerRepeatTimes,
        mulRepeatParams);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams addRepeatParams;
    addRepeatParams.dstRepStride = 0;
    addRepeatParams.src1RepStride = 0;
    for (int i = 0; i < vRepeat; i++) {
        if (hRepeat > 1) {
            Add(sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], sharedTmpBufferT[(i * hRepeat + 1) * eleCntOfOneRep],
                sharedTmpBufferT[i * hRepeat * eleCntOfOneRep], mask, hRepeat - 1, addRepeatParams);
        }
    }
    PipeBarrier<PIPE_V>();

    UnaryRepeatParams addsRepeatParams;
    addsRepeatParams.srcRepStride = hRepeat * DEFAULT_REPEAT_STRIDE;
    addsRepeatParams.dstBlkStride = dstBlkStride;
    addsRepeatParams.dstRepStride = vROffset * sizeof(PrimType) / ONE_BLK_SIZE;
    Adds(dst, sharedTmpBufferT, (PrimType)0, mask, vRepeat, addsRepeatParams);
}
}
#pragma end_pipe
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T>
[aicore] inline void BilinearInterpolation(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask, uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h"
                                                                            ;
                                                                                             ;



                         ;
    BilinearInterpolationCalc(dst, src0, src0Offset, src1, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}

template <typename T>
[aicore] inline void BilinearInterpolation(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    const LocalTensor<uint32_t> &src0Offset, const LocalTensor<T> &src1, uint64_t mask[], uint8_t hRepeat,
    bool repeatMode, uint16_t dstBlkStride, uint16_t vROffset, uint8_t vRepeat,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_bilinearinterpolation_intf_impl.h"
                                                                            ;
                                                                                             ;



                         ;
    BilinearInterpolationCalc(dst, src0, src0Offset, src1, mask, hRepeat,
        repeatMode, dstBlkStride, vROffset, vRepeat, sharedTmpBuffer);
}
}
#pragma end_pipe
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_bilinearinterpolation_intf.h" 2
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_createvecindex_intf.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_createvecindex_intf.h"
namespace AscendC {

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dst, const T &firstValue,
    uint64_t mask, uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dst, const T &firstValue,
    uint64_t mask[], uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride);

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dst, const T &firstValue,
    uint32_t count);
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_createvecindex_intf_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_createvecindex_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_createvecindex_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_createvecindex_impl.h"
namespace AscendC {
constexpr int32_t maskBitNum = 64;

template <typename T>
[aicore] inline void CreateVecIndexOneBlk(const LocalTensor<T> &dst, const T &firstValue, uint32_t count)
{
    for (int32_t i = 0; i < static_cast<int32_t>(count); i++) {
        dst.SetValue(i, static_cast<float>(firstValue) + static_cast<float>(i));
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] inline void CreateVecIndexOneRep(const LocalTensor<T> &dst, const T &firstValue, uint64_t mask[],
    uint16_t dstBlkStride)
{
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int i = 0; i < 2; i++) {
            uint64_t maskValue = 1;
            for (int j = 0; j < maskBitNum; j++) {
                if (mask[i] & maskValue) {
                    uint32_t index = i * maskBitNum + j;
                    uint32_t blkIndex = index / eleCntOfOneBlk;
                    uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + index % eleCntOfOneBlk;
                    dst.SetValue(eleIndex, static_cast<float>(firstValue) + static_cast<float>(i * maskBitNum + j));
                }
                maskValue <<= 1;
            }
        }
    } else {
        uint64_t maskValue = 1;
        for (int j = 0; j < maskBitNum; j++) {
            if (mask[0] & maskValue) {
                uint32_t blkIndex = j / eleCntOfOneBlk;
                uint32_t eleIndex = blkIndex * eleCntOfOneBlk * dstBlkStride + j % eleCntOfOneBlk;
                dst.SetValue(eleIndex, static_cast<float>(firstValue) + static_cast<float>(j));
            }
            maskValue <<= 1;
        }
    }
    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}

template <typename T>
[aicore] inline void CreateVecIndexCalc(LocalTensor<T> &dst, const T &firstValue, uint64_t mask,
    uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (mask < eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dst, firstValue, mask);
    } else {
        CreateVecIndexOneBlk(dst, firstValue, eleCntOfOneBlk);
    }
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    UnaryRepeatParams addsParams;

    int32_t loopN = mask / eleCntOfOneBlk - 1;
    int32_t tailSize = mask % eleCntOfOneBlk;
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < loopN; i++) {
        Adds(dst[(i + 1) * blkEleStride], dst[i * blkEleStride], (T)(eleCntOfOneBlk), eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    int32_t offsetTailDst = mask / eleCntOfOneBlk * eleCntOfOneBlk * dstBlkStride;
    int32_t offsetTailSrc = offsetTailDst - eleCntOfOneBlk * dstBlkStride;
    if (tailSize > 0) {
        Adds(dst[offsetTailDst], dst[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }


    for (int i = 0; i < repeatTime - 1; i++) {
        Adds(dst[(i + 1) * repEleStride], dst[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] inline void CreateVecIndexCalc(LocalTensor<T> &dst, const T &firstValue, uint64_t mask[],
    uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride)
{

    CreateVecIndexOneRep(dst, firstValue, mask, dstBlkStride);

    UnaryRepeatParams addsParams;
    addsParams.dstBlkStride = dstBlkStride;
    addsParams.srcBlkStride = dstBlkStride;
    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));
    int32_t blkEleStride = dstBlkStride * eleCntOfOneBlk;
    int32_t repEleStride = dstRepStride * eleCntOfOneBlk;
    for (int i = 0; i < repeatTime - 1; i++) {
        Adds(dst[(i + 1) * repEleStride], dst[i * repEleStride], (T)(eleCntOfOneRep), mask, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] inline void CreateVecIndexCalc(LocalTensor<T> dst, const T &firstValue, uint32_t count)
{

                                                                                                                     ;

    constexpr int32_t eleCntOfOneBlk = (ONE_BLK_SIZE / sizeof(T));
    if (count <= eleCntOfOneBlk) {
        CreateVecIndexOneBlk(dst, firstValue, count);
        return;
    }
    CreateVecIndexOneBlk(dst, firstValue, static_cast<uint32_t>(eleCntOfOneBlk));

    UnaryRepeatParams addsParams;
    constexpr int32_t eleCntOfOneRep = (ONE_BLK_SIZE * DEFAULT_REPEAT_STRIDE / sizeof(T));

    int32_t loopN = 0, tailSize = 0, offsetTailDst, offsetTailSrc;
    if (count >= eleCntOfOneRep) {
        loopN = DEFAULT_REPEAT_STRIDE - 1;
    } else {
        loopN = count / eleCntOfOneBlk - 1;
        tailSize = count % eleCntOfOneBlk;
    }
    for (int i = 0; i < loopN; i++) {
        Adds(dst[(i + 1) * eleCntOfOneBlk], dst[i * eleCntOfOneBlk], (T)eleCntOfOneBlk, eleCntOfOneBlk, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = count / eleCntOfOneBlk * eleCntOfOneBlk;
    offsetTailSrc = offsetTailDst - eleCntOfOneBlk;
    if (tailSize > 0) {
        Adds(dst[offsetTailDst], dst[offsetTailSrc], (T)eleCntOfOneBlk, tailSize, 1, addsParams);
        PipeBarrier<PIPE_V>();
    }
    if (count <= eleCntOfOneRep) {
        return;
    }

    loopN = count / eleCntOfOneRep - 1;
    tailSize = count % eleCntOfOneRep;
    for (int i = 0; i < loopN; i++) {
        Adds(dst[(i + 1) * eleCntOfOneRep], dst[i * eleCntOfOneRep], (T)(eleCntOfOneRep), eleCntOfOneRep, 1,
            addsParams);
        PipeBarrier<PIPE_V>();
    }
    offsetTailDst = count / eleCntOfOneRep * eleCntOfOneRep;
    offsetTailSrc = offsetTailDst - eleCntOfOneRep;
    if (tailSize > 0) {
        Adds(dst[offsetTailDst], dst[offsetTailSrc], (T)(eleCntOfOneRep), tailSize, 1, addsParams);
    }
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_createvecindex_intf_impl.h" 2
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_createvecindex_intf_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dst, const T &firstValue,
    uint64_t mask, uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dst, firstValue, mask, repeatTime, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> &dst, const T &firstValue,
    uint64_t mask[], uint8_t repeatTime, uint16_t dstBlkStride, uint8_t dstRepStride)
{

                                                                                                                     ;





    CreateVecIndexCalc(dst, firstValue, mask, repeatTime, dstBlkStride, dstRepStride);
}

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V"))) void CreateVecIndex(LocalTensor<T> dst, const T &firstValue,
    uint32_t count)
{





    CreateVecIndexCalc(dst, firstValue, count);
}
}
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_createvecindex_intf.h" 2
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_ternary_scalar_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_ternary_scalar_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);

template <typename T, typename U, bool isSetMask = true>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 54 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_ternary_scalar_intf.h"
template <typename T, typename U>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    const int32_t& count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_ternary_scalar_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_ternary_scalar_impl.h"
namespace AscendC {

template <typename T, typename U>
[aicore] inline void AxpyIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, U scalarValue, const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{


                                                        ;
    vaxpy(dst, src, scalarValue, repeatTime, repeatParams.dstBlkStride, repeatParams.srcBlkStride,
        repeatParams.dstRepStride, repeatParams.srcRepStride);
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        if constexpr (isSetMask) {
            if (sizeof(T) > sizeof(U)) {
                AscendCUtils::SetMask<T>(mask);
            } else {
                AscendCUtils::SetMask<U>(mask);
            }
        }
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, typename U, bool isSetMask = true>
[aicore] inline void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AxpyIntrinsicsImpl(dst, src, scalarValue, repeatTime, repeatParams);
    }
}


template <typename T, typename U>
[aicore] inline void AxpyImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) U* src, const U& scalarValue, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        SetMaskCount();
        AscendCUtils::SetMask<U>(0, count);
        if constexpr (sizeof(T) > sizeof(U)) {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
        } else {
            AxpyIntrinsicsImpl(dst, src, scalarValue, 1,
                { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        ResetMask();
        SetMaskNorm();
    }
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h"
template <typename T, typename U, bool isSetMask>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    uint64_t mask, const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}

template <typename T, typename U, bool isSetMask>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    uint64_t mask[], const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{






    AxpyImpl<T, U, isSetMask>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)src.GetPhyAddr(), scalarValue, mask,
        repeatTime, repeatParams);
}
# 88 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_ternary_scalar_intf_impl.h"
template <typename T, typename U>
[aicore] inline void Axpy(const LocalTensor<T>& dst, const LocalTensor<U>& src, const U& scalarValue,
    const int32_t& count)
{





    AxpyImpl<T, U>((__attribute__((cce_unif_buff)) T*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) U*)src.GetPhyAddr(), scalarValue, count);
}
}
#pragma end_pipe
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_ternary_scalar_intf.h" 2
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 57 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 73 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 87 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 103 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 117 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 133 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 147 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 163 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 177 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t& count);
# 194 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 208 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 224 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 238 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
# 254 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T, bool isSetMask = true>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
template <typename T, bool isSetMask = true>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams);
# 268 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h"
template <typename T>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_unary_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_unary_impl.h"
namespace AscendC {

template <typename T>
[aicore] inline void ReluIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float, int32_t>(), "Failed to check dtype in Relu, current api support dtype "
        "combination is src and dst both: half / float / int32_t.");
    vrelu(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void ExpIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Exp, current api support dtype combination "
        "is src and dst both: half / float.");
    vexp(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void LnIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Ln, current api support dtype combination "
        "is src and dst both: half / float.");
    vln(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void AbsIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Abs, current api support dtype combination "
        "is src and dst both: half / float.");
    vabs(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void ReciprocalIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Reciprocal, current api support dtype "
        "combination is src and dst both: half / float.");
    vrec(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void RsqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Rsqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vrsqrt(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void SqrtIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, half, float>(), "Failed to check dtype in Sqrt, current api support dtype "
        "combination is src and dst both: half / float.");
    vsqrt(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}


template <typename T>
[aicore] inline void NotIntrinsicsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    static_assert(SupportType<T, int16_t, uint16_t>(), "Failed to check dtype in Not, current api support dtype "
        "combination is src and dst both: int16_t / uint16_t.");
    vnot(dst, src, repeatTime, static_cast<uint16_t>(repeatParams.dstBlkStride), static_cast<uint16_t>(repeatParams.srcBlkStride),
        static_cast<uint8_t>(repeatParams.dstRepStride), static_cast<uint8_t>(repeatParams.srcRepStride));
}



template <typename T, bool isSetMask = true>
[aicore] inline void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReluIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReluIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void ReluImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vrelu(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask[], const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ExpIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const uint64_t mask, const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ExpIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void ExpImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vexp(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        LnIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        LnIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void LnImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vln(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        AbsIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        AbsIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void AbsImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vabs(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        ReciprocalIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        ReciprocalIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void ReciprocalImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vrec(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        RsqrtIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        RsqrtIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void RsqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vrsqrt(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        SqrtIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        SqrtIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void SqrtImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vsqrt(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}



template <typename T, bool isSetMask = true>
[aicore] inline void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask[], uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask[1], mask[0]);
        NotIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}

template <typename T, bool isSetMask = true>
[aicore] inline void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint64_t mask, uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    if constexpr(g_coreType == AscendC::AIV) {
        AscendCUtils::SetMask<T, isSetMask>(mask);
        NotIntrinsicsImpl(dst, src, repeatTime, repeatParams);
    }
}


template <typename T> [aicore] inline void NotImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, const int32_t& count)
{
    if constexpr(g_coreType == AscendC::AIV) {
        set_mask_count();
        set_vector_mask(0, count);
        vnot(dst, src, 1, static_cast<uint16_t>(DEFAULT_BLK_STRIDE), static_cast<uint16_t>(DEFAULT_BLK_STRIDE),
            static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
        set_mask_norm();
        set_vector_mask(static_cast<uint64_t>(-1), static_cast<uint64_t>(-1));
    }
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReluImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 94 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Relu(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    ReluImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 151 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ExpImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 203 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Exp(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    ExpImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 259 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    LnImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 309 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Ln(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    LnImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 335 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    AbsImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 373 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Abs(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    AbsImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 450 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    ReciprocalImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(),
        (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), mask, repeatTime, repeatParams);
}
# 503 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Reciprocal(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const int32_t& count)
{
    using PrimType = PrimT<T>;





    ReciprocalImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 562 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    RsqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 614 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Rsqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    RsqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 672 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    SqrtImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 724 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Sqrt(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    SqrtImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 750 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T, bool isSetMask>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask[],
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
template <typename T, bool isSetMask>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, uint64_t mask,
    const uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;







    NotImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(),
        mask, repeatTime, repeatParams);
}
# 788 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
template <typename T>
[aicore] inline void Not(const LocalTensor<T>& dst, const LocalTensor<T>& src, const int32_t& count)
{
    using PrimType = PrimT<T>;





    NotImpl((__attribute__((cce_unif_buff)) PrimType*)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType*)src.GetPhyAddr(), count);
}
# 821 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_unary_intf_impl.h"
}
#pragma end_pipe
# 274 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_unary_intf.h" 2
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vpadding_intf.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vpadding_intf.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask = true>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams);

template <typename T, bool isSetMask = true>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams);

template <typename T>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint32_t count);
}
#pragma end_pipe

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vpadding_intf_impl.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vpadding_intf_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_vpadding_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/impl/basic_api/dav_c220/kernel_operator_vec_vpadding_impl.h"
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] inline void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask, uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T, bool isSetMask>
[aicore] inline void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint64_t mask[], uint8_t repeatTime, const UnaryRepeatParams& repeatParams)
{
                                                      ;
}

template <typename T>
[aicore] inline void VectorPaddingImpl(__attribute__((cce_unif_buff)) T* dst, __attribute__((cce_unif_buff)) T* src, uint8_t padMode, bool padSide,
    const uint32_t count)
{
                                                      ;
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vpadding_intf_impl.h" 2
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_vec_vpadding_intf_impl.h"
#pragma begin_pipe(V)
namespace AscendC {
template <typename T, bool isSetMask>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint64_t mask, const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    VectorPaddingImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(), padMode,
        padSide, mask, repeatTime, repeatParams);
}

template <typename T, bool isSetMask>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint64_t mask[], const uint8_t repeatTime,
    const UnaryRepeatParams& repeatParams)
{
    using PrimType = PrimT<T>;






    VectorPaddingImpl<PrimType, isSetMask>((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(), padMode,
        padSide, mask, repeatTime, repeatParams);
}

template <typename T>
[aicore] inline void VectorPadding(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const uint8_t padMode, const bool padSide, const uint32_t count)
{
    using PrimType = PrimT<T>;





    VectorPaddingImpl((__attribute__((cce_unif_buff)) PrimType *)dst.GetPhyAddr(), (__attribute__((cce_unif_buff)) PrimType *)src.GetPhyAddr(), padMode, padSide,
        count);
}
}
#pragma end_pipe
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_vec_vpadding_intf.h" 2
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_limits_intf.h" 1
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_atomic_intf.h" 1
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_set_atomic_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_set_atomic_intf.h"
namespace AscendC {
template <typename T>
[aicore] inline void SetAtomicType();

template <typename T>
[aicore] inline void SetAtomicAdd();

[aicore] inline void SetAtomicNone();

template <typename T>
[aicore] inline void SetAtomicMax();

template <typename T>
[aicore] inline void SetAtomicMin();
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_set_atomic_intf_impl.h" 1
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/../../impl/basic_api/kernel_operator_set_atomic_intf_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void SetAtomicType()
{
    SetAtomicTypeImpl<T>();
}

template <typename T>
[aicore] inline void SetAtomicAdd()
{
    SetAtomicAddImpl<T>();
}

[aicore] inline void SetAtomicNone()
{
    SetAtomicNoneImpl();
}

template <typename T>
[aicore] inline void SetAtomicMax()
{
    SetAtomicMaxImpl<T>();
}

template <typename T>
[aicore] inline void SetAtomicMin()
{
    SetAtomicMinImpl<T>();
}
}
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_set_atomic_intf.h" 2
# 50 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/dropout.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/dropout.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_c220_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_c220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_membase_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_membase_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_membase_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/../../common/check.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/../../common/check.h"
namespace AscendC {

template <typename T>
[aicore] inline void CheckTensorPosition(const LocalTensor<T> &checkTensor, __attribute__((cce_global)) const char* tensorInfo,
    __attribute__((cce_global)) const char* supportPosInfo)
{
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/../../common/check.h"
}

template <typename T>
[aicore] inline void CheckCalCount(const uint32_t calCount, __attribute__((cce_global)) const char* calCountInfo,
    const LocalTensor<T> &checkTensor, __attribute__((cce_global)) const char* tensorInfo, __attribute__((cce_global)) const char* apiInfo)
{







}

[aicore] inline void CheckTmpBufferSize(const uint32_t checkBufferSize, const uint32_t compBufferSize,
    const uint32_t tmpBufferSize)
{



      ;
}

}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_membase_impl.h" 2

namespace AscendC {
template <typename T> [aicore] inline void DropOutBitModeInit(const LocalTensor<T>& sharedTmpBuffer)
{

                                                                                        ;

    ResetMask();
    LocalTensor<int16_t> stackBuffer = sharedTmpBuffer.template ReinterpretCast<int16_t>();


    UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(stackBuffer, stackBuffer, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetCmpMask<int16_t>(stackBuffer);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DropOutBitModeInit()
{
    LocalTensor<int16_t> stackBuffer;
    bool ans = PopStackBuffer<int16_t, TPosition::LCM>(stackBuffer);

                                                                                                                      ;
    DropOutBitModeInit(stackBuffer);
}

template <typename T, bool isInitBitMode = false>
[aicore] inline void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, dataSize);

    const BinaryRepeatParams binaryParams;
    Select<T, uint8_t>(dstLocal, maskLocal, srcLocal, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstLocal, dstLocal, static_cast<T>(divValue), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();
}

[aicore] inline void DropOutByteModeCalc(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const half divValue, const DropOutParams<half, float>& params)
{
    const LocalTensor<half>& stackBuffer = params.firstLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, uint8_t, false>(stackBuffer, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<half, false>(dstLocal, stackBuffer, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<half, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DropOutByteModeCalc(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float divValue, const DropOutParams<half, float>& params)
{
    const LocalTensor<half>& firstLocal = params.firstLocal;
    const LocalTensor<float>& secondLocal = params.secondLocal;

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, uint8_t, false>(firstLocal, maskLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(secondLocal, firstLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, params.repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(dstLocal, secondLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    Muls<float, false>(dstLocal, dstLocal, divValue, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DropOutByteModeSetTmpBuffer(LocalTensor<half>& firstLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<half, float>& params)
{
    firstLocal = sharedTmpBuffer.ReinterpretCast<half>();

    params.stackBufferSize = firstLocal.GetSize();
    params.stackBufferSize = params.stackBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;

    params.maxRepeatSize = MAX_REPEAT_HALF_SIZE;
    params.oneRepeatSize = ONE_REPEAT_HALF_SIZE;
}

[aicore] inline void DropOutByteModeSetTmpBuffer(LocalTensor<half>& firstLocal, LocalTensor<float>& secondLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, DropOutParams<half, float>& params)
{
    uint32_t popBufferLen = sharedTmpBuffer.GetSize();
    constexpr uint32_t cutBufLen = sizeof(float) + sizeof(half);
    params.stackBufferSize = popBufferLen / cutBufLen / ONE_BLK_SIZE * ONE_BLK_SIZE;

    firstLocal = sharedTmpBuffer.ReinterpretCast<half>();
    firstLocal.SetSize(params.stackBufferSize);

    secondLocal = sharedTmpBuffer[params.stackBufferSize * sizeof(half)].ReinterpretCast<float>();
    secondLocal.SetSize(params.stackBufferSize);

    params.maxRepeatSize = MAX_REPEAT_FLOAT_SIZE;
    params.oneRepeatSize = ONE_REPEAT_FLOAT_SIZE;
}

template <typename T>
[aicore] inline void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const uint32_t dataSize)
{
    DropOutParams<half, float> params;
    params.dataSize = dataSize;

    if constexpr (sizeof(T) == sizeof(half)) {
        DropOutByteModeSetTmpBuffer(params.firstLocal, sharedTmpBuffer, params);
    } else {
        DropOutByteModeSetTmpBuffer(params.firstLocal, params.secondLocal, sharedTmpBuffer, params);
    }



    const uint32_t round = params.dataSize / params.stackBufferSize;
    const uint32_t tail = params.dataSize % params.stackBufferSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackBufferSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
        offset = offset + params.stackBufferSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        DropOutByteModeCalc(dstLocal[offset], srcLocal[offset], maskLocal[offset], divValue, params);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_c220_impl.h" 2

namespace AscendC {
template <typename T, bool isInitBitMode = false>
[aicore] inline void DropOutBitMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    if constexpr (isInitBitMode == false) {
        DropOutBitModeInit(sharedTmpBuffer);
    }

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / ONE_BYTE_BIT_SIZE / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutBitMode<T, true>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue,
        info.firstAxis * info.srcLastAxis);
}

template <typename T>
[aicore] inline void DropOutByteMode(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T divValue,
    const DropOutShapeInfo& info)
{
    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = info.firstAxis;
    reducev2Params.src0RepeatStride = info.maskLastAxis / ONE_BLK_SIZE;

    LocalTensor<uint16_t> maskTmpLocal = maskLocal.ReinterpretCast<uint16_t>();

    const uint32_t mask = info.srcLastAxis / sizeof(uint16_t);
    uint64_t rsvdCnt = 0;

    GatherMask<uint16_t>(maskTmpLocal, maskTmpLocal, REDUCEV2_MODE_SEVEN, true, mask, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();

    DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, divValue, info.firstAxis * info.srcLastAxis);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_impl.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/../../api_check/kernel_api_check.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/../../../impl/adv_api/detail/filter/dropout/dropout_impl.h" 2

namespace AscendC {
#pragma begin_pipe(V)
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] inline void DropOutOpt(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    float divValue = 1.0;
    divValue = divValue / keepProb;

    const uint32_t dataSize = info.firstAxis * info.srcLastAxis;
    T actualVal;
    actualVal = static_cast<T>(divValue);
    if constexpr (dropOutMode == DROPOUT_MODE_BYTE_MISALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal, info);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BYTE_ALIGN) {
        DropOutByteMode(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal, dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_ALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal,
            dataSize);
    } else if constexpr (dropOutMode == DROPOUT_MODE_BIT_MISALIGN) {
        DropOutBitMode<T, isInitBitMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, actualVal,
            info);
    }
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] inline void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{

                                                                         ;
                                 ;

    if constexpr (dropOutMode != 0) {
        DropOutOpt<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
    } else if (info.srcLastAxis < info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    } else if (info.srcLastAxis == info.maskLastAxis) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BYTE_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else if (info.srcLastAxis == (info.maskLastAxis * ONE_BYTE_BIT_SIZE)) {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_ALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb,
            info);
    } else {
        DropOutOpt<T, isInitBitMode, DROPOUT_MODE_BIT_MISALIGN>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer,
            keepProb, info);
    }
                                ;
}

template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] inline void DropOutImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
#pragma end_pipe
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/dropout.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] inline void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const float keepProb,
    const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, sharedTmpBuffer, keepProb, info);
}
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/filter/dropout.h"
template <typename T, bool isInitBitMode = false, uint32_t dropOutMode = 0>
[aicore] inline void DropOut(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& maskLocal, const float keepProb, const DropOutShapeInfo& info)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DropOutImpl<T, isInitBitMode, dropOutMode>(dstLocal, srcLocal, maskLocal, keepProb, info);
}
#pragma end_pipe
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_common_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/kernel_tiling/kernel_tiling.h" 1








namespace AscendC {
namespace tiling {
#pragma pack(push, 8)
struct LogSoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct SoftMaxTiling {
    uint32_t srcM = 0;
    uint32_t srcK = 0;
    uint32_t srcSize = 0;
    uint32_t outMaxM = 0;
    uint32_t outMaxK = 0;
    uint32_t outMaxSize = 0;
    uint32_t splitM = 0;
    uint32_t splitK = 0;
    uint32_t splitSize = 0;
    uint32_t reduceM = 0;
    uint32_t reduceK = 0;
    uint32_t reduceSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t tailSplitSize = 0;
    uint32_t tailReduceSize = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TConv3DApiTiling {
    uint64_t orgDo = 0;
    uint32_t orgCo = 0;
    uint64_t orgHo = 0;
    uint64_t orgWo = 0;
    uint64_t orgDi = 0;
    uint32_t orgCi = 0;
    uint64_t orgHi = 0;
    uint64_t orgWi = 0;
    uint32_t kernelD = 0;
    uint32_t kernelH = 0;
    uint32_t kernelW = 0;
    uint64_t singleCoreDo = 0;
    uint32_t singleCoreCo = 0;
    uint64_t singleCoreM = 0;
    uint32_t singleCoreGroupOpt = 0;
    uint32_t groups = 0;
    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t strideD = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t dilationD = 0;
    uint32_t padHead = 0;
    uint32_t padTail = 0;
    uint32_t padUp = 0;
    uint32_t padDown = 0;
    uint32_t padLeft = 0;
    uint32_t padRight = 0;
    uint32_t mL0 = 0;
    uint32_t kL0 = 0;
    uint32_t nL0 = 0;
    uint32_t kAL1 = 0;
    uint32_t kAL1Tail = 0;
    uint32_t kBL1 = 0;
    uint32_t kBL1Tail = 0;
    uint32_t nBL1 = 0;
    uint32_t mAL1 = 0;
    uint32_t kBL1DivK0 = 0;
    uint32_t kBL1TailDivK0 = 0;
    uint32_t nBL1DivnL0 = 0;
    uint32_t mAL1DivmL0 = 0;
    uint32_t cin1InAL1 = 0;
    uint32_t cin1InAL1Tail = 0;
    uint32_t nL0xk0 = 0;
    uint64_t kL0xorgCoAlignN0 = 0;
    uint64_t kernelHxkernelW = 0;
    uint64_t cin1xOriHixOriWixk0 = 0;
    uint64_t oriHixOriWixk0 = 0;
    uint64_t oriWixk0 = 0;
    uint64_t orgHixWi = 0;
    uint64_t orgHoxWo = 0;
    uint32_t pBufferFlag = 0;
    uint32_t groupOpt = 0;
    uint32_t cinOpt = 0;
    uint32_t coutOpt = 0;
    int8_t offsetx = 0;
    uint8_t bl1FullLoad = 0;
    uint8_t al1FullLoad = 0;
    uint8_t bl1BypassFlag = 0;
    uint8_t iterateMNOrder = 0;
    uint8_t biasFullLoadFlag = 0;
    uint8_t fixpParamsFullLoadFlag = 0;
    uint8_t hf32Enable = 0;
    uint8_t hf32TransMode = 0;
    uint8_t resvered1 = 0;
    uint16_t resvered2 = 0;
    uint32_t resvered3 = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TConv3DBpFilterTiling {
    uint32_t batch = 0;
    uint32_t cin = 0;
    uint32_t cout = 0;
    uint32_t cin1G = 0;
    uint32_t cout1G = 0;
    uint32_t dout = 0;
    uint32_t ho = 0;
    uint32_t wo = 0;
    uint32_t di = 0;
    uint32_t hi = 0;
    uint32_t wi = 0;
    uint32_t dk = 0;
    uint32_t hk = 0;
    uint32_t wk = 0;
    uint32_t group = 0;
    uint32_t strideD = 0;
    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t padFront = 0;
    uint32_t padBack = 0;
    uint32_t padUp = 0;
    uint32_t padDown = 0;
    uint32_t padLeft = 0;
    uint32_t padRight = 0;
    uint32_t dilationD = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t channelSize = 0;
    uint32_t al0Pbuffer = 0;
    uint32_t bl0Pbuffer = 0;
    uint32_t cl0Pbuffer = 0;
    uint32_t al1Pbuffer = 0;
    uint32_t bl1Pbuffer = 0;
    uint32_t baseM = 0;
    uint32_t baseK = 0;
    uint32_t baseN = 0;
    uint32_t m0 = 0;
    uint32_t k0 = 0;
    uint32_t n0 = 0;
    uint32_t stepM = 0;
    uint32_t stepN = 0;
    uint32_t stepKa = 0;
    uint32_t stepKb = 0;
    uint32_t iterateOrder = 0;
    uint32_t bl1Bound = 0;
    uint32_t hf32Flag = 0;
    uint32_t singleCoreDk = 0;
    uint32_t singleCoreGroup = 0;
    uint32_t singleCoreCout = 0;
    uint32_t singleCoreHo = 0;
    uint64_t singleCoreBatch = 0;
    uint64_t singleCoreCin = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Conv3DBpFilterParams {
    uint32_t totalL1Size = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TConv3DBpFilterBasicBlockTiling {
    uint32_t singleCoreM = 0;
    uint32_t singleCoreN = 0;
    uint32_t singleCoreK = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Conv3DBackpropFilterTilingData {
    Conv3DBpFilterParams params;
    TConv3DBpFilterTiling dwTiling;
    TConv3DBpFilterBasicBlockTiling basicBlockTiling;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TConv3DBackpropInputTiling {
    uint32_t batch = 0;
    uint32_t cin = 0;
    uint32_t cout = 0;
    uint32_t cout1 = 0;
    uint32_t cin1 = 0;
    uint32_t cout1G = 0;
    uint32_t cin1G = 0;
    uint32_t c0 = 0;
    uint32_t c0Bits = 0;
    uint32_t dout = 0;
    uint32_t ho = 0;
    uint32_t wo = 0;
    uint32_t di = 0;
    uint32_t hi = 0;
    uint32_t wi = 0;
    uint32_t dk = 0;
    uint32_t hk = 0;
    uint32_t wk = 0;
    uint32_t group = 0;
    uint32_t strideD = 0;
    uint32_t strideH = 0;
    uint32_t strideW = 0;
    uint32_t padFront = 0;
    uint32_t padBack = 0;
    uint32_t padUp = 0;
    uint32_t padDown = 0;
    uint32_t padLeft = 0;
    uint32_t padRight = 0;
    uint32_t backpropPadTail = 0;
    uint32_t backpropPadUp = 0;
    uint32_t backpropPadDown = 0;
    uint32_t backpropPadLeft = 0;
    uint32_t backpropPadRight = 0;
    uint32_t dilationD = 0;
    uint32_t dilationH = 0;
    uint32_t dilationW = 0;
    uint32_t al0Pbuffer = 0;
    uint32_t bl0Pbuffer = 0;
    uint32_t cl0Pbuffer = 0;
    uint32_t al1Pbuffer = 0;
    uint32_t bl1Pbuffer = 0;
    uint32_t singleCoreGroup = 0;
    uint32_t singleCoreCout = 0;
    uint32_t singleCoreCout1 = 0;
    uint32_t singleCoreCin1 = 0;
    uint32_t singleCoreDin = 0;
    uint32_t singleCoreHo = 0;
    uint32_t baseM = 0;
    uint32_t baseK = 0;
    uint32_t baseN = 0;
    uint32_t baseD = 0;
    uint32_t baseBatch = 0;
    uint32_t baseGroup = 0;
    uint32_t stepM = 0;
    uint32_t stepN = 0;
    uint32_t stepKa = 0;
    uint32_t stepKb = 0;
    uint32_t stepBatch = 0;
    uint32_t stepGroup = 0;
    uint32_t iterateOrder = 0;
    int32_t hf32Flag = 0;
    int32_t initOutputFlag = 0;
    int32_t reserved = 0;
    uint64_t singleCoreBatch = 0;
    uint64_t singleCoreM = 0;
    uint64_t singleCoreCin = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Conv3DBackpropInputTilingData {
    TConv3DBackpropInputTiling conv3DDxTiling;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2ServerCfg {
    uint32_t version = 0;
    uint8_t debugMode = 0;
    uint8_t sendArgIndex = 0;
    uint8_t recvArgIndex = 0;
    uint8_t commOutArgIndex = 0;
    uint8_t reserved[8] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2HcommCfg {
    uint8_t skipLocalRankCopy = 0;
    uint8_t skipBufferWindowCopy = 0;
    uint8_t stepSize = 0;
    char reserved[13] = {};
    char groupName[128] = {};
    char algConfig[128] = {};
    uint32_t opType = 0;
    uint32_t reduceType = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2InitTiling {
    uint8_t reserved[64] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct Mc2CcTiling {
    uint8_t reserved[512] = {};
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TCubeTiling {
    int32_t usedCoreNum = 0;
    int32_t M = 0;
    int32_t N = 0;
    int32_t Ka = 0;
    int32_t Kb = 0;
    int32_t singleCoreM = 0;
    int32_t singleCoreN = 0;
    int32_t singleCoreK = 0;
    int32_t baseM = 0;
    int32_t baseN = 0;
    int32_t baseK = 0;
    int32_t depthA1 = 0;
    int32_t depthB1 = 0;
    int32_t stepM = 0;
    int32_t stepN = 0;
    int32_t isBias = 0;
    int32_t transLength = 0;
    int32_t iterateOrder = 0;
    int32_t shareMode = 0;
    int32_t shareL1Size = 0;
    int32_t shareL0CSize = 0;
    int32_t shareUbSize = 0;
    int32_t batchM = 0;
    int32_t batchN = 0;
    int32_t singleBatchM = 0;
    int32_t singleBatchN = 0;
    int32_t stepKa = 0;
    int32_t stepKb = 0;
    int32_t depthAL1CacheUB = 0;
    int32_t depthBL1CacheUB = 0;
    int32_t dbL0A = 0;
    int32_t dbL0B = 0;
    int32_t dbL0C = 0;
    int32_t ALayoutInfoB = 0;
    int32_t ALayoutInfoS = 0;
    int32_t ALayoutInfoN = 0;
    int32_t ALayoutInfoG = 0;
    int32_t ALayoutInfoD = 0;
    int32_t BLayoutInfoB = 0;
    int32_t BLayoutInfoS = 0;
    int32_t BLayoutInfoN = 0;
    int32_t BLayoutInfoG = 0;
    int32_t BLayoutInfoD = 0;
    int32_t CLayoutInfoB = 0;
    int32_t CLayoutInfoS1 = 0;
    int32_t CLayoutInfoN = 0;
    int32_t CLayoutInfoG = 0;
    int32_t CLayoutInfoS2 = 0;
    int32_t BatchNum = 0;
    int32_t mxTypePara = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct BatchNormTiling {
    uint32_t originalBLength = 0;
    uint32_t meanVarSize = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t shCurLength = 0;
    float firstDimValueBack = 0;
    uint32_t castHalfRepStride = 0;
    uint32_t shCurLengthBlockNum = 0;
    uint32_t castHalfOutRepStride = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct DeepNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct GroupNormTiling {
    uint32_t n = 0;
    uint32_t c = 0;
    uint32_t hw = 0;
    uint32_t g = 0;
    uint32_t d = 0;
    uint32_t hwAlignSize = 0;
    uint32_t dhwAlignSize = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float factor = 0;
    bool smallShape = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradBetaTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t bshLength = 0;
    uint32_t bsLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t bsTailSize = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t gammaTempTensorPos = 0;
    uint32_t betaTempTensorPos = 0;
    uint32_t inputDyTmpTensorPos = 0;
    uint32_t resForGammaTmpTensorPos = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormGradTiling {
    uint32_t stackBufferSize = 0;
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t oneCalSize = 0;
    uint32_t nohCalSize = 0;
    uint32_t loopNum = 0;
    uint32_t tailSize = 0;
    uint32_t nohTailSize = 0;
    uint32_t tmpTensorBSHPos = 0;
    uint32_t tmpTensorBSHSize = 0;
    uint32_t pdVarTensorPos = 0;
    uint32_t pdVarTensorSize = 0;
    uint32_t pdMeanTensorPos = 0;
    uint32_t pdMeanTensorSize = 0;
    uint32_t x1TensorPos = 0;
    uint32_t x1TensorSize = 0;
    uint32_t x2TensorPos = 0;
    uint32_t x2TensorSize = 0;
    uint32_t x3TensorPos = 0;
    uint32_t x3TensorSize = 0;
    uint32_t tmpTensorPos = 0;
    uint32_t tmpTensorSize = 0;
    uint32_t tmpTensor1Pos = 0;
    uint32_t tmpTensor1Size = 0;
    uint32_t tmpTensor2Pos = 0;
    uint32_t tmpTensor2Size = 0;
    uint32_t lastDimValueBack = 0;
    uint32_t lastDimValueBackMulTwo = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t meanTmpTensorPos = 0;
    uint32_t meanTmpTensorSize = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    float lastDimValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct LayerNormSeparateTiling {
    uint32_t aLength = 0;
    uint32_t rLength = 0;
    uint32_t halfAddRepeatTimes = 0;
    uint32_t rHeadLength = 0;
    float k2Rec = 0;
    float k2RRec = 0;
    uint32_t inputXSize = 0;
    uint32_t meanVarSize = 0;
    uint32_t numberOfTmpBuf = 0;
    uint32_t varianceTmpTensorPos = 0;
    uint32_t varianceTmpTensorSize = 0;
    uint32_t tmpBufSize = 0;
    uint32_t oneTmpSize = 0;
    uint32_t firstTmpStartPos = 0;
    uint32_t secondTmpStartPos = 0;
    uint32_t thirdTmpStartPos = 0;
    uint32_t loopRound = 0;
    uint32_t inputRoundSize = 0;
    uint32_t inputTailSize = 0;
    uint32_t inputTailPos = 0;
    uint32_t meanVarRoundSize = 0;
    uint32_t meanVarTailSize = 0;
    uint32_t meanVarTailPos = 0;
    uint32_t arCurLength = 0;
    uint32_t aCurLength = 0;
    float rValueBack = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct RmsNormTiling {
    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;
    float reciprocalOfHLength = 0;
    uint32_t mainBshLength = 0;
    uint32_t mainBsLength = 0;
    uint32_t mainBsLengthAlign = 0;
    uint32_t loopRound = 0;
    uint32_t inputTailPos = 0;
    uint32_t tailBshLength = 0;
    uint32_t tailBsLength = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct UnPadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct PadTiling {
    uint32_t srcHeight = 0;
    uint32_t srcWidth = 0;
    uint32_t srcOriWidth = 0;
    uint32_t widthWithoutLastBlock = 0;
    uint32_t blocksPerRow = 0;
    uint32_t heightTiling = 0;
    uint32_t heightFractal = 0;
    uint32_t heightFractalTail = 0;
    uint32_t mainLoopOffset = 0;
    uint32_t tailBlockOffset = 0;
    uint32_t tmpBuffer1BlockNum = 0;
    uint32_t tmpBuffer1RowNum = 0;
    uint32_t tmpBuffer2Offset = 0;
    uint32_t widthTiling = 0;
    uint32_t widthFractal = 0;
    uint32_t widthFractalTail = 0;
    uint32_t widthFractalTailAlingned = 0;
    uint32_t brcbTiling = 0;
    uint32_t brcbFractal = 0;
    uint32_t brcbFractalTail = 0;
    uint32_t maxRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimes = 0;
    uint32_t brcbTilingRepeatTimesTail = 0;
    uint32_t brcbFractalTailRepeatTimes = 0;
    uint32_t brcbFractalTailRepeatTimesTail = 0;
    uint32_t reserved = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct TopkTiling {
    int32_t tmpLocalSize = 0;
    int32_t allDataSize = 0;
    int32_t innerDataSize = 0;
    uint32_t sortRepeat = 0;
    int32_t mrgSortRepeat = 0;
    int32_t kAlignFourBytes = 0;
    int32_t kAlignTwoBytes = 0;
    int32_t maskOffset = 0;
    int32_t maskVreducev2FourBytes = 0;
    int32_t maskVreducev2TwoBytes = 0;
    int32_t mrgSortSrc1offset = 0;
    int32_t mrgSortSrc2offset = 0;
    int32_t mrgSortSrc3offset = 0;
    int32_t mrgSortTwoQueueSrc1Offset = 0;
    int32_t mrgFourQueueTailPara1 = 0;
    int32_t mrgFourQueueTailPara2 = 0;
    int32_t srcIndexOffset = 0;
    uint32_t copyUbToUbBlockCount = 0;
    int32_t topkMrgSrc1MaskSizeOffset = 0;
    int32_t topkNSmallSrcIndexOffset = 0;
    uint32_t vreduceValMask0 = 0;
    uint32_t vreduceValMask1 = 0;
    uint32_t vreduceIdxMask0 = 0;
    uint32_t vreduceIdxMask1 = 0;
    uint16_t vreducehalfValMask0 = 0;
    uint16_t vreducehalfValMask1 = 0;
    uint16_t vreducehalfValMask2 = 0;
    uint16_t vreducehalfValMask3 = 0;
    uint16_t vreducehalfValMask4 = 0;
    uint16_t vreducehalfValMask5 = 0;
    uint16_t vreducehalfValMask6 = 0;
    uint16_t vreducehalfValMask7 = 0;
};
#pragma pack(pop)
#pragma pack(push, 8)
struct ConfusionTransposeTiling {
    uint32_t param0 = 0;
    uint32_t param1 = 0;
    uint32_t param2 = 0;
    uint32_t param3 = 0;
    uint32_t param4 = 0;
    uint32_t param5 = 0;
    uint32_t param6 = 0;
    uint32_t param7 = 0;
    uint32_t param8 = 0;
    uint32_t param9 = 0;
    uint32_t param10 = 0;
    uint32_t param11 = 0;
    uint32_t param12 = 0;
    uint32_t param13 = 0;
    uint32_t param14 = 0;
    uint32_t param15 = 0;
    uint32_t param16 = 0;
    uint32_t param17 = 0;
};
#pragma pack(pop)
}
}

using AscendC::tiling::LogSoftMaxTiling;
using AscendC::tiling::SoftMaxTiling;
using AscendC::tiling::TConv3DApiTiling;
using AscendC::tiling::TConv3DBpFilterTiling;
using AscendC::tiling::Conv3DBpFilterParams;
using AscendC::tiling::TConv3DBpFilterBasicBlockTiling;
using AscendC::tiling::Conv3DBackpropFilterTilingData;
using AscendC::tiling::TConv3DBackpropInputTiling;
using AscendC::tiling::Conv3DBackpropInputTilingData;
using AscendC::tiling::Mc2ServerCfg;
using AscendC::tiling::Mc2HcommCfg;
using AscendC::tiling::Mc2InitTiling;
using AscendC::tiling::Mc2CcTiling;
using AscendC::tiling::TCubeTiling;
using AscendC::tiling::BatchNormTiling;
using AscendC::tiling::DeepNormTiling;
using AscendC::tiling::GroupNormTiling;
using AscendC::tiling::LayerNormGradBetaTiling;
using AscendC::tiling::LayerNormGradTiling;
using AscendC::tiling::LayerNormTiling;
using AscendC::tiling::LayerNormSeparateTiling;
using AscendC::tiling::RmsNormTiling;
using AscendC::tiling::UnPadTiling;
using AscendC::tiling::PadTiling;
using AscendC::tiling::TopkTiling;
using AscendC::tiling::ConfusionTransposeTiling;
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_common_impl.h" 2




# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void SigmoidIntrinsicsImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& stackBuffer)
{
    struct UnaryRepeatParams repeatParams;
    struct BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, src, static_cast<T>(-1.0), MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, repeatParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(1), MASK_PLACEHOLDER, 1, repeatParams);
    Duplicate<T, false>(stackBuffer, static_cast<T>(1.0), MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, stackBuffer, dst, MASK_PLACEHOLDER, 1, binaryRepeatParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SigmoidCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& sharedTmpBuffer, const uint32_t splitSize, const uint32_t loopCount, const uint32_t calcTail)
{
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SigmoidIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SigmoidIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/sigmoid/sigmoid_common_impl.h" 2




namespace AscendC {

template <typename T, bool isReuseSource = false>
[aicore] inline void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                            ;

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


                                                                      ;

                                                                          ;

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(T);
                                                                                               ;

    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    SigmoidCompute<T, isReuseSource>(dstTensor, srcTensor, tmpBuffer, splitSize, loopCount, calcTail);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SigmoidImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);
                                                                                 ;
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, stackBuffer, calCount);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SigmoidImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 78 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 94 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/sigmoid.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sigmoid(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sigmoid<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax_utils.h"
namespace AscendC {

enum class SoftmaxMode {
    SOFTMAX_NORMAL = 0,
    SOFTMAX_OUTPUT_WITHOUT_BRC = 1,
};

struct SoftmaxConfig {
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn)
    {
        isCheckTiling = isCheckTilingIn;
    }
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn, const uint32_t oriSrcMIn, const uint32_t oriSrcKIn)
    {
        isCheckTiling = isCheckTilingIn;
        oriSrcM = oriSrcMIn;
        oriSrcK = oriSrcKIn;
    }
    [aicore] constexpr SoftmaxConfig(const bool isCheckTilingIn, const uint32_t oriSrcMIn, const uint32_t oriSrcKIn, const enum SoftmaxMode modeIn)
    {
        isCheckTiling = isCheckTilingIn;
        oriSrcM = oriSrcMIn;
        oriSrcK = oriSrcKIn;
        mode = modeIn;
    }

    bool isCheckTiling = true;
    uint32_t oriSrcM = 0;
    uint32_t oriSrcK = 0;
    SoftmaxMode mode = SoftmaxMode::SOFTMAX_NORMAL;
};

constexpr SoftmaxConfig SOFTMAX_DEFAULT_CFG = { true, 0, 0 , SoftmaxMode::SOFTMAX_NORMAL};

struct SoftMaxParams {
    uint32_t srcM{ 0 };
    uint32_t srcK{ 0 };
    uint32_t oriSrcM{ 0 };
    uint32_t oriSrcK{ 0 };
    uint32_t loopCnt{ 1 };
    uint32_t splitMeanCnt{ 8 };
    float alpha{ 0.9375 };
};

struct SoftMaxShapeInfo {
    uint32_t srcM{ 0 };
    uint32_t srcK{ 0 };
    uint32_t oriSrcM{ 0 };
    uint32_t oriSrcK{ 0 };
};

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_utils.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_utils.h"
namespace AscendC {

constexpr uint8_t SOFTMAX_BASIC_TILE_NUM = 8;
constexpr uint8_t SOFTMAX_COMPUTE_DIM = 2;
constexpr uint8_t SOFTMAXGRAD_COMPUTE_DIM = 3;
constexpr uint8_t SOFTMAXFLASH_COMPUTE_DIM = 4;
constexpr uint8_t SOFTMAX_INNER_SHAPE_DIM = 2;
constexpr uint32_t FLOAT_NUM_PER_BLK = ONE_BLK_SIZE / B32_BYTE_SIZE;
constexpr uint32_t HALF_NUM_PER_BLK = ONE_BLK_SIZE / B16_BYTE_SIZE;
constexpr uint32_t HALF_REPEAT_STRIDE = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
constexpr uint32_t SCALAR_STACK_DEPTH = 8;
constexpr uint32_t SOFTMAX_SHAPE_NZ_BASIC_COUNT = 16;
constexpr uint32_t SOFTMAX_NZ_TILING_NEEDBLOCK = 3;
constexpr uint32_t SOFTMAX_MAX_REPEAT_STRIDE = MAX_REPEAT_TIMES * DEFAULT_REPEAT_STRIDE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_MAX_REPEAT_CLC_HALF_NUM = MAX_REPEAT_TIMES * HALF_REPEAT_SIZE;
constexpr uint32_t SOFTMAX_SPECIAL_BASICBLOCK_LEN = FLOAT_REPEAT_SIZE * SOFTMAX_BASIC_TILE_NUM * SOFTMAX_COMPUTE_DIM;
constexpr uint32_t SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE = 192;
constexpr uint32_t SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN = DEFAULT_BLOCK_SIZE * HALF_FACTOR;

struct LastAxisShapeND {
    uint32_t m;
    uint32_t k;
};

struct ReduceLastND {
    uint32_t originalSrcM;
    uint32_t originalSrcK;
    uint32_t srcM;
    uint32_t srcK;
    uint32_t dstM;
    uint32_t dstK;
};

struct BroadCastLastND {
    uint32_t dstM;
    uint32_t dstK;
    uint32_t srcM;
    uint32_t srcK;
};

};
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_shape_process.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_shape_process.h"
namespace AscendC {

[aicore] inline LastAxisShapeND GetLastAxisShapeND(const ShapeInfo& shapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < shapeInfo.shapeDim; i++) {
        calculateSize *= shapeInfo.shape[i];
    }


                                                                             ;
    ndinfo.k = shapeInfo.shape[shapeInfo.shapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}

[aicore] inline LastAxisShapeND GetLastAxisOriginShapeND(const ShapeInfo& srcShapeInfo)
{
    uint32_t calculateSize = 1;
    LastAxisShapeND ndinfo;
    for (uint32_t i = 0; i < srcShapeInfo.originalShapeDim; i++) {
        calculateSize *= srcShapeInfo.originalShape[i];
    }


                                                                                                    ;
    ndinfo.k = srcShapeInfo.originalShape[srcShapeInfo.originalShapeDim - 1];

                                                                   ;
    ndinfo.m = calculateSize / ndinfo.k;
    return ndinfo;
}
[aicore] inline constexpr uint32_t CalculateNDSplitM(const uint32_t workLocalSize, const uint32_t dataTypeSize,
    const uint32_t reduceK, const LastAxisShapeND& ndinfo, bool isBasicBlock = false)
{
    uint32_t splitM = 0;
    if (dataTypeSize == B16_BYTE_SIZE) {
        splitM = workLocalSize / (reduceK + ndinfo.k + FLOAT_REPEAT_SIZE);
    } else {
        splitM = workLocalSize / (reduceK + FLOAT_REPEAT_SIZE);
    }

    splitM = splitM < ndinfo.m ? splitM : ndinfo.m;

    if (isBasicBlock && (splitM > SOFTMAX_BASIC_TILE_NUM) && (ndinfo.m % SOFTMAX_BASIC_TILE_NUM == 0)) {
        splitM = splitM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (ndinfo.m % splitM != 0) {
            splitM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (splitM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            splitM = splitM / HALF_FACTOR;
        }
    }
    return splitM;
}

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_tiling_func.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_tiling_func.h"
namespace AscendC {

[aicore] inline bool SoftMaxTilingFunc(const uint32_t workLocalSize, const SoftMaxShapeInfo& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{

                                                                                              ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    const uint32_t srcM = ndinfo.srcM;
    const uint32_t srcK = ndinfo.srcK;
    const uint32_t oriSrcM = ndinfo.oriSrcM;

    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;
    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + srcK);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, {srcM, srcK},
            isBasicBlock);
    }

    if (softmaxTiling.reduceM < oriSrcM && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < oriSrcM ? softmaxTiling.reduceM : oriSrcM;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

                                                                                              ;
    softmaxTiling.rangeM = oriSrcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = oriSrcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] inline bool SoftMaxFlashTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isUpdate = false, bool isBasicBlock = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (!isUpdate) {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    } else {
        softmaxTiling.reduceM =
            workLocalSize / (elementNumPerBlk * SOFTMAXFLASH_COMPUTE_DIM + ndinfo.k * SOFTMAX_COMPUTE_DIM);
    }

    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                   ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] inline bool SoftMaxGradTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    SoftMaxTiling& softmaxTiling, const uint32_t elementNumPerBlk, bool isFront = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false)
{
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;

    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;

    if (elementNumPerBlk != ONE_BYTE_BIT_SIZE) {
        softmaxTiling.reduceM = workLocalSize /
            (elementNumPerBlk * SOFTMAX_COMPUTE_DIM + ndinfo.k * SOFTMAXGRAD_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
    } else {
        if (isFront && !isDataFormatNZ) {
            softmaxTiling.reduceM = workLocalSize / (elementNumPerBlk + ndinfo.k + FLOAT_REPEAT_SIZE);
        } else {
            softmaxTiling.reduceM =
                workLocalSize / (ndinfo.k + elementNumPerBlk * SOFTMAX_COMPUTE_DIM + FLOAT_REPEAT_SIZE);
        }
    }
    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;

    if (isBasicBlock && isFront && (softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) &&
        (softmaxTiling.srcM % SOFTMAX_BASIC_TILE_NUM == 0)) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
        while (softmaxTiling.srcM % softmaxTiling.reduceM != 0) {
            softmaxTiling.reduceM -= SOFTMAX_BASIC_TILE_NUM;
        }

        while (softmaxTiling.reduceM * ndinfo.k >= FLOAT_REPEAT_SIZE * DEFAULT_BLOCK_SIZE) {
            softmaxTiling.reduceM = softmaxTiling.reduceM / B16_BYTE_SIZE;
        }
    }

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                                  ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

};
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_broadcast.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_broadcast.h"
namespace AscendC {

template <typename T>
[aicore] inline void AlignedBrcbImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t brcbCount)
{
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    SetVectorMask<T>(brcbCount);
    for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
        scalarList[j] = srcLocal.GetValue(j);
    }
    for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
        Duplicate<T, false>(dstLocal[k * brcbCount], scalarList[k], MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] inline void ContinusColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] inline void AlignedColumnBrcbImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t& repeat, const uint32_t& brcbCount)
{
    float scalarList[SCALAR_STACK_DEPTH] = {0};
    SetVectorMask<float>(brcbCount);
    const uint32_t rangeM = repeat / SCALAR_STACK_DEPTH;
    const uint32_t tailM = repeat % SCALAR_STACK_DEPTH;
    uint32_t offset = 0;

    for (uint32_t i = 0; i < rangeM; i++) {
        offset = i * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < SCALAR_STACK_DEPTH; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < SCALAR_STACK_DEPTH; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailM != 0) {
        offset = rangeM * brcbCount * SCALAR_STACK_DEPTH;
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcLocal.GetValue(offset + j * brcbCount);
        }
        for (uint32_t k = 0; k < tailM; k++) {
            Duplicate<float, false>(dstLocal[offset + k * brcbCount], scalarList[k], MASK_PLACEHOLDER,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] inline void BroadCastNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM)
{
    uint8_t repeat = srcM / DEFAULT_REPEAT_STRIDE;
    for (uint8_t i = 0; i < repeat; i++) {
        Muls<float, false>(dst[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], src[i * B16_BYTE_SIZE * FLOAT_REPEAT_SIZE], 1.0,
            MASK_PLACEHOLDER, B16_BYTE_SIZE, { 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
    PipeBarrier<PIPE_V>();

    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    for (int32_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)dst[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
        srcList[i] = (uint64_t)src[i * FLOAT_NUM_PER_BLK].GetPhyAddr();
    }
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = repeat;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
        transDataParams.srcRepStride = B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE;
    }
    TransDataTo5HD<float>(dstList, srcList, transDataParams);
}

template <typename T>
[aicore] inline void BroadCastLastCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const BroadCastLastND& brcParam, const uint32_t scalarStackDepth, const uint32_t index)
{
    const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    const uint32_t rangeK = brcParam.dstK / elementNumPerRep;
    const uint32_t tailK = brcParam.dstK % elementNumPerRep;
    T scalarList[SCALAR_STACK_DEPTH] = {0};

    for (uint32_t j = 0; j < scalarStackDepth; j++) {
        scalarList[j] = src[(index * SCALAR_STACK_DEPTH + j) * brcParam.srcK].GetValue(0);
    }
    for (uint32_t j = 0; j < rangeK; j++) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[j * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                elementNumPerRep, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
    if (tailK != 0) {
        for (uint32_t k = 0; k < scalarStackDepth; k++) {
            Duplicate(dst[rangeK * elementNumPerRep + (index * SCALAR_STACK_DEPTH + k) * brcParam.dstK], scalarList[k],
                tailK, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

};
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_reduce.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_reduce.h"
namespace AscendC {

[aicore] inline void ReduceMaxBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Max<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceMax<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}

[aicore] inline void ReduceSumBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);

    Add<float, false>(dst, src, src[FLOAT_NUM_PER_BLK], 1, 1,
        { B16_BYTE_SIZE, B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
    PipeBarrier<PIPE_V>();
    BlockReduceSum<float, false>(dst, dst, reduceParam.srcM / FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
        DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE);

    SetMaskNorm();
    ResetMask();
}

[aicore] inline void BigBlockReduceMax(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceMax<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

[aicore] inline void BigBlockReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    for (uint32_t i = 0; i < splitM; i++) {
        BlockReduceSum<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * splitK], FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER,
            1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitBlock - FLOAT_NUM_PER_BLK;
    if (remainRepeat == 0) {
        return;
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitM; ++j) {
        Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * splitK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
    }
}

};
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_arithmetic.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_arithmetic.h"
namespace AscendC {

[aicore] inline void TailMaxImpl(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = HALF_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * HALF_REPEAT_SIZE], dst[i * HALF_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK], mask,
                1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_HALF_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] inline void TailMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Max(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Max(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Max(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}
[aicore] inline void TailAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const ReduceLastND& reduceParam, const uint64_t mask, const uint8_t srcRepeatStride, const uint32_t splitCount)
{
    const uint32_t tailStartOffset = FLOAT_REPEAT_SIZE * splitCount;
    if (reduceParam.srcK > SOFTMAX_MAX_REPEAT_STRIDE) {
        for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
            Add(dst[i * FLOAT_REPEAT_SIZE], dst[i * FLOAT_REPEAT_SIZE], src[tailStartOffset + i * reduceParam.srcK],
                mask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
    } else {
        const uint32_t range = reduceParam.originalSrcM / MAX_REPEAT_TIMES;
        const uint32_t tail = reduceParam.originalSrcM % MAX_REPEAT_TIMES;
        for (uint32_t i = 0; i < range; i++) {
            Add(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
        if (tail != 0) {
            Add(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM],
                src[tailStartOffset + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepeatStride });
        }
    }
}

[aicore] inline void NextBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {





        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] inline void NextBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint8_t splitM, const uint8_t srcRepstride, const uint32_t splitBlock, const uint32_t srcK)
{
    if (splitM > splitBlock) {
        for (uint32_t i = HALF_FACTOR; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, srcRepstride });
        }
    } else {





        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[HALF_REPEAT_SIZE + j * srcK], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] inline void BasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Max<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Max<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] inline void BasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src, uint8_t splitM,
    uint8_t offset, const uint32_t splitBlock)
{
    Add<float, false>(dst, src, src[FLOAT_REPEAT_SIZE], 1, splitM, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
    if (splitM > splitBlock) {
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(dst, dst, src[FLOAT_REPEAT_SIZE * i], 1, splitM,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
    } else {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitM; ++j) {
            Add<float, false>(dst[j * FLOAT_REPEAT_SIZE],
                src[HALF_FACTOR * FLOAT_REPEAT_SIZE + splitBlock * FLOAT_REPEAT_SIZE * j], dst[j * FLOAT_REPEAT_SIZE],
                1, (uint8_t)(splitBlock - HALF_FACTOR), { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
}

[aicore] inline void GenericSubNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Sub<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Sub<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] inline void GenericDivNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Div<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Div<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] inline void GenericMulNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t dstBlockStride = srcK / FLOAT_NUM_PER_BLK;
        const uint8_t src1BlockStride = srcReduceK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < dstBlockStride; j++) {
            Mul<float, false>(dst[j * FLOAT_NUM_PER_BLK], src0[j * FLOAT_NUM_PER_BLK], src1, 1, 1,
                { dstBlockStride, dstBlockStride, src1BlockStride, (uint8_t)srcK, (uint8_t)srcK, (uint8_t)srcReduceK });
        }
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < originalSrcM; j++) {
            Mul<float, false>(dst[j * srcK], src0[j * srcK], src1[j * srcReduceK], 1, 1,
                { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0 });
        }
        SetMaskNorm();
        ResetMask();
    }
}

[aicore] inline void TransDivToMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint32_t originalSrcM, const uint32_t srcK, const uint32_t srcReduceK)
{
    const uint32_t curReduceSize = originalSrcM * srcReduceK;
    Duplicate(tmpbuffer, (float)1.0, curReduceSize);
    PipeBarrier<PIPE_V>();
    Div(tmpbuffer, tmpbuffer, src, curReduceSize);
    PipeBarrier<PIPE_V>();
    GenericMulNDImpl(dst, dst, tmpbuffer, originalSrcM, srcK, srcReduceK);
}

};
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common.h" 2

namespace AscendC {
[aicore] inline void CreateSpecialFormatMask(uint64_t& lowMask, const uint32_t& maskLen, const uint32_t& nzBlockCount,
    const uint32_t& totalLen = SOFTMAX_SHAPE_NZ_BASIC_COUNT)
{



                                                                                       ;
    if (totalLen == SOFTMAX_SHAPE_NZ_BASIC_COUNT) {

                                                                                                          ;
    }
    if (totalLen >= B32_DATA_NUM_PER_BLOCK) {

                                                                                                                         ;
    }

                                                                                      ;
    uint16_t originalMask = totalLen == SOFTMAX_SHAPE_NZ_BASIC_COUNT ? 0xFFFF : 0xFF;
    uint64_t defaultMask = originalMask >> (totalLen - maskLen);
    lowMask = defaultMask;

    for (uint32_t i = 0; i < nzBlockCount - 1; i++) {
        lowMask = lowMask << totalLen;
        lowMask = lowMask | defaultMask;
    }
}

[aicore] inline void BinaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*,
    const uint8_t, const BinaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src0[i * offsetCount], src1[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src0[repeatRange * offsetCount], src1[repeatRange * offsetCount], mask,
            repeatTail, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src0[repeat * FLOAT_REPEAT_SIZE], src1[repeat * FLOAT_REPEAT_SIZE],
            tailMask, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] inline void UnaryComputeWithSpecialMask(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    uint64_t mask[2], const uint32_t& lastBlockMaskLen, const uint32_t& splitCount,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, uint64_t*, const uint8_t,
    const UnaryRepeatParams&))
{
    uint32_t repeat = splitCount / FLOAT_REPEAT_SIZE;
    uint32_t tail = splitCount % FLOAT_REPEAT_SIZE;

    uint32_t repeatRange = repeat / MAX_REPEAT_TIMES;
    uint32_t repeatTail = repeat % MAX_REPEAT_TIMES;
    const auto offsetCount = MAX_REPEAT_TIMES * FLOAT_REPEAT_SIZE;
    uint32_t dstOffset = 0;
    uint32_t src0Offset = 0;
    uint32_t src1Offset = 0;

    for (uint32_t i = 0; i < repeatRange; i++) {
        func(dst[i * offsetCount], src[i * offsetCount], mask, MAX_REPEAT_TIMES,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    if (repeatTail != 0) {
        func(dst[repeatRange * offsetCount], src[repeatRange * offsetCount], mask, repeatTail,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    if (tail != 0) {
        uint64_t tailMask[2] = { 0, 0 };
        CreateSpecialFormatMask(tailMask[0], lastBlockMaskLen, tail / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        func(dst[repeat * FLOAT_REPEAT_SIZE], src[repeat * FLOAT_REPEAT_SIZE], tailMask, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

};
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_base_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h"
namespace AscendC {

template <typename T>
[aicore] inline void BroadCastLastImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const struct BroadCastLastND& brcParam)
{


    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint64_t lowMask =
        brcParam.srcM * elementNumPerBlk;
    uint16_t repeat = brcParam.dstK / elementNumPerBlk;
    uint16_t srcBlkStride = brcParam.srcK / elementNumPerBlk;
    uint64_t mask[2] = { lowMask, 0 };

    uint32_t range = repeat / MAX_REPEAT_TIMES;
    uint32_t tail = repeat % MAX_REPEAT_TIMES;

    SetMaskCount();
    for (uint32_t i = 0; i < range; i++) {
        Copy<T>(dst[i * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, MAX_REPEAT_TIMES,
            { repeat, srcBlkStride, 1, 0 });
    }
    if (tail != 0) {
        Copy<T>(dst[range * elementNumPerBlk * MAX_REPEAT_TIMES], src, mask, tail, { repeat, srcBlkStride, 1, 0 });
    }

    SetMaskNorm();
    ResetMask();
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl/softmax_common_broadcast.h"
}

[aicore] inline void SingleBlockBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam)
{
    BrcbRepeatParams brcbParams;
    brcbParams.dstBlkStride = 1;
    brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER;
    const uint32_t range = reduceParam.originalSrcM / BRCB_BROADCAST_NUMBER;
    const uint32_t tail = reduceParam.originalSrcM % BRCB_BROADCAST_NUMBER;

    if (range != 0) {
        if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
            brcbParams.dstBlkStride = HALF_FACTOR;
            brcbParams.dstRepStride = BRCB_BROADCAST_NUMBER * HALF_FACTOR;
            Brcb(dst[0], src, range, brcbParams);
            Brcb(dst[BRCB_BROADCAST_NUMBER], src, range, brcbParams);
        } else {
            Brcb(dst, src, range, brcbParams);
        }
    }

    if (tail != 0) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        float scalarList[SCALAR_STACK_DEPTH] = {0};
        for (uint32_t j = 0; j < tail; j++) {
            scalarList[j] = src[(range * BRCB_BROADCAST_NUMBER + j)].GetValue(0);
        }

        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        for (uint32_t k = 0; k < tail; k++) {
            Duplicate(dst[(range * SCALAR_STACK_DEPTH + k) * reduceParam.dstK], scalarList[k], reduceParam.dstK, 1,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        }
    }
}

[aicore] inline void AlignedBroadCastImpl(const LocalTensor<float>& dst, const LocalTensor<float>& tmpbuffer,
    const struct ReduceLastND& reduceParam)
{
    const uint32_t repeat = (reduceParam.originalSrcM + BRCB_BROADCAST_NUMBER - 1) / BRCB_BROADCAST_NUMBER;

    if (reduceParam.dstK == BRCB_BROADCAST_NUMBER * HALF_FACTOR) {
        if (reduceParam.originalSrcM != 1) {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
            Brcb(tmpbuffer[BRCB_BROADCAST_NUMBER], dst, (uint8_t)repeat,
                { HALF_FACTOR, BRCB_BROADCAST_NUMBER * HALF_FACTOR });
        } else {
            Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
            PipeBarrier<PIPE_V>();
            DataCopy(tmpbuffer[DEFAULT_REPEAT_STRIDE], tmpbuffer, { 1, 1, 0, 0 });
        }
    } else {
        Brcb(tmpbuffer, dst, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
    }
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * reduceParam.dstK);
    Copy<float, false>(dst, tmpbuffer, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

};
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl/softmax_common_nd_reduce.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl/softmax_common_nd_reduce.h"
namespace AscendC {

[aicore] inline void FirstBlockCopyImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t srcM, const uint32_t srcK, const uint16_t dstRepeatStride, const uint16_t srcRepeatStride)
{

    const uint32_t range = srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = srcM % MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < range; i++) {
        Copy<float, false>(dst[i * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[i * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, MAX_REPEAT_TIMES, { 1, 1, dstRepeatStride, srcRepeatStride });
    }
    if (tail != 0) {
        Copy<float, false>(dst[range * SOFTMAX_MAX_REPEAT_CLC_FLOAT_NUM], src[range * MAX_REPEAT_TIMES * srcK],
            MASK_PLACEHOLDER, tail, { 1, 1, dstRepeatStride, srcRepeatStride });
    }





}

[aicore] inline void ReduceMaxLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceMax(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK, ReduceOrder::ORDER_ONLY_VALUE);
    }
}

[aicore] inline void AlignedReduceMaxNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceMax<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceMaxParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
        Max<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceMax<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void AlignedReduceSumNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam, const uint32_t splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
    BlockReduceSum<float, false>(tmpTensor, src, 1, MASK_PLACEHOLDER, 1, 1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    DataCopy(dst, tmpTensor, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 1; i < splitCount; i++) {
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(tmpTensor, src[i * FLOAT_REPEAT_SIZE], 1, MASK_PLACEHOLDER, 1, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
        Add<float, false>(dst, dst, tmpTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
    BlockReduceSum<float, false>(dst, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void ReduceMaxLastNDImpl(const LocalTensor<float>& dstMax, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceMaxParam)
{
    const uint32_t splitCount = reduceMaxParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceMaxParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceMaxNDImpl(tmpTensor, src, dstMax, reduceMaxParam, splitCount);
    }
    if (tailSrcK != 0) {
        ReduceMaxLastNDSplitImpl(dstMax, src, reduceMaxParam, tailSrcK, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dstMax, { 1, (uint16_t)reduceMaxParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceMaxParam.srcM * FLOAT_NUM_PER_BLK);
            Max<float, false>(tmpTensor, tmpTensor, dstMax, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dstMax, tmpTensor, reduceMaxParam);
}

[aicore] inline void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceMax<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0,
            ReduceOrder::ORDER_ONLY_VALUE);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceMax<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceMax<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

template <bool isBroadCast = true>
[aicore] inline void NewReduceMaxLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceMaxLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceMax(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Max<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockMaxImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailMaxImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if constexpr (isBroadCast) {
        if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
            PipeBarrier<PIPE_V>();
            AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
        }
    }
}

[aicore] inline void ReduceSumLastNDSplitImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const struct ReduceLastND& reduceParam, uint64_t mask, uint32_t dstRepStride, uint32_t splitNum)
{
    uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;

    for (uint32_t i = 0; i < range; i++) {
        WholeReduceSum(dst[i * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + i * MAX_REPEAT_TIMES * reduceParam.srcK], mask, MAX_REPEAT_TIMES,
            dstRepStride, 1,
            reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES],
            src[splitNum * FLOAT_REPEAT_SIZE + range * MAX_REPEAT_TIMES * reduceParam.srcK], mask, tail, dstRepStride,
            1, reduceParam.srcK / FLOAT_NUM_PER_BLK);
    }
}

[aicore] inline void ReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    if (splitCount > 0) {
        AlignedReduceSumNDImpl(tmpTensor, src, dst, reduceParam, splitCount);
    }

    if (tailSrcK != 0) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, tailSrcK, 1, splitCount);
        PipeBarrier<PIPE_V>();
        if (splitCount == 0) {
            DataCopy(tmpTensor, dst, { 1, (uint16_t)reduceParam.srcM, 0, 0 });
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * FLOAT_NUM_PER_BLK);
            Add<float, false>(tmpTensor, tmpTensor, dst, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            SetMaskNorm();
            ResetMask();
        }
    }

    PipeBarrier<PIPE_V>();
    SingleBlockBroadCastImpl(dst, tmpTensor, reduceParam);
}

[aicore] inline void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t reduceK)
{
    if (originalSrcM == 1) {
        WholeReduceSum<float, false>(dst, src, MASK_PLACEHOLDER, DEFAULT_REPEAT_STRIDE, 1, 1, 0);
        if (reduceK == DEFAULT_REPEAT_STRIDE * HALF_FACTOR) {
            PipeBarrier<PIPE_V>();
            DataCopy(dst[DEFAULT_REPEAT_STRIDE], dst, { 1, 1, 0, 0 });
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_REPEAT_SIZE);
        BlockReduceSum<float, false>(src, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        BlockReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        SetMaskNorm();
        ResetMask();
    }
}

template <bool isBroadCast = true>
[aicore] inline void NewReduceSumLastNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    if (reduceParam.originalSrcK < FLOAT_REPEAT_SIZE) {
        ReduceSumLastNDSplitImpl(dst, src, reduceParam, reduceParam.originalSrcK, 1, 0);
        ResetMask();
    } else {
        if (reduceParam.originalSrcK >= SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN) {
            BigBlockReduceSum(tmpTensor, src, splitCount, reduceParam.originalSrcM, reduceParam.srcK);
        } else if (reduceParam.originalSrcK >= HALF_REPEAT_SIZE) {
            Add<float, false>(tmpTensor, src, src[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(reduceParam.originalSrcM),
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, (uint8_t)srcRepeatStride, (uint8_t)srcRepeatStride });
            NextBlockAddImpl(tmpTensor, src, (uint8_t)(reduceParam.originalSrcM), srcRepeatStride, splitCount,
                reduceParam.srcK);
        } else {
            FirstBlockCopyImpl(tmpTensor, src, reduceParam.originalSrcM, reduceParam.srcK, DEFAULT_REPEAT_STRIDE,
                srcRepeatStride);
        }

        if (tailSrcK != 0) {
            PipeBarrier<PIPE_V>();
            TailAddImpl(tmpTensor, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
            ResetMask();
        }
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(dst, tmpTensor, reduceParam.originalSrcM, reduceParam.dstK);
    }
    if constexpr (isBroadCast) {
        if (reduceParam.originalSrcM != 1 || reduceParam.originalSrcK <= FLOAT_REPEAT_SIZE) {
            PipeBarrier<PIPE_V>();
            AlignedBroadCastImpl(dst, tmpTensor, reduceParam);
        }
    }
}

};
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_common_nz_reduce.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_common_nz_reduce.h"
namespace AscendC {
[aicore] inline void ReduceMaxSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceMax(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceMax(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, DEFAULT_REPEAT_STRIDE, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] inline void SingleUnAlignedReduceMaxNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceMaxSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] inline void ReduceMaxLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const ReduceLastND& reduceParam)
{
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceMaxNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Max<float>);

            PipeBarrier<PIPE_V>();
            ReduceMaxBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}
[aicore] inline void ReduceSumSingleBlockNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint64_t& mask, const ReduceLastND& reduceParam)
{
    const uint32_t range = reduceParam.srcM / MAX_REPEAT_TIMES;
    const uint32_t tail = reduceParam.srcM % MAX_REPEAT_TIMES;
    for (uint32_t j = 0; j < range; j++) {
        WholeReduceSum(dst[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[j * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, MAX_REPEAT_TIMES,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1, SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
    if (tail != 0) {
        WholeReduceSum(dst[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[range * MAX_REPEAT_TIMES * SOFTMAX_SHAPE_NZ_BASIC_COUNT], mask, tail, SOFTMAX_SHAPE_NZ_BASIC_COUNT, 1,
            SOFTMAX_SHAPE_NZ_BASIC_COUNT / FLOAT_NUM_PER_BLK);
    }
}

[aicore] inline void SingleUnAlignedReduceSumNZImpl(const LocalTensor<float>& tmpBuffer1,
    const LocalTensor<float>& tmpBuffer0, const uint32_t lastBlockMaskLen, const ReduceLastND& reduceParam)
{
    ReduceSumSingleBlockNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);

    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    AlignedColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    ResetMask();
}

[aicore] inline void ReduceSumLastNZImpl(const LocalTensor<float>& tmpBuffer1, const LocalTensor<float>& tmpBuffer0,
    uint64_t mask[2], const struct ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = reduceParam.dstM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitCount = reduceParam.originalSrcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = reduceParam.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    if (__builtin_expect(!!(splitNZBlockCount == 1 && lastBlockMaskLen != SOFTMAX_SHAPE_NZ_BASIC_COUNT), 0)) {
        SingleUnAlignedReduceSumNZImpl(tmpBuffer1, tmpBuffer0, lastBlockMaskLen, reduceParam);
    } else {
        if (__builtin_expect(!!(splitNZBlockCount == 1), 0)) {
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer0, reduceParam);
        } else {
            SetMaskCount();
            SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
            Muls<float, false>(tmpBuffer1, tmpBuffer0, 1.0, MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            PipeBarrier<PIPE_V>();
            for (uint32_t j = 1; j < splitNZBlockCount - 1; j++) {
                Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
            }
            SetMaskNorm();
            ResetMask();

            BinaryComputeWithSpecialMask(tmpBuffer1, tmpBuffer1, tmpBuffer0[splitOffset * (splitNZBlockCount - 1)],
                mask, lastBlockMaskLen, splitCount, Add<float>);

            PipeBarrier<PIPE_V>();
            ReduceSumBlockNZImpl(tmpBuffer1, tmpBuffer1, reduceParam);
        }

        if (reduceParam.originalSrcM % DEFAULT_REPEAT_STRIDE == 0) {
            PipeBarrier<PIPE_V>();
            BroadCastNZImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM);
        } else {
            event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
            SetFlag<HardEvent::V_S>(eventIdVToS);
            WaitFlag<HardEvent::V_S>(eventIdVToS);

            ContinusColumnBrcbImpl(tmpBuffer1, tmpBuffer1, reduceParam.originalSrcM, SOFTMAX_SHAPE_NZ_BASIC_COUNT);
            ResetMask();
        }
    }
}

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_common_impl.h" 2

namespace AscendC {
constexpr RoundMode FLOAT2HALF_ROUND_MODE = RoundMode::CAST_ROUND;

};
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
namespace AscendC {

[aicore] inline void SoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_BLK_NUM);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
# 112 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], reduceSumBuffer, splitCeilM,
            { HALF_FACTOR, HALF_FACTOR * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}

[aicore] inline void SoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, src[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 176 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, dst[offset1], (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer2, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 210 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(sumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], dst[offset1 + FLOAT_REPEAT_SIZE * j],
                sumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void SoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    uint8_t stride = (uint8_t)(tiling.splitK / FLOAT_NUM_PER_BLK);
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockMaxImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 268 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(maxTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            BasicBlockAddImpl(tmpBuffer1, tmpBuffer0, (uint8_t)(tiling.splitM), offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(reduceSumBuffer, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
# 304 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_basic_block_impl.h"
        Brcb(sumTensor[offset2], reduceSumBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], sumTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nz_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nz_impl.h"
namespace AscendC {

template <bool isFlashV2 = false>
[aicore] inline void SoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <bool isFlashV2 = false>
[aicore] inline void SoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    if constexpr (isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount; j++) {
            DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
                splitCount);
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
                tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(
            dst[offset1 + (splitNZBlockCount - 1) * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1, mask, lastBlockMaskLen, splitCount, Div<float>);
    }
}

template <bool isFlashV2 = false>
[aicore] inline void SoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();
    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    if constexpr (!isFlashV2) {
        for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
            Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
                { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        }
        SetMaskNorm();
        ResetMask();
        BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
            mask, lastBlockMaskLen, splitCount, Div<float>);

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT : SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftMaxNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nd_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_impl/softmax_generic_nd_impl.h"
namespace AscendC {

[aicore] inline void SoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();




    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

}

[aicore] inline void SoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();



    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] inline void SoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();




    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T>
[aicore] inline void SoftMaxNDExtImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] inline void SoftMaxNDExtImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_common_impl.h"
namespace AscendC {
template <typename T1, typename T2, uint8_t stepSizeMode = 0>
[aicore] inline bool AdjustSoftMaxResNZImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    uint32_t floatStepSize = ONE_BLK_FLOAT_NUM;
    uint32_t halfStepSize = ONE_BLK_HALF_NUM;

    bool isUpdateNeedCheck = false;
    const uint32_t splitNZBlockCount = softmaxShapeInfo.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetVectorMask<float>(SOFTMAX_SHAPE_NZ_BASIC_COUNT);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        uint32_t offset = j * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint32_t splitCount = softmaxShapeInfo.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * floatStepSize);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * halfStepSize);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                for (uint32_t k = 0; k < splitNZBlockCount; k++) {
                    Duplicate<T1, false>(softMaxRes[offset + splitCount * k], to, MASK_PLACEHOLDER, 1, 1,
                        DEFAULT_REPEAT_STRIDE);
                }
                isUpdateNeedCheck = true;
            }
        }
    }
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2, uint8_t stepSizeMode = 0>
[aicore] inline bool AdjustSoftMaxResNDImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    uint32_t floatStepSize = ONE_BLK_FLOAT_NUM;
    uint32_t halfStepSize = ONE_BLK_HALF_NUM;
    if constexpr (stepSizeMode) {
        floatStepSize = 1;
        halfStepSize = 1;
    }

    bool isUpdateNeedCheck = false;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, softmaxShapeInfo.srcK);
    for (uint32_t j = 0; j < softmaxShapeInfo.srcM; j++) {
        if constexpr (sizeof(T2) == sizeof(float)) {
            T2 maxValue = maxTensor.GetValue(j * floatStepSize);
            uint32_t checkValue = *reinterpret_cast<uint32_t*>(&maxValue);
            if (checkValue == from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        } else {
            T2 maxValue = maxTensor.GetValue(j * halfStepSize);
            uint16_t checkValue = *reinterpret_cast<uint16_t*>(&maxValue);
            if (checkValue == (uint16_t)from) {
                Duplicate<T1, false>(softMaxRes[j * softmaxShapeInfo.srcK], to, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                isUpdateNeedCheck = true;
            }
        }
    }
    SetMaskNorm();
    ResetMask();
    return isUpdateNeedCheck;
}

template <typename T1, typename T2, bool isDataFormatNZ = false, uint8_t stepSizeMode = 0>
[aicore] inline bool AdjustSoftMaxResBaseImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
                                                const uint32_t from, const T1 to,
                                                const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
    bool isUpdateNeedCheck = false;
    if constexpr (isDataFormatNZ) {
        isUpdateNeedCheck =
            AdjustSoftMaxResNZImpl<T1, T2, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    } else {
        isUpdateNeedCheck =
            AdjustSoftMaxResNDImpl<T1, T2, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
    }
    return isUpdateNeedCheck;
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_impl.h" 2

namespace AscendC {

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxNDImpl(const LocalTensor<T1>& dst, const LocalTensor<T1>& sumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T1, half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T1, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftMaxNDExtImpl<T1>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftMaxBasicBlock(dst, sumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftMaxNDExtImpl(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <bool isReuseSource = false>
[aicore] inline void SingleSoftMaxImpl(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer1, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer1, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <bool isReuseSource = false>
[aicore] inline void SingleSoftMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const uint32_t& offset, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    if constexpr(isReuseSource) {
        Exp(src[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, src[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], src[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    } else {
        Exp(dst[offset], dst[offset], splitSize);
        PipeBarrier<PIPE_V>();
        NewReduceSumLastNDImpl(tmpBuffer0, dst[offset], tmpBuffer1, reduceParam);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset], dst[offset], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    }
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    uint32_t offset = 0;
    uint32_t splitSize = tiling.splitSize;
    ReduceLastND reduceParam;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM,
            tiling.reduceK };
    } else {
        uint32_t splitK = 0;
        if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
            splitK = config.oriSrcK;
        } else {
            splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
        }
        if constexpr (SupportType<T, half>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
        } else if constexpr (SupportType<T, float>()) {
            reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
        }
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SingleSoftMaxImpl<isReuseSource>(dst, src, workLocal, tiling, offset, splitSize, reduceParam);
        offset += tiling.splitSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_base_impl.h" 2





namespace AscendC {
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
                                                                                                                                ;

    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
        SoftMaxTiling newTiling = tiling;
        SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
            newTiling, sizeof(T), sizeof(float), isBasicBlock);
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, newTiling);
    } else {
        SoftMaxNDImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, originalSrcShape, tiling);
    }
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dst, src, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{

                                                                              ;

    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || originalSrcShape.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, newTiling);
        } else {
            SoftMaxNZImpl<T1, T2, isBasicBlock>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape, tiling);
        }
    } else {

        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                newTiling);
        } else {
            SoftMaxNDImpl<T1, T2, isBasicBlock, config>(dst, sumTensor, maxTensor, src, workLocal, originalSrcShape,
                tiling);
        }
    }
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftMaxImpl<T1, T2, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftMaxImpl<T1, T2, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dst, sumTensor, maxTensor, src, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T1, typename T2, bool isDataFormatNZ = false, uint8_t stepSizeMode = 0>
[aicore] inline bool AdjustSoftMaxResImpl(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
                                                                                                                                           ;
    return AdjustSoftMaxResBaseImpl<T1, T2, isDataFormatNZ, stepSizeMode>(softMaxRes, maxTensor, from, to,
        softmaxShapeInfo);
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<half, float, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor, tiling,
        softmaxShapeInfo);
                                ;
}
# 97 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, tiling, softmaxShapeInfo);
                                ;
}
# 123 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, isReuseSource, isBasicBlock, config>(dstTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
                                ;
}
# 155 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<T, T, isReuseSource, isBasicBlock, isDataFormatNZ,config>(dstTensor, sumTensor, maxTensor, srcTensor, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                ;
}
# 186 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                 ;
    SoftMaxImpl<half, float, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
                                ;
}
# 214 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmax.h"
template <typename T1, typename T2, bool isDataFormatNZ = false, uint8_t stepSizeMode = 0>
[aicore] inline bool AdjustSoftMaxRes(const LocalTensor<T1>& softMaxRes, const LocalTensor<T2>& maxTensor,
    const uint32_t from, const T1 to, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return false;
    }
    return AdjustSoftMaxResImpl<T1, T2, isDataFormatNZ, stepSizeMode>(softMaxRes, maxTensor, from, to, softmaxShapeInfo);
}
}

#pragma end_pipe
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/simple_softmax_base_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/simple_softmax_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/simple_softmax_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/simple_softmax_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h"
namespace AscendC {
[aicore] inline void SimpleSoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.reduceSize + tiling.reduceSize];
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer0, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer0[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER,
            1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SimpleSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& inSumTensor,
    const LocalTensor<half>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Cast<float, half, false>(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    Cast<float, half, false>(tmpBuffer1, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SimpleSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t originalSrcM = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);

    DataCopyParams copyParams = { originalSrcM, 1, 0, 1 };
    DataCopy(tmpBuffer1, inMaxTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTensor[offset2], copyParams);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inSumTensor[offset2], copyParams);
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inSumTensor[offset2], copyParams);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2>
[aicore] inline void SimpleSoftMaxNZImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceM * ONE_BLK_SIZE / sizeof(T2);
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceM * ONE_BLK_SIZE / sizeof(T2);
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SimpleSoftMaxGenericNZImpl(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1, offset2, splitCount);
    }
}

[aicore] inline void SimpleSoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;

        Cast<float, half, false>(tmpBuffer1, maxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Cast<float, half, false>(tmpBuffer1, expSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] inline void SimpleSoftMaxBasicBlock(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
# 262 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/simple_softmax_common_impl.h"
    const uint32_t splitBlock = tiling.srcK / FLOAT_REPEAT_SIZE;
    const uint8_t repstride = tiling.srcK / FLOAT_NUM_PER_BLK;
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Sub<float, false>(dst[FLOAT_REPEAT_SIZE * j], src[FLOAT_REPEAT_SIZE * j], maxTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcSize);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.srcM * FLOAT_REPEAT_SIZE);
    for (uint32_t j = 0; j < splitBlock; ++j) {
        Div<float, false>(dst[FLOAT_REPEAT_SIZE * j], dst[FLOAT_REPEAT_SIZE * j], expSumTensor, MASK_PLACEHOLDER, 1,
            { 1, 1, 0, repstride, repstride, 1 });
    }
    SetMaskNorm();
    ResetMask();

}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& inSumTensor,
    const LocalTensor<half>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const uint32_t splitSize = curSplitM * tiling.splitK;
    const uint32_t reduceSize = curSplitM * tiling.reduceK;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        Cast(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();

        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, splitSize);

        Cast(tmpBuffer2, inSumTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    } else {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        Cast(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();

        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE * HALF_FACTOR);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, splitSize);

        Cast(tmpBuffer2, inSumTensor[offset2], RoundMode::CAST_NONE, reduceSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE * HALF_FACTOR);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const uint32_t splitSize = curSplitM * tiling.splitK;

    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        GenericSubNDImpl(dst[offset1], src[offset1], inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
        PipeBarrier<PIPE_V>();
        Exp(dst[offset1], dst[offset1], splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset1], dst[offset1], inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);
    } else {
        GenericSubNDImpl(dst[offset1], src[offset1], inMaxTensor[offset2], curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Exp(dst[offset1], dst[offset1], splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(dst[offset1], dst[offset1], inSumTensor[offset2], curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& src, const LocalTensor<float> workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        if constexpr (sizeof(T) == sizeof(float)) {
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, 0, 0,
                tiling.srcM);
        } else {
            uint32_t offset1 = 0;
            uint32_t offset2 = 0;
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                offset1 = i * tiling.splitSize;
                offset2 = i * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                    offset2, tiling.splitM);
            }
            PipeBarrier<PIPE_V>();
            if (tiling.tailM != 0) {
                offset1 = tiling.rangeM * tiling.splitSize;
                offset2 = tiling.rangeM * tiling.reduceSize;
                SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                    offset2, tiling.tailM);
            }
        }
    }
}

[aicore] inline void SimpleSoftMaxBasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceM * FLOAT_NUM_PER_BLK;
        offset1 = i * tiling.splitSize;

        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inMaxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                inSumTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

template <const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t offset1, const uint32_t offset2, const uint32_t curSplitM)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const uint32_t splitSize = curSplitM * tiling.splitK;
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        PipeBarrier<PIPE_V>();




        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, inMaxTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, tiling.splitSize);
        PipeBarrier<PIPE_V>();




        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, inSumTensor[offset2], curSplitM, tiling.srcK, tiling.reduceK);

        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    } else {
        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
        PipeBarrier<PIPE_V>();
        GenericSubNDImpl(tmpBuffer0, tmpBuffer0, inMaxTensor[offset2], curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer0, tmpBuffer0, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        GenericDivNDImpl(tmpBuffer0, tmpBuffer0, inSumTensor[offset2], curSplitM, tiling.splitK,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

template <typename T, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock) {
        SimpleSoftMaxBasicBlock(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
    } else {
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset1 = i * tiling.splitSize;
            offset2 = i * tiling.reduceM * FLOAT_NUM_PER_BLK;
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                offset2, tiling.splitM);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset1 = tiling.rangeM * tiling.splitSize;
            offset2 = tiling.rangeM * tiling.reduceM * FLOAT_NUM_PER_BLK;
            SimpleSoftMaxGenericNDImpl<config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling, offset1,
                offset2, tiling.tailM);
        }
    }
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/simple_softmax_impl.h" 2
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/simple_softmax_base_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_simple.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_simple.h"
namespace AscendC {

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxBaseImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    SetMaskNorm();
    ResetMask();
    ShapeInfo srcShape = src.GetShapeInfo();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), false, isDataFormatNZ);
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNZImpl<T1, T2>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxTilingFunc(workLocal.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
                newTiling, sizeof(T1), sizeof(T2), isBasicBlock);
            SimpleSoftMaxNDImpl<T1, isBasicBlock, config>(dst, inSumTensor, inMaxTensor, src, workLocal, newTiling);
        } else {
            SimpleSoftMaxNDImpl<T1, isBasicBlock, config>(dst, inSumTensor, inMaxTensor, src, workLocal, tiling);
        }
    }
}

}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/simple_softmax_base_impl.h" 2






namespace AscendC {
template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{

                                                                                  ;
    SimpleSoftMaxBaseImpl<T1, T2, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dst, inSumTensor,
        inMaxTensor, src, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SimpleSoftMaxImpl<T1, T2, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dst, inSumTensor, inMaxTensor, src, workLocal,
        tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMaxImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& inSumTensor,
    const LocalTensor<T2>& inMaxTensor, const LocalTensor<T1>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SimpleSoftMaxImpl<T1, T2, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dst, inSumTensor, inMaxTensor, src, workLocal,
        tiling, softmaxShapeInfo);
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        tiling, softmaxShapeInfo);
}
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& srcTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<half, float, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        tiling, softmaxShapeInfo);
}
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& inSumTensor,
    const LocalTensor<T>& inMaxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<T, T, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
# 128 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/simplesoftmax.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false, bool isDataFormatNZ = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SimpleSoftMax(const LocalTensor<half>& dstTensor, const LocalTensor<float>& inSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SimpleSoftMaxImpl<half, float, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, inSumTensor, inMaxTensor, srcTensor,
        sharedTmpBuffer, tiling, softmaxShapeInfo);
}
}

#pragma end_pipe
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv2_base_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv2_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_block_reduce_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_block_reduce_impl.h"
namespace AscendC {
[aicore] inline void SpecialBasicBlockMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint8_t splitCeilM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceMax<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpbuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceMax<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpbuffer, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpbuffer, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] inline void SpecialBasicBlockAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpbuffer, const uint8_t splitM, const uint8_t splitCeilM, const uint32_t splitK)
{
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
        BlockReduceSum<float, false>(tmpbuffer, src, splitM * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, splitM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpbuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    } else {
        BlockReduceSum<float, false>(tmpbuffer, src, HALF_FACTOR * FLOAT_REPEAT_SIZE, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpbuffer, tmpbuffer, HALF_FACTOR * FLOAT_NUM_PER_BLK, MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpbuffer, tmpbuffer, tmpbuffer[FLOAT_NUM_PER_BLK], 1, 1,
            { 1, HALF_FACTOR, HALF_FACTOR, DEFAULT_REPEAT_STRIDE, B16_DATA_NUM_PER_BLOCK, B16_DATA_NUM_PER_BLOCK });
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpbuffer, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}

[aicore] inline void BasicBlockReduceMaxImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockMaxImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitCeilM, splitK);
    } else {
        if (splitBlock == 1) {
            BlockReduceMax<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceMax(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockMaxImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceMax<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}
[aicore] inline void BasicBlockReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer, const uint32_t splitBlock, const uint32_t splitM, const uint32_t splitK)
{
    const uint8_t splitCeilM = (uint8_t)(DivCeil(splitM, FLOAT_NUM_PER_BLK));
    if (splitK == DEFAULT_BLOCK_SIZE * HALF_FACTOR || splitK == SOFTMAX_SPECIAL_BASICBLOCK_LEN) {
        SpecialBasicBlockAddImpl(dst, src, tmpBuffer, (uint8_t)splitM, splitCeilM, splitK);
    } else {
        if (splitBlock == 1) {
            BlockReduceSum<float, false>(tmpBuffer, src, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else if (splitK > DEFAULT_BLOCK_SIZE * HALF_FACTOR) {
            BigBlockReduceSum(tmpBuffer, src, splitBlock, splitM, splitK);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        } else {
            uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (splitK / FLOAT_REPEAT_SIZE));
            BasicBlockAddImpl(tmpBuffer, src, (uint8_t)splitM, offset, splitBlock);
            PipeBarrier<PIPE_V>();
            BlockReduceSum<float, false>(tmpBuffer, tmpBuffer, (uint8_t)splitM, MASK_PLACEHOLDER, 1, 1,
                DEFAULT_REPEAT_STRIDE);
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(dst, tmpBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h" 2

namespace AscendC {
[aicore] inline void SetWaitFlagVToS()
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
}

[aicore] inline void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inExpSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;

    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<float, half, false>(tmpBuffer2, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });


        Sub<float, false>(tmpBuffer2, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer2, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer3, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 111 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], tmpBuffer3, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();

        Cast<float, half, false>(tmpBuffer3, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(tmpBuffer3, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expSumTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] inline void SoftmaxFlashV2BasicBlockImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& expMaxTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 162 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Copy<float, false>(tmpBuffer2, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dst[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }


        Sub<float, false>(tmpBuffer2, tmpBuffer2, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(expMaxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Exp<float, false>(dst[offset1], dst[offset1], MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer3, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();







        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Mul<float, false>(inExpSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER,
            reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inExpSumTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}

[aicore] inline void SoftmaxFlashV2BasicBlock(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];

    const LocalTensor<float>& inMaxTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    const LocalTensor<float>& inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE / B16_BYTE_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 248 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Max<float, false>(maxTensor[offset2], inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
# 282 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });
        Copy<float, false>(tmpBuffer1[DEFAULT_REPEAT_STRIDE], inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { B16_BYTE_SIZE, 1, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue * B16_BYTE_SIZE, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceSumImpl(inSumTmp, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 309 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, inSumTmp, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        Mul<float, false>(inSumTmp, inMaxTmp, inExpSumTensor[offset2], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(expSumTensor[offset2], inSumTmp, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
    }
}
[aicore] inline void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<half>& dst,
    const LocalTensor<half>& expSumTensor, const LocalTensor<half>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceSumBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceMaxImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 356 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(reduceSumBuffer, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 387 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(tmpBuffer1, reduceSumBuffer, splitCeilM, { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });
        Brcb(tmpBuffer1[DEFAULT_REPEAT_STRIDE], reduceSumBuffer, splitCeilM,
            { B16_BYTE_SIZE, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expSumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
}

[aicore] inline void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<half>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 432 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

        PipeBarrier<PIPE_V>();

        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], maxTensor[offset2],
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }

        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        BasicBlockReduceSumImpl(tmpBuffer2, tmpBuffer0, tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 459 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

    }
}

[aicore] inline void SoftmaxFlashV2NoUpdateBasicBlock(const LocalTensor<float>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        BasicBlockReduceMaxImpl(tmpBuffer2, src[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 494 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(maxTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });


        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                maxTensor[offset2], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(dst[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        BasicBlockReduceSumImpl(tmpBuffer2, dst[offset1], tmpBuffer1, splitBlock, tiling.splitM, tiling.splitK);
        PipeBarrier<PIPE_V>();
# 517 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_basic_block_impl.h"
        Brcb(expSumTensor[offset2], tmpBuffer2, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

    }
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_update_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_update_impl.h"
namespace AscendC {

[aicore] inline void SoftmaxFlashV2UpdateImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inExpSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];

    Cast(tmpBuffer1, inMaxTensor[offset2], RoundMode::CAST_NONE, reduceSize);
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer3, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer3, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer3, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitSize);

    Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Exp<float, false>(tmpBuffer1, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expMaxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<float, half, false>(tmpBuffer2, inExpSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer2, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expSumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SoftmaxFlashV2UpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDImpl(tmpBuffer0, src[offset1], tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Max<float, false>(tmpBuffer0, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Sub<float, false>(tmpBuffer1, inMaxTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Exp<float, false>(expMaxTensor[offset2], tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
    GenericSubNDImpl(dst[offset1], src[offset1], tmpBuffer0, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);



    DataCopy(maxTensor[offset2], tmpBuffer0, reduceSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(tmpBuffer0, dst[offset1], tmpBuffer1, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);

    Mul<float, false>(expSumTensor[offset2], expMaxTensor[offset2], inExpSumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(expSumTensor[offset2], expSumTensor[offset2], tmpBuffer0, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SoftmaxFlashV2UpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling,
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& tmpBuffer3 =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.splitM * FLOAT_REPEAT_SIZE];



    DataCopy(tmpBuffer1, inMaxTensor[offset2], reduceSize);

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Max(maxTensor[offset2], inMaxTensor[offset2], tmpBuffer3, reduceSize);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);

    NewReduceSumLastNDImpl(tmpBuffer3, tmpBuffer0, tmpBuffer2, reduceParam);

    Sub(tmpBuffer1, tmpBuffer1, maxTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer1, tmpBuffer1, reduceSize);
    PipeBarrier<PIPE_V>();

    BroadCastLastImpl(tmpBuffer0, tmpBuffer1,
        { tiling.reduceM, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE, tiling.reduceM, tiling.reduceK });
    PipeBarrier<PIPE_V>();
    Cast(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer0, FLOAT2HALF_ROUND_MODE, reduceSize * B16_BYTE_SIZE);

    Mul(tmpBuffer1, tmpBuffer1, inExpSumTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();
    Add(expSumTensor[offset2], tmpBuffer1, tmpBuffer3, reduceSize);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_no_update_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_no_update_impl.h"
namespace AscendC {
[aicore] inline void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<half>& dst, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceBuffer = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.reduceSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, reduceBuffer, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(reduceBuffer, tmpBuffer0, tmpBuffer2, reduceParam);
    PipeBarrier<PIPE_V>();
    Cast(expSumTensor[offset2], reduceBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
}

[aicore] inline void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<float>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
}

template <typename T>
[aicore] inline void SoftmaxFlashV2NoUpdateExtImpl(const LocalTensor<T>& dst, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
            offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] inline void SoftmaxFlashV2NoUpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
}

[aicore] inline void SoftmaxFlashV2NoUpdateExtImpl(const LocalTensor<half>& dst,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling,
    ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2NoUpdateImpl(dst, expSumTensor, maxTensor, src, workLocal, reduceParam, tiling, offset1,
            offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_nz_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_nz_impl.h"
namespace AscendC {

[aicore] inline void FlashV2NZUpdateGenericImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            splitCount);
    }
    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        DataCopy(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            splitCount);
    }

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, expMaxTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void FlashV2NZUpdateGenericImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    Cast<float, half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    Cast<float, half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    Max<float, false>(tmpBuffer1, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer1, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void FlashV2NZUpdateGenericImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> inMaxTmp = workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    LocalTensor<float> inSumTmp =
        workLocal[tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint32_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint16_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    DataCopy(inMaxTmp, inMaxTensor[offset2], { 1, copyBlockCount, 0, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(maxTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);
    Max<float, false>(maxTensor[offset2], inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inMaxTmp, inMaxTmp, maxTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, inMaxTmp, { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], inMaxTmp, { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
        1, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();

    DataCopy(tmpBuffer1, maxTensor[offset2], { copyBlockCount, 1, 0, 1 });
    DataCopy(tmpBuffer1[FLOAT_NUM_PER_BLK], maxTensor[offset2], { copyBlockCount, 1, 0, 1 });

    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
    DataCopy(inSumTmp, inSumTensor[offset2], { 1, copyBlockCount, 0, 0 });

    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(sumTensor[offset2], tmpBuffer1, { copyBlockCount, 1, 1, 0 });
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount / B16_BYTE_SIZE);

    Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Add<float, false>(sumTensor[offset2], inMaxTmp, sumTensor[offset2], MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashV2NZUpdateImpl(const LocalTensor<half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<half>& expMaxTensor, const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        FlashV2NZUpdateGenericImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
            workLocal, tiling, mask, offset1, offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashV2NZNoUpdateImpl(const LocalTensor<half>& dstTensor,
    const LocalTensor<float>& sumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint32_t paddingTailCount = (tiling.srcM - originalSrcShape.m) * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        if (tiling.tailM == 0 && i == tiling.rangeM - 1 && splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        if (splitCount > paddingTailCount) {
            splitCount -= paddingTailCount;
        }
        SoftMaxGenericNZImpl<true>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, tiling, mask, offset1,
            offset2, splitCount, tailReduceParam);
    }
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_flashv2_impl/softmax_flashv2_common_impl.h"
namespace AscendC {

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] inline void SoftMaxFlashV2NZImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false>
[aicore] inline void SoftMaxFlashV2NZImpl(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NZNoUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NZUpdateImpl<T1, T2, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor,
            inSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2NoUpdate(const LocalTensor<T1>& dst, const LocalTensor<T1>& expSumTensor,
    const LocalTensor<T1>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NoUpdateExtImpl<T1>(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T1, half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T1, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftmaxFlashV2NoUpdateExtImpl<T1>(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2NoUpdate(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NoUpdateExtImpl(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2NoUpdateBasicBlock(dst, expSumTensor, maxTensor, src, workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftmaxFlashV2NoUpdateExtImpl(dst, expSumTensor, maxTensor, src, workLocal, originalSrcShape, tiling,
                reduceParam);
        }
    }
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2NDExtImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2NDImpl(const LocalTensor<T1>& dst, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& src, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2BasicBlockImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NDExtImpl<T1, T2>(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2BasicBlockImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            ReduceLastND reduceParam;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            if constexpr (SupportType<T2, half>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE * HALF_FACTOR };
            } else if constexpr (SupportType<T2, float>()) {
                reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                    DEFAULT_REPEAT_STRIDE };
            }
            SoftmaxFlashV2NDExtImpl<T1, T2>(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

[aicore] inline void SoftmaxFlashV2NDExtImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        SoftmaxFlashV2UpdateImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
            workLocal, reduceParam, tiling, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T1, typename T2, bool isBasicBlock = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2NDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    if constexpr (config.oriSrcM == 0 || config.oriSrcK == 0) {
        if constexpr (isBasicBlock) {
            SoftmaxFlashV2BasicBlock(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                tiling.splitK, tiling.reduceM, tiling.reduceK };
            SoftmaxFlashV2NDExtImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, originalSrcShape, tiling, reduceParam);
        }
    } else {
        constexpr uint32_t basicBlockMaxK = 2048;
        constexpr bool localIsBasicBlock = config.oriSrcK % FLOAT_REPEAT_SIZE == 0 &&
            config.oriSrcK < basicBlockMaxK && config.oriSrcM % FLOAT_NUM_PER_BLK == 0;
        if constexpr (localIsBasicBlock) {
            SoftmaxFlashV2BasicBlock(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, tiling);
        } else {
            uint32_t splitK = 0;
            if constexpr (config.oriSrcK % FLOAT_NUM_PER_BLK == 0) {
                splitK = config.oriSrcK;
            } else {
                splitK = AlignUp(config.oriSrcK, FLOAT_NUM_PER_BLK);
            }
            ReduceLastND reduceParam = { tiling.splitM, config.oriSrcK, tiling.splitM, splitK, tiling.reduceM,
                DEFAULT_REPEAT_STRIDE };
            SoftmaxFlashV2NDExtImpl(dst, expSumTensor, maxTensor, src, expMaxTensor, inExpSumTensor, inMaxTensor,
                workLocal, originalSrcShape, tiling, reduceParam);
        }
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2PostProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    if constexpr (!isUpdate) {
        SoftmaxFlashV2NoUpdate<T1, T2, isBasicBlock, config>(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal,
            originalSrcShape, tiling);
    } else {
        SoftmaxFlashV2NDImpl<T1, T2, isBasicBlock, config>(dstTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor,
            inExpSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling);
    }
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv2_impl.h" 2

namespace AscendC {
[aicore] inline void SoftMaxFlashV2M1CastIntrinsicsImpl(const LocalTensor<float>& dstLocal, const LocalTensor<half>& srcLocal,
    const uint32_t calCount)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<float, half, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void SoftMaxFlashV2M1CastIntrinsicsImpl(const LocalTensor<half>& dstLocal, const LocalTensor<float>& srcLocal,
    const uint32_t calCount)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<half, float, false>(dstLocal, srcLocal, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashV2M1BrcbSubImpl(const LocalTensor<float>& dstLocal, const LocalTensor<float>& src0Local,
    const LocalTensor<float>& src1Local, const LocalTensor<float>& tmpBuffer, const uint32_t srcM, const uint32_t srcK)
{

    uint32_t splitCeilM = DivCeil(srcM, FLOAT_NUM_PER_BLK);
    Brcb(tmpBuffer, src1Local, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    if constexpr (isBasicBlock) {
        uint32_t splitBlock = srcK / FLOAT_REPEAT_SIZE;
        uint8_t offset = srcK / FLOAT_NUM_PER_BLK;
        SetVectorMask<float, MaskMode::COUNTER>(0, srcM * FLOAT_REPEAT_SIZE);
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(dstLocal[FLOAT_REPEAT_SIZE * j], src0Local[FLOAT_REPEAT_SIZE * j], tmpBuffer,
                MASK_PLACEHOLDER, 1, { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    } else if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetVectorMask<float, MaskMode::COUNTER>(0, srcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Sub<float, false>(dstLocal[j * FLOAT_NUM_PER_BLK], src0Local[j * FLOAT_NUM_PER_BLK], tmpBuffer,
                MASK_PLACEHOLDER, 1, { blockStride, blockStride, DEFAULT_BLK_STRIDE, (uint8_t)srcK, (uint8_t)srcK, DEFAULT_REPEAT_STRIDE});
        }
        PipeBarrier<PIPE_V>();
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, srcK);
        for (uint32_t j = 0; j < srcM; j++) {
            Sub<float, false>(dstLocal[j * srcK], src0Local[j * srcK], tmpBuffer[j * FLOAT_NUM_PER_BLK],
                MASK_PLACEHOLDER, 1, { 1, 1, 0, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, 0});
        }
        PipeBarrier<PIPE_V>();
    }
}

template <bool isBroadCast = true>
[aicore] inline void NewReduceMaxLastNDNormImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    SetMaskNorm();
    ResetMask();

    NewReduceMaxLastNDImpl<isBroadCast>(dst, src, tmpTensor, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
}

template <bool isBroadCast = true>
[aicore] inline void NewReduceSumLastNDNormImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const struct ReduceLastND& reduceParam)
{
    SetMaskNorm();
    ResetMask();

    NewReduceSumLastNDImpl<isBroadCast>(dst, src, tmpTensor, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
}

template <typename T, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M12ReduceMax(const LocalTensor<T>& reduceMaxTensorOut,
    const LocalTensor<float>& reduceMaxTensorIn, const uint32_t calCount)
{
    if constexpr (isOutputReduceMax) {
        if constexpr (std::is_same<T, float>::value) {
            CopyRepeatParams copyParams(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
            Copy<float, false>(reduceMaxTensorOut, reduceMaxTensorIn, MASK_PLACEHOLDER, 1, copyParams);
            PipeBarrier<PIPE_V>();
        } else {
            SoftMaxFlashV2M1CastIntrinsicsImpl(reduceMaxTensorOut, reduceMaxTensorIn, calCount);
        }
    }
}

[aicore] inline void SoftmaxFlashV2M1NoUpdateBasicBlockProcess(const LocalTensor<float>& dstLocal,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    UnaryRepeatParams unaryParams;
    const LocalTensor<float>& reduceBuffer = workLocal;
    uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;

    SetMaskNorm();
    ResetMask();


    BasicBlockReduceMaxImpl(maxTensor, srcLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();


    SoftmaxFlashV2M1BrcbSubImpl<true>(srcLocal, srcLocal, maxTensor, reduceBuffer, tiling.splitM, tiling.splitK);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitSize);
    Exp<float, false>(dstLocal, srcLocal, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();


    BasicBlockReduceSumImpl(expSumTensor, dstLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
}

[aicore] inline void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<half>& dstLocal,
    const LocalTensor<half>& expSumTensor, const LocalTensor<half>& maxTensor, const LocalTensor<half>& srcLocal,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& srcBuffer = workLocal;
    const LocalTensor<float>& sumBuffer = srcBuffer[tiling.splitSize];
    const LocalTensor<float>& maxBuffer = sumBuffer[tiling.splitM];
    const LocalTensor<float>& tmpBuffer = maxBuffer[tiling.splitM];

    SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
    SoftmaxFlashV2M1NoUpdateBasicBlockProcess(srcBuffer, sumBuffer, maxBuffer, srcBuffer, tmpBuffer, tiling);
    SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
    SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensor, sumBuffer, tiling.splitM);
    SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensor, maxBuffer, tiling.splitM);
}

[aicore] inline void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<float>& dstLocal,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    SoftmaxFlashV2M1NoUpdateBasicBlockProcess(dstLocal, expSumTensor, maxTensor, srcLocal, workLocal, tiling);
}

[aicore] inline void SoftmaxFlashV2M1NoUpdateBasicBlock(const LocalTensor<half>& dstLocal,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcLocal,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& srcBuffer = workLocal;
    const LocalTensor<float>& tmpBuffer = srcBuffer[tiling.splitSize];

    SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
    SoftmaxFlashV2M1NoUpdateBasicBlockProcess(srcBuffer, expSumTensor, maxTensor, srcBuffer, tmpBuffer, tiling);
    SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
}

template <typename T, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1BasicBlockImplProcess(const LocalTensor<float>& dstLocal,
    const LocalTensor<T>& outReduceMax, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    CopyRepeatParams copyParams(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);

    const LocalTensor<float>& reduceBuffer = workLocal;
    const LocalTensor<float>& tmpBufferM1 = workLocal[tiling.splitM * FLOAT_REPEAT_SIZE];

    uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;

    SetMaskNorm();
    ResetMask();


    BasicBlockReduceMaxImpl(tmpBufferM1, srcLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitM);

    SoftmaxFlashV2M12ReduceMax<T, isOutputReduceMax>(outReduceMax, tmpBufferM1, tiling.splitM);


    Copy<float, false>(reduceBuffer, inMaxTensor, MASK_PLACEHOLDER, 1, copyParams);
    PipeBarrier<PIPE_V>();


    Max<float, false>(maxTensor, inMaxTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(tmpBufferM1, reduceBuffer, maxTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(expMaxTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    SoftmaxFlashV2M1BrcbSubImpl<true>(dstLocal, srcLocal, maxTensor, tmpBufferM1, tiling.splitM, tiling.splitK);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitSize);
    Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();

    BasicBlockReduceSumImpl(tmpBufferM1, dstLocal, reduceBuffer, splitBlock, tiling.splitM, tiling.splitK);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.splitM);


    Mul<float, false>(inExpSumTensor, expMaxTensor, inExpSumTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(expSumTensor, inExpSumTensor, tmpBufferM1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<float>& dstLocal,
    const LocalTensor<float>& outReduceMax, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal,
    const LocalTensor<float>& expMaxTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    SoftmaxFlashV2M1BasicBlockImplProcess<float, isOutputReduceMax>(dstLocal, outReduceMax, expSumTensor,
        maxTensor, srcLocal, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, tiling);
}

template <bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<half>& dstLocal,
    const LocalTensor<half>& outReduceMax, const LocalTensor<half>& expSumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& srcLocal, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<half>& inExpSumTensor, const LocalTensor<half>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& srcBuffer = workLocal;
    const LocalTensor<float>& sumBuffer = srcBuffer[tiling.splitSize];
    const LocalTensor<float>& maxBuffer = sumBuffer[tiling.splitM];
    const LocalTensor<float>& expMaxBuffer = maxBuffer[tiling.splitM];
    const LocalTensor<float>& tmpBuffer = expMaxBuffer[tiling.splitM];

    SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
    SoftMaxFlashV2M1CastIntrinsicsImpl(sumBuffer, inExpSumTensor, tiling.splitM);
    SoftMaxFlashV2M1CastIntrinsicsImpl(maxBuffer, inMaxTensor, tiling.splitM);

    SoftmaxFlashV2M1BasicBlockImplProcess<half, isOutputReduceMax>(srcBuffer, outReduceMax, sumBuffer, maxBuffer,
        srcBuffer, expMaxBuffer, sumBuffer, maxBuffer, tmpBuffer, tiling);

    SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
    SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensor, sumBuffer, tiling.splitM);
    SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensor, maxBuffer, tiling.splitM);
    SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensor, expMaxBuffer, tiling.splitM);
}

template <bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1BasicBlockImpl(const LocalTensor<half>& dstLocal,
    const LocalTensor<float>& outReduceMax, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcLocal, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling)
{
    const LocalTensor<float>& srcBuffer = workLocal;
    const LocalTensor<float>& expMaxBuffer = srcBuffer[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer = expMaxBuffer[tiling.splitM];

    SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocal, tiling.splitSize);
    SoftmaxFlashV2M1BasicBlockImplProcess<float, isOutputReduceMax>(srcBuffer, outReduceMax, expSumTensor, maxTensor,
        srcBuffer, expMaxBuffer, inExpSumTensor, inMaxTensor, tmpBuffer, tiling);
    SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocal, srcBuffer, tiling.splitSize);
    SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensor, expMaxBuffer, tiling.splitM);
}

[aicore] inline void SoftmaxFlashV2M1NoUpdateImplProcess(const LocalTensor<float>& dstLocal, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam)
{
    UnaryRepeatParams unaryParams;
    const LocalTensor<float>& tmpBuffer = workLocal;

    NewReduceMaxLastNDNormImpl<false>(maxTensor, srcLocal, tmpBuffer, reduceParam);

    SoftmaxFlashV2M1BrcbSubImpl(dstLocal, srcLocal, maxTensor, tmpBuffer, reduceParam.srcM, reduceParam.srcK);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * reduceParam.srcK);
    Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDNormImpl<false>(expSumTensor, dstLocal, tmpBuffer, reduceParam);
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2M1NoUpdateImplPreCast(LocalTensor<float>& dstLocalOut,
    LocalTensor<float>& expSumTensorOut, LocalTensor<float>& maxTensorOut, LocalTensor<float>& srcLocalOut,
    LocalTensor<float>& workLocalOut, const LocalTensor<T1>& dstLocalIn, const LocalTensor<T2>& expSumTensorIn,
    const LocalTensor<T2>& maxTensorIn, const LocalTensor<T1>& srcLocalIn, const LocalTensor<float>& workLocalIn,
    const ReduceLastND& param)
{
    uint32_t splitM = DivCeil(param.srcM, FLOAT_NUM_PER_BLK) * FLOAT_NUM_PER_BLK;
    uint32_t splitSize = param.srcM * param.srcK;
    if constexpr (SupportType<T1, float>() && SupportType<T2, float>()) {
        dstLocalOut = dstLocalIn;
        expSumTensorOut = expSumTensorIn;
        maxTensorOut = maxTensorIn;
        srcLocalOut = srcLocalIn;
        workLocalOut = workLocalIn;
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, half>()) {
        LocalTensor<float> srcBuffer = workLocalIn;
        LocalTensor<float> sumBuffer = srcBuffer[splitSize];
        LocalTensor<float> maxBuffer = sumBuffer[splitM];
        LocalTensor<float> tmpBuffer = maxBuffer[splitM];
        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
        dstLocalOut = srcBuffer;
        expSumTensorOut = sumBuffer;
        maxTensorOut = maxBuffer;
        srcLocalOut = srcBuffer;
        workLocalOut = tmpBuffer;
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, float>()) {
        LocalTensor<float> srcBuffer = workLocalIn;
        LocalTensor<float> tmpBuffer = srcBuffer[splitM];
        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
        dstLocalOut = srcBuffer;
        expSumTensorOut = expSumTensorIn;
        maxTensorOut = maxTensorIn;
        srcLocalOut = srcBuffer;
        workLocalOut = tmpBuffer;
    }
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2M1NoUpdateImplPostCast(const LocalTensor<T1>& dstLocalOut,
    const LocalTensor<T2>& expSumTensorOut, const LocalTensor<T2>& maxTensorOut,
    const LocalTensor<float>& dstLocalIn, const LocalTensor<float>& expSumTensorIn, const LocalTensor<float>& maxTensorIn,
    const ReduceLastND& param)
{
    uint32_t splitM = param.srcM;
    uint32_t splitSize = param.srcM * param.srcK;
    if constexpr (SupportType<T1, half>() && SupportType<T2, half>()) {
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensorOut, expSumTensorIn, splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensorOut, maxTensorIn, splitM);
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, float>()) {
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
    }
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2M1NoUpdateImpl(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& expSumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcLocal, const LocalTensor<float>& workLocal,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
{
    LocalTensor<float> dstLocalOut;
    LocalTensor<float> expSumTensorOut;
    LocalTensor<float> maxTensorOut;
    LocalTensor<float> srcLocalOut;
    LocalTensor<float> tmpBuffer;

    SoftmaxFlashV2M1NoUpdateImplPreCast<T1, T2>(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut, tmpBuffer,
        dstLocal, expSumTensor, maxTensor, srcLocal, workLocal, reduceParam);
    SoftmaxFlashV2M1NoUpdateImplProcess(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut, tmpBuffer, reduceParam);
    SoftmaxFlashV2M1NoUpdateImplPostCast<T1, T2>(dstLocal, expSumTensor, maxTensor, dstLocalOut, expSumTensorOut, maxTensorOut, reduceParam);
}

template <typename T, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1UpdateImplProcess(const LocalTensor<float>& dstLocal,
    const LocalTensor<T>& outReduceMax, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& srcLocal, const LocalTensor<float>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    CopyRepeatParams copyParams(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& reduceBuffer = workLocal[tiling.reduceSize];

    NewReduceMaxLastNDNormImpl<false>(tmpBuffer0, srcLocal, reduceBuffer, reduceParam);

    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);
    SoftmaxFlashV2M12ReduceMax<T, isOutputReduceMax>(outReduceMax, tmpBuffer0, reduceParam.srcM);

    Max<float, false>(tmpBuffer0, inMaxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(reduceBuffer, inMaxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(expMaxTensor, reduceBuffer, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SoftmaxFlashV2M1BrcbSubImpl(dstLocal, srcLocal, tmpBuffer0, reduceBuffer, reduceParam.srcM, reduceParam.srcK);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM * reduceParam.srcK);
    Exp<float, false>(dstLocal, dstLocal, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);
    Copy<float, false>(maxTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, copyParams);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDNormImpl<false>(tmpBuffer0, dstLocal, reduceBuffer, reduceParam);

    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcM);

    Mul<float, false>(expSumTensor, expMaxTensor, inExpSumTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(expSumTensor, expSumTensor, tmpBuffer0, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2M1UpdateImplPreCast(LocalTensor<float>& dstLocalOut, LocalTensor<float>& expSumTensorOut,
    LocalTensor<float>& maxTensorOut, LocalTensor<float>& srcLocalOut, LocalTensor<float>& expMaxTensorOut, LocalTensor<float>& inExpSumTensorOut,
    LocalTensor<float>& inMaxTensorOut, LocalTensor<float>& workLocalOut, const LocalTensor<T1>& dstLocalIn, const LocalTensor<T2>& expSumTensorIn,
    const LocalTensor<T2>& maxTensorIn, const LocalTensor<T1>& srcLocalIn, const LocalTensor<T1>& expMaxTensorIn,
    const LocalTensor<T2>& inExpSumTensorIn, const LocalTensor<T2>& inMaxTensorIn, const LocalTensor<float>& workLocalIn, const ReduceLastND& param)
{
    uint32_t splitM = DivCeil(param.srcM, FLOAT_NUM_PER_BLK) * FLOAT_NUM_PER_BLK;
    uint32_t splitSize = param.srcM * param.srcK;
    if constexpr (SupportType<T1, float>() && SupportType<T2, float>()) {
        dstLocalOut = dstLocalIn;
        expSumTensorOut = expSumTensorIn;
        maxTensorOut = maxTensorIn;
        srcLocalOut = srcLocalIn;
        expMaxTensorOut = expMaxTensorIn;
        inExpSumTensorOut = inExpSumTensorIn;
        inMaxTensorOut = inMaxTensorIn;
        workLocalOut = workLocalIn;
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, half>()) {
        const LocalTensor<float>& srcBuffer = workLocalIn;
        const LocalTensor<float>& sumBuffer = srcBuffer[splitSize];
        const LocalTensor<float>& maxBuffer = sumBuffer[splitM];
        const LocalTensor<float>& expMaxBuffer = maxBuffer[splitM];
        const LocalTensor<float>& tmpBuffer = expMaxBuffer[splitM];
        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(sumBuffer, inExpSumTensorIn, param.srcM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxBuffer, inMaxTensorIn, param.srcM);
        dstLocalOut = srcBuffer;
        expSumTensorOut = sumBuffer;
        maxTensorOut = maxBuffer;
        srcLocalOut = srcBuffer;
        expMaxTensorOut = expMaxBuffer;
        inExpSumTensorOut = sumBuffer;
        inMaxTensorOut = maxBuffer;
        workLocalOut = tmpBuffer;
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, float>()) {
        const LocalTensor<float>& srcBuffer = workLocalIn;
        const LocalTensor<float>& expMaxBuffer = srcBuffer[splitSize];
        const LocalTensor<float>& tmpBuffer = expMaxBuffer[splitM];
        SoftMaxFlashV2M1CastIntrinsicsImpl(srcBuffer, srcLocalIn, splitSize);
        dstLocalOut = srcBuffer;
        expSumTensorOut = expSumTensorIn;
        maxTensorOut = maxTensorIn;
        srcLocalOut = srcBuffer;
        expMaxTensorOut = expMaxBuffer;
        inExpSumTensorOut = inExpSumTensorIn;
        inMaxTensorOut = inMaxTensorIn;
        workLocalOut = tmpBuffer;
    }
}

template <typename T1, typename T2>
[aicore] inline void SoftmaxFlashV2M1UpdateImplPostCast(const LocalTensor<T1>& dstLocalOut, const LocalTensor<T2>& expSumTensorOut,
    const LocalTensor<T2>& maxTensorOut, const LocalTensor<T1>& expMaxTensorOut,
    const LocalTensor<float>& dstLocalIn, const LocalTensor<float>& expSumTensorIn, const LocalTensor<float>& maxTensorIn,
    const LocalTensor<float>& expMaxTensorIn, const ReduceLastND& param)
{
    uint32_t splitM = param.srcM;
    uint32_t splitSize = param.srcM * param.srcK;
    if constexpr (SupportType<T1, half>() && SupportType<T2, half>()) {
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expSumTensorOut, expSumTensorIn, splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(maxTensorOut, maxTensorIn, splitM);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensorOut, expMaxTensorIn, splitM);
    } else if constexpr (SupportType<T1, half>() && SupportType<T2, float>()) {
        SoftMaxFlashV2M1CastIntrinsicsImpl(dstLocalOut, dstLocalIn, splitSize);
        SoftMaxFlashV2M1CastIntrinsicsImpl(expMaxTensorOut, expMaxTensorIn, splitM);
    }
}

template <typename T1, typename T2, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1UpdateImpl(const LocalTensor<T1>& dstLocal, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& expSumTensor, const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcLocal,
    const LocalTensor<T1>& expMaxTensor, const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
{
    LocalTensor<float> dstLocalOut;
    LocalTensor<float> expSumTensorOut;
    LocalTensor<float> maxTensorOut;
    LocalTensor<float> srcLocalOut;
    LocalTensor<float> expMaxTensorOut;
    LocalTensor<float> inExpSumTensorOut;
    LocalTensor<float> inMaxTensorOut;
    LocalTensor<float> tmpBuffer;

    SoftmaxFlashV2M1UpdateImplPreCast<T1, T2>(dstLocalOut, expSumTensorOut, maxTensorOut, srcLocalOut,
        expMaxTensorOut, inExpSumTensorOut, inMaxTensorOut, tmpBuffer,
        dstLocal, expSumTensor, maxTensor, srcLocal, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, reduceParam);

    SoftmaxFlashV2M1UpdateImplProcess<T2, isOutputReduceMax>(dstLocalOut, outReduceMax, expSumTensorOut, maxTensorOut,
        srcLocalOut, expMaxTensorOut, inExpSumTensorOut, inMaxTensorOut, tmpBuffer, reduceParam, tiling);

    SoftmaxFlashV2M1UpdateImplPostCast<T1, T2>(dstLocal, expSumTensor, maxTensor, expMaxTensor,
        dstLocalOut, expSumTensorOut, maxTensorOut, expMaxTensorOut, reduceParam);
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1ImplProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& expSumTensor, const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor,
    const LocalTensor<T1>& expMaxTensor, const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<float>& workLocal, const ReduceLastND& reduceParam, const SoftMaxTiling& tiling)
{
    if constexpr (isBasicBlock && !isUpdate) {
        SoftmaxFlashV2M1NoUpdateBasicBlock(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal, tiling);
    } else if constexpr (isBasicBlock && isUpdate) {
        SoftmaxFlashV2M1BasicBlockImpl<isOutputReduceMax>(dstTensor, outReduceMax, expSumTensor, maxTensor,
            srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, tiling);
    } else if constexpr (!isBasicBlock && !isUpdate) {
        SoftmaxFlashV2M1NoUpdateImpl<T1, T2>(dstTensor, expSumTensor, maxTensor, srcTensor, workLocal, reduceParam, tiling);
    } else if constexpr (!isBasicBlock && isUpdate) {
        SoftmaxFlashV2M1UpdateImpl<T1, T2, isOutputReduceMax>(dstTensor, outReduceMax, expSumTensor, maxTensor,
            srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, workLocal, reduceParam, tiling);
    }
}

template <typename T1, typename T2, bool isUpdate = false, bool isBasicBlock = false, bool isOutputReduceMax = false>
[aicore] inline void SoftmaxFlashV2M1PostProcess(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& expSumTensor, const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor,
    const LocalTensor<T1>& expMaxTensor, const LocalTensor<T2>& inExpSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK};
    ReduceLastND tailParam = { tiling.tailM, originalSrcShape.k, tiling.tailM, tiling.splitK, tiling.tailM, tiling.reduceK};

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxFlashV2M1ImplProcess<T1, T2, isUpdate, isBasicBlock, isOutputReduceMax>(dstTensor[offset1],
            outReduceMax[offset2], expSumTensor[offset2], maxTensor[offset2], srcTensor[offset1],
            expMaxTensor[offset2], inExpSumTensor[offset2], inMaxTensor[offset2], workLocal, reduceParam, tiling);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceM;
    }

    if constexpr (!isBasicBlock) {
        if (tiling.tailM != 0) {
            offset1 = tiling.rangeM * tiling.splitSize;
            offset2 = tiling.rangeM * tiling.reduceM;
            SoftmaxFlashV2M1ImplProcess<T1, T2, isUpdate, isBasicBlock, isOutputReduceMax>(dstTensor[offset1],
                outReduceMax[offset2], expSumTensor[offset2], maxTensor[offset2], srcTensor[offset1],
                expMaxTensor[offset2], inExpSumTensor[offset2], inMaxTensor[offset2], workLocal, tailParam, tiling);
        }
    }
    SetMaskNorm();
    ResetMask();
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv2_base_impl.h" 2





namespace AscendC {
[aicore] inline constexpr SoftMaxTiling SoftMaxFlashV2TilingFuncImpl(const uint32_t srcM, const uint32_t srcK,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false,
    const bool isFlashOutputBrc = false)
{
    SoftMaxTiling softmaxTiling;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = srcM;
    softmaxTiling.srcK = srcK;
    softmaxTiling.srcSize = srcM * srcK;

    softmaxTiling.outMaxM = srcM;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = srcM * elementNumPerBlk;

    if (isDataFormatNZ) {
        softmaxTiling.reduceM = localWorkSpaceSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT * SOFTMAX_NZ_TILING_NEEDBLOCK + srcK);
    } else {
        if (isBasicBlock && srcK % FLOAT_REPEAT_SIZE == 0 && srcM % SOFTMAX_BASIC_TILE_NUM == 0) {
            softmaxTiling.reduceM =
                CalculateNDSplitM(localWorkSpaceSize, dataTypeSize1, elementNumPerBlk, { srcM, srcK }, isBasicBlock);
        } else {
            softmaxTiling.reduceM = (dataTypeSize1 == B16_BYTE_SIZE) ?
                localWorkSpaceSize / (SOFTMAX_COMPUTE_DIM * elementNumPerBlk + srcK + FLOAT_REPEAT_SIZE) :
                localWorkSpaceSize / (elementNumPerBlk + FLOAT_REPEAT_SIZE);
        }
    }

    uint32_t softmaxBasicTileNum = SOFTMAX_BASIC_TILE_NUM;
    if (isFlashOutputBrc && dataTypeSize1 == B16_BYTE_SIZE) {
        softmaxBasicTileNum = HALF_NUM_PER_BLK;
    }

    if (softmaxTiling.reduceM < srcM && softmaxTiling.reduceM > softmaxBasicTileNum) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / softmaxBasicTileNum * softmaxBasicTileNum;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < srcM ? softmaxTiling.reduceM : srcM;

    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = srcK;
    softmaxTiling.splitSize = softmaxTiling.reduceM * srcK;

    softmaxTiling.rangeM = srcM / softmaxTiling.reduceM;
    softmaxTiling.tailM = srcM % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * srcK;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;

    if (isFlashOutputBrc && (softmaxTiling.rangeM > MIN_BLOCK_LEN || softmaxTiling.tailM != 0)) {


                                                                                                           ;
    }
    return softmaxTiling;
}

template <typename T1, typename T2, bool isUpdate, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline SoftMaxTiling SoftmaxFlashV2UpdateTilingImpl(const LocalTensor<T1>& srcTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    if constexpr (isDataFormatNZ) {
        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }
        uint32_t workLocalSize = workLocal.GetSize();
        return SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, false, true);
    } else {
        if constexpr (!config.isCheckTiling) {
            return tiling;
        }

        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }

        if (srcNDinfo.m == tiling.srcM && srcNDinfo.k == tiling.srcK) {
            return tiling;
        }

        SoftMaxTiling softmaxTiling;
        uint32_t workLocalSize = workLocal.GetSize();

        if constexpr (config.mode == SoftmaxMode::SOFTMAX_OUTPUT_WITHOUT_BRC) {
            softmaxTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, isBasicBlock, false, true);
        } else {
            softmaxTiling = SoftMaxFlashV2TilingFuncImpl(srcNDinfo.m, srcNDinfo.k, sizeof(T1), sizeof(T2), workLocalSize, isUpdate, isBasicBlock);
        }
        return softmaxTiling;
    }
}

template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{


                                                          ;

    SoftMaxTiling newTiling = SoftmaxFlashV2UpdateTilingImpl<T1, T2, isUpdate, isBasicBlock, isDataFormatNZ, config>(
        srcTensor, workLocal, tiling, softmaxShapeInfo);

    LastAxisShapeND originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    }

    if constexpr (isDataFormatNZ) {
        SoftMaxFlashV2NZImpl<T1, T2, isUpdate, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
            expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    } else if constexpr (config.mode == SoftmaxMode::SOFTMAX_OUTPUT_WITHOUT_BRC) {
        SoftmaxFlashV2M1PostProcess<T1, T2, isUpdate, isBasicBlock, false>(dstTensor, maxTensor, sumTensor, maxTensor,
            srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    } else {
        SoftmaxFlashV2PostProcess<T1, T2, isUpdate, isBasicBlock, config>(dstTensor, sumTensor, maxTensor, srcTensor,
            expMaxTensor, inSumTensor, inMaxTensor, workLocal, originalSrcShape, newTiling);
    }
}

template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxFlashV2Impl<T1, T2, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2Impl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& sumTensor,
    const LocalTensor<T2>& maxTensor, const LocalTensor<T1>& srcTensor, const LocalTensor<T1>& expMaxTensor,
    const LocalTensor<T2>& inSumTensor, const LocalTensor<T2>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    if constexpr (isDataFormatNZ) {
        LastAxisShapeND srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
            ShapeInfo srcShape = srcTensor.GetShapeInfo();
            srcNDinfo = GetLastAxisShapeND(srcShape);
        }
        uint32_t workLocalSize = workLocal.GetSize();
        if (workLocalSize < SOFTMAX_SHAPE_NZ_BASIC_COUNT * SOFTMAX_NZ_TILING_NEEDBLOCK + srcNDinfo.k) {
            PopStackBuffer<float, TPosition::LCM>(workLocal);
        }
    }

    SoftmaxFlashV2Impl<T1, T2, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, workLocal, tiling, softmaxShapeInfo);
}


template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ,
    const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2MaxImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& outSum, const LocalTensor<T2>& outMax, const LocalTensor<T1>& srcTensor,
    const LocalTensor<T1>& outexpMax, const LocalTensor<T2>& inSum, const LocalTensor<T2>& inMax,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{

                                                                                                         ;

    LastAxisShapeND originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    if constexpr (config.mode == SoftmaxMode::SOFTMAX_OUTPUT_WITHOUT_BRC) {
        SoftmaxFlashV2M1PostProcess<T1, T2, isUpdate, isBasicBlock, true>(dstTensor, outReduceMax, outSum, outMax,
            srcTensor, outexpMax, inSum, inMax, workLocal, originalSrcShape, tiling);
    }
}

template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ,
    const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2MaxImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& outSum, const LocalTensor<T2>& outMax, const LocalTensor<T1>& srcTensor,
    const LocalTensor<T1>& outexpMax, const LocalTensor<T2>& inSum, const LocalTensor<T2>& inMax,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxFlashV2MaxImpl<T1, T2, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor,
        outReduceMax, outSum, outMax, srcTensor, outexpMax, inSum, inMax, workLocal, tiling, softmaxShapeInfo);
}

template <typename T1, typename T2, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ,
    const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV2MaxImpl(const LocalTensor<T1>& dstTensor, const LocalTensor<T2>& outReduceMax,
    const LocalTensor<T2>& outSum, const LocalTensor<T2>& outMax, const LocalTensor<T1>& srcTensor,
    const LocalTensor<T1>& outexpMax, const LocalTensor<T2>& inSum, const LocalTensor<T2>& inMax,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxFlashV2MaxImpl<T1, T2, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor,
        outReduceMax, outSum, outMax, srcTensor, outexpMax, inSum, inMax, workLocal, tiling, softmaxShapeInfo);
}

}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
[aicore] inline constexpr SoftMaxTiling SoftMaxFlashV2TilingFunc(const SoftMaxShapeInfo& shapeInfo,
    const uint32_t dataTypeSize1, const uint32_t dataTypeSize2, const uint32_t localWorkSpaceSize,
    const bool isUpdate = false, const bool isBasicBlock = false, const bool isDataFormatNZ = false, const bool isFlashOutputBrc = false)
{
    return SoftMaxFlashV2TilingFuncImpl(shapeInfo.srcM, shapeInfo.srcK, dataTypeSize1, dataTypeSize2,
        localWorkSpaceSize / B32_BYTE_SIZE, isUpdate, isBasicBlock, isDataFormatNZ, isFlashOutputBrc);
}
# 73 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 113 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<half, float, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, tiling, softmaxShapeInfo);
                                       ;
}
# 156 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& expSumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inExpSumTensor, const LocalTensor<T>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<T, T, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
# 199 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<half>& dstTensor, const LocalTensor<float>& expSumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inExpSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2Impl<half, float, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}
# 242 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& outReduceMax,
    const LocalTensor<T>& outExpSum, const LocalTensor<T>& outMax, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& outExpMax, const LocalTensor<T>& inExpSum, const LocalTensor<T>& inMax,
    const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2MaxImpl<T, T, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, outReduceMax,
        outExpSum, outMax, srcTensor, outExpMax, inExpSum, inMax, tiling, softmaxShapeInfo);
                                       ;
}
# 285 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv2.h"
template <typename T, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& outReduceMax,
    const LocalTensor<T>& outExpSum, const LocalTensor<T>& outMax, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& outExpMax, const LocalTensor<T>& inExpSum, const LocalTensor<T>& inMax,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV2MaxImpl<T, T, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, outReduceMax,
        outExpSum, outMax, srcTensor, outExpMax, inExpSum, inMax, sharedTmpBuffer, tiling, softmaxShapeInfo);
                                       ;
}

}
#pragma end_pipe
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv3.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv3.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv3_base_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv3_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv3_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_flashv3_impl.h"
namespace AscendC {

[aicore] inline void SoftmaxFlashV3ReduceSumImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& rowMeanLocal, const LocalTensor<float>& rowMeanGlobalTmp,
    const LocalTensor<float>& meanTmp, const struct ReduceLastND& reduceParam, const uint32_t& baseK,
    const uint32_t& reduceSize)
{
    const uint32_t splitCount = reduceParam.originalSrcK / FLOAT_REPEAT_SIZE;
    const uint32_t tailSrcK = reduceParam.originalSrcK % FLOAT_REPEAT_SIZE;
    const uint16_t srcRepeatStride = reduceParam.srcK / FLOAT_NUM_PER_BLK;

    for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
        BlockReduceSum<float, false>(dst[FLOAT_REPEAT_SIZE * i], src[i * reduceParam.srcK], FLOAT_NUM_PER_BLK,
            MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    }
    uint8_t remainRepeat = splitCount - FLOAT_NUM_PER_BLK;
    if (remainRepeat != 0) {
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < reduceParam.originalSrcM; j++) {
        Add<float, false>(dst[j * FLOAT_REPEAT_SIZE], src[SOFTMAX_FLOAT_SPECIAL_BLOCKREDUCE_LEN + j * reduceParam.srcK],
            dst[j * FLOAT_REPEAT_SIZE], 1, remainRepeat, { 1, 1, 1, 0, DEFAULT_REPEAT_STRIDE, 0 });
        }
    }
    if (tailSrcK != 0) {
      PipeBarrier<PIPE_V>();
      TailAddImpl(dst, src, reduceParam, tailSrcK, srcRepeatStride, splitCount);
      ResetMask();
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * FLOAT_REPEAT_SIZE);
    BlockReduceSum<float, false>(rowMeanLocal, dst, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    UnaryRepeatParams unaryParams;
    if (baseK != 0) {
        Muls<float, false>(rowMeanLocal, rowMeanLocal, static_cast<float>(1.0f / baseK), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    BlockReduceSum<float, false>(rowMeanGlobalTmp, rowMeanLocal, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    const uint32_t repeat = (reduceParam.originalSrcM + BRCB_BROADCAST_NUMBER - 1) / BRCB_BROADCAST_NUMBER;
    Brcb(dst, rowMeanGlobalTmp, (uint8_t)repeat, { 1, BRCB_BROADCAST_NUMBER });
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.originalSrcM * reduceParam.dstK);
    Copy<float, false>(rowMeanGlobalTmp, dst, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Muls<float, false>(meanTmp, rowMeanGlobalTmp, static_cast<float>(1.0f / FLOAT_NUM_PER_BLK), MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void ModifyInputImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& meanTmp, const LocalTensor<float>& workLocal, const LocalTensor<float>& tmpBuffer2,
    const ReduceLastND& reduceParam, const SoftMaxTiling& tiling, const SoftMaxParams& params, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& rowMeanLocal = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& rowMeanGlobalTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const uint32_t baseK = reduceParam.originalSrcK / params.splitMeanCnt;

    SoftmaxFlashV3ReduceSumImpl(tmpBuffer1, src, rowMeanLocal, rowMeanGlobalTmp, meanTmp, reduceParam, baseK, reduceSize);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, reduceSize);
    Sub<float, false>(rowMeanGlobalTmp, meanTmp, rowMeanLocal, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    UnaryRepeatParams unaryParams;
    Muls<float, false>(rowMeanGlobalTmp, rowMeanGlobalTmp, static_cast<float>(params.alpha / (1.0f - params.alpha)),
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < reduceParam.originalSrcM; i++) {
        Brcb(tmpBuffer1, rowMeanGlobalTmp[i * params.splitMeanCnt], 1, {1, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, baseK);
        const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)(baseK / FLOAT_NUM_PER_BLK), 1};
        Copy<float, false>(tmpBuffer2, tmpBuffer1, MASK_PLACEHOLDER, params.splitMeanCnt, copyRepeatParams);
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, reduceParam.srcK);
        Sub<float, false>(src[i * reduceParam.srcK], src[i * reduceParam.srcK], tmpBuffer2, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}
[aicore] inline void ModifyMaxImpl(const LocalTensor<float>& maxTensor, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& src, const LocalTensor<float>& meanTmp, const LocalTensor<float>& maxTmp,
    const LocalTensor<float>& shiftVal, const SoftMaxParams& params, const uint32_t& reduceSize)
{
    float scalar = params.alpha / (1 - params.alpha);

    Sub(shiftVal, meanTmp, meanTensor, reduceSize);
    PipeBarrier<PIPE_V>();
    Muls(shiftVal, shiftVal, scalar, reduceSize);
    PipeBarrier<PIPE_V>();
    Add(maxTensor, maxTmp, shiftVal, reduceSize);
}

[aicore] inline void SoftmaxFlashV3NoUpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, const SoftMaxParams& params,
    const ReduceLastND& reduceParam, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& meanTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& maxTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& shiftCurr = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 3];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    ModifyInputImpl(tmpBuffer0, tmpBuffer0, meanTmp, workLocal, tmpBuffer2, reduceParam, tiling, params, reduceSize);
    PipeBarrier<PIPE_V>();
    Copy(meanTensor[offset2], meanTmp, reduceSize, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    ResetMask();

    NewReduceMaxLastNDImpl(maxTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTensor[offset2], meanTensor[offset2], tmpBuffer0, meanTmp, maxTmp, shiftCurr, params, reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(shiftCurr, maxTensor[offset2], shiftCurr, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, shiftCurr, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(expSumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
}

[aicore] inline void SoftmaxFlashV3TailImpl(const SoftMaxTiling& tiling, ReduceLastND& reduceParam,
    uint32_t& offset1, uint32_t& offset2, uint32_t& splitSize, uint32_t& reduceSize)
{
    offset2 = tiling.rangeM * tiling.reduceSize;
    offset1 = tiling.rangeM * tiling.splitSize;
    splitSize = tiling.tailSplitSize;
    reduceSize = tiling.tailReduceSize;
    reduceParam.originalSrcM = tiling.tailM;
    reduceParam.srcM = tiling.tailM;
    reduceParam.dstM = tiling.tailM;
}

[aicore] inline void SoftmaxFlashV3NoUpdateExtImpl(const LocalTensor<half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;

    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxFlashV3NoUpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, workLocal, tiling, params, reduceParam,
            offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        PipeBarrier<PIPE_V>();
    }

    if (tiling.tailM != 0) {
        SoftmaxFlashV3TailImpl(tiling, reduceParam, offset1, offset2, splitSize, reduceSize);
        SoftmaxFlashV3NoUpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, workLocal, tiling, params, reduceParam,
            offset1, offset2, splitSize, reduceSize);
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void SoftmaxFlashV3UpdateMeanImpl(const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& meanTmp,
    const SoftMaxParams& params, const uint32_t& reduceSize)
{

    Muls(meanTensor, inMeanTensor, params.loopCnt - 1.0f, reduceSize);
    PipeBarrier<PIPE_V>();
    Add(meanTensor, meanTensor, meanTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Muls(meanTensor, meanTensor, static_cast<float>(1.0f / params.loopCnt), reduceSize);
}

[aicore] inline void SoftmaxFlashV3UpdateImpl(const LocalTensor<half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<half>& expMaxTensor, const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, const ReduceLastND& reduceParam, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitSize, const uint32_t& reduceSize)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& meanTmp = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float>& maxTmp = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize];
    const LocalTensor<float>& shiftCurr = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 2];
    const LocalTensor<float>& shiftPrev = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 3];
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitM * 64 + tiling.reduceSize * 4];

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    ModifyInputImpl(tmpBuffer0, tmpBuffer0, meanTmp, workLocal, tmpBuffer2, reduceParam, tiling, params, reduceSize);
    PipeBarrier<PIPE_V>();
    SoftmaxFlashV3UpdateMeanImpl(meanTensor[offset2], inMeanTensor[offset2], meanTmp, params, reduceSize);
    PipeBarrier<PIPE_V>();

    NewReduceMaxLastNDImpl(maxTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTensor[offset2], meanTensor[offset2], tmpBuffer0, meanTmp, maxTmp, shiftCurr, params, reduceSize);
    PipeBarrier<PIPE_V>();

    ModifyMaxImpl(maxTmp, meanTensor[offset2], tmpBuffer0, inMeanTensor[offset2], inMaxTensor[offset2], shiftPrev,
        params, reduceSize);
    PipeBarrier<PIPE_V>();

    Max(maxTensor[offset2], maxTensor[offset2], maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(maxTmp, inMaxTensor[offset2], maxTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();

    Add(maxTmp, shiftPrev, maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Exp(maxTmp, maxTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    Mul(expSumTensor[offset2], maxTmp, inExpSumTensor[offset2], reduceSize);
    PipeBarrier<PIPE_V>();

    Sub(shiftCurr, maxTensor[offset2], shiftCurr, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, shiftCurr, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();
    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(meanTmp, tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();
    Add(expSumTensor[offset2], expSumTensor[offset2], meanTmp, reduceSize);
    PipeBarrier<PIPE_V>();
    BroadCastLastImpl(tmpBuffer0, maxTmp,
        { tiling.reduceM, B16_BYTE_SIZE * DEFAULT_REPEAT_STRIDE, tiling.reduceM, tiling.reduceK });
    PipeBarrier<PIPE_V>();
    Cast(expMaxTensor[offset2 * B16_BYTE_SIZE], tmpBuffer0, FLOAT2HALF_ROUND_MODE, reduceSize * B16_BYTE_SIZE);
}

[aicore] inline void SoftmaxFlashV3NDExtImpl(const LocalTensor<half>& dst, const LocalTensor<float>& meanTensor,
    const LocalTensor<float>& expSumTensor, const LocalTensor<float>& maxTensor, const LocalTensor<half>& src,
    const LocalTensor<half>& expMaxTensor, const LocalTensor<float>& inMeanTensor, const LocalTensor<float>& inExpSumTensor,
    const LocalTensor<float>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params, ReduceLastND& reduceParam)
{
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;

    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxFlashV3UpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, expMaxTensor, inMeanTensor, inExpSumTensor,
            inMaxTensor, workLocal, tiling, params, reduceParam, offset1, offset2, splitSize, reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        PipeBarrier<PIPE_V>();
    }

    if (tiling.tailM != 0) {
        SoftmaxFlashV3TailImpl(tiling, reduceParam, offset1, offset2, splitSize, reduceSize);
        SoftmaxFlashV3UpdateImpl(dst, meanTensor, expSumTensor, maxTensor, src, expMaxTensor, inMeanTensor, inExpSumTensor,
            inMaxTensor, workLocal, tiling, params, reduceParam, offset1, offset2, splitSize, reduceSize);
        PipeBarrier<PIPE_V>();
    }
}
template <typename T, typename U, bool isUpdate = false, bool isBasicBlock = false,
    const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV3Process(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, const SoftMaxParams& params)
{
    SetMaskNorm();
    ResetMask();

                                                                                                               ;
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM, tiling.splitK, tiling.reduceM,
        tiling.reduceK };
    if constexpr (!isUpdate) {
        SoftmaxFlashV3NoUpdateExtImpl(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor, workLocal,
            tiling, params, reduceParam);
    } else {
        SoftmaxFlashV3NDExtImpl(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor, expMaxTensor, inMeanTensor,
            inExpSumTensor, inMaxTensor, workLocal, tiling, params, reduceParam);
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flashv3_base_impl.h" 2





namespace AscendC {
template <typename T, typename U, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{


                                                                              ;
    static_assert((SupportType<Tuple<T, U>, Tuple<half, float>>()), "Failed to check dtype in SoftmaxFlashV3, "
        "Current api support dtype combination is T : half, U : float");

    LastAxisShapeND originalSrcShape = { params.oriSrcM, params.oriSrcK };
    if (params.srcM == 0 || params.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    }
    SoftmaxFlashV3Process<T, U, isUpdate, isBasicBlock, config>(dstTensor, meanTensor, expSumTensor, maxTensor, srcTensor,
        expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, originalSrcShape, tiling, params);
}

template <typename T, typename U, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const SoftMaxTiling& tiling, const SoftMaxParams& params)
{
    LocalTensor<float> workLocal;
    bool ans = PopStackBuffer<float, TPosition::LCM>(workLocal);
                                                                                                ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, tiling, params);
}

template <typename T, typename U, bool isUpdate, bool isReuseSource, bool isBasicBlock, bool isDataFormatNZ, const SoftmaxConfig& config>
[aicore] inline void SoftmaxFlashV3Impl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inexpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxFlashV3Impl<T, U, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor, maxTensor,
        srcTensor, expMaxTensor, inMeanTensor, inexpSumTensor, inMaxTensor, workLocal, tiling, params);
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv3.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv3.h"
template <typename T, typename U, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV3(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const SoftMaxTiling& tiling, const SoftMaxParams& params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inMeanTensor, inExpSumTensor, inMaxTensor, tiling, params);
                                       ;
}
# 107 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflashv3.h"
template <typename T, typename U, bool isUpdate = false, bool isReuseSource = false, bool isBasicBlock = false,
    bool isDataFormatNZ = false, const SoftmaxConfig& config = SOFTMAX_DEFAULT_CFG>
[aicore] inline void SoftmaxFlashV3(const LocalTensor<T>& dstTensor, const LocalTensor<U>& meanTensor,
    const LocalTensor<U>& expSumTensor, const LocalTensor<U>& maxTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& expMaxTensor, const LocalTensor<U>& inMeanTensor, const LocalTensor<U>& inExpSumTensor,
    const LocalTensor<U>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxParams& params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                        ;
    SoftmaxFlashV3Impl<T, U, isUpdate, isReuseSource, isBasicBlock, isDataFormatNZ, config>(dstTensor, meanTensor, expSumTensor,
        maxTensor, srcTensor, expMaxTensor, inMeanTensor, inExpSumTensor, inMaxTensor, sharedTmpBuffer, tiling, params);
                                       ;
}
}
#pragma end_pipe
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_grad_base_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_grad_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_grad_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_grad_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
namespace AscendC {

[aicore] inline void SoftmaxGradNDGenericImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const ReduceLastND& reduceSumParam, const BroadCastLastND& brcParam, const bool isFront, const uint32_t offset1,
    const uint32_t offset2, const uint32_t splitSize, const uint32_t reduceSize)
{
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    Cast(srcBuffer, srcTensor[offset1], RoundMode::CAST_NONE, splitSize);
    Cast(gradBuffer, gradTensor[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    Mul(dstBuffer, srcBuffer, gradBuffer, splitSize);
    PipeBarrier<PIPE_V>();
    ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
    PipeBarrier<PIPE_V>();
    if (isFront) {
        Cast(dstTensor[offset2], addBuffer, FLOAT2HALF_ROUND_MODE, reduceSize);
    } else {
        BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
        BroadCastLastImpl(dstBuffer, addBuffer, brcParam);
        PipeBarrier<PIPE_V>();
        Sub(dstBuffer, gradBuffer, dstBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Mul(dstBuffer, dstBuffer, srcBuffer, splitSize);
        PipeBarrier<PIPE_V>();
        Cast(dstTensor[offset1], dstBuffer, FLOAT2HALF_ROUND_MODE, splitSize);
    }
}

[aicore] inline void SoftmaxGradNDImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    const ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                          tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, tiling.reduceK };
    LocalTensor<float> srcBuffer = workLocal;
    LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
    LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

    LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    LocalTensor<float> addBuffer =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, reduceSumParam, brcParam, isFront,
            offset1, offset2, tiling.splitSize, tiling.reduceSize);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
    }
    if (tiling.tailM != 0) {
        const ReduceLastND tailReduceSumParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                                  tiling.splitK, tiling.tailM, tiling.reduceK };
        const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.srcK, tiling.tailM, tiling.reduceK };
        SoftmaxGradNDGenericImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, tailReduceSumParam, tailBrcParam,
            isFront, offset1, offset2, tiling.tailSplitSize, tiling.tailReduceSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void SoftmaxGradFrontNDImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(half)) {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> gradBuffer = workLocal[tiling.splitSize];
        LocalTensor<float> dstBuffer = workLocal[tiling.splitSize + tiling.splitSize];

        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
        LocalTensor<float> addBuffer =
            workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize + tiling.reduceSize];
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
        const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
        const uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                Cast<float, half, false>(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                Cast<float, half, false>(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE,
                    MASK_PLACEHOLDER, repeatTimes, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
                PipeBarrier<PIPE_V>();
                Mul<float, false>(dstBuffer, srcBuffer, gradBuffer, MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(dstBuffer, dstBuffer, dstBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(dstBuffer, dstBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    offset);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, dstBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
# 142 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
                Brcb(dstBuffer, reduceBuffer, splitCeilM, { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });
                Brcb(dstBuffer[DEFAULT_BLK_NUM], reduceBuffer, splitCeilM,
                    { B16_BYTE_SIZE, DEFAULT_REPEAT_STRIDE * B16_BYTE_SIZE });

                PipeBarrier<PIPE_V>();
                Cast<half, float, false>(dstTensor[i * tiling.reduceSize], dstBuffer, FLOAT2HALF_ROUND_MODE,
                    MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
            } else {
                Cast(srcBuffer, srcTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                Cast(gradBuffer, gradTensor[i * tiling.splitSize], RoundMode::CAST_NONE, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstBuffer, srcBuffer, gradBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                Cast(dstTensor[i * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            }
        }
        if (tiling.tailM != 0) {
            Cast(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            Cast(gradBuffer, gradTensor[tiling.rangeM * tiling.splitSize], RoundMode::CAST_NONE, tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();
            Mul(dstBuffer, srcBuffer, gradBuffer, tiling.tailSplitSize);
            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(addBuffer, dstBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
            Cast(dstTensor[tiling.rangeM * tiling.reduceSize], addBuffer, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        }
    } else {
        LocalTensor<float> srcBuffer = workLocal;
        LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
        uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
        uint32_t offset1 = 0;
        uint32_t offset2 = 0;
        const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
        const uint32_t elementNumPerBlk = DEFAULT_C0_SIZE / B32_BYTE_SIZE;
        uint8_t offset = (uint8_t)(splitBlock * elementNumPerBlk);
        const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, elementNumPerBlk));
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            if constexpr (isBasicBlock) {
                offset2 = i * tiling.reduceSize;
                offset1 = i * tiling.splitSize;
                PipeBarrier<PIPE_V>();
                Mul<float, false>(srcBuffer, srcTensor[offset1], gradTensor[offset1], MASK_PLACEHOLDER, repeatTimes,
                    { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

                for (uint32_t j = 1; j < splitBlock; ++j) {
                    PipeBarrier<PIPE_V>();
                    Add<float, false>(srcBuffer, srcBuffer, srcBuffer[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                        (uint8_t)(tiling.splitM), { 1, 1, 1, offset, offset, offset });
                }
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(srcBuffer, srcBuffer, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
                    splitBlock * DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                BlockReduceSum<float, false>(reduceBuffer, srcBuffer, splitCeilM, MASK_PLACEHOLDER, 1, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
# 215 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nd_impl.h"
                Brcb(dstTensor[offset2], reduceBuffer, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });

            } else {
                Mul(srcBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(dstTensor[i * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
            }
        }

        if (tiling.tailM != 0) {
            Mul(srcBuffer, srcTensor[tiling.rangeM * tiling.splitSize], gradTensor[tiling.rangeM * tiling.splitSize],
                tiling.tailSplitSize);
            PipeBarrier<PIPE_V>();

            reduceSumParam.srcM = tiling.tailM;
            reduceSumParam.dstM = tiling.tailM;
            reduceSumParam.originalSrcM = tiling.tailM;
            ReduceSumLastNDImpl(dstTensor[tiling.rangeM * tiling.reduceSize], srcBuffer, reduceBuffer, reduceSumParam);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T>
[aicore] inline void SoftmaxGradPostProcess(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const LastAxisShapeND& originalSrcShape, bool isFront = false)
{
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    ReduceLastND reduceSumParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };

    if constexpr (sizeof(T) == sizeof(half)) {
        SoftmaxGradNDImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
    } else {
        if (isFront) {
            SoftmaxGradFrontNDImpl<float>(dstTensor, srcTensor, gradTensor, workLocal, tiling, originalSrcShape);
        } else {
            LocalTensor<float> splitBuffer = workLocal;
            LocalTensor<float> reduceBuffer = workLocal[tiling.splitSize];
            LocalTensor<float> addBuffer = workLocal[tiling.splitSize + tiling.reduceSize];

            BroadCastLastND brcParam = { tiling.reduceM, tiling.srcK, tiling.reduceM, elementNumPerBlk };
            for (uint32_t i = 0; i < tiling.rangeM; i++) {
                Mul(splitBuffer, srcTensor[i * tiling.splitSize], gradTensor[i * tiling.splitSize], tiling.splitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[i * tiling.splitSize], srcTensor[i * tiling.splitSize], splitBuffer, tiling.splitSize);
            }
            if (tiling.tailM != 0) {
                reduceSumParam.srcM = tiling.tailM;
                reduceSumParam.dstM = tiling.tailM;
                reduceSumParam.originalSrcM = tiling.tailM;
                Mul(splitBuffer, srcTensor[tiling.rangeM * tiling.splitSize],
                    gradTensor[tiling.rangeM * tiling.splitSize], tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                ReduceSumLastNDImpl(addBuffer, splitBuffer, reduceBuffer, reduceSumParam);
                PipeBarrier<PIPE_V>();
                BroadCastLastImpl(splitBuffer, addBuffer, brcParam);
                PipeBarrier<PIPE_V>();
                Sub(splitBuffer, gradTensor[tiling.rangeM * tiling.splitSize], splitBuffer, tiling.tailSplitSize);
                PipeBarrier<PIPE_V>();
                Mul(dstTensor[tiling.rangeM * tiling.splitSize], srcTensor[tiling.rangeM * tiling.splitSize],
                    splitBuffer, tiling.tailSplitSize);
            }
        }
    }
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_grad_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nz_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/../common/softmax_grad_nz_impl.h"
namespace AscendC {

[aicore] inline void SoftMaxGradFrontGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer2, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(dst[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SoftMaxGradFrontGenericNZImpl(const LocalTensor<float>& dst,
    const LocalTensor<float>& gradTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    DataCopy(dst[offset2], tmpBuffer1, { (uint16_t)reduceParam.originalSrcM, 1, 1, 0 });
}

template <typename T>
[aicore] inline void SoftmaxGradFrontNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling)
{
    const ReduceLastND& mainReduceParam = { tiling.splitM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, tiling.splitK, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };

    const uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        SoftMaxGradFrontGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset1, offset2,
            splitCount, tailReduceParam);
    }
}

[aicore] inline void SoftMaxGradGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& gradTensor,
    const LocalTensor<half>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    LocalTensor<float> tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    LocalTensor<float> tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        Cast<float, half, false>(tmpBuffer1[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer3, tmpBuffer2, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer1[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer3, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer2[splitOffset * j], tmpBuffer1[splitOffset * j], tmpBuffer0[splitOffset * j],
            MASK_PLACEHOLDER, 1, { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Cast<half, float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer2[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }

    SetMaskNorm();
    ResetMask();
}

[aicore] inline void SoftMaxGradGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& gradTensor,
    const LocalTensor<float>& src, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, uint64_t mask[2],
    const uint32_t& offset, const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint32_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint32_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(tmpBuffer0[splitOffset * j], src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j],
            gradTensor[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Mul<float, false>(dst[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], tmpBuffer0[splitOffset * j],
            src[offset + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] inline void SoftmaxGradNZImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const LastAxisShapeND& originalSrcShape,
    const SoftMaxTiling& tiling, bool isFront = false)
{
    if (isFront) {
        SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
    } else {
        const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
            tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
        uint32_t lastBlockMaskLen = tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
            tiling.splitK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
            SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        uint64_t mask[2] = { 0, 0 };
        CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);
        uint32_t offset = 0;
        uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;

        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset = i * splitCount;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                mainReduceParam);
        }
        PipeBarrier<PIPE_V>();
        if (tiling.tailM != 0) {
            offset = tiling.rangeM * splitCount;
            splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
            SoftMaxGradGenericNZImpl(dstTensor, gradTensor, srcTensor, workLocal, tiling, mask, offset, splitCount,
                tailReduceParam);
        }
    }
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/v220/softmax_grad_impl.h" 2
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_grad_base_impl.h" 2





namespace AscendC {
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
                                                                                                                                                          ;
    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, false, true);
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling);
        } else {
            SoftmaxGradFrontNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, true, isBasicBlock);
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, newTiling,
                originalSrcShape);
        } else {
            SoftmaxGradFrontNDImpl<T, isBasicBlock>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
                originalSrcShape);
        }
    }
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradFrontImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling,
        softmaxShapeInfo);
}

template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<float>& workLocal, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{

                                                      ;

    ShapeInfo srcShape = srcTensor.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);

    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (isDataFormatNZ) {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false, true);
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, newTiling, isFront);
        } else {
            SoftmaxGradNZImpl(dstTensor, gradTensor, srcTensor, workLocal, originalSrcShape, tiling, isFront);
        }
    } else {
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxGradTilingFunc(workLocal.GetSize(), srcNDinfo, newTiling, elementNumPerBlk, isFront, false);
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, newTiling, originalSrcShape,
                isFront);
        } else {
            SoftmaxGradPostProcess<T>(dstTensor, gradTensor, srcTensor, workLocal, tiling, originalSrcShape, isFront);
        }
    }
}

template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront,
    const SoftMaxShapeInfo& softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    SoftmaxGradImpl<T, isReuseSource, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront, const SoftMaxShapeInfo& softmaxShapeInfo)
{
    auto workLocal = sharedTmpBuffer.ReinterpretCast<float>();
    SoftmaxGradImpl<T, isReuseSource, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, workLocal, tiling, isFront, softmaxShapeInfo);
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, bool isFront = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isReuseSource, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, isFront, softmaxShapeInfo);
                                    ;
}
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const SoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, tiling, softmaxShapeInfo);
                                    ;
}
# 100 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGrad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    bool isFront = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                     ;
    SoftmaxGradImpl<T, isReuseSource, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling, isFront,
        softmaxShapeInfo);
                                    ;
}
# 129 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxgrad.h"
template <typename T, bool isBasicBlock = false, bool isDataFormatNZ = false>
[aicore] inline void SoftmaxGradFront(const LocalTensor<T>& dstTensor, const LocalTensor<T>& gradTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SoftmaxGradFrontImpl<T, isBasicBlock, isDataFormatNZ>(dstTensor, gradTensor, srcTensor, sharedTmpBuffer, tiling,
        softmaxShapeInfo);
}
}
#pragma end_pipe
# 31 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_membase_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_membase_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void XorCalcSimplified(const LocalTensor<T>& dstAddr, const LocalTensor<T> &src0Addr,
    const LocalTensor<T> &src1Addr, const LocalTensor<T>& tmpTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    And<T, false>(dstAddr, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);

    Or<T, false>(tmpTensor, src0Addr, src1Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Not<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    And<T, false>(dstAddr, tmpTensor, dstAddr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] inline void XorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                                     ;




    uint32_t stackSize = sharedTmpBuffer.GetSize() / sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    const uint32_t loopCount = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);
    for (uint32_t i = 0; i < loopCount; i++) {
        XorCalcSimplified(dstTensor[i * stackSize],
            src0Tensor[i * stackSize],
            src1Tensor[i * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    if (tail != 0) {
        SetVectorMask<T>(0, tail);
        XorCalcSimplified(dstTensor[loopCount * stackSize],
            src0Tensor[loopCount * stackSize],
            src1Tensor[loopCount * stackSize],
            sharedTmpBuffer.ReinterpretCast<T>());
    }
    SetMaskNorm();
    SetVectorMask<T>(FULL_MASK, FULL_MASK);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_common_impl.h" 2




namespace AscendC {
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/xor/xor_common_impl.h"
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, int16_t>::value || std::is_same<T, uint16_t>::value),
        "Failed to check the data types, current api support data types are int16_t/uint16_t.");

    XorImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Xor(const LocalTensor<T>& dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{






    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 103 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/xor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Xor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor)
{






    Xor<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
#pragma end_pipe
}
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/floor.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/floor.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/floor/floor_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/floor/floor_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/floor/floor_v220_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/floor/floor_v220_impl.h"
namespace AscendC {
[aicore] inline void FloorProcess(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
[aicore] inline void FloorProcess(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();



    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    FloorProcess(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
template <typename T, bool isReuseSource = false>
[aicore] inline void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                          ;



    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        FloorProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        FloorProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}
template <typename T, bool isReuseSource = false>
[aicore] inline void FloorImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FloorImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
template <typename T, bool isReuseSource = false>
[aicore] inline void FloorImpl(const LocalTensor<float>& dstTensor,
    const LocalTensor<float>& srcTensor, const uint32_t calCount)
{
                                                                                             ;

    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
template <typename T, bool isReuseSource = false>
[aicore] inline void FloorImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                              ;

    (void)sharedTmpBuffer;
    Cast<float, float>(dstTensor, srcTensor, RoundMode::CAST_FLOOR, calCount);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/floor/floor_common_impl.h" 2
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/floor.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    FloorImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/floor.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Floor(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    FloorImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/sort.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/sort.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/sort.h" 2
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2





# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/../utils/std/algorithm.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/../utils/std/algorithm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/algorithm/max.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/algorithm/max.h"
namespace AscendC {
namespace Std {
template <typename T, typename U>
[host, aicore] inline T max(const T src0, const U src1)
{
    static_assert(Std::is_same<T, U>::value, "Only support compare with same type!");
    return (src0 > src1) ? src0 : src1;
}

}
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/../utils/std/algorithm.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/algorithm/min.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/impl/utils/std/algorithm/min.h"
namespace AscendC {
namespace Std {
template <typename T, typename U>
[host, aicore] inline T min(const T src0, const U src1)
{
    static_assert(Std::is_same<T, U>::value, "Only support compare with same type!");
    return (src0 < src1) ? src0 : src1;
}
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/../utils/std/algorithm.h" 2

namespace AscendC {
namespace Std {
template <typename T, typename U> [host, aicore] inline T min(const T src0, const U src1);
template <typename T, typename U> [host, aicore] inline T max(const T src0, const U src1);
}
}
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/arithprogression.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/arithprogression.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/../../../impl/adv_api/detail/index/arithprogression/arithprogression_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/../../../impl/adv_api/detail/index/arithprogression/arithprogression_common_impl.h"
namespace AscendC {

template <typename T>
[aicore] inline void GetBaseArithProgression(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<T>(firstValue) +
            static_cast<T>(diffValue) * static_cast<T>(i));
    }
}


template <>
[aicore] inline void GetBaseArithProgression(const LocalTensor<half> &dstLocal, const half firstValue,
    const half diffValue, const int32_t count)
{
    for (int i = 0; i < count; i++) {
        dstLocal.SetValue(i, static_cast<float>(firstValue) +
            static_cast<float>(diffValue) * static_cast<float>(i));
    }
}

template <typename T>
[aicore] inline void ArithProgressionImpl(const LocalTensor<T> &dstLocal, const T firstValue, const T diffValue,
    const int32_t count)
{
                                                                                             ;


    if (g_coreType == AIC) {
        return;
    }

    struct UnaryRepeatParams addsParamsStride1(1, 1, 1, 1);
    struct UnaryRepeatParams addsParamsStride8(1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);

    constexpr int32_t BLOCK_NUM = (ONE_BLK_SIZE / sizeof(T));
    constexpr int32_t REPEAT_NUM = (ONE_REPEAT_BYTE_SIZE / sizeof(T));
    if (count > BLOCK_NUM) {

        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, BLOCK_NUM);
        auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
        if (count > REPEAT_NUM) {

            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < DEFAULT_BLK_NUM - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
            int32_t repeat = count / REPEAT_NUM;
            int32_t tail = count % REPEAT_NUM;
            ResetMask();
            PipeBarrier<PIPE_V>();

            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * REPEAT_NUM], dstLocal[i * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
            if (tail > 0) {
                int32_t tail_aligned = (tail + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
                SetVectorMask<T>(tail_aligned);
                PipeBarrier<PIPE_V>();
                Adds<T, false>(dstLocal[repeat * REPEAT_NUM], dstLocal[(repeat - 1) * REPEAT_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(REPEAT_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride8);
                PipeBarrier<PIPE_V>();
            }
        } else {

            int32_t countAligned = (count + BLOCK_NUM - 1) / BLOCK_NUM * BLOCK_NUM;
            int32_t repeat = countAligned / BLOCK_NUM;
            SetVectorMask<T>(0, (((static_cast<uint64_t>(1)) << static_cast<uint32_t>(BLOCK_NUM)) - 1));
            PipeBarrier<PIPE_V>();
            for (int i = 0; i < repeat - 1; i++) {
                Adds<T, false>(dstLocal[(i + 1) * BLOCK_NUM], dstLocal[i * BLOCK_NUM],
                    static_cast<T>(static_cast<float>(diffValue) * static_cast<float>(BLOCK_NUM)), MASK_PLACEHOLDER,
                    (uint16_t)1, addsParamsStride1);
                PipeBarrier<PIPE_V>();
            }
        }
    } else {

        auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
        GetBaseArithProgression<T>(dstLocal, firstValue, diffValue, count);
        auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventIdSToV);
        WaitFlag<HardEvent::S_V>(eventIdSToV);
    }
}

template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V, S"))) void ArithProgression(const LocalTensor<T> &dstLocal,
    const T firstValue, const T diffValue, const int32_t count)
{
    ArithProgressionImpl(dstLocal, firstValue, diffValue, count);
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/arithprogression.h" 2

namespace AscendC {
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/index/arithprogression.h"
template <typename T>
[aicore] inline __attribute__((in_pipe("S"))) __attribute__((out_pipe("V, S"))) void Arange(const LocalTensor<T> &dst,
    const T firstValue, const T diffValue, const int32_t count)
{
    ArithProgressionImpl(dst, firstValue, diffValue, count);
}
}
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/layernormgrad_utils.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/layernormgrad_utils.h"
namespace AscendC {

struct LayerNormGradShapeInfo {
    DataFormat dataFormat = DataFormat::ND;
};

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernormgrad/layernormgrad_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernormgrad/layernormgrad_common_impl.h"
const uint32_t LAYERNORM_GRAD_B32_BYTE_SIZE = 4;
const uint32_t LAYERNORM_GRAD_B16_BYTE_SIZE = 2;

namespace AscendC {
struct LayerNormGradParams {
    [aicore] LayerNormGradParams(LayerNormGradTiling &tiling, LocalTensor<float> &stackBuffer)
        : bLength(tiling.bLength),
          sLength(tiling.sLength),
          hLength(tiling.hLength),
          loopNum(tiling.loopNum),
          tailSize(tiling.tailSize),
          nohTailSize(tiling.nohTailSize),
          oneCalSize(tiling.oneCalSize),
          nohCalSize(tiling.nohCalSize),
          x1Tensor(stackBuffer[tiling.x1TensorPos]),
          x2Tensor(stackBuffer[tiling.x2TensorPos]),
          x3Tensor(stackBuffer[tiling.x3TensorPos]),
          pdVarTensor(stackBuffer[tiling.pdVarTensorPos]),
          pdMeanTensor(stackBuffer[tiling.pdMeanTensorPos]),
          tmpTensor(stackBuffer[tiling.tmpTensorPos]),
          tmpTensor1(stackBuffer[tiling.tmpTensor1Pos]),
          tmpTensor2(stackBuffer[tiling.tmpTensor2Pos]),
          tmpTensorBSH(stackBuffer[tiling.tmpTensorBSHPos]),
          lastDimValueBack(*(reinterpret_cast<float *>(&tiling.lastDimValueBack))),
          lastDimValueBackMulTwo(*(reinterpret_cast<float *>(&tiling.lastDimValueBackMulTwo)))
    {
        x1Tensor.SetSize(tiling.x1TensorSize);
        x2Tensor.SetSize(tiling.x2TensorSize);
        x3Tensor.SetSize(tiling.x3TensorSize);
        pdVarTensor.SetSize(tiling.pdVarTensorSize);
        pdMeanTensor.SetSize(tiling.pdMeanTensorSize);
        tmpTensor.SetSize(tiling.tmpTensorSize);
        tmpTensor1.SetSize(tiling.tmpTensor1Size);
        tmpTensor2.SetSize(tiling.tmpTensor2Size);
        tmpTensorBSH.SetSize(tiling.tmpTensorBSHSize);
    }

    [aicore] LayerNormGradParams(uint32_t b, uint32_t s, uint32_t h)
    {
        bLength = b;
        sLength = s;
        hLength = h;
    }

    uint32_t bLength;
    uint32_t sLength;
    uint32_t hLength;

    uint32_t loopNum;
    uint32_t tailSize;
    uint32_t nohTailSize;
    uint32_t oneCalSize;
    uint32_t nohCalSize;

    float lastDimValueBack;
    float lastDimValueBackMulTwo;

    LocalTensor<float> x1Tensor;
    LocalTensor<float> x2Tensor;
    LocalTensor<float> x3Tensor;
    LocalTensor<float> pdVarTensor;
    LocalTensor<float> pdMeanTensor;
    LocalTensor<float> tmpTensor;
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensorBSH;
};

[aicore] inline void DuplicateLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    float scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = bsLength % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = src[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dst[(i * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = src[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dst[(rangeM * BRCB_BROADCAST_NUMBER + j) * hLength], scalarList[j], hLength);
        }
    }
}


[aicore] inline void BrcbLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t maxRepeatHSize = BRCB_MAX_REPEAT_SIZE * hLength;

    const uint32_t lineRound = hLength / BRCB_BROADCAST_NUMBER;

    const uint32_t rowRound = bsLength / BRCB_BROADCAST_NUMBER;
    const uint32_t rowTail = bsLength % BRCB_BROADCAST_NUMBER;
    const uint32_t rowRoundLen = bsLength - rowTail;

    const uint32_t repeatTimes = rowRound / MAX_REPEAT_TIMES;
    const uint32_t tailTimes = rowRound % MAX_REPEAT_TIMES;

    BrcbRepeatParams repeatParams;
    repeatParams.dstBlkStride = lineRound;
    repeatParams.dstRepStride = hLength;

    for (uint32_t i = 0; i < lineRound; i++) {
        for (uint32_t j = 0; j < repeatTimes; j++) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + j * maxRepeatHSize], src[j * BRCB_MAX_REPEAT_SIZE], MAX_REPEAT_TIMES,
                repeatParams);
        }

        if (tailTimes > 0) {
            Brcb(dst[i * BRCB_BROADCAST_NUMBER + repeatTimes * maxRepeatHSize], src[repeatTimes * BRCB_MAX_REPEAT_SIZE],
                tailTimes, repeatParams);
        }
        PipeBarrier<PIPE_V>();
    }

    if (rowTail != 0) {
        DuplicateLastDimImpl(dst[rowRoundLen * hLength], src[rowRoundLen], rowTail, hLength);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void BroadcastLastDimImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t dstSize, const uint32_t srcSize)
{

    BrcbLastDimImpl(dst, src, srcSize, dstSize / srcSize);



}

[aicore] inline void ReduceSumImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t calSize, const uint32_t hLength)
{
                                                                                                   ;
    const uint32_t count = calSize / hLength;

    for (uint32_t i = 0; i < count; i++) {
        uint32_t totalNum = hLength;
        uint32_t iMulhLength = i * hLength;

        LocalTensor<float> srctmp = src;
        LocalTensor<float> dstTmp = dst;

        while (totalNum > 1) {
            uint32_t repeatTimes = totalNum / ONE_REPEAT_FLOAT_SIZE;
            uint32_t tailSize = totalNum % ONE_REPEAT_FLOAT_SIZE;

            uint32_t blockNum = repeatTimes / MAX_REPEAT_TIMES;
            uint32_t blockTail = repeatTimes % MAX_REPEAT_TIMES;

            for (uint32_t j = 0; j < blockNum; j++) {
                WholeReduceSum(dst[iMulhLength + j * MAX_REPEAT_TIMES], srctmp[iMulhLength + j * MAX_REPEAT_FLOAT_SIZE],
                    ONE_REPEAT_FLOAT_SIZE, MAX_REPEAT_TIMES, 1, 1, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();

            if (totalNum == ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + blockNum * MAX_REPEAT_TIMES];
            }

            if (blockTail > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + blockNum * MAX_REPEAT_FLOAT_SIZE], ONE_REPEAT_FLOAT_SIZE,
                    blockTail, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }

            if (totalNum < ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            } else {
                dstTmp = dst[iMulhLength + totalNum / ONE_REPEAT_FLOAT_SIZE];
            }

            if (tailSize > 0) {
                WholeReduceSum(dstTmp, srctmp[iMulhLength + repeatTimes * ONE_REPEAT_FLOAT_SIZE], tailSize,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srctmp = dst;
        }
    }
}

template <typename T>
[aicore] inline void DuplicateTensor(const LocalTensor<T> &dst, const LocalTensor<T> &src, const uint32_t count,
    const uint32_t length)
{
    BroadcastLastDimImpl(dst, src, count * length, count);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void ComputePdX1(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputGamma,
    LayerNormGradParams &param, const uint32_t nohSize, const uint32_t hLength)
{

    for (size_t i = 0; i < nohSize; ++i) {
        Mul(param.x1Tensor[i * hLength], inputDy[i * hLength], inputGamma, hLength);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void ComputePdX2(const LocalTensor<T> &inputX, const LocalTensor<T> &inputMean,
    const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, const uint32_t hLength)
{

    DuplicateTensor(param.tmpTensorBSH, inputMean, nohSize, hLength);
    PipeBarrier<PIPE_V>();

    Sub(param.x2Tensor, inputX, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DoOneDiv(LocalTensor<float> &dstTensor, LocalTensor<float> &oneTensor,
    LocalTensor<float> &src1Tensor, const uint32_t nohSize)
{
    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(oneTensor, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, nohSize);
    Div<float, false>(dstTensor, oneTensor, src1Tensor, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
}

[aicore] inline void ComputePdVar(const LocalTensor<float> &inputVariance, float epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize)
{
    const float multiplier1 = -1.5;
    const float multiplier2 = -0.5;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();

    Mul(param.tmpTensorBSH, param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(param.tmpTensorBSH, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensorBSH, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.tmpTensorBSH, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();



    ReduceSumImpl(param.pdVarTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void ComputePdMean(const LocalTensor<float> &inputVariance, const LocalTensor<float> &resForGamma,
    float epsilon, LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{
    constexpr float exponent = -0.5;
    constexpr float multiplier = -1.0;
    constexpr float multiplier2 = -2.0;

    Adds(param.tmpTensor, inputVariance, epsilon, nohSize);
    PipeBarrier<PIPE_V>();


    Sqrt(param.tmpTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();

    DoOneDiv(param.tmpTensor, param.tmpTensorBSH, param.tmpTensor, nohSize);


    DuplicateTensor(param.tmpTensorBSH, param.tmpTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Mul(resForGamma, param.x2Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();


    Mul(param.x3Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();
    Muls(param.tmpTensorBSH, param.x3Tensor, static_cast<float>(multiplier), calSize);
    PipeBarrier<PIPE_V>();


    ReduceSumImpl(param.pdMeanTensor, param.tmpTensorBSH, calSize, param.hLength);


    Muls(param.tmpTensorBSH, param.x2Tensor, static_cast<float>(multiplier2), calSize);
    PipeBarrier<PIPE_V>();

    ReduceSumImpl(param.tmpTensor, param.tmpTensorBSH, calSize, param.hLength);
    PipeBarrier<PIPE_V>();


    Muls(param.tmpTensor, param.tmpTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    Mul(param.tmpTensor, param.pdVarTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();


    Add(param.pdMeanTensor, param.pdMeanTensor, param.tmpTensor, nohSize);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void ComputePdX(const LocalTensor<float> &inputVariance, const LocalTensor<float> &outputPdX,
    float epsilon, const LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize)
{


    Muls(param.pdVarTensor, param.pdVarTensor, static_cast<float>(param.lastDimValueBackMulTwo), nohSize);
    PipeBarrier<PIPE_V>();

    DuplicateTensor(param.tmpTensorBSH, param.pdVarTensor, nohSize, param.hLength);
    PipeBarrier<PIPE_V>();

    Mul(param.x1Tensor, param.tmpTensorBSH, param.x2Tensor, calSize);
    PipeBarrier<PIPE_V>();


    Muls(param.pdMeanTensor, param.pdMeanTensor, static_cast<float>(param.lastDimValueBack), nohSize);
    PipeBarrier<PIPE_V>();
    DuplicateTensor(param.tmpTensorBSH, param.pdMeanTensor, nohSize, param.hLength);


    Add(param.x1Tensor, param.x1Tensor, param.tmpTensorBSH, calSize);
    PipeBarrier<PIPE_V>();

    Add(outputPdX, param.x1Tensor, param.x3Tensor, calSize);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetTmpTensor(const LocalTensor<float> &outputPdX, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &inputX, LayerNormGradParams &param, bool isReuseSource = false)
{
    param.tmpTensor = outputPdX;
    if (isReuseSource == true) {
        param.x1Tensor = inputDy;
        param.x2Tensor = inputX;
    }
}

template <typename T>
[aicore] inline void ComputeProcess(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{}

template <>
[aicore] inline void ComputeProcess<half>(const LocalTensor<half> &inputDy, const LocalTensor<half> &inputX,
    const LocalTensor<half> &inputVariance, const LocalTensor<half> &inputMean, const LocalTensor<half> &inputGamma,
    const LocalTensor<half> &outputPdX, const LocalTensor<half> &resForGamma, half epsilon, LayerNormGradParams &param,
    const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    Cast(param.tmpTensor1, inputDy, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputGamma, RoundMode::CAST_NONE, param.hLength);
    PipeBarrier<PIPE_V>();

    ComputePdX1(param.tmpTensor1, param.tmpTensor2, param, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputX, RoundMode::CAST_NONE, calSize);
    Cast(param.tmpTensor2, inputMean, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdX2(param.tmpTensor1, param.tmpTensor2, param, calSize, nohSize, param.hLength);

    Cast(param.tmpTensor1, inputVariance, RoundMode::CAST_NONE, nohSize);
    PipeBarrier<PIPE_V>();
    ComputePdVar(param.tmpTensor1, epsilon, param, calSize, nohSize);


    ComputePdMean(param.tmpTensor1, param.tmpTensor2, epsilon, param, calSize, nohSize);



    ComputePdX(param.tmpTensor1, param.tmpTensor, epsilon, param, calSize, nohSize);

    Cast(outputPdX, param.tmpTensor, RoundMode::CAST_NONE, calSize);
    Cast(resForGamma, param.tmpTensor2, RoundMode::CAST_NONE, calSize);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void ComputeProcess<float>(const LocalTensor<float> &inputDy, const LocalTensor<float> &inputX,
    const LocalTensor<float> &inputVariance, const LocalTensor<float> &inputMean, const LocalTensor<float> &inputGamma,
    const LocalTensor<float> &outputPdX, const LocalTensor<float> &resForGamma, float epsilon,
    LayerNormGradParams &param, const uint32_t calSize, const uint32_t nohSize, bool isReuseSource)
{
    GetTmpTensor(outputPdX, inputDy, inputX, param, isReuseSource);

    ComputePdX1(inputDy, inputGamma, param, nohSize, param.hLength);

    ComputePdX2(inputX, inputMean, param, calSize, nohSize, param.hLength);

    ComputePdVar(inputVariance, epsilon, param, calSize, nohSize);


    ComputePdMean(inputVariance, resForGamma, epsilon, param, calSize, nohSize);
    PipeBarrier<PIPE_V>();



    ComputePdX(inputVariance, outputPdX, epsilon, param, calSize, nohSize);
}

template <typename T>
[aicore] inline void LayerNormGradComputeND(const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX,
    const LocalTensor<T> &inputVariance, const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma,
    const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma, T epsilon, LayerNormGradParams &param,
    bool isReuseSource)
{
    int offset0 = 0;
    int offset1 = 0;

    for (size_t i = 0; i < param.loopNum; ++i) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.oneCalSize, param.nohCalSize,
            isReuseSource);
        offset0 += param.oneCalSize;
        offset1 += param.nohCalSize;
    }

    if (param.tailSize != 0) {
        ComputeProcess<T>(inputDy[offset0], inputX[offset0], inputVariance[offset1], inputMean[offset1], inputGamma,
            outputPdX[offset0], resForGamma[offset0], epsilon, param, param.tailSize, param.nohTailSize, isReuseSource);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
                                       ;

                                                                 ;


                                                                                  ;

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradParams param(tiling, stackBuffer);

    if (shapeInfo.dataFormat == DataFormat::ND) {
        LayerNormGradComputeND(inputDy, inputX, inputVariance, inputMean, inputGamma, outputPdX, resForGamma, epsilon,
            param, isReuseSource);
    } else {
                                                                                           ;
    }
                                      ;
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradImpl(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LocalTensor<uint8_t> stackBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackBuffer);
                                                                                 ;

    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        stackBuffer, epsilon, tiling, shapeInfo);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 55 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, LocalTensor<uint8_t> &sharedTmpBuffer, T epsilon,
    LayerNormGradTiling &tiling, const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        sharedTmpBuffer, epsilon, tiling, shapeInfo);
}
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgrad.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGrad(const LocalTensor<T> &outputPdX, const LocalTensor<T> &resForGamma,
    const LocalTensor<T> &inputDy, const LocalTensor<T> &inputX, const LocalTensor<T> &inputVariance,
    const LocalTensor<T> &inputMean, const LocalTensor<T> &inputGamma, T epsilon, LayerNormGradTiling &tiling,
    const LayerNormGradShapeInfo &shapeInfo = {})
{
    LayerNormGradImpl<T, isReuseSource>(outputPdX, resForGamma, inputDy, inputX, inputVariance, inputMean, inputGamma,
        epsilon, tiling, shapeInfo);
}
#pragma end_pipe
}
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgradbeta.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgradbeta.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernormgrad/layernormgradbeta_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernormgrad/layernormgradbeta_common_impl.h"
namespace AscendC {
struct LayerNormGradBetaParams {
    [aicore] LayerNormGradBetaParams(){};

    uint32_t bLength = 0;
    uint32_t sLength = 0;
    uint32_t hLength = 0;
    uint32_t originalHLength = 0;

    uint32_t bshCurLength = 0;
    uint32_t bsCurLength = 0;
    uint32_t hCurLength = 0;

    LocalTensor<float> gammaTempTensor;
    LocalTensor<float> betaTempTensor;
    LocalTensor<float> inputDyTmpTensor;
    LocalTensor<float> resForGammaTmpTensor;
};


template <bool isClearDst = false>
[aicore] inline void ReduceSumFirstN(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    uint32_t startIndex = 0;
    if constexpr (isClearDst) {
        const UnaryRepeatParams unaryRepeatParams;
        Adds<float, false>(dst, src, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryRepeatParams);
        startIndex = 1;
        PipeBarrier<PIPE_V>();
    }

    const BinaryRepeatParams binaryParams;
    for (; startIndex < bsLength; startIndex++) {
        Add<float, false>(dst, src[startIndex * hLength], dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool isClearDst = false>
[aicore] inline void ComputeProcess(const LocalTensor<float> &resForGamma, const LocalTensor<float> &inputDy,
    const LocalTensor<float> &outputPdGamma, const LocalTensor<float> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    SetVectorMask<float, MaskMode::COUNTER>(0, params.bshCurLength);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(resForGammaTmpTensor, inputDy, resForGamma, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    ReduceSumFirstN<isClearDst>(outputPdGamma, resForGammaTmpTensor, params.bsCurLength, params.hCurLength);

    ReduceSumFirstN<isClearDst>(outputPdBeta, inputDy, params.bsCurLength, params.hCurLength);
}

template <bool isClearDst = false>
[aicore] inline void ComputeProcess(const LocalTensor<half> &resForGamma, const LocalTensor<half> &inputDy,
    const LocalTensor<half> &outputPdGamma, const LocalTensor<half> &outputPdBeta,
    const LayerNormGradBetaParams &params)
{
    const LocalTensor<float> &inputDyTmpTensor = params.inputDyTmpTensor;
    const LocalTensor<float> &resForGammaTmpTensor = params.resForGammaTmpTensor;

    const LocalTensor<float> &gammaTempTensor = params.gammaTempTensor;
    const LocalTensor<float> &betaTempTensor = params.betaTempTensor;

    SetVectorMask<half, MaskMode::COUNTER>(0, params.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);


    Cast<float, half, false>(inputDyTmpTensor, inputDy, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(resForGammaTmpTensor, resForGamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    ComputeProcess<isClearDst>(resForGammaTmpTensor, inputDyTmpTensor, gammaTempTensor, betaTempTensor, params);

    SetVectorMask<float, MaskMode::COUNTER>(0, params.hCurLength);

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(outputPdGamma, gammaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(outputPdBeta, betaTempTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void LayerNormGradBetaComputeND(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta, const LayerNormGradBetaTiling &tiling,
    LayerNormGradBetaParams &params)
{
    ComputeProcess<true>(resForGamma, inputDy, outputPdGamma, outputPdBeta, params);

    uint32_t inputOffset = tiling.oneCalSize;

    for (uint32_t index = 1; index < tiling.loopRound; index++) {
        ComputeProcess(resForGamma[inputOffset], inputDy[inputOffset], outputPdGamma, outputPdBeta, params);
        inputOffset += tiling.oneCalSize;
    }

    if (tiling.inputTailSize > 0) {
        params.bshCurLength = tiling.inputTailSize;
        params.bsCurLength = tiling.bsTailSize;

        ComputeProcess(resForGamma[tiling.inputTailPos], inputDy[tiling.inputTailPos], outputPdGamma, outputPdBeta,
            params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] inline void GetLayerNormGradBetaTensorInfo(const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy,
    const LocalTensor<float> &stackBuffer, const LayerNormGradBetaTiling &tiling, LayerNormGradBetaParams &params)
{
    params.bLength = tiling.bLength;
    params.sLength = tiling.sLength;
    params.hLength = tiling.hLength;
    params.originalHLength = tiling.originalHLength;

    params.bshCurLength = tiling.bshCurLength;
    params.bsCurLength = tiling.bsCurLength;
    params.hCurLength = tiling.originalHLength;

    if constexpr (sizeof(T) == sizeof(half)) {
        params.gammaTempTensor = stackBuffer[tiling.gammaTempTensorPos];
        params.betaTempTensor = stackBuffer[tiling.betaTempTensorPos];
        params.inputDyTmpTensor = stackBuffer[tiling.inputDyTmpTensorPos];
        params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];




          ;
    }

    if constexpr (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            params.resForGammaTmpTensor = resForGamma;
        } else {
            params.resForGammaTmpTensor = stackBuffer[tiling.resForGammaTmpTensorPos];





              ;
        }
    }




      ;
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
                                           ;

                ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                              ;
        return;
    }

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    LayerNormGradBetaParams params;
    GetLayerNormGradBetaTensorInfo<T, isReuseSource>(resForGamma, inputDy, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormGradBetaComputeND(resForGamma, inputDy, outputPdGamma, outputPdBeta, tiling, params);

    SetMaskNorm();
    ResetMask();
                                          ;
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradBetaImpl(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgradbeta.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, sharedTmpBuffer, tiling);
}
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernormgradbeta.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormGradBeta(const LocalTensor<T> &outputPdGamma, const LocalTensor<T> &outputPdBeta,
    const LocalTensor<T> &resForGamma, const LocalTensor<T> &inputDy, LayerNormGradBetaTiling &tiling)
{
    LayerNormGradBetaImpl<T, isReuseSource>(outputPdGamma, outputPdBeta, resForGamma, inputDy, tiling);
}
#pragma end_pipe
}
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_v220_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_base_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_base_impl.h" 2



namespace AscendC {
template <typename T>
[aicore] inline void DuplicateLastDimImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcSize, const uint32_t brcbSize)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    T scalarList[BRCB_BROADCAST_NUMBER] = {0};
    const uint32_t rangeM = srcSize / BRCB_BROADCAST_NUMBER;
    const uint32_t tailM = srcSize % BRCB_BROADCAST_NUMBER;

    for (uint32_t i = 0; i < rangeM; i++) {
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            scalarList[j] = srcTensor[i * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < BRCB_BROADCAST_NUMBER; j++) {
            Duplicate(dstTensor[(i * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }
    if (tailM != 0) {
        for (uint32_t j = 0; j < tailM; j++) {
            scalarList[j] = srcTensor[rangeM * BRCB_BROADCAST_NUMBER + j].GetValue(0);
        }
        for (uint32_t j = 0; j < tailM; j++) {
            Duplicate(dstTensor[(rangeM * BRCB_BROADCAST_NUMBER + j) * brcbSize], scalarList[j], brcbSize);
        }
    }

    event_t eventIdSToV = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
    SetFlag<HardEvent::S_V>(eventIdSToV);
    WaitFlag<HardEvent::S_V>(eventIdSToV);
}




template <typename T>
[aicore] inline void AlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;

    uint32_t elementsPerBlock = ONE_BLK_SIZE / sizeof(T);

    DataCopy(dstTensor, srcTensor, height * width);
    PipeBarrier<PIPE_V>();

    uint64_t mask[2];
    mask[0] = ((1 << rightPad) - 1) << (elementsPerBlock - (width - oriWidth));
    mask[1] = 0;

    uint32_t widthWithoutLastBlock = tiling.widthWithoutLastBlock;
    uint32_t blocksPerRow = tiling.blocksPerRow;

    uint32_t heightTiling = tiling.heightTiling;
    uint32_t heightFractal = tiling.heightFractal;
    uint32_t heightFractalTail = tiling.heightFractalTail;
    for (uint32_t i = 0; i < heightFractal; i++) {
        Duplicate<T, true>(dstTensor[i * tiling.mainLoopOffset + widthWithoutLastBlock], static_cast<T>(padValue), mask,
            heightTiling, 1, blocksPerRow);
    }
    if (heightFractalTail) {
        Duplicate<T, true>(dstTensor[tiling.tailBlockOffset], static_cast<T>(padValue), mask, heightFractalTail, 1,
            blocksPerRow);
    }
}
# 105 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_base_impl.h"
template <typename T>
[aicore] inline void UnAlignedPad(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    PadParams& padParams, const LocalTensor<T>& tmpBuffer, PadTiling& tiling)
{
    uint16_t leftPad = padParams.leftPad;
    uint16_t rightPad = padParams.rightPad;
    int32_t padValue = padParams.padValue;

    uint32_t height = tiling.srcHeight;
    uint32_t width = tiling.srcWidth;
    uint32_t oriWidth = tiling.srcOriWidth;

    uint32_t tmp1BlockNum = tiling.tmpBuffer1BlockNum;

    LocalTensor<T> tmp1 = tmpBuffer;
    LocalTensor<T> tmp2 = tmpBuffer[tiling.tmpBuffer2Offset];

    uint32_t widthTiling = tiling.widthTiling;
    uint32_t widthFractal = tiling.widthFractal;
    uint32_t widthFractalTail = tiling.widthFractalTail;

    uint32_t widthFractalTailAlingned = tiling.widthFractalTailAlingned;

    uint32_t brcbTiling = tiling.brcbTiling;
    uint32_t brcbFractal = tiling.brcbFractal;
    uint32_t brcbFractalTail = tiling.brcbFractalTail;
    uint32_t brcbFractalCount = 0;

    uint32_t maxRepeatTimes = tiling.maxRepeatTimes;
    uint32_t brcbTilingRepeatTimes = tiling.brcbTilingRepeatTimes;
    uint32_t brcbTilingRepeatTimesTail = tiling.brcbTilingRepeatTimesTail;
    uint32_t brcbFractalTailRepeatTimes = tiling.brcbFractalTailRepeatTimes;
    uint32_t brcbFractalTailRepeatTimesTail = tiling.brcbFractalTailRepeatTimesTail;

    uint32_t tmp1RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RowCount = tiling.tmpBuffer1RowNum;
    uint32_t tmp1RemainRow = 0;

    uint32_t tmp2RowFull = tiling.tmpBuffer1RowNum;
    uint32_t tmp2RowCount = 0;
    uint32_t tmp2NeedRow = tiling.tmpBuffer1RowNum;

    uint32_t tmpWidth = ONE_BLK_SIZE / sizeof(T);


    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tmp1BlockNum;
    if (transDataParams.repeatTimes > 1) {
        transDataParams.dstRepStride = 16;
        transDataParams.srcRepStride = 16;
    }

    uint64_t srcList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t dstList[NCHW_CONV_ADDR_LIST_SIZE];


    for (uint16_t i = 0; i < NCHW_CONV_ADDR_LIST_SIZE; i++) {
        dstList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
        srcList[i] = (uint64_t)(tmp2[i * tmpWidth].GetPhyAddr());
    }




    for (uint32_t j = 0; j < height; j++) {
        for (uint32_t i = 0; i < (widthFractal + 1); i++) {
            tmp2RowCount = 0;

            if (i == 0 && leftPad != 0) {
                Duplicate<T, true>(tmp2, static_cast<T>(padValue), tmpWidth, leftPad, 1, 1);
                tmp2RowCount += leftPad;
            }


            if (i == widthFractal) {
                if (rightPad != 0) {
                    Duplicate<T, true>(tmp2[(widthFractalTailAlingned - rightPad) * tmpWidth], static_cast<T>(padValue),
                        tmpWidth, rightPad, 1, 1);
                }
                tmp2NeedRow = widthFractalTailAlingned - tmp2RowCount - rightPad;
            } else {
                tmp2NeedRow = tmp2RowFull - tmp2RowCount;
            }

            while (tmp2NeedRow != 0) {
                PipeBarrier<PIPE_V>();
                tmp1RemainRow = tmp1RowFull - tmp1RowCount;
                if (tmp2NeedRow > tmp1RemainRow) {
                    if (tmp1RemainRow != 0) {
                        DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                            { 1, static_cast<uint16_t>(tmp1RemainRow), 0, 0 });
                        tmp1RowCount += tmp1RemainRow;
                        tmp2RowCount += tmp1RemainRow;
                        tmp2NeedRow -= tmp1RemainRow;
                        PipeBarrier<PIPE_V>();
                    }


                    if (brcbFractalCount == brcbFractal) {
                        for (uint32_t i = 0; i < brcbFractalTailRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbFractalTailRepeatTimesTail) {
                            Brcb(tmp1[brcbFractalTailRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling +
                                brcbFractalTailRepeatTimes * maxRepeatTimes * 8],
                                brcbFractalTailRepeatTimesTail, { 1, 8 });
                        }
                        tmp1RowFull = brcbFractalTail;
                    } else {
                        for (uint32_t i = 0; i < brcbTilingRepeatTimes; i++) {
                            Brcb(tmp1[i * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + i * maxRepeatTimes * 8], maxRepeatTimes,
                                { 1, 8 });
                        }
                        if (brcbTilingRepeatTimesTail) {
                            Brcb(tmp1[brcbTilingRepeatTimes * maxRepeatTimes * 8 * tmpWidth],
                                srcTensor[brcbFractalCount * brcbTiling + brcbTilingRepeatTimes * maxRepeatTimes * 8],
                                brcbTilingRepeatTimesTail, { 1, 8 });
                        }
                    }
# 238 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_base_impl.h"
                    brcbFractalCount += 1;
                    tmp1RowCount = 0;
                } else {
                    DataCopy(tmp2[tmp2RowCount * tmpWidth], tmp1[tmp1RowCount * tmpWidth],
                        { 1, static_cast<uint16_t>(tmp2NeedRow), 0, 0 });
                    tmp1RowCount += tmp2NeedRow;
                    tmp2RowCount += tmp2NeedRow;
                    tmp2NeedRow = 0;
                }
            }

            PipeBarrier<PIPE_V>();


            TransDataTo5HD<T>(dstList, srcList, transDataParams);
            PipeBarrier<PIPE_V>();
            if (i == widthFractal) {

                if (sizeof(T) == sizeof(half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(widthFractalTailAlingned / 16), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    if (widthFractalTailAlingned / 16 != 0) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum], tmp2,
                            { static_cast<uint16_t>(widthFractalTailAlingned / 16), 2, 14, 0 });
                    }
                    if (widthFractalTailAlingned % 16) {
                        DataCopy(dstTensor[j * (width + leftPad + rightPad) + widthFractal * 16 * tmp1BlockNum +
                            widthFractalTailAlingned / 16 * 16],
                            tmp2[widthFractalTailAlingned / 16 * 16 * 8], { 1, 1, 15, 0 });
                    }
                }
            } else {

                if (sizeof(T) == sizeof(half)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 1, 15, 0 });
                } else if (sizeof(T) == sizeof(float)) {
                    DataCopy(dstTensor[j * (width + leftPad + rightPad) + i * 16 * tmp1BlockNum], tmp2,
                        { static_cast<uint16_t>(tmp1BlockNum), 2, 14, 0 });
                }
            }
        }
    }
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_v220_impl.h" 2

namespace AscendC {
template <typename T>
[aicore] inline void PadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    PadParams &padParams, const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
    uint32_t width = tiling.srcWidth;


    if (width * sizeof(T) % ONE_BLK_SIZE == 0) {
        AlignedPad(dstTensor, srcTensor, padParams, tiling);
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        UnAlignedPad(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
    }
}
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_v220_impl.h"
template <typename T>
[aicore] inline void UnPadCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    uint16_t rightPad = unPadParams.rightPad;
    uint16_t height = tiling.srcHeight;
    uint16_t width = tiling.srcWidth;

    GatherMaskParams reducev2Params;
    reducev2Params.repeatTimes = height;
    reducev2Params.src0RepeatStride = static_cast<uint16_t>(width * sizeof(T) / ONE_BLK_SIZE);
    uint64_t rsvdCnt = 0;
    GatherMask(dstTensor, srcTensor, REDUCEV2_MODE_SEVEN, true, (width - rightPad), reducev2Params, rsvdCnt);
    ResetMask();
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/pad/pad_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void PadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
                                                                                                  ;
    PadCompute<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
}

template <typename T>
[aicore] inline void UnPadImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    UnPadParams &unPadParams, LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
                                                                                                      ;

    UnPadCompute<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h" 2

namespace AscendC {
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h"
#pragma begin_pipe(V)
template <typename T>
[aicore] inline void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    const LocalTensor<uint8_t> &sharedTmpBuffer, PadTiling &tiling)
{
                             ;
    PadImpl<T>(dstTensor, srcTensor, padParams, sharedTmpBuffer, tiling);
                            ;
}
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h"
template <typename T>
[aicore] inline void Pad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, PadParams &padParams,
    PadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    PadImpl<T>(dstTensor, srcTensor, padParams, tmpBuffer, tiling);
}
# 83 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h"
template <typename T>
[aicore] inline void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
     LocalTensor<uint8_t> &sharedTmpBuffer, UnPadTiling &tiling)
{
    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, sharedTmpBuffer, tiling);
}
# 101 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/pad.h"
template <typename T>
[aicore] inline void UnPad(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, UnPadParams &unPadParams,
    UnPadTiling &tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    UnPadImpl<T>(dstTensor, srcTensor, unPadParams, tmpBuffer, tiling);
}
#pragma end_pipe
}
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl_common.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl_common.h"
namespace AscendC {
constexpr uint32_t HCCL_GROUP_ID_0 = 0;
using HcclHandle = int8_t;

enum class HcclCMDType {
    HCCL_CMD_INVALID = 0,
    HCCL_CMD_BROADCAST = 1,
    HCCL_CMD_ALLREDUCE,
    HCCL_CMD_REDUCE,
    HCCL_CMD_SEND,
    HCCL_CMD_RECEIVE,
    HCCL_CMD_ALLGATHER,
    HCCL_CMD_REDUCE_SCATTER,
    HCCL_CMD_ALLTOALLV,
    HCCL_CMD_ALLTOALLVC,
    HCCL_CMD_ALLTOALL,
    HCCL_CMD_GATHER,
    HCCL_CMD_SCATTER,
    HCCL_CMD_BATCH_SEND_RECV,
    HCCL_CMD_BATCH_PUT,
    HCCL_CMD_BATCH_GET,
    HCCL_CMD_ALLGATHER_V,
    HCCL_CMD_REDUCE_SCATTER_V,
    HCCL_CMD_BATCH_WRITE,
    HCCL_CMD_HALF_ALLTOALLV = 20,
    HCCL_CMD_ALL
};

enum HcclReduceOp {
    HCCL_REDUCE_SUM = 0,
    HCCL_REDUCE_PROD = 1,
    HCCL_REDUCE_MAX = 2,
    HCCL_REDUCE_MIN = 3,
    HCCL_REDUCE_RESERVED
};

enum class MC2_BUFFER_LOCATION {
    MC2_BUFFER_TYPE_DEFAULT = 0,
    MC2_BUFFER_TYPE_OUTPUT,
    MC2_BUFFER_TYPE_WINDOW_IN,
    MC2_BUFFER_TYPE_WINDOW_OUT,
    MC2_BUFFER_TYPE_WORKSPACE,
    MC2_BUFFER_TYPE_INPUT,
    MC2_BUFFER_TYPE_COMMOUT,
    MC2_BUFFER_TYPE_END
};

enum HcclServerType {
    HCCL_SERVER_TYPE_AICPU = 0,
    HCCL_SERVER_TYPE_CCU = 5,
    HCCL_SERVER_TYPE_END
};

enum class CoreType: uint8_t {
    DEFAULT,
    ON_AIV,
    ON_AIC
};

struct HcclServerConfig {
    CoreType type;
    int64_t blockId;
};

enum HcclDataType {
    HCCL_DATA_TYPE_INT8 = 0,
    HCCL_DATA_TYPE_INT16 = 1,
    HCCL_DATA_TYPE_INT32 = 2,
    HCCL_DATA_TYPE_FP16 = 3,
    HCCL_DATA_TYPE_FP32 = 4,
    HCCL_DATA_TYPE_INT64 = 5,
    HCCL_DATA_TYPE_UINT64 = 6,
    HCCL_DATA_TYPE_UINT8 = 7,
    HCCL_DATA_TYPE_UINT16 = 8,
    HCCL_DATA_TYPE_UINT32 = 9,
    HCCL_DATA_TYPE_FP64 = 10,
    HCCL_DATA_TYPE_BFP16 = 11,
    HCCL_DATA_TYPE_INT128 = 12,
    HCCL_DATA_TYPE_HIF8 = 14,
    HCCL_DATA_TYPE_FP8E4M3 = 15,
    HCCL_DATA_TYPE_FP8E5M2 = 16,
    HCCL_DATA_TYPE_FP8E8M0 = 17,
    HCCL_DATA_TYPE_RESERVED
};

enum class ScopeType: uint8_t {
    ALL,
    QUEUE,
    BLOCK,

    INVALID_TYPE
};
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl_def.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl_def.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_base.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_base.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_inner_def.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_inner_def.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/hccl/internal/hccl_msg.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/hccl/internal/hccl_msg.h"
namespace HcclApi {
constexpr uint32_t HCCL_MSG_VALID_MASK = 0x5CDF123A;
constexpr uint64_t COMMIT_VALID_MASK = 987654321U;
constexpr uint64_t FINALIZE_FINISH_CNT = 1234567899999999999UL;
constexpr int8_t INVALID_HANDLE_ID = static_cast<int8_t>(-1);
constexpr int8_t HCCL_MAX_HANDLE_ID = 63;

enum class HcclTilingVersion: uint8_t {
    DEPRECATED_TILING_VERSION,
    NEW_TILING_VERSION,
    ONLINE_COMPILATION_TILING_VERSION,
    INVALID_TILING_VERSION
};

struct V0MsgAdditionInfo {
    AscendC::HcclDataType hcclDataType;
    uint32_t p2pSrcDestRankId;
    uint32_t valid;
    uint8_t repeatCnt;
    uint8_t everyTurnRsp;
    uint8_t everyTurnWait;
    int8_t commDepGroupID;

    AscendC::HcclHandle commDepHandleID;

    AscendC::HcclHandle selfHandleID;
    uint8_t seqNum;
    HcclTilingVersion version;
    uint32_t xorCheck;
};

struct V1MsgAdditionInfo {
    uint64_t ccOpTilingData;
    uint32_t valid;
    AscendC::HcclDataType hcclDataType;
    uint8_t repeatCnt;
    AscendC::HcclHandle selfHandleID;
    uint8_t seqNum;
    HcclTilingVersion version;
    uint32_t xorCheck;
};

enum class ControlMsgType: uint32_t {
    HCCL_CMD_FINALIZE = 100,
    HCCL_CMD_INTER_GROUP_SYNC,
    HCCL_CMD_INIT,
    HCCL_CMD_BARRIER,
    HCCL_CMD_MAX
};
constexpr uint32_t HCCL_MSG_TYPE_CNT = static_cast<uint32_t>(ControlMsgType::HCCL_CMD_MAX) -
        static_cast<uint32_t>(ControlMsgType::HCCL_CMD_FINALIZE);

union HcclCommType {
    AscendC::HcclCMDType prepareType;
    ControlMsgType msgType;
};

struct HcclMsg {
    HcclCommType commType;
    AscendC::HcclReduceOp opType;
    uint64_t sendBuffer;
    uint64_t recvBuffer;
    uint64_t dataCnt;
    uint64_t strideCount;



    union {
        V0MsgAdditionInfo v0Msg;
        V1MsgAdditionInfo v1Msg;
    } addMsg;
};




constexpr uint32_t HCCL_MAX_RANK_NUM_V2 = 256;
struct HcclMsgExt {

    uint64_t sendCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t sendOffset[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvCounts[HCCL_MAX_RANK_NUM_V2];

    uint64_t recvOffset[HCCL_MAX_RANK_NUM_V2];
    uint64_t reserved[6U];
    uint64_t valid;
    uint64_t xorCheck;
};

struct AlltoAllVParamExt {
    uint64_t *sendCounts;
    uint64_t *sdispls;
    uint64_t *recvCounts;
    uint64_t *rdispls;
};

constexpr uint32_t HCCL_MSG_CNT = 64;
constexpr uint32_t BYTE_PER_KB = 1024U;
constexpr uint32_t BYTE_PER_MB = BYTE_PER_KB * BYTE_PER_KB;





struct TurnCnt {
    uint64_t valid;
    uint64_t cnt;
    uint64_t reserved[6];
};

struct SingleQueueMsg {
    HcclMsg sendMsgs[HCCL_MSG_CNT];
    HcclMsg recvMsgs[HCCL_MSG_CNT];
    uint8_t reserved0[8 * BYTE_PER_KB];
    TurnCnt commitTurnCnt[HCCL_MSG_CNT];
    TurnCnt finishedTurnCnt[HCCL_MSG_CNT];
    uint8_t reserved1[BYTE_PER_MB];
    HcclMsgExt paramExtMsgList[HCCL_MSG_CNT];
};

constexpr uint32_t MAX_QUE_NUM = 48U;
struct MultiQueueMsg {
    HcclMsg sendMsgs[MAX_QUE_NUM][HCCL_MSG_CNT];
    TurnCnt commitTurnCnt[MAX_QUE_NUM][HCCL_MSG_CNT];
    TurnCnt finishedTurnCnt[MAX_QUE_NUM][HCCL_MSG_CNT];
};

struct ControlHcclMsg {
    uint8_t restart;
    uint8_t restarting;
    uint8_t restartCnt;
    uint8_t resetSeq;
    uint8_t reserved[60];
};

constexpr uint32_t HCCL_API_SNAPSHOTS_CNT = 15U;
struct ApiStates {
    TurnCnt commitStats[static_cast<uint32_t>(AscendC::HcclCMDType::HCCL_CMD_ALL)];
    TurnCnt waitStats[static_cast<uint32_t>(AscendC::HcclCMDType::HCCL_CMD_ALL)];
    TurnCnt msgStats[HCCL_MSG_TYPE_CNT];
    TurnCnt snapshots[HCCL_API_SNAPSHOTS_CNT + 1U];
};

struct HcclMsgArea {
    union {
        SingleQueueMsg singleMsg;
        MultiQueueMsg multiMsg;
    } commMsg;
    ControlHcclMsg controlMsg;
    ApiStates apiStats;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_inner_def.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/hccl/internal/hccl_tiling_msg.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/hccl/internal/hccl_tiling_msg.h"
namespace HcclApi {
constexpr uint32_t INIT_TILING_VERSION = 100U;
constexpr uint8_t MC2_CC_TILING_STRUCT = 1U;
constexpr uint8_t MC2_CC_TILING_INTERFACE = 2U;
constexpr uint32_t MAX_CC_TILING_NUM = 8U;
struct Mc2InitTilingInner {
    uint32_t version;
    uint32_t mc2HcommCnt;
    uint32_t offset[MAX_CC_TILING_NUM];
    uint8_t debugMode;
    uint8_t preparePosition;
    uint16_t queueNum;
    uint16_t commBlockNum;
    uint8_t devType;
    char reserved[17];
};

constexpr uint32_t GROUP_NAME_SIZE = 128U;
constexpr uint32_t ALG_CONFIG_SIZE = 128U;
struct AicpuContext {
    uint64_t version;
    uint64_t workSpace;
    uint64_t workSpaceSize;
    uint32_t rankId;
    uint32_t rankNum;
    uint64_t xnAddr;
    uint64_t ckeAddr;
    char reserved[120];
};

struct Mc2CcTilingInner {
    uint8_t skipLocalRankCopy;
    uint8_t skipBufferWindowCopy;
    uint8_t stepSize;
    uint8_t version;
    char reserved[9];
    uint8_t commEngine;
    uint8_t srcDataType;
    uint8_t dstDataType;
    char groupName[GROUP_NAME_SIZE];
    char algConfig[ALG_CONFIG_SIZE];
    uint32_t opType;
    uint32_t reduceType;
    char reserved2[64];
    AicpuContext aicpuContext;
};
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_inner_def.h" 2

using namespace HcclApi;
namespace AscendC {
constexpr int32_t HCCL_FAILED = -1;
constexpr int32_t HCCL_SUCCESS = 0;
constexpr uint8_t HCCL_ONLY_COMPUTE = 1U;
constexpr uint8_t HCCL_ASCEND910B = 1U;
constexpr uint32_t MAX_DCCI_CNT = 64U;
constexpr uint64_t DATA_TYPE_MAP[] = {1, 2, 4, 2, 4, 8, 8, 1, 8, 4, 8, 2, 0, 0, 1, 1, 1, 1, 0};


constexpr uint32_t HCCL_MSG_DATA_CNT = 16U;
struct DataBlock {
    uint32_t data[HCCL_MSG_DATA_CNT];
};

struct AlltoAllvWriteParamExt {
    uint64_t sendOffsets;
    uint64_t sendSizes;
    uint64_t remoteWinOffset;
};

struct CommonPrepareParam {
    HcclCommType commType;
    __attribute__((cce_global)) uint8_t* sendBuf;
    __attribute__((cce_global)) uint8_t* recvBuf;
    uint64_t count;
    HcclDataType dataType;
    HcclDataType dstDataType;
    HcclReduceOp op;
    uint64_t strideCount;
    uint8_t repeat = 1U;
    AlltoAllVParamExt paramExt;
    AlltoAllvWriteParamExt wParamExt;
};

struct MemDetails {
    uint64_t size;
    uint64_t addr;
    uint32_t key;
};

struct IbVerbsData {
    MemDetails remoteInput;
    MemDetails remoteOutput;
    MemDetails localInput;
    MemDetails localOutput;
    uint8_t res[24];
};

constexpr uint32_t HCCL_MAX_RANK_NUM = 32U;
struct HcclCombineOpParam {
    uint64_t workSpace;

    uint64_t workSpaceSize;
    uint32_t rankId;
    uint32_t rankNum;
    uint64_t winSize;
    uint64_t windowsIn[HCCL_MAX_RANK_NUM];


    uint64_t windowsOut[HCCL_MAX_RANK_NUM];







    uint8_t res[8328];

    uint8_t multiFlag;
    __attribute__((cce_global)) IbVerbsData *data;
};

namespace HcclContextDef {
struct HcclRankRelationResV2 {
    uint32_t remoteUsrRankId = 0;
    uint32_t remoteWorldRank = 0;
    uint64_t windowsIn = 0;
    uint64_t windowsOut = 0;
    uint64_t windowsExp = 0;
};

struct RemoteResPtr {
    HcclRankRelationResV2 *nextHostPtr;
    HcclRankRelationResV2 *nextDevicePtr;
};

struct HcclOpResParam {
    uint64_t workSpace;
    uint64_t workSpaceSize;
    uint32_t rankId;
    uint32_t rankNum;
    uint64_t winSize;
    uint64_t localWindowsIn;
    uint64_t localWindowsOut;
    char hcomId[128];
    uint64_t winExpSize;
    uint64_t localWindowsExp;
    uint32_t rWinStart;
};
}
constexpr uint16_t CCU_CKE_SIZE = 8;
constexpr uint64_t CCU_XN_DATA_SIZE = 8;
constexpr uint16_t CCU_USED_XN_NUM = 9;
constexpr uint16_t CCU_MAX_MSG_NUM = 8;
constexpr uint16_t CCU_MSG_XN_NUM = 64;

constexpr uint64_t CCU_LOOP_COUNT = 64;
constexpr uint64_t CCU_LOOP_COUNT_ATAVW = 8;
constexpr uint64_t ALIGN_64_BYTE = 64;
constexpr uint64_t CCU_MEMSLICE_SIZE = 4096;
constexpr uint8_t CCU_MSG_CKE_INIT_VALUE = 0;
constexpr uint8_t CCU_MSG_CKE_SET_VALUE = 1;

struct CCUConfig {
    __attribute__((cce_global)) uint8_t* xnAddr;

    __attribute__((cce_global)) uint8_t* ckeAddr;



};

struct CCUMsg {
    __attribute__((cce_global)) uint8_t* xnData;
    __attribute__((cce_global)) uint8_t* xnAddr;
    __attribute__((cce_global)) uint8_t* commitCKEAddr;
    __attribute__((cce_global)) uint8_t* waitCKEAddr;
};

struct ReduceDataTypeAbility {
    HcclReduceOp op;
    HcclDataType dstDataType;
    HcclDataType srcDataType;
};

template <typename T, int Size>
class CircularFifo {
public:
    [aicore] CircularFifo() : mHead(0), mTail(0), mSize(0)
    {}

    [aicore] inline bool push(const T &value)
    {
        if (mSize == Size) {
            return false;
        }

        m_buffer[mTail] = value;
        mTail = (mTail + 1) % Size;
        ++mSize;

        return true;
    }

    [aicore] inline bool pop(T &value)
    {
        if (mSize == 0) {
            return false;
        }

        value = m_buffer[mHead];
        mHead = (mHead + 1) % Size;
        --mSize;

        return true;
    }

    [aicore] inline bool isFull() const
    {
        return mSize == Size;
    }

    [aicore] inline bool isEmpty() const
    {
        return mSize == 0;
    }

    [aicore] inline T Head() const
    {
        return m_buffer[mHead];
    }

    [aicore] inline T Tail() const
    {
        return m_buffer[mTail];
    }

public:
    int mHead;
    int mTail;

private:
    T m_buffer[Size];
    int mSize;
};

struct CCUMsgExt {
    uint64_t sendSize;
    uint64_t sendOffset;
    uint64_t recvSize;
    uint64_t recvOffset;
};

struct CCUMsgCommOp {
    int8_t resourceId;
    int8_t isFinish;
    uint8_t reserved[6];
    uint64_t xnData[CCU_USED_XN_NUM];
};

struct HandleCommOp {
    uint8_t reqId;
    uint8_t repeatCnt;
    uint8_t commitCnt;
    uint8_t waitCnt;
    uint8_t finishCnt;
    uint8_t reserved[3];
};

struct CCUParam {
    uint32_t rankNum;
    uint32_t rankId;
    CommonPrepareParam commParam;
    uint32_t repeatIndex;
    uint8_t alltoallvCnt = 0;
    __attribute__((cce_global)) CCUMsgExt *ccuMsgExt;
};

constexpr uint64_t CCU_MSG_EXT_RANK_OFFSET = sizeof(CCUMsgExt) * HCCL_MAX_RANK_NUM_V2;
constexpr uint64_t CCU_MSG_EXT_MAX_OFFSET = CCU_MSG_EXT_RANK_OFFSET * HCCL_MSG_CNT;
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_base.h" 2

namespace AscendC {
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/../common/hccl_base.h"
template<HcclServerType serverType, const auto &config>
class HcclImpl {
public:
    template <bool commit = false>
    [aicore] inline HcclHandle AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
            HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
            HcclDataType dataType,uint64_t strideCount, uint8_t repeat = 1) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
            HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat = 1) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
            HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
            __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType, uint8_t repeat = 1) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum, uint16_t queueID) { return -1; }

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllvWrite(__attribute__((cce_global)) uint8_t* usrIn, __attribute__((cce_global)) uint8_t* sendOffsets, __attribute__((cce_global)) uint8_t* sendSizes,
            uint64_t remoteWinOffset, uint64_t localDataSize) { return -1; }

    [aicore] inline void Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling = nullptr) {}

    [aicore] inline void InitV2(__attribute__((cce_global)) uint8_t* context, const void *initTiling) {}

    [aicore] inline int32_t SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData) { return -1; }

    [aicore] inline int32_t SetCcTilingV2(uint64_t offset) { return -1; }

    [aicore] inline void Commit(HcclHandle handleId) {}

    [aicore] inline int32_t Wait(HcclHandle handleId) { return -1; }

    [aicore] inline int32_t Query(HcclHandle handleId) { return -1; }

    [aicore] inline void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID) {}

    template <ScopeType type = ScopeType::ALL>
    [aicore] inline void QueueBarrier(uint16_t queueID) {}

    template <bool sync = true>
    [aicore] inline int32_t Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen) { return -1; }

    template <bool sync = true>
    [aicore] inline void Finalize() {}

    [aicore] inline bool
    SetReduceDataTypeAbility(HcclReduceOp op, HcclDataType dstDataType, HcclDataType srcDataType) { return true; }

    public:
    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId) { return nullptr; }

    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId) { return nullptr; }

    [aicore] inline uint32_t GetRankId() { return 0; }

    [aicore] inline uint32_t GetRankDim() { return 0; }

    [aicore] inline uint16_t GetQueueNum() { return 0; }
};

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl_def.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_v220_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_def.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_def.h"
namespace AscendC {

template<const auto &config>
class HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config> {
public:
    template <bool commit = false>
    [aicore] inline HcclHandle AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                           HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] inline HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] inline HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
                                               HcclDataType dataType, HcclReduceOp op, uint64_t strideCount,
                                               uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                           __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                           uint8_t repeat = 1);

    template <bool commit = false>
    [aicore] inline HcclHandle BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum, uint16_t queueID);

    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllvWrite(__attribute__((cce_global)) uint8_t* usrIn, __attribute__((cce_global)) uint8_t* sendOffsets, __attribute__((cce_global)) uint8_t* sendSizes,
                                                uint64_t remoteWinOffset, uint64_t localDataSize);

public:
    [aicore] inline void Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling = nullptr);

    [aicore] inline void InitV2(__attribute__((cce_global)) uint8_t* context, const void *initTiling);

    [aicore] inline int32_t SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData);

    [aicore] inline int32_t SetCcTilingV2(uint64_t offset);

    [aicore] inline void Commit(HcclHandle handleId);

    [aicore] inline int32_t Wait(HcclHandle handleId);

    [aicore] inline int32_t Query(HcclHandle handleId);

    [aicore] inline void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID);

    template <ScopeType type = ScopeType::ALL>
    [aicore] inline void QueueBarrier(uint16_t queueID);

    template <bool sync = true>
    [aicore] inline int32_t Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen);

    template <bool sync = true>
    [aicore] inline void Finalize();

public:
    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);

    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);

    [aicore] inline uint32_t GetRankId() { return hcclContext_->rankId; }

    [aicore] inline uint32_t GetRankDim() { return hcclContext_->rankNum; }

    [aicore] inline uint16_t GetQueueNum() { return queueNum_; }

private:


    template <bool commit = false>
    [aicore] inline HcclHandle CommonPrepareImpl(const CommonPrepareParam &param);

    [aicore] inline bool CheckCommonPrepareParamValid(const CommonPrepareParam &param);


    [aicore] inline void ResetFinishedTurnCnt();

    template <bool sync>
    [aicore] inline void SendFinalizeMsg();

    [aicore] inline void SendMsgToServer(uint16_t queId, const CommonPrepareParam &para,
                                           int8_t srcGroupID = -1, HcclHandle srcHandleID = INVALID_HANDLE_ID);

    [aicore] inline void SendMsgToServer(const AlltoAllVParamExt &para);

    [aicore] inline uint16_t GetStepSizeByHandle(HcclHandle handle);

    [aicore] inline uint16_t GetStepCntsPerRepeatByHandle(HcclHandle handle);

    [aicore] inline void SetCommitTurnCntToGm(uint8_t msgPos, uint64_t turnCnt, HcclHandle handleId);

    [aicore] inline uint64_t WaitFinishCntFromGm(uint8_t msgPos, uint64_t expectedCnt);

    [aicore] inline void InitWorkingFlag();

    [aicore] inline void InitInner(__attribute__((cce_global)) uint8_t* context, HcclTilingVersion version);

    [aicore] inline void InitContext(uint64_t ccTilingAddr);

private:
    uint64_t ccOpTilingDataTable_[static_cast<uint32_t>(HcclCMDType::HCCL_CMD_ALL)] = {0UL};
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext_;
    __attribute__((cce_global)) AicpuContext *aicpuContext_ = nullptr;
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea_;
    uint64_t tilingBaseAddr_;
    uint16_t queueNum_ = 0U;
    uint16_t handleId2CurrSliceId_[HCCL_MAX_HANDLE_ID] = {0U};
    uint16_t handleIdCommitTurnCnt_[HCCL_MAX_HANDLE_ID] = {0U};
    uint16_t handleIdWaitCallNum_[HCCL_MAX_HANDLE_ID] = {0U};
    uint8_t handleId2CmdType_[HCCL_MAX_HANDLE_ID] = {0U};
    int8_t handleIdMsgPosition_[HCCL_MAX_HANDLE_ID];
    uint8_t handleIdRepeat_[HCCL_MAX_HANDLE_ID] = {0U};
    uint8_t curMsgPosition_[MAX_QUE_NUM] = {0U};
    HcclHandle curHandleId_ = INVALID_HANDLE_ID;




    HcclTilingVersion curVersion_ = HcclTilingVersion::INVALID_TILING_VERSION;
    uint8_t workingFlag_ = false;
    uint8_t debugMode_ = 0U;
    uint8_t devType_;
};
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_v220_impl.h" 2
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl_def.h" 2
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h" 2

namespace AscendC {
static constexpr HcclServerConfig DEFAULT_CFG = {CoreType::DEFAULT, 0};
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
template <HcclServerType serverType = HcclServerType::HCCL_SERVER_TYPE_AICPU, const auto &config = DEFAULT_CFG>
class Hccl {
public:
# 66 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle AllReduce(
        __attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count, HcclDataType dataType, HcclReduceOp op, uint8_t repeat = 1);
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                           HcclDataType dataType, uint64_t strideCount, uint8_t repeat = 1);
# 120 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount,
        HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat = 1);
# 151 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                          HcclDataType dataType, uint64_t strideCount = 0, uint8_t repeat = 1);
# 188 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                                           __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                                           uint8_t repeat = 1);
# 215 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum, uint16_t queueID = 0U);
# 237 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool commit = false>
    [aicore] inline HcclHandle AlltoAllvWrite(__attribute__((cce_global)) uint8_t* usrIn, __attribute__((cce_global)) uint8_t* sendOffsets, __attribute__((cce_global)) uint8_t* sendSizes,
                                                uint64_t remoteWinOffset, uint64_t localDataSize);

public:







    [[deprecated("It is obsoleted and use SetCcTilingV2 instead.")]]
    [aicore] inline int32_t SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData);
# 259 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline int32_t SetCcTilingV2(uint64_t offset);
# 268 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [[deprecated("It is obsoleted and use InitV2 instead.")]]
    [aicore] inline void Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling = nullptr);
# 278 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline void InitV2(__attribute__((cce_global)) uint8_t* context, const void *initTiling);
# 289 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline void Commit(HcclHandle handleId);
# 301 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline int32_t Wait(HcclHandle handleId);
# 311 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline int32_t Query(HcclHandle handleId);
# 322 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline void InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID);






    template <ScopeType type = ScopeType::ALL>
    [aicore] inline void QueueBarrier(uint16_t queueID);
# 341 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool sync = true>
    [aicore] inline int32_t Iterate(HcclHandle handleId, uint16_t *seqSlices, uint16_t seqSliceLen);
# 352 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    template <bool sync = true>
    [aicore] inline void Finalize();

public:
# 365 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsInAddr(uint32_t rankId);
# 376 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline __attribute__((cce_global)) uint8_t* GetWindowsOutAddr(uint32_t rankId);
# 385 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline uint32_t GetRankId();
# 394 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline uint32_t GetRankDim();
# 404 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h"
    [aicore] inline bool SetReduceDataTypeAbility(HcclReduceOp op,
        HcclDataType dstDataType, HcclDataType srcDataType);

    [aicore] inline uint16_t GetQueueNum();

private:
    HcclImpl<serverType, config> impl_;
};
}

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_utils.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_utils.h"
namespace AscendC {
[aicore] inline void FlushDataCache(GlobalTensor<int64_t> &globalHcclMsgArea, __attribute__((cce_global)) void *gmAddr)
{
    AscendC::Barrier();
    globalHcclMsgArea.SetGlobalBuffer((__attribute__((cce_global)) int64_t *)gmAddr);
    __asm__("NOP");
    DataCacheCleanAndInvalid<int64_t, CacheLine::SINGLE_CACHE_LINE, DcciDst::CACHELINE_OUT>(globalHcclMsgArea);
    DataSyncBarrier<MemDsbT::ALL>();
}

[aicore] inline void FlushDataCache(__attribute__((cce_global)) void *gmAddr)
{
    GlobalTensor<int64_t> globalHcclMsgArea;
    FlushDataCache(globalHcclMsgArea, gmAddr);
}

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_impl_dfx.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_impl_dfx.h"
namespace AscendC {
enum class HcclApiOperType : uint32_t {
    MIN_TYPE = 0x1000,

    INIT = MIN_TYPE,
    COMMIT = 0x1010,
    WAIT = 0x1020,
    QUERY = 0x1030,
    FINALIZE = 0x1040,
    GROUP_SYNC = 0x1050,
    GET_WINDOW_IN = 0x1060,
    GET_WINDOW_OUT = 0x1062,
    GET_RANK_ID = 0x1064,
    GET_RANK_DIM = 0x1066,
    SET_CCTILING = 0x1068,
    ITERATE = 0x1070,
    BARRIER = 0x1072,

    ALL_REDUCE_PREPARE = 0x1100,
    ALL_GATHER_PREPARE = 0x1110,
    REDUCE_SCATTER_PREPARE = 0x1120,
    ALL_TO_ALL_PREPARE = 0x1130,
    ALL_TO_ALL_V_PREPARE = 0x1140,
    BATCH_WRITE_PREPARE = 0x1150,

    MAX_TYPE = 0x1FFF
};
# 54 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_impl_dfx.h"
class DfxScopeGuard {
public:
    [aicore] inline DfxScopeGuard(HcclApiOperType operType): operType_(operType) {
        PrintTimeStamp(static_cast<uint32_t>(operType_));



    }
# 148 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_impl_dfx.h"
    [aicore] inline ~DfxScopeGuard() {



        PrintTimeStamp(static_cast<uint32_t>(operType_) + 1);
    }

private:
    HcclApiOperType operType_;







};
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_utils.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_control.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_control.h"
namespace AscendC {

constexpr uint8_t SINGLE_COMM_NUM = 1;
constexpr uint8_t MULTI_COMM_NUM = 2;
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/../../common/hccl_control.h"
[aicore] inline void GetRestartFromContext(__attribute__((cce_global)) uint8_t* context, uint8_t &restart)
{
    do { if (context == nullptr) { return; } } while (0);
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext = (__attribute__((cce_global)) HcclCombineOpParam *)context;
    uint64_t msgAddr = hcclContext->workSpace;
    do { if (msgAddr == 0) { return; } } while (0);
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    __attribute__((cce_global)) ControlHcclMsg *controlMsgGM = &hcclMsgArea->controlMsg;
    FlushDataCache(controlMsgGM);
    restart += controlMsgGM->restart;
}

[aicore] inline void ResetRestartFlag(__attribute__((cce_global)) uint8_t* context)
{
    do { if (context == nullptr) { return; } } while (0);
    __attribute__((cce_global)) HcclCombineOpParam *hcclContext = (__attribute__((cce_global)) HcclCombineOpParam *)context;
    uint64_t msgAddr = hcclContext->workSpace;
    do { if (msgAddr == 0) { return; } } while (0);
    __attribute__((cce_global)) HcclMsgArea *hcclMsgArea = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    __attribute__((cce_global)) ControlHcclMsg *controlMsgGM = &hcclMsgArea->controlMsg;
    controlMsgGM->restart = 0;
    controlMsgGM->restarting = 1;
    controlMsgGM->resetSeq = 1;
    FlushDataCache(controlMsgGM);
}

[aicore] inline uint8_t GetRestart(uint8_t ctxNum)
{
    uint8_t restart = 0;

    if (ctxNum >= SINGLE_COMM_NUM) {
        GetRestartFromContext(AscendC::GetHcclContext<0>(), restart);
    }
    if (ctxNum >= MULTI_COMM_NUM) {
        GetRestartFromContext(AscendC::GetHcclContext<1>(), restart);
    }
    return restart;
}

[aicore] inline void SetRestart(uint8_t ctxNum)
{
    if (GetBlockIdx() == 0) {

        if (ctxNum >= SINGLE_COMM_NUM) {
            ResetRestartFlag(AscendC::GetHcclContext<0>());
        }
        if (ctxNum >= MULTI_COMM_NUM) {
            ResetRestartFlag(AscendC::GetHcclContext<1>());
        }
    }
}

[aicore] inline bool CheckIfRestart(__attribute__((cce_global)) HcclMsgArea *msgArea)
{







    return false;
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu_utils.h" 2

namespace AscendC {
[aicore] inline void CopyHcclMsg(const uint8_t *src, __attribute__((cce_global)) HcclMsg *dst)
{
    constexpr uint32_t HCCL_VALID_POS = 12U;
    __attribute__((cce_global)) DataBlock *tmpDst = reinterpret_cast<__attribute__((cce_global)) DataBlock *>(dst);
    volatile uint32_t xorCheck = 0U;
    for (uint32_t i = 0; i < HCCL_MSG_DATA_CNT - 1U; ++i) {
        if (i == HCCL_VALID_POS) {
            xorCheck ^= HCCL_MSG_VALID_MASK;
        } else {
            xorCheck ^= tmpDst->data[i] = *(reinterpret_cast<const uint32_t *>(src));
        }
        src += sizeof(tmpDst->data[i]);
    }
    tmpDst->data[HCCL_MSG_DATA_CNT - 1U] = xorCheck;
    tmpDst->data[HCCL_VALID_POS] = HCCL_MSG_VALID_MASK;
}


[aicore] inline void AssembleHcclMsgExt(const AlltoAllVParamExt &param, uint32_t rankDim, __attribute__((cce_global)) HcclMsgExt *dst)
{
    uint64_t xorCheck = 0U;
    for (uint32_t i = 0U; i < rankDim; ++i) {
        xorCheck ^= dst->sendCounts[i] = param.sendCounts[i];
        xorCheck ^= dst->sendOffset[i] = param.sdispls[i];
        xorCheck ^= dst->recvCounts[i] = param.recvCounts[i];
        xorCheck ^= dst->recvOffset[i] = param.rdispls[i];
    }
    dst->xorCheck = (xorCheck ^ HCCL_MSG_VALID_MASK);
    dst->valid = HCCL_MSG_VALID_MASK;
}

[aicore] inline void AssembleHcclMsg(const CommonPrepareParam &param, HcclTilingVersion ver, HcclHandle handle,
                                       uint64_t tiling, __attribute__((cce_global)) HcclMsg *dst, __attribute__((cce_global)) ControlHcclMsg *controlMsgGM)
{
    HcclMsg tmp{};
    static uint8_t primitiveId = 0U;
    static bool isResetPrimitiveId = false;
    FlushDataCache(controlMsgGM);
    if (controlMsgGM->resetSeq > 0) {
        controlMsgGM->resetSeq = 0;
        if (!isResetPrimitiveId) {
            primitiveId = 0U;
            isResetPrimitiveId = true;
        }
    }
    tmp.commType.msgType = param.commType.msgType;
    if (param.commType.msgType == ControlMsgType::HCCL_CMD_FINALIZE) {
        primitiveId = 0U;
        isResetPrimitiveId = false;
    } else {
        tmp.opType = param.op;
        tmp.sendBuffer = reinterpret_cast<uint64_t>(param.sendBuf);
        tmp.recvBuffer = reinterpret_cast<uint64_t>(param.recvBuf);
        tmp.dataCnt = param.count;
        tmp.strideCount = param.strideCount;
        if (ver == HcclTilingVersion::DEPRECATED_TILING_VERSION) {
            tmp.addMsg.v0Msg.hcclDataType = param.dataType;
            tmp.addMsg.v0Msg.repeatCnt = param.repeat;
            tmp.addMsg.v0Msg.selfHandleID = handle;
            tmp.addMsg.v0Msg.seqNum = primitiveId++;
            tmp.addMsg.v0Msg.version = ver;
        } else {
            tmp.addMsg.v1Msg.ccOpTilingData = tiling;
            tmp.addMsg.v1Msg.hcclDataType = param.dataType;
            tmp.addMsg.v1Msg.repeatCnt = param.repeat;
            tmp.addMsg.v1Msg.selfHandleID = handle;
            tmp.addMsg.v1Msg.seqNum = primitiveId++;
            tmp.addMsg.v1Msg.version = ver;
        }
    }
    tmp.addMsg.v0Msg.valid = HCCL_MSG_VALID_MASK;
    CopyHcclMsg(reinterpret_cast<const uint8_t *>(&tmp), dst);
}

[aicore] inline void AssembleHcclMsg(const CommonPrepareParam &param, int8_t srcGroupID,
                                       HcclHandle srcHandleID, __attribute__((cce_global)) HcclMsg *dst)
{
    HcclMsg tmp{};
    tmp.commType.msgType = param.commType.msgType;
    tmp.addMsg.v0Msg.commDepGroupID = srcGroupID;
    tmp.addMsg.v0Msg.commDepHandleID = srcHandleID;
    tmp.addMsg.v0Msg.valid = HCCL_MSG_VALID_MASK;
    CopyHcclMsg(reinterpret_cast<const uint8_t *>(&tmp), dst);
}

[aicore] inline HcclContextDef::HcclRankRelationResV2 *GetRemoteRankAddrs(__attribute__((cce_global)) HcclContextDef::HcclOpResParam *ctx,
                                                                            uint32_t rankId)
{
    const HcclContextDef::RemoteResPtr *remoteRes =
            reinterpret_cast<const HcclContextDef::RemoteResPtr *>(reinterpret_cast<uintptr_t>(ctx) + ctx->rWinStart);
    return remoteRes[rankId].nextDevicePtr;
}

[aicore] inline void UpdateControlMsgCount(__attribute__((cce_global)) HcclMsgArea *hcclMsgArea, ControlMsgType msg)
{

                                                                               ;
    __attribute__((cce_global)) TurnCnt *apiInfo = &(hcclMsgArea->apiStats.msgStats[
            static_cast<uint32_t>(msg) - static_cast<uint32_t>(ControlMsgType::HCCL_CMD_FINALIZE)]);
    FlushDataCache(apiInfo);
    ++(apiInfo->cnt);
    FlushDataCache(apiInfo);
}

template <const auto &config>
[aicore] inline bool
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::CheckCommonPrepareParamValid(const CommonPrepareParam &param)
{
    const HcclCMDType commType = param.commType.prepareType;
    uint64_t tiling = 0UL;
    if (commType < HcclCMDType::HCCL_CMD_ALL) {
        tiling = ccOpTilingDataTable_[static_cast<uint32_t>(commType)];
    }
    if (curVersion_ > HcclTilingVersion::DEPRECATED_TILING_VERSION) {


                                                                ;
    } else {


                                                                ;
    }


                                                                                             ;



                                                                                                 ;
    if (commType == HcclCMDType::HCCL_CMD_ALLTOALLV) {



                                                                                                                      ;
    } else {


                                                               ;
    }
    return true;
}


template<const auto &config>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::InitWorkingFlag()
{
    using T = decltype(config);
    static_assert(std::is_same<T, const HcclServerConfig &>::value);
                                                                                                             ;
    if constexpr (config.type == CoreType::ON_AIV) {
        workingFlag_ = (g_coreType == AscendC::AIV && GetBlockIdx() == config.blockId);
    } else if constexpr (config.type == CoreType::ON_AIC) {
        workingFlag_ = (g_coreType == AscendC::AIC && GetBlockIdx() == config.blockId);
    } else {
        workingFlag_ = (GetBlockIdx() == config.blockId);
    }
}

template <const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::InitInner(__attribute__((cce_global)) uint8_t* context, HcclTilingVersion version)
{

                                                              ;
    hcclContext_ = (__attribute__((cce_global)) HcclCombineOpParam *)context;
                                                                                                               ;
    if (__builtin_expect(!!(hcclContext_->workSpace == 0UL), 0)) {
        return;
    }

    uint64_t msgAddr = hcclContext_->workSpace;
    if (msgAddr & 0x1ff) {
        msgAddr = (msgAddr & (~((uint64_t)0x1ff))) + 0x200;
    }
    hcclMsgArea_ = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    for (uint32_t i = 0U; i < HCCL_MAX_HANDLE_ID; ++i) {
        handleIdMsgPosition_[i] = -1;
    }
    InitWorkingFlag();
    curVersion_ = version;
}

template <const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::InitContext(uint64_t ccTilingAddr)
{
    __attribute__((cce_global)) Mc2CcTilingInner *ccTilingPtr = reinterpret_cast<__attribute__((cce_global)) Mc2CcTilingInner *>(ccTilingAddr);
    if (ccTilingPtr->version == MC2_CC_TILING_INTERFACE) {
        aicpuContext_ = reinterpret_cast<__attribute__((cce_global)) AicpuContext *>(&(ccTilingPtr->aicpuContext));
        uint64_t msgAddr = aicpuContext_->workSpace;
        if (__builtin_expect(!!(msgAddr == 0UL), 0)) {
            return;
        }

        if (msgAddr & 0x1ff) {
            msgAddr = (msgAddr & (~((uint64_t)0x1ff))) + 0x200;
        }
        hcclMsgArea_ = (__attribute__((cce_global)) HcclMsgArea *)msgAddr;
    } else {
        aicpuContext_ = nullptr;
    }
}

template<const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SendMsgToServer(uint16_t queId,
        const CommonPrepareParam &param, int8_t srcGroupID, HcclHandle srcHandleID)
{
    if (!workingFlag_ && queueNum_ == 0U) {
        return;
    }
    __attribute__((cce_global)) HcclMsg *hcclSendMsg;
    if (queueNum_ == 0U) {
        hcclSendMsg = hcclMsgArea_->commMsg.singleMsg.sendMsgs + curMsgPosition_[0U];
    } else {
        hcclSendMsg = hcclMsgArea_->commMsg.multiMsg.sendMsgs[queId + GetBlockIdx() * queueNum_] +
                curMsgPosition_[queId];
    }

    do {
        do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);
        FlushDataCache(hcclSendMsg);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) &&
            ((curVersion_ == HcclTilingVersion::DEPRECATED_TILING_VERSION &&
            hcclSendMsg->addMsg.v0Msg.valid == HCCL_MSG_VALID_MASK) ||
            (curVersion_ != HcclTilingVersion::DEPRECATED_TILING_VERSION &&
            hcclSendMsg->addMsg.v1Msg.valid == HCCL_MSG_VALID_MASK)));
                                                                                          ;
    if (srcGroupID < 0) {
        uint64_t tiling = 0UL;
        if (param.commType.prepareType < HcclCMDType::HCCL_CMD_ALL) {
            tiling = ccOpTilingDataTable_[static_cast<uint32_t>(param.commType.prepareType)];
        }
        AssembleHcclMsg(param, curVersion_, curHandleId_, tiling, hcclSendMsg, &hcclMsgArea_->controlMsg);
    } else {
        AssembleHcclMsg(param, srcGroupID, srcHandleID, hcclSendMsg);
    }
    FlushDataCache(reinterpret_cast<__attribute__((cce_global)) void *>(hcclSendMsg));
}

template<const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SendMsgToServer(const AlltoAllVParamExt &param)
{
    if (!workingFlag_) {
        return;
    }
    __attribute__((cce_global)) HcclMsgExt *hcclSendMsg = &(hcclMsgArea_->commMsg.singleMsg.paramExtMsgList[curMsgPosition_[0U]]);
    do {
        do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);
        FlushDataCache(hcclSendMsg);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (hcclSendMsg->valid == HCCL_MSG_VALID_MASK));
                                                                                          ;
    uint32_t rankNum = aicpuContext_ != nullptr ? aicpuContext_->rankNum : hcclContext_->rankNum;
    AssembleHcclMsgExt(param, rankNum, hcclSendMsg);
    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i < rankNum; i += MAX_DCCI_CNT / sizeof(uint64_t)) {
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->sendCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->sendOffset + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->recvCounts + i));
        FlushDataCache(globalHcclMsgArea, (hcclSendMsg->recvOffset + i));
    }
    FlushDataCache(globalHcclMsgArea, hcclSendMsg->reserved);
}

template<const auto &config>
[aicore] inline uint16_t
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::GetStepSizeByHandle(HcclHandle handle)
{
    const uint8_t commType = handleId2CmdType_[handle];
    if (commType != static_cast<uint8_t>(HcclCMDType::HCCL_CMD_ALLTOALLV)) {
        return 0U;
    }
    __attribute__((cce_global)) Mc2CcTilingInner *tilingPtr;
    if (curVersion_ == HcclTilingVersion::ONLINE_COMPILATION_TILING_VERSION) {
        tilingPtr = reinterpret_cast<__attribute__((cce_global)) Mc2CcTilingInner *>(ccOpTilingDataTable_[commType] + tilingBaseAddr_);
    } else {
        tilingPtr = reinterpret_cast<__attribute__((cce_global)) Mc2CcTilingInner *>(ccOpTilingDataTable_[commType]);
    }
    if (tilingPtr == nullptr) {
        return 0U;
    }
    return tilingPtr->stepSize;
}

template<const auto &config>
[aicore] inline uint16_t
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::GetStepCntsPerRepeatByHandle(HcclHandle handle)
{
    return (GetStepSizeByHandle(handle) == 0U ? 1U : GetRankDim());
}

template<const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SetCommitTurnCntToGm(uint8_t msgPos, uint64_t turnCnt,
                                                                               HcclHandle handleId)
{
    handleIdCommitTurnCnt_[handleId] += turnCnt;
    if (queueNum_ != 0U || !workingFlag_) {
        return;
    }

    __attribute__((cce_global)) TurnCnt *commitGM = hcclMsgArea_->commMsg.singleMsg.commitTurnCnt + msgPos;
    do {
        do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);
        FlushDataCache(commitGM);
    } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (commitGM->cnt >= handleIdCommitTurnCnt_[handleId]));

                                                                     ;
    commitGM->cnt = handleIdCommitTurnCnt_[handleId];
    commitGM->valid = COMMIT_VALID_MASK;
    FlushDataCache(commitGM);
    if (workingFlag_) {
        __attribute__((cce_global)) TurnCnt *apiInfo = &(hcclMsgArea_->apiStats.commitStats[handleId2CmdType_[handleId]]);
        FlushDataCache(apiInfo);
        apiInfo->cnt += turnCnt;
        FlushDataCache(apiInfo);
    }
}

template<const auto &config>
[aicore] inline uint64_t
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::WaitFinishCntFromGm(uint8_t msgPos, uint64_t expectedCnt)
{
    __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->commMsg.singleMsg.finishedTurnCnt + msgPos;
    GlobalTensor<int64_t> globalHcclMsgArea;
    while (true) {
        do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return finishGM->cnt; } } while (0);
        FlushDataCache(globalHcclMsgArea, finishGM);
        if ((debugMode_ == HCCL_ONLY_COMPUTE) || (finishGM->cnt >= expectedCnt)) {
            break;
        }
    }
    return finishGM->cnt;
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::CommonPrepareImpl(
        const CommonPrepareParam &param)
{
    do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return INVALID_HANDLE_ID; } } while (0);
    if (__builtin_expect(!!(param.repeat == 0U), 0)) {
        return INVALID_HANDLE_ID;
    }


                                                                             ;

    HcclHandle handleId = ++curHandleId_;


                                                                                                               ;
    if (param.commType.prepareType == HcclCMDType::HCCL_CMD_ALLTOALLV) {
        SendMsgToServer(param.paramExt);
    }
    const uint16_t queId = (queueNum_ == 0U ? 0U : static_cast<uint16_t>(param.dataType));
    SendMsgToServer(queId, param);
    handleIdMsgPosition_[handleId] = curMsgPosition_[queId];
    handleIdRepeat_[handleId] = param.repeat;
    handleId2CmdType_[handleId] = static_cast<uint8_t>(param.commType.prepareType);
    if constexpr (commit) {
        SetCommitTurnCntToGm(curMsgPosition_[queId], param.repeat * GetStepCntsPerRepeatByHandle(handleId), handleId);
    }
    ++(curMsgPosition_[queId]);

                                                                                     ;
    return handleId;
}

template<const auto &config>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::ResetFinishedTurnCnt()
{
    __attribute__((cce_global)) TurnCnt *finishArea = hcclMsgArea_->commMsg.singleMsg.finishedTurnCnt;
    GlobalTensor<int64_t> globalHcclMsgArea;
    for (uint32_t i = 0U; i <= curMsgPosition_[0U]; ++i) {
        __attribute__((cce_global)) TurnCnt *finishGM = finishArea + i;
        finishGM->cnt = 0;
        FlushDataCache(globalHcclMsgArea, finishGM);
    }
}

template<const auto &config>
template <bool sync>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SendFinalizeMsg()
{
    const uint16_t totalQueNum = (queueNum_ == 0U ? 1U : queueNum_);
    CommonPrepareParam param;
    param.commType.msgType = ControlMsgType::HCCL_CMD_FINALIZE;
    for (uint16_t idx = 0U; idx < totalQueNum; ++idx) {

                                                       ;
        SendMsgToServer(idx, param);
        if constexpr (!sync) {
            ++(curMsgPosition_[idx]);

                                                                                              ;
        }
    }
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/platform_v220/hccl_aicpu.h" 2

namespace AscendC {
template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
        HcclDataType dataType, HcclReduceOp op, uint8_t repeat)
{

                                                                                                                  ;

    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLREDUCE, sendBuf, recvBuf, count, dataType, dataType,
                                       op, 0, repeat });
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf,
        uint64_t sendCount, HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLGATHER, sendBuf, recvBuf, sendCount, dataType, dataType,
                                       HCCL_REDUCE_RESERVED, strideCount, repeat });
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf,
        uint64_t recvCount, HcclDataType dataType, HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{

                                                                                                                      ;
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_REDUCE_SCATTER, sendBuf, recvBuf, recvCount,
                                       dataType, dataType, op, strideCount, repeat });
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf,
        uint64_t dataCount, HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLTOALL, sendBuf, recvBuf, dataCount, dataType, dataType,
                                       HCCL_REDUCE_RESERVED, strideCount, repeat });
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls,
        HcclDataType sendType, __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType, uint8_t repeat)
{


                                                                                           ;
    return CommonPrepareImpl<commit>({ HcclCMDType::HCCL_CMD_ALLTOALLV, sendBuf, recvBuf, 0U, sendType, recvType,
                                       HCCL_REDUCE_RESERVED, 0U, repeat,
                                       {static_cast<uint64_t *>(sendCounts), static_cast<uint64_t *>(sdispls),
                                        static_cast<uint64_t *>(recvCounts), static_cast<uint64_t *>(rdispls)} });
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum,
        uint16_t queueID)
{
    return CommonPrepareImpl<true>({HcclCMDType::HCCL_CMD_BATCH_WRITE, batchWriteInfo, batchWriteInfo, itemNum,
                                    static_cast<HcclDataType>(queueID), static_cast<HcclDataType>(queueID),
                                    static_cast<HcclReduceOp>(queueID + GetBlockIdx() * queueNum_)});
}

template<const auto &config>
template <bool commit>
[aicore] inline HcclHandle
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::AlltoAllvWrite(__attribute__((cce_global)) uint8_t* usrIn, __attribute__((cce_global)) uint8_t* sendOffsets,
        __attribute__((cce_global)) uint8_t* sendSizes, uint64_t remoteWinOffset, uint64_t localDataSize)
{
    CommonPrepareParam commonPrepareParam = {HcclCMDType::HCCL_CMD_HALF_ALLTOALLV,
        usrIn,
        usrIn,
        localDataSize,
        HCCL_DATA_TYPE_INT8,
        HCCL_DATA_TYPE_INT8,
        HCCL_REDUCE_RESERVED,
        0,
        1,
        {},
        {reinterpret_cast<uint64_t>(sendOffsets), reinterpret_cast<uint64_t>(sendSizes), remoteWinOffset}
    };

    return CommonPrepareImpl<commit>(commonPrepareParam);
}

template<const auto &config>
[aicore] inline int32_t HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Query(HcclHandle handleId)
{

                                                                                                      ;


                                                         ;
    if (queueNum_ != 0U) {
        return 0;
    }
    int8_t curMsgPos = handleIdMsgPosition_[handleId];

                                                                                                          ;
    return WaitFinishCntFromGm(curMsgPos, 0UL);
}

template<const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID)
{

                                                                                                                   ;
    CommonPrepareParam param;
    param.commType.msgType = ControlMsgType::HCCL_CMD_INTER_GROUP_SYNC;
    SendMsgToServer(0U, param, srcGroupID, srcHandleID);
    ++(curMsgPosition_[0U]);

                                                                                        ;
    if (workingFlag_) {
        UpdateControlMsgCount(hcclMsgArea_, ControlMsgType::HCCL_CMD_INTER_GROUP_SYNC);
    }
}

template<const auto &config>
[aicore] inline __attribute__((cce_global)) uint8_t* HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::GetWindowsInAddr(uint32_t rankId)
{

                                                                                                                ;
    if (devType_ != HCCL_ASCEND910B) {
        __attribute__((cce_global)) HcclContextDef::HcclOpResParam *hcclContext = (__attribute__((cce_global)) HcclContextDef::HcclOpResParam *)hcclContext_;
        if (rankId == hcclContext->rankId) {
            return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(hcclContext->localWindowsIn);
        } else {
            const auto addr = GetRemoteRankAddrs(hcclContext, rankId);
            return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(addr != nullptr ? addr->windowsIn : 0UL);
        }
    } else {
        if (hcclContext_->multiFlag == 0U) {
            return (__attribute__((cce_global)) uint8_t*)hcclContext_->windowsIn[rankId];
        } else {
            if (rankId == hcclContext_->rankId) {
                return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].localInput.addr);
            } else {
                return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].remoteInput.addr);
            }
        }
    }
}

template<const auto &config>
[aicore] inline __attribute__((cce_global)) uint8_t* HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::GetWindowsOutAddr(uint32_t rankId)
{

                                                                                                                 ;
    if (devType_ != HCCL_ASCEND910B) {
        __attribute__((cce_global)) HcclContextDef::HcclOpResParam *hcclContext = (__attribute__((cce_global)) HcclContextDef::HcclOpResParam *)hcclContext_;
        if (rankId == hcclContext->rankId) {
            return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(hcclContext->localWindowsOut);
        } else {
            const auto addr = GetRemoteRankAddrs(hcclContext, rankId);
            return reinterpret_cast<__attribute__((cce_global)) uint8_t*>(addr != nullptr ? addr->windowsOut : 0UL);
        }
    } else {
        if (hcclContext_->multiFlag == 0U) {
            return (__attribute__((cce_global)) uint8_t*)hcclContext_->windowsOut[rankId];
        } else {
            if (rankId == hcclContext_->rankId) {
                return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].localOutput.addr);
            } else {
                return (__attribute__((cce_global)) uint8_t*)(hcclContext_->data[rankId].remoteOutput.addr);
            }
        }
    }
}

template <const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling)
{
    HcclTilingVersion version;
    if (initTiling != nullptr) {
        version = HcclTilingVersion::NEW_TILING_VERSION;
        auto initTilingPtr = static_cast<__attribute__((cce_global)) Mc2InitTilingInner *>(initTiling);
        debugMode_ = initTilingPtr->debugMode;
        queueNum_ = initTilingPtr->queueNum;
        devType_ = initTilingPtr->devType;
    } else {
        version = HcclTilingVersion::DEPRECATED_TILING_VERSION;
        devType_ = HCCL_ASCEND910B;
    }
    InitInner(context, version);
}

template <const auto &config>
[aicore] inline void
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::InitV2(__attribute__((cce_global)) uint8_t* context, const void *initTiling)
{
                                                                                      ;
    const Mc2InitTilingInner *initTilingPtr = static_cast<const Mc2InitTilingInner *>(initTiling);
    debugMode_ = initTilingPtr->debugMode;
    queueNum_ = initTilingPtr->queueNum;
    devType_ = initTilingPtr->devType;
    InitInner(context, HcclTilingVersion::ONLINE_COMPILATION_TILING_VERSION);
    tilingBaseAddr_ = reinterpret_cast<uint64_t>(initTiling);
}

template<const auto &config>
[aicore] inline int32_t
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData)
{

                                                                                                              ;

                                                                                            ;
    const uint32_t opType = (static_cast<__attribute__((cce_global)) Mc2CcTilingInner *>(ccOpTilingData))->opType;

                                                                                                        ;
                                                                                                                      ;
    ccOpTilingDataTable_[opType] = reinterpret_cast<uint64_t>(ccOpTilingData);
    InitContext(ccOpTilingDataTable_[opType]);
    return HCCL_SUCCESS;
}

template <const auto &config>
[aicore] inline int32_t HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::SetCcTilingV2(uint64_t offset)
{


                                                                                                              ;
    const uint32_t opType = (reinterpret_cast<Mc2CcTilingInner *>(tilingBaseAddr_ + offset))->opType;

                                                                                                        ;
    ccOpTilingDataTable_[opType] = offset;
    InitContext(tilingBaseAddr_ + offset);
    return HCCL_SUCCESS;
}

template<const auto &config>
[aicore] inline int32_t HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Wait(HcclHandle handleId)
{
    do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return HCCL_FAILED; } } while (0);

                                                                                                     ;
    if (queueNum_ != 0U) {
        return HCCL_SUCCESS;
    }
    if (__builtin_expect(!!(handleId <= INVALID_HANDLE_ID || handleId >= HCCL_MAX_HANDLE_ID), 0)) {

                                                ;
        return HCCL_FAILED;
    }
    uint16_t &waitCnt = handleIdWaitCallNum_[handleId];
    if (__builtin_expect(!!(waitCnt >= handleIdCommitTurnCnt_[handleId]), 0)) {

                                                                                                                 ;
        return HCCL_FAILED;
    }
    if (workingFlag_) {
        __attribute__((cce_global)) TurnCnt *apiInfo = &(hcclMsgArea_->apiStats.waitStats[handleId2CmdType_[handleId]]);
        FlushDataCache(apiInfo);
        ++(apiInfo->cnt);
        FlushDataCache(apiInfo);
    }
    int8_t curMsgPos = handleIdMsgPosition_[handleId];

                                                                                                         ;
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    waitCnt += (stepSize == 0U ? 1U : stepSize);
    (void)WaitFinishCntFromGm(curMsgPos, waitCnt);
    return HCCL_SUCCESS;
}

template<const auto &config>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Commit(HcclHandle handleId)
{
    do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);

                                                                                                       ;
    if (__builtin_expect(!!(handleId <= INVALID_HANDLE_ID || handleId >= HCCL_MAX_HANDLE_ID), 0)) {

                                                ;
        return;
    }
    const uint16_t commitCnt = handleIdCommitTurnCnt_[handleId];
    if (__builtin_expect(!!(commitCnt >= handleIdRepeat_[handleId] * GetStepCntsPerRepeatByHandle(handleId)), 0)) {


                                                                                                    ;
        return;
    }
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    SetCommitTurnCntToGm(handleIdMsgPosition_[handleId], (stepSize == 0U ? 1U : stepSize),
                         handleId);
}

template<const auto &config>
template <ScopeType type>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::QueueBarrier(uint16_t queueID)
{
    CommonPrepareParam param;
    param.commType.msgType = ControlMsgType::HCCL_CMD_BARRIER;
    SendMsgToServer(queueID, param);
    ++(curMsgPosition_[queueID]);

                                                                                     ;
    if (workingFlag_) {
        UpdateControlMsgCount(hcclMsgArea_, ControlMsgType::HCCL_CMD_BARRIER);
    }
}

template<const auto &config>
template <bool sync>
[aicore] inline int32_t
HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Iterate(HcclHandle handleId,
        uint16_t *seqSlices, uint16_t seqSliceLen)
{


                                                                                                  ;

                                                         ;
    const uint16_t stepSize = GetStepSizeByHandle(handleId);
    const uint16_t stepsPerRepeat = GetStepCntsPerRepeatByHandle(handleId);

                                                                              ;
    uint16_t &curSlice = handleId2CurrSliceId_[handleId];

                                                            ;


    if (curSlice >= stepsPerRepeat * handleIdRepeat_[handleId]) {
                                                                                                         ;
        return 0;
    }
    const uint16_t slicesPerRepeat = stepsPerRepeat;
    const uint32_t rankId = GetRankId();
    const uint32_t rankDim = GetRankDim();
                                                                                        ;
    for (uint16_t i = 0U; i < seqSliceLen; ++i) {
        if constexpr (sync) {
            if ((curSlice + 1) % stepSize == 0) {
                (void)Wait(handleId);
            }
            seqSlices[i] = (rankId + rankDim - curSlice % slicesPerRepeat) % rankDim;
        } else {
            seqSlices[i] = (rankId + curSlice % slicesPerRepeat) % rankDim;
        }
        ++curSlice;
    }
    return seqSliceLen;
}

template<const auto &config>
template <bool sync>
[aicore] inline void HcclImpl<HcclServerType::HCCL_SERVER_TYPE_AICPU, config>::Finalize()
{

                                                                                                         ;
    do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);

    if (!workingFlag_ && queueNum_ == 0U) {
        ++(curMsgPosition_[0U]);

                                                                                          ;
        return;
    }



    if constexpr (sync) {
        if (curHandleId_ > INVALID_HANDLE_ID) {
                                                                                                                 ;
            while ((debugMode_ != HCCL_ONLY_COMPUTE) && (Query(curHandleId_) < handleIdRepeat_[curHandleId_])) {
                do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);
            }
        }
    }


    SendFinalizeMsg<sync>();

    if constexpr (sync) {


        __attribute__((cce_global)) TurnCnt *finishGM = hcclMsgArea_->commMsg.singleMsg.finishedTurnCnt + curMsgPosition_[0U];
                                                                                                            ;
        do {
            do { if (__builtin_expect(!!(CheckIfRestart(hcclMsgArea_)), 0)) { return; } } while (0);
            FlushDataCache(finishGM);
        } while ((debugMode_ != HCCL_ONLY_COMPUTE) && (finishGM->cnt != FINALIZE_FINISH_CNT));
                                                                                                ;
        ResetFinishedTurnCnt();
        ++(curMsgPosition_[0U]);

                                                                                          ;
    }
    if (workingFlag_) {
        UpdateControlMsgCount(hcclMsgArea_, ControlMsgType::HCCL_CMD_FINALIZE);
        __attribute__((cce_global)) TurnCnt *snapshots = hcclMsgArea_->apiStats.snapshots;
        FlushDataCache(snapshots);
        for (auto handleId = 0; handleId <= curHandleId_; ++handleId) {
            auto &apiSnapshot = snapshots[snapshots->cnt % HCCL_API_SNAPSHOTS_CNT + 1UL];
            apiSnapshot.cnt = handleId2CmdType_[handleId];
            FlushDataCache(&apiSnapshot);
            ++(snapshots->cnt);
        }
        FlushDataCache(snapshots);
    }
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/../../../impl/adv_api/detail/hccl/impl/hccl_impl.h" 2






namespace AscendC {
template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle Hccl<serverType, config>::AllReduce(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t count,
                                                         HcclDataType dataType, HcclReduceOp op, uint8_t repeat)
{

                                            ;
    return impl_.template AllReduce<commit>(sendBuf, recvBuf, count, dataType, op, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle Hccl<serverType, config>::AllGather(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t sendCount,
                                                         HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{

                                                     ;
    return impl_.template AllGather<commit>(sendBuf, recvBuf, sendCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle Hccl<serverType, config>::AlltoAll(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t dataCount,
                                                        HcclDataType dataType, uint64_t strideCount, uint8_t repeat)
{

                                                              ;
    return impl_.template AlltoAll<commit>(sendBuf, recvBuf, dataCount, dataType, strideCount, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle
Hccl<serverType, config>::AlltoAllV(__attribute__((cce_global)) uint8_t* sendBuf, void *sendCounts, void *sdispls, HcclDataType sendType,
                            __attribute__((cce_global)) uint8_t* recvBuf, void *recvCounts, void *rdispls, HcclDataType recvType,
                            uint8_t repeat)
{

                                                                                                  ;
    return impl_.template AlltoAllV<commit>(sendBuf, sendCounts, sdispls, sendType,
                                            recvBuf, recvCounts, rdispls, recvType, repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle
Hccl<serverType, config>::ReduceScatter(__attribute__((cce_global)) uint8_t* sendBuf, __attribute__((cce_global)) uint8_t* recvBuf, uint64_t recvCount, HcclDataType dataType,
                                HcclReduceOp op, uint64_t strideCount, uint8_t repeat)
{

                                                     ;
    return impl_.template ReduceScatter<commit>(sendBuf, recvBuf, recvCount, dataType, op, strideCount,
                                                repeat);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle Hccl<serverType, config>::BatchWrite(__attribute__((cce_global)) uint8_t* batchWriteInfo, uint32_t itemNum,
                                                                  uint16_t queueID)
{
                                                        ;
    return impl_.template BatchWrite<commit>(batchWriteInfo, itemNum, queueID);
}

template <HcclServerType serverType, const auto &config>
template <bool commit>
[aicore] inline HcclHandle Hccl<serverType, config>::AlltoAllvWrite(__attribute__((cce_global)) uint8_t* usrIn, __attribute__((cce_global)) uint8_t* sendOffsets,
    __attribute__((cce_global)) uint8_t* sendSizes, uint64_t remoteWinOffset, uint64_t localDataSize)
{
    return impl_.template AlltoAllvWrite<commit>(usrIn, sendOffsets, sendSizes, remoteWinOffset, localDataSize);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline void Hccl<serverType, config>::Init(__attribute__((cce_global)) uint8_t* context, __attribute__((cce_global)) void *initTiling)
{
                                         ;
    impl_.Init(context, initTiling);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline void Hccl<serverType, config>::InitV2(__attribute__((cce_global)) uint8_t* context, const void *initTiling)
{
                                         ;
    impl_.InitV2(context, initTiling);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline int32_t Hccl<serverType, config>::SetCcTiling(__attribute__((cce_global)) void *ccOpTilingData)
{
                                                 ;
    return impl_.SetCcTiling(ccOpTilingData);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline int32_t Hccl<serverType, config>::SetCcTilingV2(uint64_t offset)
{
                                                 ;
    return impl_.SetCcTilingV2(offset);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline void Hccl<serverType, config>::Commit(HcclHandle handleId)
{
                                           ;
    impl_.Commit(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline int32_t Hccl<serverType, config>::Wait(HcclHandle handleId)
{
                                         ;
    return impl_.Wait(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline int32_t Hccl<serverType, config>::Query(HcclHandle handleId)
{
                                          ;
    return impl_.Query(handleId);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline void Hccl<serverType, config>::InterHcclGroupSync(int8_t srcGroupID, HcclHandle srcHandleID)
{
                                               ;
    impl_.InterHcclGroupSync(srcGroupID, srcHandleID);
}

template <HcclServerType serverType, const auto &config>
template <ScopeType type>
[aicore] inline void Hccl<serverType, config>::QueueBarrier(uint16_t queueID)
{
                                            ;
    impl_.template QueueBarrier<type>(queueID);
}

template <HcclServerType serverType, const auto &config>
template <bool sync>
[aicore] inline int32_t Hccl<serverType, config>::Iterate(HcclHandle handleId, uint16_t *seqSlices,
                                                            uint16_t seqSliceLen)
{
                                            ;
    return impl_.template Iterate<sync>(handleId, seqSlices, seqSliceLen);
}

template <HcclServerType serverType, const auto &config>
template <bool sync>
[aicore] inline void Hccl<serverType, config>::Finalize()
{
                                             ;
    impl_.template Finalize<sync>();
}

template <HcclServerType serverType, const auto &config>
[aicore] inline __attribute__((cce_global)) uint8_t* Hccl<serverType, config>::GetWindowsInAddr(uint32_t rankId)
{
    return impl_.GetWindowsInAddr(rankId);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline __attribute__((cce_global)) uint8_t* Hccl<serverType, config>::GetWindowsOutAddr(uint32_t rankId)
{
    return impl_.GetWindowsOutAddr(rankId);
}

template <HcclServerType serverType, const auto &config>
[aicore] inline uint32_t Hccl<serverType, config>::GetRankId()
{
    return impl_.GetRankId();
}

template <HcclServerType serverType, const auto &config>
[aicore] inline uint32_t Hccl<serverType, config>::GetRankDim()
{
    return impl_.GetRankDim();
}

template <HcclServerType serverType, const auto &config>
[aicore] inline uint16_t Hccl<serverType, config>::GetQueueNum()
{
    return impl_.GetQueueNum();
}

template <HcclServerType serverType, const auto &config>
[aicore] inline bool Hccl<serverType, config>::SetReduceDataTypeAbility(HcclReduceOp op,
    HcclDataType dstDataType, HcclDataType srcDataType)
{
    return impl_.SetReduceDataTypeAbility(op, dstDataType, srcDataType);
}
}
# 415 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/hccl/hccl.h" 2
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/frac/frac_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/frac/frac_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/frac/frac_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/frac/frac_v220_impl.h"
namespace AscendC {
[aicore] inline void TruncCastForFrac(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/frac/frac_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void FracCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{

    TruncCastForFrac(dstTensor, srcTensor, dstTensor);

    Sub<T, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void FracCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    LocalTensor<float> srcTmpTensor = tmpTensor;
    LocalTensor<float> dstTmpTensor = tmpTensor[splitSize];


    Cast<float, half, false>(srcTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();

    TruncCastForFrac(dstTmpTensor, srcTmpTensor, dstTmpTensor);

    Sub<float, false>(dstTmpTensor, srcTmpTensor, dstTmpTensor, MASK_PLACEHOLDER, 1,
        { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dstTensor, dstTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    uint32_t round = 1;
    uint32_t tail = 0;
    constexpr uint8_t FRAC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(half)) {
        stackSize = tmpBufferSize / FRAC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        CheckTmpBufferSize(stackSize, 0, bufferSize);
        round = calCount / stackSize;
        tail = calCount % stackSize;
        SetMaskCount();
        SetVectorMask<half, MaskMode::COUNTER>(0, stackSize);
    } else {
        SetMaskCount();
        SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    }

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        FracCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void FracImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FracImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FracImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 79 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Frac<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 95 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/frac.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Frac(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    FracImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 48 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_int_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_int_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_v220_impl.h" 1
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_utils.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_utils.h"
namespace AscendC {

struct AscPowerFParams {
    [aicore] AscPowerFParams() {};
    LocalTensor<float> tmpTensor1;
    LocalTensor<float> tmpTensor2;
    LocalTensor<float> tmpTensor3;
    LocalTensor<float> tmpTensor4;

    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    LocalTensor<uint8_t> finiteIntegerYMask;
};


struct AscPowerIParams {
    [aicore] AscPowerIParams() {};
    float expIterateSum;

    LocalTensor<int32_t> expUBIterate;
    LocalTensor<int32_t> oriAbsExp;
    LocalTensor<int32_t> recordExpNode;
    LocalTensor<int32_t> tmpTensor1;
    LocalTensor<int32_t> tmpTensor2;
    LocalTensor<int32_t> tmpTensor3;

    LocalTensor<uint8_t> negMask;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpScalar;
};


[aicore] inline void VselPowerTensorScalar(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams &binaryParam, const uint32_t calCount)
{
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();

    Select<float, uint8_t>(dst, sel, src0, repeat, binaryParam);
}


[aicore] inline void VselPowerTensorTensor(const LocalTensor<float>& dst, const LocalTensor<uint8_t>& sel,
    const LocalTensor<float>& src0, const LocalTensor<float>& src1, const LocalTensor<float>& tmpScalar,
    SELMODE selMode, int32_t repeat, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{




    uint32_t selAddr = static_cast<uint32_t>(
        reinterpret_cast<int64_t>(reinterpret_cast<__attribute__((cce_unif_buff)) int64_t*>(sel.GetPhyAddr())));
    SetVectorMask<uint32_t>(0, 1);
    Duplicate<uint32_t, false>(tmpScalar.ReinterpretCast<uint32_t>(), selAddr, MASK_PLACEHOLDER, 1,
        DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, calCount);
    SetCmpMask<int64_t>(tmpScalar.ReinterpretCast<int64_t>());
    PipeBarrier<PIPE_V>();
    Select<float, SELMODE::VSEL_TENSOR_TENSOR_MODE>(dst, src0, src1, repeat, binaryParam);
}
}
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_v220_impl.h" 2

namespace AscendC {

constexpr uint32_t TENSOR_TENSOR_FLOAT = 4;
constexpr uint32_t TENSOR_TENSOR_INT = 6;
constexpr uint32_t TENSOR_TENSOR_HALF = 7;
constexpr uint32_t TENSOR_SCALAR_FLOAT = 5;
constexpr uint32_t TENSOR_SCALAR_INT = 7;
constexpr uint32_t TENSOR_SCALAR_HALF = 7;


[aicore] inline void PowerIParamsCalc(const LocalTensor<uint8_t>& tmpTensor, AscPowerIParams& param,
    uint32_t splitSize)
{
    param.expUBIterate = tmpTensor.ReinterpretCast<int32_t>();
    param.oriAbsExp = param.expUBIterate[splitSize];
    param.recordExpNode = param.oriAbsExp[splitSize];
    param.tmpTensor1 = param.recordExpNode[splitSize];
    param.tmpTensor2 = param.tmpTensor1[splitSize];
    param.negMask = param.tmpTensor2[splitSize].ReinterpretCast<uint8_t>();
    param.mask = param.negMask[splitSize];
    param.tmpScalar = param.mask[splitSize];
    param.expUBIterate.SetSize(splitSize);
    param.oriAbsExp.SetSize(splitSize);
    param.recordExpNode.SetSize(splitSize);
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.negMask.SetSize(splitSize);
    param.mask.SetSize(splitSize);
    param.tmpScalar.SetSize(ONE_BLK_SIZE);
}


[aicore] inline void PowerFParamsCalc(const LocalTensor<float>& tmpTensor,
    AscPowerFParams& param, uint32_t splitSize)
{
    param.tmpTensor1 = tmpTensor;
    param.tmpTensor2 = tmpTensor[splitSize];
    param.tmpTensor3 = param.tmpTensor2[splitSize];
    param.tmpMask1 = param.tmpTensor3[splitSize].ReinterpretCast<uint8_t>();
    param.tmpMask2 = param.tmpMask1[splitSize];
    param.tmpMask3 = param.tmpMask2[splitSize];
    param.finiteIntegerYMask = param.tmpMask3[splitSize];
    param.tmpTensor1.SetSize(splitSize);
    param.tmpTensor2.SetSize(splitSize);
    param.tmpTensor3.SetSize(splitSize);
    param.tmpMask1.SetSize(splitSize);
    param.tmpMask2.SetSize(splitSize);
    param.tmpMask3.SetSize(splitSize);
    param.finiteIntegerYMask.SetSize(splitSize);
}


[aicore] inline void CompareIntZero(const LocalTensor<uint8_t>& mask, const LocalTensor<int32_t>& intInput,
    const LocalTensor<int32_t>& tmpTensor, const UnaryRepeatParams& unaryParam, const uint8_t repeat)
{
    (void)(tmpTensor);
    CompareScalar<int32_t, uint8_t, false>(mask, intInput, static_cast<int32_t>(0), CMPMODE::EQ, MASK_PLACEHOLDER,
        repeat, unaryParam);
}


[aicore] inline void CastFloat2Float(const LocalTensor<float>& dst, const LocalTensor<float>& src, RoundMode mode,
    const UnaryRepeatParams& unaryParam)
{
    Cast<float, float, false>(dst, src, mode, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] inline void GrepSignBit(const LocalTensor<uint8_t>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor1, const LocalTensor<float>& tmpTensor2,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    constexpr uint32_t signBit = 31;
    const uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    ShiftRight<uint32_t, false>(tmpTensor1.ReinterpretCast<uint32_t>(), src.ReinterpretCast<uint32_t>(),
        signBit, MASK_PLACEHOLDER, 1, unaryParam, false);
    PipeBarrier<PIPE_V>();
    CompareScalar<int32_t, uint8_t, false>(dst, tmpTensor1.ReinterpretCast<int32_t>(),
        static_cast<int32_t>(1), CMPMODE::EQ, MASK_PLACEHOLDER, repeat, unaryParam);
}


[aicore] inline void ShiftRightOneBit(const LocalTensor<int32_t>& srcDst, const LocalTensor<int32_t>& tmpTensor,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    (void)(binaryParam);
    (void)(calCount);
    ShiftRight<int32_t, false>(srcDst, srcDst, 1, MASK_PLACEHOLDER, 1, unaryParam, false);
}


[aicore] inline void CompareZeroPositive(const LocalTensor<uint8_t>& dst, const LocalTensor<int32_t>& src,
    const UnaryRepeatParams& unaryParam, const uint8_t repeat)
{
    CompareScalar<int32_t, uint8_t, false>(dst, src, static_cast<int32_t>(0), CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
}

[aicore] inline void ReduceSumCount(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpTensor, const uint32_t calCount)
{
    ReduceSum<float, false>(dst, src, tmpTensor, calCount);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_int_impl.h" 2




namespace AscendC {




[aicore] inline void InitFinePowerI(AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Sub<int32_t, false>(param.recordExpNode, param.oriAbsExp, param.recordExpNode, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.recordExpNode, unaryParam, repeat);
}

[aicore] inline void FineProcessPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.recordExpNode,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();

    ReduceSumCount(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, src0, MASK_PLACEHOLDER, 1, binaryParam);
        Adds<int32_t, false>(param.tmpTensor2, param.recordExpNode, -1, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        PipeBarrier<PIPE_V>();
        CompareZeroPositive(param.mask, param.recordExpNode, unaryParam, repeat);
    }
}

[aicore] inline void BulkProcessPowerI(const LocalTensor<int32_t>& dst, AscPowerIParams& param,
    const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(param.tmpTensor1.ReinterpretCast<float>(), param.expUBIterate,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();

    ReduceSumCount(param.tmpScalar.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    param.expIterateSum = param.tmpScalar.ReinterpretCast<float>().GetValue(0);
    PipeBarrier<PIPE_V>();
    if (param.expIterateSum != 0) {
        Mul<int32_t, false>(param.tmpTensor1, dst, dst, MASK_PLACEHOLDER, 1, binaryParam);
        int32_t scalarValue = 2;
        Muls<int32_t, false>(param.tmpTensor2, param.recordExpNode, scalarValue, MASK_PLACEHOLDER, 1, unaryParam);
        PipeBarrier<PIPE_V>();
        VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, dst.ReinterpretCast<float>(),
            param.tmpTensor1.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(),
            SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
            param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
            param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
        ShiftRightOneBit(param.expUBIterate, param.tmpTensor3, unaryParam, binaryParam, calCount);
        PipeBarrier<PIPE_V>();
        CompareIntZero(param.mask, param.expUBIterate, param.tmpTensor3, unaryParam, repeat);
    }
}

[aicore] inline void InitBulkPowerI(AscPowerIParams& param, const LocalTensor<int32_t>& src0,
    const LocalTensor<int32_t>& src1, const LocalTensor<int32_t>& dst, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    Cast<float, int32_t, false>(
        param.tmpTensor1.ReinterpretCast<float>(), src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        static_cast<float>(0), CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.tmpTensor1, src1, -1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.expUBIterate.ReinterpretCast<float>(), param.negMask,
        param.tmpTensor1.ReinterpretCast<float>(), src1.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Muls<int32_t, false>(param.oriAbsExp, param.expUBIterate, 1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.oriAbsExp, unaryParam, repeat);
    Duplicate<int32_t, false>(param.recordExpNode, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(param.tmpTensor2, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(param.recordExpNode.ReinterpretCast<float>(), param.mask,
        param.recordExpNode.ReinterpretCast<float>(), param.tmpTensor2.ReinterpretCast<float>(),
        param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE, 1, binaryParam, calCount);
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        src0.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    ShiftRightOneBit(param.expUBIterate, param.tmpTensor3, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    CompareZeroPositive(param.mask, param.expUBIterate, unaryParam, repeat);
}

[aicore] inline void HandleNegativeExpPowerI(const LocalTensor<int32_t>& dst, const LocalTensor<int32_t>& src0,
    AscPowerIParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint8_t& repeat, const uint32_t calCount)
{
    CompareIntZero(param.mask, dst, param.tmpTensor3, unaryParam, repeat);
    LocalTensor<float> resF32 = param.oriAbsExp.ReinterpretCast<float>();
    LocalTensor<float> oneTensor = param.expUBIterate.ReinterpretCast<float>();
    Cast<float, int32_t, false>(resF32, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParam);
    Duplicate<float, false>(oneTensor, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(resF32, oneTensor, resF32, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int32_t, float, false>(param.tmpTensor1, resF32, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.negMask, param.tmpTensor1.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
    Duplicate<int32_t, false>(param.tmpTensor2, 0, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    VselPowerTensorTensor(dst.ReinterpretCast<float>(), param.mask, param.tmpTensor2.ReinterpretCast<float>(),
        dst.ReinterpretCast<float>(), param.tmpScalar.ReinterpretCast<float>(), SELMODE::VSEL_TENSOR_TENSOR_MODE,
        1, binaryParam, calCount);
}

[aicore] inline void CommonPowerI(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, AscPowerIParams& param, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;
    const uint8_t repeat = DivCeil(calCount * sizeof(int32_t), ONE_REPEAT_BYTE_SIZE);

    PipeBarrier<PIPE_V>();
    InitBulkPowerI(param, srcTensor0, srcTensor1, dstTensor, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    param.expIterateSum = 1;
    do {
        BulkProcessPowerI(dstTensor, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    InitFinePowerI(param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
    do {
        FineProcessPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
        PipeBarrier<PIPE_V>();
    } while (param.expIterateSum != 0);
    HandleNegativeExpPowerI(dstTensor, srcTensor0, param, unaryParam, binaryParam, repeat, calCount);
    PipeBarrier<PIPE_V>();
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_float_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_float_impl.h"
namespace AscendC {





[aicore] inline void InitTmpScalar(const LocalTensor<float>& tmpScalar)
{
    NotNumUnion notNum;
    notNum.i = F32_NAN;
    SetVectorMask<float>(0, ONE_BLK_SIZE / sizeof(float));
    Duplicate<float, false>(tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    Duplicate<float, false>(
        tmpScalar[ONE_BLK_SIZE / sizeof(float)], notNum.f, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
}


[aicore] inline void InitDst(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(dst, src0, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParam);
}


[aicore] inline void DetermineSign(const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam)
{
    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor1, 0.5f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor2, param.tmpTensor2, RoundMode::CAST_FLOOR, unaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor2, param.tmpTensor2, 2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor1, param.tmpTensor1, param.tmpTensor2, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(param.tmpTensor1, param.tmpTensor1, -2.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(param.tmpTensor1, param.tmpTensor1, 1.0f, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor1, param.tmpTensor1, RoundMode::CAST_RINT, unaryParam);
}


[aicore] inline void GenMaskForOne(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, static_cast<float>(0), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(-1), CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    notNum.i = F32_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, param.tmpTensor1, notNum.f, CMPMODE::NE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] inline void GenMaskForNan(const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const AscPowerFParams& param,
    const UnaryRepeatParams& unaryParam, const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    NotNumUnion notNum;
    notNum.i = F32_INF;
    uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    Abs<float, false>(param.tmpTensor1, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.tmpMask1, param.tmpTensor1, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
    CompareScalar<float, uint8_t, false>(param.tmpMask2, src0, static_cast<float>(0), CMPMODE::GE,
        MASK_PLACEHOLDER, repeat, unaryParam);
    notNum.i = F32_NEG_INF;
    CompareScalar<float, uint8_t, false>(param.tmpMask3, src0, notNum.f, CMPMODE::EQ,
        MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Or<uint16_t, false>(param.tmpMask2.ReinterpretCast<uint16_t>(), param.tmpMask3.ReinterpretCast<uint16_t>(),
        param.tmpMask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Or<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(), param.tmpMask2.ReinterpretCast<uint16_t>(),
        param.tmpMask1.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}


[aicore] inline void GenMaskForSign(const LocalTensor<float>& src0, const LocalTensor<float>& src1,
    const AscPowerFParams& param, const UnaryRepeatParams& unaryParam,
    const BinaryRepeatParams& binaryParam, const uint32_t calCount)
{
    constexpr float intThreshold = 0.00000001f;
    const uint8_t repeat = DivCeil(calCount * sizeof(float), ONE_REPEAT_BYTE_SIZE);

    GrepSignBit(param.tmpMask1, src0, param.tmpTensor2, param.tmpTensor4, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor2, src1, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CastFloat2Float(param.tmpTensor3, param.tmpTensor2, RoundMode::CAST_RINT, unaryParam);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(param.tmpTensor3, param.tmpTensor2, param.tmpTensor3, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(param.tmpTensor3, param.tmpTensor3, MASK_PLACEHOLDER, 1, unaryParam);
    PipeBarrier<PIPE_V>();
    CompareScalar<float, uint8_t, false>(param.finiteIntegerYMask, param.tmpTensor3, static_cast<float>(intThreshold),
        CMPMODE::LT, MASK_PLACEHOLDER, repeat, unaryParam);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(calCount, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(param.tmpMask1.ReinterpretCast<uint16_t>(),
        param.finiteIntegerYMask.ReinterpretCast<uint16_t>(), param.tmpMask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, binaryParam);
    SetVectorMask<float>(0, calCount);
}

[aicore] inline void CommonPowerF(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<float>& tmpScalar,
    const AscPowerFParams& powerParam, const uint32_t calCount)
{
    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;

    PipeBarrier<PIPE_V>();
    InitDst(dstTensor, srcTensor0, srcTensor1, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    DetermineSign(srcTensor1, powerParam, unaryParam, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForSign(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(powerParam.tmpTensor2, powerParam.tmpMask1, powerParam.tmpTensor1, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, powerParam.tmpTensor2, dstTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    GenMaskForOne(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar,
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    GenMaskForNan(srcTensor0, srcTensor1, powerParam, unaryParam, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
    VselPowerTensorScalar(dstTensor, powerParam.tmpMask1, dstTensor, tmpScalar[ONE_BLK_SIZE / sizeof(float)],
        SELMODE::VSEL_TENSOR_SCALAR_MODE, 1, binaryParam, calCount);
    PipeBarrier<PIPE_V>();
}

}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/power/power_common_impl.h" 2



namespace AscendC {

[aicore] inline void PowerImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor0,
    const LocalTensor<half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc1.SetSize(splitSize);
    stackSrc0.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_TENSOR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>(), powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            srcTensor1[i * splitSize], tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], tmpScalar, powerParam, calcTail);
    }
}


[aicore] inline void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_TENSOR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor, powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], srcTensor1[i * splitSize],
            powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        PipeBarrier<PIPE_V>();
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            srcTensor1[loopCount * splitSize], powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void PowerImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor0,
    const half& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) / TENSOR_SCALAR_HALF /
        ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackDst.SetSize(splitSize);
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    Duplicate<float, false>(stackSrc1, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc0, srcTensor0[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc0, srcTensor0[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void PowerImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const float& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);
    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc1 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc1.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], srcTensor0[i * splitSize],
            stackSrc1, tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize],
            stackSrc1, tmpScalar, powerParam, calcTail);
    }
}


[aicore] inline void PowerImpl(const LocalTensor<int32_t>& dstTensor, const LocalTensor<int32_t>& srcTensor0,
    const int32_t& scalarValue, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<int32_t> stackSrc1 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc1.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc1, scalarValue, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], srcTensor0[i * splitSize], stackSrc1,
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], srcTensor0[loopCount * splitSize], stackSrc1,
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void PowerImpl(const LocalTensor<half>& dstTensor, const half& scalarValue,
    const LocalTensor<half>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    constexpr uint32_t tripleFactor = 3;
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_HALF / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    LocalTensor<float> stackSrc1 = stackSrc0[splitSize];
    LocalTensor<float> stackDst = stackSrc1[splitSize];
    stackSrc0.SetSize(splitSize);
    stackSrc1.SetSize(splitSize);
    stackDst.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(
        stackTensor[ONE_REPEAT_BYTE_SIZE + tripleFactor * splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<half>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, static_cast<float>(scalarValue), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    struct UnaryRepeatParams fp162fp32Param(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
    struct UnaryRepeatParams fp322fp16Param(1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE);
    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(
            stackSrc1, srcTensor1[i * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, splitSize);
        Cast<half, float, false>(
            dstTensor[i * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        Cast<float, half, false>(
            stackSrc1, srcTensor1[loopCount * splitSize], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp162fp32Param);
        PipeBarrier<PIPE_V>();
        CommonPowerF(stackDst, stackSrc0, stackSrc1, tmpScalar, powerParam, calcTail);
        Cast<half, float, false>(
            dstTensor[loopCount * splitSize], stackDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, fp322fp16Param);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void PowerImpl(const LocalTensor<float>& dstTensor, const float& scalarValue,
    const LocalTensor<float>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    CheckTmpBufferSize(tmpBufferSize, ONE_REPEAT_BYTE_SIZE, tmpBufferSize);

    uint32_t splitSize = (tmpBufferSize - ONE_REPEAT_BYTE_SIZE) / sizeof(float) /
        TENSOR_SCALAR_FLOAT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<float> tmpScalar = stackTensor.ReinterpretCast<float>();
    tmpScalar.SetSize(ONE_REPEAT_BYTE_SIZE / sizeof(float));
    LocalTensor<float> stackSrc0 = stackTensor[ONE_REPEAT_BYTE_SIZE].ReinterpretCast<float>();
    stackSrc0.SetSize(splitSize);
    AscPowerFParams powerParam;
    PowerFParamsCalc(stackTensor[ONE_REPEAT_BYTE_SIZE + splitSize * sizeof(float)].ReinterpretCast<float>(),
        powerParam, splitSize);
    InitTmpScalar(tmpScalar);
    SetVectorMask<float>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerF(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            tmpScalar, powerParam, splitSize);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        CommonPowerF(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            tmpScalar, powerParam, calcTail);
    }
}


[aicore] inline void PowerImpl(const LocalTensor<int32_t>& dstTensor, const int32_t& scalarValue,
    const LocalTensor<int32_t>& srcTensor1, const LocalTensor<uint8_t>& stackTensor, uint32_t calCount)
{
    uint32_t tmpBufferSize = stackTensor.GetSize();
    uint32_t splitSize = tmpBufferSize / sizeof(int32_t) /
        TENSOR_SCALAR_INT / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);

    LocalTensor<int32_t> stackSrc0 = stackTensor.ReinterpretCast<int32_t>();
    stackSrc0.SetSize(splitSize);
    AscPowerIParams powerParam;
    PowerIParamsCalc(stackTensor[splitSize * sizeof(int32_t)], powerParam, splitSize);
    SetVectorMask<int32_t>(0, splitSize);
    PipeBarrier<PIPE_V>();
    Duplicate<int32_t, false>(stackSrc0, scalarValue, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        CommonPowerI(dstTensor[i * splitSize], stackSrc0, srcTensor1[i * splitSize],
            powerParam, splitSize);
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<int32_t>(0, calcTail);
        CommonPowerI(dstTensor[loopCount * splitSize], stackSrc0, srcTensor1[loopCount * splitSize],
            powerParam, calcTail);
        PipeBarrier<PIPE_V>();
    }
}





template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& scalarValue,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                        ;

    SetMaskCount();
    PowerImpl(dstTensor, scalarValue, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const T& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                        ;
    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, scalarValue, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}





template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                       ;
    SetMaskCount();
    PowerImpl(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, stackTensor, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, stackTensor, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void PowerCommonImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;

    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, stackTensor, calCount);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, calCount);
}
# 81 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, src0Tensor.GetSize());
}
# 121 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor, const T& src1Scalar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, calCount);
}
# 139 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, calCount);
}
# 161 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, sharedTmpBuffer, src0Tensor.GetSize());
}
# 178 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const T& src1Scalar)
{
    Power<T, isReuseSource>(dstTensor, src0Tensor, src1Scalar, src0Tensor.GetSize());
}
# 201 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, calCount);
}
# 219 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar,
    const LocalTensor<T>& src1Tensor, uint32_t calCount)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, calCount);
}
# 241 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, sharedTmpBuffer, src1Tensor.GetSize());
}
# 258 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/power.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Power(const LocalTensor<T>& dstTensor, const T& src0Scalar, const LocalTensor<T>& src1Tensor)
{
    PowerCommonImpl<T, isReuseSource>(dstTensor, src0Scalar, src1Tensor, src1Tensor.GetSize());
}
#pragma end_pipe
}
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/log/log_common_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/log/log_common_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    const T Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;
    Ln<float, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, Ln2Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
}

template <typename T>
[aicore] inline void Log2Compute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    const float Ln2Reciprocal = 1.4426950408889634;
    const UnaryRepeatParams unaryParams;


    Cast<float, T, false>(tmpTensor.ReinterpretCast<float>(), srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpTensor.ReinterpretCast<float>(),
        tmpTensor.ReinterpretCast<float>(),
        static_cast<float>(Ln2Reciprocal),
        MASK_PLACEHOLDER,
        1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<T, float, false>(dstTensor, tmpTensor.ReinterpretCast<float>(),
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LogImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{

                                                                                       ;

    const UnaryRepeatParams unaryParams;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void Log2Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{
    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(float)) {
                                                                                            ;
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Log2Compute(dstTensor, srcTensor);
    } else {
                                                                                                             ;
        uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
        uint32_t splitSize = tmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
        CheckTmpBufferSize(splitSize, 0, tmpBufferSize);
        uint32_t loopCount = calCount / splitSize;
        uint32_t calcTail = calCount % splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            Log2Compute(dstTensor[i * splitSize], srcTensor[i * splitSize], sharedTmpBuffer);
        }
        if (calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            Log2Compute(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize], sharedTmpBuffer);
        }
    }
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void Log10Impl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{

                                                                                         ;

    const T Ln10Reciprocal = 0.43429448190325176;
    const UnaryRepeatParams unaryParams;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
    Ln<T, false>(dstTensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dstTensor, dstTensor, Ln10Reciprocal, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(FULL_MASK, FULL_MASK);
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h" 2



namespace AscendC {

#pragma begin_pipe(V)







template <typename T, bool isReuseSource = false>
[aicore] inline void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LogImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Log(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log2Impl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 85 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;

    if constexpr (std::is_same<T, half>::value) {
        bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                       ;
    }

    Log2<T, isReuseSource>(dstTensor, srcTensor, stackTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Log2(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log2<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 133 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/log.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    Log10Impl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Log10(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Log10<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}

#pragma end_pipe
}
# 50 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h" 1
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_v220_impl.h"
namespace AscendC {
[aicore] inline void SinCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_common_impl.h" 2




namespace AscendC {
const uint8_t SIN_HALF_CALC_PROCEDURE = 4;
const uint8_t SIN_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t SIN_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float SIN_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float SIN_PI_V2 = 3.140625;
constexpr float SIN_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float SIN_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float SIN_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float SIN_RES_MULIT_SCA = 2.604926501e-6;
constexpr float SIN_RES_ADDICT_UP = -0.0001980894471;
constexpr float SIN_2ADDS = 0.008333049340;
constexpr float SIN_3ADDS = -0.1666665792;
constexpr float SIN_POINT_FIVE = 0.5;
constexpr float SIN_M4_SCA = 4.0;
constexpr float SIN_K2_SCA = -2.0;

[aicore] inline void SinSignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, SIN_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);

    Muls<float, false>(dstTensor, dstTensor, SIN_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, SIN_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void SinPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SinSignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, SIN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, SIN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void SinKpi(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<float, false>(kpi, roundTensor, SIN_PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, SIN_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void SinRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
                                const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 159 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sin/sin_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, SIN_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    SinCast(roundTensor, roundTensor, RoundMode::CAST_ROUND);
    SinKpi(inputX, srcTensor, roundTensor, kpi);
}

template <typename T>
[aicore] inline void SinCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    const BinaryRepeatParams binParams;
    LocalTensor<T> roundTensor = tmpTensor;
    LocalTensor<T> kpi = tmpTensor[splitSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = tmpTensor[splitSize * 2];
    }
    SinRound(inputX, srcTensor, roundTensor, kpi);
    SinPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template <>
[aicore] inline void SinCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize, bool isReuseSource)
{
    (void)isReuseSource;
    const BinaryRepeatParams binParams;
    const LocalTensor<float>& tmpBuffer = tmpTensor;
    const LocalTensor<float>& roundTensor = tmpBuffer[splitSize];
    const LocalTensor<float>& kpi = roundTensor[splitSize];
    const LocalTensor<float>& inputX = kpi[splitSize];

    Cast<float, half, false>(tmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    SinRound(inputX, tmpBuffer, roundTensor, kpi);
    SinPolynomialApproximation(tmpBuffer, inputX, roundTensor, kpi);

    Cast<half, float, false>(dstTensor, tmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}


template <typename T, bool isReuseSource = false>
[aicore] inline void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                        ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        stackSize = tmpBufferSize / SIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            stackSize = tmpBufferSize / SIN_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            stackSize = tmpBufferSize / SIN_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }
    CheckTmpBufferSize(stackSize, 0, bufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        SinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, stackSize, isReuseSource);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 115 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 51 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h" 1
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_v220_impl.h"
namespace AscendC {
[aicore] inline void CosCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_common_impl.h" 2




namespace AscendC {
const uint8_t COS_HALF_CALC_PROCEDURE = 4;
const uint8_t COS_FLOAT_NOREUSE_CALC_PROCEDURE = 3;
const uint8_t COS_FLOAT_REUSE_CALC_PROCEDURE = 2;


constexpr float COS_PI_FOR_X_TODIV = 0.3183098733425140380859375;

constexpr float PI_0 = 3.140625;
constexpr float COS_KPI_FIRS_PI_MULS = 0.0009670257568359375;
constexpr float COS_KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float COS_KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;
constexpr float COS_KPI_FOR_PI_MULS = -1.0290623200529979163359041220560e-13;

constexpr float COS_PI_DOWN = 1.57079637050628662109375;

constexpr float COS_PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float COS_RES_MULIT_SCA = 2.604926501e-6;
constexpr float COS_RES_ADDICT_UP = -0.0001980894471;
constexpr float COS_2ADDS = 0.008333049340;
constexpr float COS_3ADDS = -0.1666665792;
constexpr float COS_POINT_FIVE = 0.5;
constexpr float COS_M4_SCA = 4.0;
constexpr float COS_K2_SCA = -2.0;

[aicore] inline void KPI(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<float, false>(kpi, roundTensor, PI_0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, srcTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(kpi, roundTensor, COS_KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(inputX, inputX, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(inputX, inputX, COS_PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void CosRound(const LocalTensor<float>& inputX, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
# 114 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cos/cos_common_impl.h"
    const UnaryRepeatParams unaryParams;
    Muls<float, false>(roundTensor, srcTensor, COS_PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(roundTensor, roundTensor, RoundMode::CAST_RINT);
    KPI(inputX, srcTensor, roundTensor, kpi);
}

[aicore] inline void SignCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(kpi, inputX, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, COS_POINT_FIVE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    CosCast(dstTensor, dstTensor, RoundMode::CAST_FLOOR);


    Muls<float, false>(dstTensor, dstTensor, COS_M4_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(roundTensor, roundTensor, COS_K2_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void CosPolynomialApproximation(const LocalTensor<float>& dstTensor, const LocalTensor<float>& inputX,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& kpi)
{





    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SignCompute(dstTensor, inputX, roundTensor, kpi);


    Muls<float, false>(roundTensor, kpi, COS_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, COS_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, kpi, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(roundTensor, roundTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, inputX, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, roundTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mins<float, false>(dstTensor, dstTensor, 1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dstTensor, dstTensor, -1.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void CosCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    const LocalTensor<T>& roundTensor = tmpBuffer;
    const LocalTensor<T>& kpi = roundTensor[calSize];
    LocalTensor<T> inputX = srcTensor;
    if (!isReuseSource) {
        inputX = roundTensor[calSize * 2];
    }

    CosRound(inputX, srcTensor, roundTensor, kpi);
    CosPolynomialApproximation(dstTensor, inputX, roundTensor, kpi);
}

template <>
[aicore] inline void CosCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpBuffer, const uint32_t calSize, bool isReuseSource)
{
    (void)isReuseSource;
    const LocalTensor<float>& tempTensorConv = tmpBuffer;
    const LocalTensor<float>& roundTensor = tempTensorConv[calSize];
    const LocalTensor<float>& kpi = roundTensor[calSize];
    const LocalTensor<float>& inputX = kpi[calSize];

    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    CosRound(inputX, tempTensorConv, roundTensor, kpi);
    CosPolynomialApproximation(tempTensorConv, inputX, roundTensor, kpi);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                        ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / COS_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        if constexpr (isReuseSource) {
            calSize = tmpBufferSize / COS_FLOAT_REUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            calSize = tmpBufferSize / COS_FLOAT_NOREUSE_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
        offset = offset + calSize;
    }

    if (tail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize, isReuseSource);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void CosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 85 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 119 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    CosImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/asin_common_impl.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/asin_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/../math_constant_util.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/../math_constant_util.h"
namespace AscendC {

constexpr float NUM_ONE = 1.0;
constexpr float NEG_ONE = -1.0;
constexpr float HALF_PI = 1.5707963267948966192313216916398;
constexpr float BOUNDARY = 0.70710678118654752440084436210485;

constexpr uint8_t ASIN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ASIN_FLOAT_CALC_PROCEDURE = 4;
constexpr uint32_t ASIN_TAYLOR_EXPAND_COUNT = 7;

constexpr float kCOEF[] = {
    1.0,
    0.16666666666666666666666666666667,
    0.075,
    0.04464285714285714285714285714286,
    0.03038194444444444444444444444444,
    0.02237215909090909090909090909091,
    0.01735276442307692307692307692308,
    0.01396484375,
};
}
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/asin_common_impl.h" 2



namespace AscendC {




template <typename T>
[aicore] inline void GetSign(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float FP16_MAX = 32768;
    constexpr float FP16_MIN = 3.0517578125e-05;
    constexpr float FP32_MAX = 4611686018427387904;
    constexpr float FP32_MIN = 2.168404344971009e-19;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? FP32_MAX : FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? FP32_MIN : FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] inline void AsinTaylorCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& localTemp)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Mul<T, false>(dst, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<T, false>(localTemp, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, localTemp, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] inline void AsinTaylorComputeBySquareValue(const LocalTensor<T>& dst, const LocalTensor<T>& src)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Muls<T, false>(dst, src, static_cast<T>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(kCOEF[ASIN_TAYLOR_EXPAND_COUNT]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = ASIN_TAYLOR_EXPAND_COUNT - 1; i > 0; i--) {

        Adds<T, false>(dst, dst, static_cast<T>(kCOEF[i]), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<T, false>(dst, dst, static_cast<T>(kCOEF[0]), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(src, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] inline void AsinFp16Compute(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& stackBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    const LocalTensor<float>& tmpFloatBuffer1 = stackBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const uint32_t tmpHalfBuffer1Offset = calSize * 4;
    const uint32_t tmpHalfBuffer2Offset = calSize * 5;
    const LocalTensor<half>& tmpHalfBuffer1 = stackBuffer[tmpHalfBuffer1Offset];
    const LocalTensor<half>& tmpHalfBuffer2 = stackBuffer[tmpHalfBuffer2Offset];



    Cast<float, half, false>(tmpFloatBuffer2, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(tmpFloatBuffer1, tmpFloatBuffer2);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Abs<half, false>(tmpHalfBuffer2, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(dst, tmpHalfBuffer2, tmpHalfBuffer1);
    PipeBarrier<PIPE_V>();
# 149 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/asin_common_impl.h"
    Mins<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(-BOUNDARY), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const LocalTensor<int8_t>& tmpS8Buffer = tmpHalfBuffer1.ReinterpretCast<int8_t>();
    Cast<int8_t, half, false>(tmpS8Buffer, tmpHalfBuffer2, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Cast<half, int8_t, false>(tmpHalfBuffer2, tmpS8Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<half, false>(dst, dst, tmpHalfBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(tmpHalfBuffer2, tmpHalfBuffer2, static_cast<half>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer2, tmpHalfBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer2, dst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    GetSign(tmpHalfBuffer1, src, tmpHalfBuffer2);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(tmpFloatBuffer2, tmpHalfBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}






template <typename T>
[aicore] inline void AsinCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpBuffer2 = tmpBuffer[calSize];
    const LocalTensor<T>& dupBuffer = tmpBuffer[calSize * 2];

    Mul<T, false>(tmpBuffer2, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer2, tmpBuffer2, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Sqrt<T, false>(dst, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorCompute(tmpBuffer2, dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer2, tmpBuffer2, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpBuffer2, tmpBuffer2, HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpBuffer, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    AsinTaylorComputeBySquareValue(dst, tmpBuffer);
    PipeBarrier<PIPE_V>();
# 249 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asin/asin_common_impl.h"
    Mins<T, false>(tmpBuffer, tmpBuffer, BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, -BOUNDARY, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LocalTensor<int32_t> tmpS32Buffer = tmpBuffer.template ReinterpretCast<int32_t>();
    Cast<int32_t, T, false>(tmpS32Buffer, tmpBuffer, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<T, int32_t, false>(tmpBuffer, tmpS32Buffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    PipeBarrier<PIPE_V>();
    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tmpBuffer, tmpBuffer, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(tmpBuffer, tmpBuffer, NUM_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpBuffer2, tmpBuffer2, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();



    GetSign(tmpBuffer2, src, tmpBuffer);
    PipeBarrier<PIPE_V>();
    Mul<T, false>(dst, dst, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void AsinCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{


    AsinFp16Compute(dst, src, tmpBuffer, calSize);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(float)) {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AsinCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AsinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 79 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AsinImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 101 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asin.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asin<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h" 1
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/acos_common_impl.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/acos_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/../math_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/../math_common_impl.h"
namespace AscendC {
namespace Internal {

template <typename T>
[aicore] inline void CommonCheckInputsValidness(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                        const uint32_t calCount) {
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/../math_common_impl.h"
}
}
}
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acos/acos_common_impl.h" 2




namespace AscendC {


template <typename T>
[aicore] inline void AcosCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    AsinCompute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(dst, dst, static_cast<T>(-HALF_PI), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<T, false>(dst, dst, static_cast<T>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
}

template <>
[aicore] inline void AcosCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;

    AsinFp16Compute(dst, src, tmpBuffer, calSize);
    PipeBarrier<PIPE_V>();



    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, -HALF_PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(dst, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();



    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ASIN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ASIN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AcosCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcosImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AcosImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 64 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AcosImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 81 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acos.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acos(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Acos<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 54 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asinh/asinh_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/asinh/asinh_common_impl.h"
namespace AscendC {
constexpr uint32_t ASINH_HALF_CALC_PROC = 3;
constexpr uint32_t ASINH_FLOAT_CALC_PROC = 3;
constexpr float ASINH_ONE = 1;
constexpr float ASINH_FP16_MAX = 32768;
constexpr float ASINH_FP16_MIN = 3.0517578125e-05;
constexpr float ASINH_FP32_MAX = 4611686018427387904;
constexpr float ASINH_FP32_MIN = 2.168404344971009e-19;
constexpr uint32_t ASINH_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] inline void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AsinhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AsinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                          ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float);
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ASINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    } else {
        splitCount = splitCount / ASINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AsinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AsinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T> [aicore] inline void AsinhGetSign(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ASINH_FP32_MAX : ASINH_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ASINH_FP32_MIN : ASINH_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void AsinhCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;





    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];


    Abs<T, false>(tmpFloatBuffer1, src, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<T>(ASINH_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, src, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dst, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void AsinhCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer3 = tmpFloatBuffer1[calCount];
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount * 2];





    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Abs<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<half>(ASINH_ONE), MASK_PLACEHOLDER,
        1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    AsinhGetSign(tmpFloatBuffer1, tmpFloatBuffer1, tmpFloatBuffer3);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / ASINH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 48 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Asinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/asinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Asinh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AsinhImpl<T, isReuseSource>(dstTensor, srcTensor);
}
#pragma end_pipe
}
# 55 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acosh.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acosh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acosh/acosh_common_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/acosh/acosh_common_impl.h"
namespace AscendC {
constexpr uint32_t ACOSH_HALF_CALC_PROC = 2;
constexpr uint32_t ACOSH_FLOAT_CALC_PROC = 1;
constexpr float ACOSH_NEG_ONE = -1;
constexpr uint32_t ACOSH_STRIDE_DIGITS = 2;

template <typename T>
[aicore] inline void AcoshCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;





    Mul<T, false>(tmpFloatBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(ACOSH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tmpFloatBuffer1, src, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<T, false>(dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void AcoshCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<float> &tmpBuffer, uint32_t calCount)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpFloatBuffer1[calCount];






    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<half>(ACOSH_NEG_ONE),
        MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE / ACOSH_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                          ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(float);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ACOSH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ACOSH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AcoshCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AcoshCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcoshImpl(const LocalTensor<T> &dstTensor,
 const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AcoshImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acosh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}






template <typename T, bool isReuseSource = false>
[aicore] inline void Acosh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor);
}
# 85 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/acosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Acosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AcoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/atan/atan_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/atan/atan_common_impl.h"
namespace AscendC {
constexpr uint8_t ATAN_HALF_CALC_PROCEDURE = 6;
constexpr uint8_t ATAN_FLOAT_CALC_PROCEDURE = 5;
constexpr float ATAN_FP16_MAX = 32768;
constexpr float ATAN_FP16_MIN = 3.0517578125e-05;
constexpr float ATAN_FP32_MAX = 4611686018427387904;
constexpr float ATAN_FP32_MIN = 2.168404344971009e-19;
constexpr uint8_t TAYLOR_COUNT_FOUR = 4;
constexpr uint8_t TAYLOR_COUNT_SIX = 6;
constexpr float MIN_INPUT_VALUE = -10000;
constexpr float MAX_INPUT_VALUE = 10000;





template <typename T>
[aicore] inline void Sign(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& denominator)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    constexpr float kFpMax = sizeof(T) == sizeof(float) ? ATAN_FP32_MAX : ATAN_FP16_MAX;
    constexpr float kFpMin = sizeof(T) == sizeof(float) ? ATAN_FP32_MIN : ATAN_FP16_MIN;
    Muls<T, false>(dst, src, static_cast<T>(kFpMax), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Abs<T, false>(denominator, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<T, false>(denominator, denominator, static_cast<T>(kFpMin), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<T, false>(dst, dst, denominator, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void TaylorExpand(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& squareTensor, int32_t expandLevel)
{


    constexpr float factorList[7] = {1, -0.3333333333333333, 0.2, -0.14285714285714285,
        0.1111111111111111, - 0.09090909090909091, 0.07692307692307693};
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Mul<float, false>(squareTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    Mul<float, false>(dstTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(dstTensor, dstTensor, factorList[expandLevel], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    for (uint32_t i = expandLevel - 1; i > 0; --i) {

        Adds<float, false>(dstTensor, dstTensor, factorList[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Mul<float, false>(dstTensor, dstTensor, squareTensor, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Adds<float, false>(dstTensor, dstTensor, factorList[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binaryParams);
}

[aicore] inline void AtanTransform(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const float transFactor)
{

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float transFactorNeg = 0 - transFactor;


    Muls<float, false>(dstTensor, srcTensor, transFactor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);

    Adds<float, false>(tmpTensor, srcTensor, transFactorNeg, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}





[aicore] inline void AtanFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;
    const float piByFour = 0.78539816339744830961566084581988;
    const float piByEight = 0.39269908169872415480783042290994;
    const float tanPiByEight = 0.4142135623730950;
    LocalTensor<float> clipTensor = tmpTensor[splitSize];
    LocalTensor<float> absTensor = clipTensor[splitSize];
    LocalTensor<float> tmpTensor2 = absTensor[splitSize];
    LocalTensor<float> squareTensor = tmpTensor2[splitSize];




    Mins<float, false>(clipTensor, srcTensor, MAX_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(clipTensor, clipTensor, MIN_INPUT_VALUE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(absTensor, clipTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(dstTensor, absTensor, squareTensor, TAYLOR_COUNT_FOUR);



    AtanTransform(tmpTensor, absTensor, tmpTensor2, tanPiByEight);
    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor2, tmpTensor2, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);




    Adds<float, false>(tmpTensor2, absTensor, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    Adds<float, false>(tmpTensor, absTensor, -static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpTensor, tmpTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
    Abs<float, false>(tmpTensor, tmpTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    TaylorExpand(tmpTensor2, tmpTensor, squareTensor, TAYLOR_COUNT_FOUR);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor2, tmpTensor2, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor2, MASK_PLACEHOLDER, 1, binParams);


    AtanTransform(tmpTensor2, tmpTensor, squareTensor, tanPiByEight);
    TaylorExpand(tmpTensor, tmpTensor2, squareTensor, TAYLOR_COUNT_SIX);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpTensor, tmpTensor, piByEight, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmpTensor, tmpTensor, piByFour, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Min<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sign(tmpTensor, clipTensor, tmpTensor2);


    Mul<float, false>(dstTensor, dstTensor, tmpTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void AtanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    AtanFormulaImpl(dstTensor, srcTensor, tmpTensor, splitSize);
}

template <>
[aicore] inline void AtanCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& tempTensorConv = tmpTensor[splitSize * 5];
    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    AtanFormulaImpl(tempTensorConv, tempTensorConv, tmpTensor, splitSize);
    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ATAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ATAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AtanCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AtanImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AtanImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 86 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Atan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 103 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    AtanImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 57 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cosh/cosh_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cosh/cosh_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cosh/cosh_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cosh/cosh_v220_impl.h"
namespace AscendC {
[aicore] inline void CoshCast(const LocalTensor<half>& dst, const LocalTensor<float>& src)
{
    Cast<half, float, false>(dst, src, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cosh/cosh_common_impl.h" 2




namespace AscendC {
constexpr float SCALAR_LN2 = -0.69314718055994530941723212145818;
constexpr float SCALAR_BROAD_CAST = 0.25;
const uint8_t COSH_HALF_CALC_PROCEDURE = 6;
const uint8_t COSH_FLOAT_CALC_PROCEDURE = 2;



template <typename T>
[aicore] inline void CoshCompute(
    const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<T> tmpBuffer2 = tmpBuffer[calSize];

    Adds<T, false>(tmpBuffer2, src, static_cast<T>(SCALAR_LN2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<T, false>(tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<T, false>(dst, static_cast<T>(SCALAR_BROAD_CAST), MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<T, false>(tmpBuffer2, dst, tmpBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<T, false>(dst, tmpBuffer, tmpBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void CoshCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[calSize];
    const LocalTensor<float>& tmpFloatBuffer3 = tmpFloatBuffer2[calSize];

    Cast<float, half, false>(tmpFloatBuffer3, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, SCALAR_LN2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(tmpFloatBuffer3, SCALAR_BROAD_CAST, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer3, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmpFloatBuffer3, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    CoshCast(dst, tmpFloatBuffer3);
}


template <typename T, bool isReuseSource = false>
[aicore] inline void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    const uint32_t bufferSize = sharedTmpBuffer.GetSize();
    const uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / COSH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / COSH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);


    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        CoshCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void CoshImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    CoshImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Cosh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 97 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cosh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Cosh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    CoshImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/erf/erf_common_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/erf/erf_common_impl.h"
namespace AscendC {
[aicore] inline constexpr RoundMode GetErfCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] inline void ErfClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    constexpr float ERF_BOUNDARY_MAX = 3.92;
    UnaryRepeatParams unaryParams;

    Mins<float, false>(tmpBuffer, src, static_cast<float>(ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(dst, tmpBuffer, static_cast<float>(-ERF_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] inline void ErfComputeP(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_P0 = 0.29639384698e5;
    constexpr float SCALAR_P1 = 0.50637915060e4;
    constexpr float SCALAR_P2 = 0.13938061484e4;
    constexpr float SCALAR_P3 = 0.10162808918e3;
    constexpr float SCALAR_P4 = 0.75517016694e1;
    constexpr float SCALAR_P5 = 0.053443748819;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];

    Mul<float, false>(tmpBuffer1, src, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpBuffer2, tmpBuffer1, static_cast<float>(SCALAR_P5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, tmpBuffer2, static_cast<float>(SCALAR_P0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpBuffer2, src, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void ErfComputeQ(const LocalTensor<float>& tmpBuffer, const LocalTensor<float>& src,
    const uint32_t calSize)
{
    constexpr float SCALAR_Q0 = 0.26267224157e5;
    constexpr float SCALAR_Q1 = 0.13243365831e5;
    constexpr float SCALAR_Q2 = 0.30231248150e4;
    constexpr float SCALAR_Q3 = 0.39856963806e3;
    constexpr float SCALAR_Q4 = 0.31212858877e2;

    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer;
    LocalTensor<float> tmpBuffer3 = tmpBuffer1[calSize * 2];

    Adds<float, false>(tmpBuffer3, tmpBuffer1, static_cast<float>(SCALAR_Q4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(src, tmpBuffer1, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpBuffer3, src, static_cast<float>(SCALAR_Q0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void ErfCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<T>& tmpBuffer,
    const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<T> tmpBuffer1 = tmpBuffer;
    LocalTensor<T> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<T> tmpBuffer3 = tmpBuffer2[calSize];


    ErfClip(dst, src, tmpBuffer1);

    ErfComputeP(tmpBuffer1, dst, calSize);
    ErfComputeQ(tmpBuffer1, dst, calSize);

    Div<T, false>(dst, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void ErfCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, const uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpBuffer1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpBuffer2 = tmpBuffer1[calSize];
    LocalTensor<float> tmpBuffer3 = tmpBuffer2[calSize];
    LocalTensor<float> tmpBuffer4 = tmpBuffer3[calSize];


    Cast<float, half, false>(tmpBuffer4, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfClip(tmpBuffer4, tmpBuffer4, tmpBuffer1);

    ErfComputeP(tmpBuffer1, tmpBuffer4, calSize);
    ErfComputeQ(tmpBuffer1, tmpBuffer4, calSize);

    Div<float, false>(tmpBuffer1, tmpBuffer2, tmpBuffer3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    constexpr RoundMode castType = GetErfCastType();

    Cast<half, float, false>(dst, tmpBuffer1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                        ;

    constexpr uint8_t ERF_HALF_CALC_PROCEDURE = 8;
    constexpr uint8_t ERF_FLOAT_CALC_PROCEDURE = 3;

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ERF_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERF_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        ErfCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ErfImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    ErfImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erf(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    ErfImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erf.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erf(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Erf<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 59 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h" 1
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/erfc/erfc_common_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/erfc/erfc_common_impl.h"
namespace AscendC {
constexpr float ERFC_BOUNDARY_MAX = 10;
constexpr uint8_t TMPBUF_IDX_3 = 2;
constexpr uint8_t TMPBUF_IDX_5 = 4;
constexpr uint8_t TMPBUF_IDX_6 = 5;

[aicore] inline constexpr RoundMode GetErfcCastType()
{

    return RoundMode::CAST_ROUND;



}


[aicore] inline void ErfcPreCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    Abs<float, false>(tmpCompBuf1, srcBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr float SCALAR_ERFC_FP32_MIN = 2.168404344971009e-19;
    Adds<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(SCALAR_ERFC_FP32_MIN), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstBuf1, srcBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] inline void ErfcComputeR(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];


    constexpr float R0 = 0.1735313680e-7;
    constexpr float R1 = -0.9856738394e-6;
    constexpr float R2 = 0.2517003236e-4;
    constexpr float R3 = -0.3848015171e-3;
    constexpr float R4 = 0.5681528564e0;
    constexpr float R5 = 0.5245623129e1;
    constexpr float R6 = 0.2107740710e2;
    constexpr float R7 = 0.4212761755e2;
    constexpr float R8 = 0.4380524149e2;


    Muls<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(R0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R6), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R7), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf4, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf4, tmpCompBuf3, static_cast<float>(R8), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] inline void ErfcComputeS(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf3 = tmpCompBuf1[TMPBUF_IDX_3 * calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf1[TMPBUF_IDX_5 * calSize];


    constexpr float S1 = 0.9349684299e1;
    constexpr float S2 = 0.3756930664e2;
    constexpr float S3 = 0.8058268949e2;
    constexpr float S4 = 0.9155653738e2;
    constexpr float S5 = 0.4380524152e2;


    Adds<float, false>(tmpCompBuf3, tmpCompBuf1, static_cast<float>(S1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S2), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S3), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S4), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf5, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf3, tmpCompBuf5, static_cast<float>(S5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void ErfcPostCompute(const LocalTensor<float>& dstBuf1, const LocalTensor<float>& srcBuf1,
    const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];

    Muls<float, false>(tmpCompBuf2, srcBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpCompBuf2, tmpCompBuf2, static_cast<float>(NUM_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf3, srcBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstBuf1, tmpCompBuf3, tmpCompBuf2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void ErfcPublicSteps(const LocalTensor<float>& tmpCompBuf1, uint32_t calSize)
{
    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;

    LocalTensor<float> tmpCompBuf2 = tmpCompBuf1[calSize];
    LocalTensor<float> tmpCompBuf3 = tmpCompBuf2[calSize];
    LocalTensor<float> tmpCompBuf4 = tmpCompBuf3[calSize];
    LocalTensor<float> tmpCompBuf5 = tmpCompBuf4[calSize];

    Mins<float, false>(tmpCompBuf2, tmpCompBuf1, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
    ErfcComputeR(tmpCompBuf1, calSize);
    ErfcComputeS(tmpCompBuf1, calSize);


    Div<float, false>(tmpCompBuf3, tmpCompBuf4, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf1, tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmpCompBuf1, tmpCompBuf1, static_cast<float>(NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpCompBuf1, tmpCompBuf1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmpCompBuf3, tmpCompBuf1, tmpCompBuf3, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void ErfcClip(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const LocalTensor<float>& tmpBuffer)
{
    UnaryRepeatParams unaryParams;

    Mins<float, false>(dst, src, static_cast<float>(ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpBuffer, dst, static_cast<float>(-ERFC_BOUNDARY_MAX), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] inline void ErfcCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<T> tmpCompBuf1 = tmpBuffer;


    ErfcClip(dst, src, dst);
    ErfcPreCompute(dst, dst, tmpCompBuf1, calSize);


    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(dst, dst, tmpCompBuf1, calSize);
}

template <>
[aicore] inline void ErfcCompute<half>(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<half>& tmpBuffer, uint32_t calSize)
{

    LocalTensor<float> tmpCompBuf1 = tmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpCompBuf6 = tmpCompBuf1[TMPBUF_IDX_6 * calSize];


    Cast<float, half, false>(tmpCompBuf6, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    ErfcClip(tmpCompBuf1, tmpCompBuf6, tmpCompBuf6);

    ErfcPreCompute(tmpCompBuf6, tmpCompBuf6, tmpCompBuf1, calSize);

    ErfcPublicSteps(tmpCompBuf1, calSize);

    ErfcPostCompute(tmpCompBuf1, tmpCompBuf6, tmpCompBuf1, calSize);

    constexpr RoundMode castType = GetErfcCastType();

    Cast<half, float, false>(dst, tmpCompBuf1, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(T);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    constexpr uint8_t ERFC_HALF_CALC_PROCEDURE = 12;
    constexpr uint8_t ERFC_FLOAT_CALC_PROCEDURE = 5;
    uint32_t calSize = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        calSize = tmpBufferSize / ERFC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        calSize = tmpBufferSize / ERFC_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(calSize, 0, bufferSize);

    const uint32_t round = calCount / calSize;
    const uint32_t tail = calCount % calSize;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, calSize);

    uint32_t offset = 0;

    for (uint32_t i = 0; i < round; i++) {
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
        offset = offset + calSize;
    }

    if (tail != 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        ErfcCompute(dstTensor[offset], srcTensor[offset], tmpBuffer, calSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ErfcImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ErfcImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    ErfcImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 77 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 93 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    ErfcImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 113 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/erfc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Erfc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Erfc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

#pragma end_pipe
}
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/clamp.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/clamp.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/clamp/clamp_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/clamp/clamp_common_impl.h"
namespace AscendC {
template <typename T, bool isReuseSource = false>
[aicore] inline void ClampCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount, CLAMPMODE selMode)
{
    if (selMode == CLAMPMODE::CLAMP_MIN) {
                                                                                                                         ;
        Maxs(dstTensor, srcTensor, scalar, calCount);
    } else if (selMode == CLAMPMODE::CLAMP_MAX) {
                                                                                                                         ;
        Mins(dstTensor, srcTensor, scalar, calCount);
    }
}



#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMaxImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MAX);
}





template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{
    ClampCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount, CLAMPMODE::CLAMP_MIN);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/clamp.h" 2


namespace AscendC {
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/clamp.h"
#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMaxImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMax(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMaxImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
# 82 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/clamp.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T scalar, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ClampMinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ClampMin(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const T scalar,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ClampMinImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, scalar, calCount);
}
#pragma end_pipe
}
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_common_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_common_impl.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_v220_impl.h" 2

namespace AscendC {
namespace RmsNormAPI {


[aicore] inline void RmsNormBasicBlockBrc(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t hLength, const uint32_t bsLength)
{
    constexpr uint32_t BASIC_BLK_HLENGTH = 64;
    constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
    constexpr uint32_t FLOAT_PER_BLOCK = 8;
    const uint16_t dstBlkStride = hLength / FLOAT_PER_BLOCK;
    const uint16_t dstRepStride = dstBlkStride * BASIC_BLK_BSLENGTH;
    const uint32_t repTime = bsLength / BASIC_BLK_BSLENGTH;

    SetMaskNorm();
    ResetMask();
    BrcbRepeatParams brcParams(dstBlkStride, dstRepStride);
    Brcb(dst, reduceAddr, repTime, brcParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
    const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
    const int32_t loop = hLength / BASIC_BLK_HLENGTH;
    UnaryRepeatParams unaryParams(1, 0, repStride, repStride);
    for (int32_t i = 0; i < loop; i++) {
        const uint32_t offset = i * BASIC_BLK_HLENGTH;
        Adds<float, false>(dst[offset], dst, 0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/rmsnorm/rmsnorm_common_impl.h" 2




namespace AscendC {
namespace RmsNormAPI {
constexpr uint32_t BASIC_BLK_HLENGTH = 64;
constexpr uint32_t BASIC_BLK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr float RSQRT_EXPONENT = -0.5;

struct RmsNormParams {
    [aicore] RmsNormParams() {};
    uint32_t curBsLength = 0;
    uint32_t curBshLength = 0;
    LocalTensor<float> tmpAddr;
    LocalTensor<float> reducedAddr;
    LocalTensor<float> srcFp32Addr;
};

template <typename T>
[aicore] inline void GetRmsNormInfo(
    const LocalTensor<float>& tmpLocal, const RmsNormTiling& tiling, RmsNormParams& params)
{
    params.reducedAddr = tmpLocal;
    params.tmpAddr = tmpLocal[tiling.mainBsLengthAlign];
    if constexpr (sizeof(T) == sizeof(half)) {
        params.srcFp32Addr = tmpLocal[tiling.mainBshLength + tiling.mainBsLengthAlign];
    }
    params.curBsLength = tiling.mainBsLength;
    params.curBshLength = tiling.mainBshLength;
}


[aicore] inline void RmsNormGenericReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = srcTmp;

        while (totalNum > 1) {
            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                dstTmp = dst[i];
            }
            SetVectorMask<float>(0, totalNum);
            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();
            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
        }
    }
}

template <bool isBasicBlock = false>
[aicore] inline void RmsNormReduceSum(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    if constexpr (isBasicBlock) {







        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint32_t basicBlockNum = hLength / BASIC_BLK_HLENGTH;
        const uint8_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, repStride);
        for (uint32_t i = 1; i < basicBlockNum; i++) {
            const uint32_t offset = i * BASIC_BLK_HLENGTH;
            Add<float, false>(src, src, src[offset], MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }


        RepeatReduceSum<float, false>(dst, src, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, repStride);
        PipeBarrier<PIPE_V>();
    } else {
        RmsNormGenericReduceSum(dst, src, bsLength, hLength, originalHLength);
    }
}



[aicore] inline void RmsNormGeneralFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<float>(0, hLength);
    UnaryRepeatParams unaryParams;
    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    for (uint32_t i = 0; i < bsLength; i++) {
        const uint32_t offset = i * hLength;
        Muls<float, false>(dst[offset], src0[offset], src1.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, bshLength);
}


template <bool isBasicBlock = false>
[aicore] inline void RmsNormFirstAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& inputAddr,
    const LocalTensor<float>& reduceAddr, const uint32_t bshLength, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        if (bsLength > BASIC_BLK_BSLENGTH && bsLength > hLength / BASIC_BLK_HLENGTH) {
            RmsNormBasicBlockBrc(dst, inputAddr, reduceAddr, hLength, bsLength);
            SetVectorMask<float>(0, bshLength);
            BinaryRepeatParams binaryParams;
            Mul<float, false>(dst, dst, inputAddr, MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        } else {
            RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
        }
    } else {
        RmsNormGeneralFirstAxisBrcMul(dst, inputAddr, reduceAddr, bshLength, bsLength, hLength);
    }
}

[aicore] inline void RmsNormLastAxisBrcMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const uint32_t loop = hLength / BASIC_BLK_HLENGTH;
    if (loop >= bsLength) {

        BinaryRepeatParams binaryParams;
        SetVectorMask<float>(0, hLength);
        for (uint32_t i = 0; i < bsLength; ++i) {
            uint32_t offset = i * hLength;
            Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
        }
    } else {

        SetVectorMask<float>(0, bsLength * BASIC_BLK_HLENGTH);
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
        for (uint32_t i = 0; i < loop; ++i) {
            uint32_t offset = i * BASIC_BLK_HLENGTH;
            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
        }
        if (hLength % BASIC_BLK_HLENGTH != 0) {
            uint32_t offset = loop * BASIC_BLK_HLENGTH;
            uint32_t tail = hLength - offset;
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << tail) - 1);

            Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        }
    }
}


template <bool isBasicBlock = false>
[aicore] inline void RmsNormLastAxisBrcMul(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr (isBasicBlock) {
        RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
    } else {
        if (hLength == BASIC_BLK_HLENGTH) {
            BinaryRepeatParams binaryParams;
            binaryParams.src1RepStride = 0;
            SetVectorMask<float>(0, bsLength * hLength);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);
        } else if (hLength < BASIC_BLK_HLENGTH) {
            SetMaskNorm();
            SetVectorMask<float>(0, (1ull << hLength) - 1);
            uint32_t repStride = hLength / FLOAT_PER_BLOCK;
            BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);
            Mul<float, false>(dst, src0, src1, MASK_PLACEHOLDER, bsLength, binaryParams);
            SetMaskCount();
        } else {
            RmsNormLastAxisBrcMulImpl(dst, src0, src1, bsLength, hLength);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void RmsNormCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& gamma, const T epsilon, const RmsNormTiling& tiling, RmsNormParams& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<T>(0, params.curBshLength);
    if constexpr (sizeof(T) == sizeof(half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        Cast<float, half, false>(params.srcFp32Addr, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    } else {
        params.srcFp32Addr = src;
    }

    BinaryRepeatParams binaryParams;
    Mul<float, false>(params.tmpAddr, params.srcFp32Addr, params.srcFp32Addr, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    RmsNormReduceSum<isBasicBlock>(
        params.reducedAddr, params.tmpAddr, params.curBsLength, tiling.hLength, tiling.originalHLength);

    SetVectorMask<T>(0, params.curBsLength);
    Muls<float, false>(
        params.reducedAddr, params.reducedAddr, tiling.reciprocalOfHLength, MASK_PLACEHOLDER, 1, unaryParams);

    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.reducedAddr, params.reducedAddr, epsilon, MASK_PLACEHOLDER, 1, unaryParams);


    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.reducedAddr, params.reducedAddr, RSQRT_EXPONENT, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(params.reducedAddr, params.reducedAddr, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    RmsNormFirstAxisBrcMul<isBasicBlock>(params.tmpAddr, params.srcFp32Addr, params.reducedAddr, params.curBshLength,
        params.curBsLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
    if constexpr (sizeof(T) == sizeof(half)) {
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        SetVectorMask<T>(0, tiling.hLength);
        Cast<float, half, false>(
            params.srcFp32Addr, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);

        PipeBarrier<PIPE_V>();
        RmsNormLastAxisBrcMul<isBasicBlock>(
            params.tmpAddr, params.tmpAddr, params.srcFp32Addr, params.curBsLength, tiling.hLength);
        unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
        unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
        SetVectorMask<T>(0, params.curBshLength);
        Cast<half, float, false>(dst, params.tmpAddr, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {

        RmsNormLastAxisBrcMul<isBasicBlock>(dst, params.tmpAddr, gamma, params.curBsLength, tiling.hLength);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void RmsNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                            ;

                                                                                    ;

    LocalTensor<float> tmpLocal = sharedTmpBuffer.ReinterpretCast<float>();
    RmsNormParams params;
    GetRmsNormInfo<T>(tmpLocal, tiling, params);
    SetMaskCount();
    for (uint32_t i = 0; i < tiling.loopRound; ++i) {
        uint32_t offset = i * tiling.mainBshLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[offset], srcLocal[offset], gammaLocal, epsilon, tiling, params);
    }
    if (tiling.tailBsLength != 0) {
        params.curBshLength = tiling.tailBshLength;
        params.curBsLength = tiling.tailBsLength;
        RmsNormCompute<T, isBasicBlock>(
            dstLocal[tiling.inputTailPos], srcLocal[tiling.inputTailPos], gammaLocal, epsilon, tiling, params);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] inline void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon,
    const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                    ;
    RmsNormAPI::RmsNormImpl<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, sharedTmpBuffer, epsilon, tiling);
}
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/rmsnorm.h"
template <typename T, bool isBasicBlock = false>
[aicore] inline void RmsNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<T>& gammaLocal, const T epsilon, const RmsNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> stackBufer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(stackBufer);
                                                                                                   ;
    RmsNorm<T, isBasicBlock>(dstLocal, srcLocal, gammaLocal, stackBufer, epsilon, tiling);
}
#pragma end_pipe
}
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/batchnorm.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/batchnorm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/batchnorm.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_impl.h" 2







# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_v220_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_v220_impl.h"
namespace AscendC {
namespace BatchNormAPI {
constexpr uint32_t FLOAT_BLOCK_NUM_V220 = 8;
constexpr uint32_t BRC_ADDS_LOOP = 7;
constexpr uint32_t BASIC_BLOCK_LEN_V220 = 64;

[aicore] inline void BrcFirstBlockByAdds(const LocalTensor<float>& dst, const uint32_t repeat,
    const uint32_t firstOffset, UnaryRepeatParams& addsUnaryParams, const BatchNormParams<float>& params)
{
    for (uint32_t m = 0; m < repeat; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[firstOffset + m * firstOffset], dst, 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
    ResetMask();
    addsUnaryParams.srcBlkStride = DEFAULT_BLK_STRIDE;
    for (uint32_t m = 0; m < (params.basicLoop - 1); m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + i * params.oriBTmpLoopOffset],
                dst[i * params.oriBTmpLoopOffset], 0, MASK_PLACEHOLDER, MAX_REPEAT_TIMES, addsUnaryParams);
        }
        if (params.oriBTail) {
            Adds<float, false>(dst[BASIC_BLOCK_LEN_V220 + m * BASIC_BLOCK_LEN_V220 + params.oriBTmpTailOffset],
                dst[params.oriBTmpTailOffset], 0, MASK_PLACEHOLDER, (uint8_t)params.oriBTail, addsUnaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void BrcFirstDimByBrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    BrcbRepeatParams repeatParams;
    repeatParams.dstBlkStride = (uint16_t)tiling.shCurLengthBlockNum;
    repeatParams.dstRepStride = tiling.shCurLength * FLOAT_BLOCK_NUM_V220 / FLOAT_BLOCK_NUM_V220;


    Brcb(dst, src, (uint8_t)params.brcRepeatTimes, repeatParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::NORMAL>(FLOAT_BLOCK_NUM_V220);
    UnaryRepeatParams addsUnaryParams;
    addsUnaryParams.dstBlkStride = DEFAULT_BLK_STRIDE;
    addsUnaryParams.srcBlkStride = 0;
    addsUnaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addsUnaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    BrcFirstBlockByAdds(dst, BRC_ADDS_LOOP, FLOAT_BLOCK_NUM_V220, addsUnaryParams, params);
}
}
}
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_pre_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_pre_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_pre_impl.h" 2
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_pre_impl.h"
namespace AscendC {
namespace BatchNormAPI {
constexpr uint32_t FLOAT_BLOCK_NUMBER = 8;
constexpr uint32_t BASIC_BLOCK_LEN = 64;


[aicore] inline void StackBufferChecker(const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling)
{



      ;
}

template <bool needCast = false> [aicore] inline void GetSrcOffset(uint32_t& srcOffset, const BatchNormTiling& tiling)
{
    if constexpr (!needCast) {
        srcOffset = tiling.meanVarSize;
    } else {
        srcOffset = tiling.shCurLength;
    }
}

[aicore] inline void GetUpdataParams(const BatchNormTiling& tiling, BatchNormParams<float>& params)
{
    params.srcRepeatStride = params.srcOffset / FLOAT_BLOCK_NUMBER;
    params.brcRepeatTimes = tiling.originalBLength / FLOAT_BLOCK_NUMBER;
    params.oriBloop = tiling.originalBLength / MAX_REPEAT_TIMES;
    params.oriBTail = tiling.originalBLength % MAX_REPEAT_TIMES;
    params.oriBTmpLoopOffset = tiling.shCurLength * MAX_REPEAT_TIMES;
    params.oriBTmpTailOffset = params.oriBloop * params.oriBTmpLoopOffset;
    params.oriBOutLoopOffset = tiling.meanVarSize * MAX_REPEAT_TIMES;
    params.oriBOutTailOffset = params.oriBloop * params.oriBOutLoopOffset;
    params.reduceAddLoop = (tiling.originalBLength - 1) / MAX_REPEAT_TIMES;
    params.reduceAddTail = (tiling.originalBLength - 1) % MAX_REPEAT_TIMES;
    params.reduceAddTailOffset = BASIC_BLOCK_LEN + params.reduceAddLoop * params.oriBTmpLoopOffset;
    params.basicLoop = tiling.shCurLength / BASIC_BLOCK_LEN;
}

template <typename T, bool needCast = false>
[aicore] inline void GetMainTailOffset(uint64_t& inputMainOffset, uint64_t& inputTailOffset,
    const BatchNormParams<float>& params)
{
    inputMainOffset = params.oriBTmpLoopOffset;
    inputTailOffset = params.oriBTmpTailOffset;
    if constexpr (!needCast) {
        inputMainOffset = params.oriBOutLoopOffset;
        inputTailOffset = params.oriBOutTailOffset;
    }
}

template <bool isBasicBlock = false>
[aicore] inline void CastGammBeta(const LocalTensor<float>& dst, const LocalTensor<half>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams castUnaryParams;
    castUnaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.originalBLength);
    Cast<float, half, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, castUnaryParams);

    if constexpr (!isBasicBlock) {
        event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(eventIdVToS);
        WaitFlag<HardEvent::V_S>(eventIdVToS);
    } else {
        PipeBarrier<PIPE_V>();
    }
}

template <bool isBasicBlock = false>
[aicore] inline void CastOutput(const LocalTensor<half>& output, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfOutRepStride;
        unaryParams.srcRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<half, float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                    src[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<half, float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                    src[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<half, float, false>(output[i * tiling.meanVarSize], src[i * tiling.shCurLength], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void CastInput(const LocalTensor<float>& dst, const LocalTensor<half>& input,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfOutRepStride;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Cast<float, half, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    input[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Cast<float, half, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    input[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        unaryParams.srcRepStride = (uint8_t)tiling.castHalfRepStride;
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Cast<float, half, false>(dst[i * tiling.shCurLength], input[i * tiling.meanVarSize], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, unaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void GetReduceAddResult(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();
    DataCopyParams datacopyParams;
    datacopyParams.blockCount = 1;
    datacopyParams.blockLen = (uint16_t)tiling.shCurLengthBlockNum;
    DataCopy<float>(dst, src, datacopyParams);
    PipeBarrier<PIPE_V>();
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        binaryParams.dstRepStride = 0;
        binaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
        binaryParams.src1RepStride = 0;
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.reduceAddLoop; i++) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN],
                    src[tiling.shCurLength + i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES, binaryParams);
            }
            if (params.reduceAddTail) {
                Add<float, false>(dst[m * BASIC_BLOCK_LEN], src[tiling.shCurLength + m * BASIC_BLOCK_LEN],
                    dst[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.reduceAddTail, binaryParams);
            }
        }
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 1; i < tiling.originalBLength; i++) {
            Add<float, false>(dst, dst, src[i * tiling.shCurLength], MASK_PLACEHOLDER, (uint8_t)1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] inline void BrcFirstDimByDup(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t shLength, const uint32_t bLength)
{
    for (uint32_t i = 0; i < bLength; i++) {
        Duplicate<float, false>(dst[i * shLength], static_cast<float>(src.GetValue(i)), MASK_PLACEHOLDER,
            static_cast<uint8_t>(1), static_cast<uint16_t>(1), static_cast<uint8_t>(DEFAULT_REPEAT_STRIDE));
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void BrcFirstDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    SetMaskNorm();
    ResetMask();

    BrcFirstDimByBrcb<isBasicBlock>(dst, src, tiling, params);



}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] inline void GetBatchNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        unaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
        unaryParams.srcRepStride = params.srcRepeatStride;
        uint64_t inputMainOffset = 0;
        uint64_t inputTailOffset = 0;
        GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
        for (uint32_t m = 0; m < params.basicLoop; m++) {
            for (uint32_t i = 0; i < params.oriBloop; i++) {
                Muls<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                    inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    MAX_REPEAT_TIMES, unaryParams);
            }
            if (params.oriBTail) {
                Muls<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                    inputX[inputTailOffset + m * BASIC_BLOCK_LEN], params.firstDimValueBack, MASK_PLACEHOLDER,
                    (uint8_t)params.oriBTail, unaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Muls<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset],
                params.firstDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
        }
        PipeBarrier<PIPE_V>();

        GetReduceAddResult<isBasicBlock>(outputMean, params.tempTensorC, tiling, params);
    }
}

template <bool needCast = false>
[aicore] inline void GetBatchNormOutputVarianceBasicBlock(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams subBinaryParams;
    const BinaryRepeatParams mulBinaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskNorm();
    ResetMask();
    subBinaryParams.src0RepStride = params.srcRepeatStride;
    subBinaryParams.src1RepStride = 0;
    subBinaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;

    uint64_t inputMainOffset = 0;
    uint64_t inputTailOffset = 0;
    GetMainTailOffset<float, needCast>(inputMainOffset, inputTailOffset, params);
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Sub<float, false>(params.tempTensorC[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                inputX[i * inputMainOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (params.oriBTail) {
            Sub<float, false>(params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                inputX[inputTailOffset + m * BASIC_BLOCK_LEN], outputMean[m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, subBinaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, mulBinaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();
    GetReduceAddResult<true>(outputVariance, params.tempTensorA, tiling, params);
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void GetBatchNormOutputVarianceNorm(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    const UnaryRepeatParams mulsUnaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);

    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Sub<float, false>(params.tempTensorC[i * tiling.shCurLength], inputX[i * params.srcOffset], outputMean,
            MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorB, params.tempTensorC, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(params.tempTensorA, params.tempTensorB, params.firstDimValueBack, MASK_PLACEHOLDER, 1,
        mulsUnaryParams);
    PipeBarrier<PIPE_V>();

    GetReduceAddResult<false>(outputVariance, params.tempTensorA, tiling, params);
}

template <bool isBasicBlock = false, bool needCast = false>
[aicore] inline void GetBatchNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputVarianceBasicBlock<needCast>(outputVariance, inputX, outputMean, tiling, params);
    } else {
        GetBatchNormOutputVarianceNorm(outputVariance, inputX, outputMean, tiling, params);
    }
}

[aicore] inline void GetBatchNormOutputPreProcess(const LocalTensor<float>& addSrc,
    const LocalTensor<float>& addDst, const LocalTensor<float>& tmpDst, const float epsilon,
    const BatchNormTiling& tiling)
{
    const UnaryRepeatParams unaryParams;
    constexpr float exponent = -0.5;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    Adds<float, false>(addDst, addSrc, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(addDst, tmpDst, exponent, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Exp<float, false>(tmpDst, addDst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetBatchNormOutputPreBasicBlock(const LocalTensor<float>& addSrc,
    const LocalTensor<float>& addDst, const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    GetBatchNormOutputPreProcess(addSrc, addDst, tmpDst, epsilon, tiling);
    SetMaskNorm();
    ResetMask();
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = 0;
    binaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    binaryParams.dstRepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Mul<float, false>(dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[i * params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
        }
        if (params.oriBTail) {
            Mul<float, false>(dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], tmpDst[m * BASIC_BLOCK_LEN],
                dst[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, (uint8_t)params.oriBTail,
                binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetBatchNormOutputPreNorm(const LocalTensor<float>& addSrc, const LocalTensor<float>& addDst,
    const LocalTensor<float>& tmpDst, const LocalTensor<float>& dst, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    GetBatchNormOutputPreProcess(addSrc, addDst, tmpDst, epsilon, tiling);
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Mul<float, false>(dst[i * tiling.shCurLength], dst[i * tiling.shCurLength], tmpDst, MASK_PLACEHOLDER, 1,
            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void GetBatchNormOutputPre(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const float epsilon, const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputPreBasicBlock(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    } else {
        GetBatchNormOutputPreNorm(src, params.tempTensorA, params.tempTensorB, dst, epsilon, tiling, params);
    }
}
}
}
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/batchnorm/batchnorm_common_impl.h" 2

namespace AscendC {
namespace BatchNormAPI {
[aicore] inline void GetBatchNormOutputBasicBlock(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    BrcFirstDim(params.tempTensorB, gamm, tiling, params);
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    BrcFirstDim(params.tempTensorB, beta, tiling, params);
    BinaryRepeatParams addBinaryParams;
    addBinaryParams.dstRepStride = tiling.meanVarSize / FLOAT_BLOCK_NUMBER;
    addBinaryParams.src0RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    addBinaryParams.src1RepStride = (uint8_t)tiling.shCurLengthBlockNum;
    for (uint32_t m = 0; m < params.basicLoop; m++) {
        for (uint32_t i = 0; i < params.oriBloop; i++) {
            Add<float, false>(output[i * params.oriBOutLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpLoopOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                addBinaryParams);
        }
        if (params.oriBTail) {
            Add<float, false>(output[params.oriBOutTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorB[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN],
                params.tempTensorC[params.oriBTmpTailOffset + m * BASIC_BLOCK_LEN], MASK_PLACEHOLDER,
                (uint8_t)params.oriBTail, addBinaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetBatchNormOutputNorm(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, gamm, tiling.shCurLength, tiling.originalBLength);
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
    BrcFirstDimByDup(params.tempTensorB, beta, tiling.shCurLength, tiling.originalBLength);
    for (uint32_t i = 0; i < tiling.originalBLength; i++) {
        Add<float, false>(output[i * tiling.meanVarSize], params.tempTensorB[i * tiling.shCurLength],
            params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<float>& output,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        GetBatchNormOutputBasicBlock(src, output, gamm, beta, tiling, params);
    } else {
        GetBatchNormOutputNorm(src, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] inline void GetBatchNormOutput(const LocalTensor<float>& src, const LocalTensor<half>& output,
    const LocalTensor<half>& gamm, const LocalTensor<half>& beta, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    BinaryRepeatParams binaryParams;
    if constexpr (isBasicBlock) {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        BrcFirstDim(params.tempTensorB, params.tempTensorA, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Add<float, false>(params.tempTensorB, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        CastGammBeta<isBasicBlock>(params.tempTensorA, gamm, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
        Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorC, MASK_PLACEHOLDER, 1,
            binaryParams);
        PipeBarrier<PIPE_V>();
        CastGammBeta<isBasicBlock>(params.tempTensorA, beta, tiling, params);
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        BrcFirstDimByDup(params.tempTensorB, params.tempTensorA, tiling.shCurLength, tiling.originalBLength);
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < tiling.originalBLength; i++) {
            Add<float, false>(params.tempTensorB[i * tiling.shCurLength], params.tempTensorB[i * tiling.shCurLength],
                params.tempTensorC[i * tiling.shCurLength], MASK_PLACEHOLDER, 1, binaryParams);
        }
    }
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void GetOutputMeanVariance(const LocalTensor<half>& dst, const LocalTensor<float>& src,
    const BatchNormTiling& tiling, const BatchNormParams<float>& params)
{
    if constexpr (isBasicBlock) {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        SetMaskNorm();
        ResetMask();
    } else {
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.shCurLength);
        UnaryRepeatParams unaryParams;
        unaryParams.dstRepStride = (uint8_t)tiling.castHalfRepStride;
        Cast<half, float, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void GetBatchNormInfo(const LocalTensor<half>& inputX, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isReuseSource = false>
[aicore] inline void GetBatchNormInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



      ;
    StackBufferChecker(stackBuffer, tiling);
}

template <bool isBasicBlock = false>
[aicore] inline void BatchNormExeImpl(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const float epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = false;
    if constexpr (isBasicBlock) {
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    } else {

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, inputX, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, inputX, tmpOutputMean, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
    }
}

template <bool isBasicBlock = false>
[aicore] inline void BatchNormExeImpl(const LocalTensor<half>& inputX, const LocalTensor<half>& gamm,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& tmpOutputMean,
    const LocalTensor<float>& tmpOutputVariance, const half epsilon, const BatchNormTiling& tiling,
    const BatchNormParams<float>& params)
{
    constexpr bool needCast = true;
    UnaryRepeatParams unaryParams;
    if constexpr (isBasicBlock) {
        SetMaskNorm();
        ResetMask();
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);

        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);

        GetOutputMeanVariance<isBasicBlock>(outputMean, tmpOutputMean, tiling, params);

        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);

        GetOutputMeanVariance<isBasicBlock>(outputVariance, tmpOutputVariance, tiling, params);

        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);

        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);

        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    } else {
        CastInput<isBasicBlock>(params.tempTensorA, inputX, tiling, params);
        GetBatchNormOutputMean<isBasicBlock, needCast>(tmpOutputMean, params.tempTensorA, tiling, params);
        GetOutputMeanVariance(outputMean, tmpOutputMean, tiling, params);
        GetBatchNormOutputVariance<isBasicBlock, needCast>(tmpOutputVariance, params.tempTensorA, tmpOutputMean, tiling,
            params);
        GetOutputMeanVariance(outputVariance, tmpOutputVariance, tiling, params);
        GetBatchNormOutputPre<isBasicBlock>(tmpOutputVariance, params.tempTensorC, epsilon, tiling, params);
        GetBatchNormOutput<isBasicBlock>(params.tempTensorC, output, gamm, beta, tiling, params);
        CastOutput<isBasicBlock>(output, params.tempTensorB, tiling, params);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void BatchNormCompute(const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const T epsilon, BatchNormTiling& tiling,
    BatchNormParams<float>& params)
{
    constexpr bool needCast = IsSameType<T, half>::value;
    uint32_t mvOffset = 0;

    GetSrcOffset<needCast>(params.srcOffset, tiling);
    GetUpdataParams(tiling, params);

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        BatchNormExeImpl<isBasicBlock>(inputX[mvOffset], gamm, beta, output[mvOffset], outputMean[mvOffset],
            outputVariance[mvOffset], params.meanTmpTensor[mvOffset], params.varianceTmpTensor[mvOffset], epsilon,
            tiling, params);

        mvOffset += tiling.shCurLength;
    }
    if (tiling.inputTailSize > 0) {

        tiling.bshCurLength = tiling.inputTailSize;
        tiling.shCurLength = tiling.meanVarTailSize;
        tiling.shCurLengthBlockNum = tiling.shCurLength / FLOAT_BLOCK_NUMBER;
        GetSrcOffset<needCast>(params.srcOffset, tiling);
        GetUpdataParams(tiling, params);

        BatchNormExeImpl<isBasicBlock>(inputX[tiling.inputTailPos], gamm, beta, output[tiling.inputTailPos],
            outputMean[tiling.meanVarTailPos], outputVariance[tiling.meanVarTailPos],
            params.meanTmpTensor[tiling.meanVarTailPos], params.varianceTmpTensor[tiling.meanVarTailPos], epsilon,
            tiling, params);
    }
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{

                                                                                                   ;

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    BatchNormParams<float> params;
    GetBatchNormInfo<isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);
    params.firstDimValueBack = tiling.firstDimValueBack;

    SetMaskCount();
    BatchNormCompute<T, isBasicBlock>(inputX, gamm, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void BatchNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/batchnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/batchnorm.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormAPI::BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        sharedTmpBuffer, epsilon, tiling);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void BatchNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamm,
    const LocalTensor<T>& beta, const T epsilon, BatchNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BatchNormAPI::BatchNormImpl<T, isReuseSource, isBasicBlock>(output, outputMean, outputVariance, inputX, gamm, beta,
        epsilon, tiling);
}
#pragma end_pipe
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tanh/tanh_common_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tanh/tanh_common_impl.h"
namespace AscendC {
constexpr float FP32_MIN_V2 = -8.8;
constexpr float FP32_MAX_V2 = 8.8;
constexpr float DOUBLE_X = 2;
const uint8_t TANH_HALF_CALC_PROCEDURE = 2;
const uint8_t TANH_FLOAT_CALC_PROCEDURE = 1;



[aicore] inline void TanhFormulaImpl(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tmpClip = params.tmpClip;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mins<float, false>(tmpClip, srcTensor, FP32_MAX_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Maxs<float, false>(tmpClip, tmpClip, FP32_MIN_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpClip, tmpClip, DOUBLE_X, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpClip, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(dstTensor, tmpClip, -1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpClip, tmpClip, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, tmpClip, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TanhCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const TanhParams<float>& params)
{
    TanhFormulaImpl(dstTensor, srcTensor, params);
}

template <>
[aicore] inline void TanhCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const TanhParams<float>& params)
{
    const LocalTensor<float>& tempTensorConv = params.tempTensorConv;
    Cast<float, half, false>(tempTensorConv, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    TanhFormulaImpl(tempTensorConv, tempTensorConv, params);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TanhFormulasTmpCalc(TanhParams<float>& params, uint32_t tmpBufferSize)
{
    uint32_t tmpUbIndex = 0;
    if constexpr (sizeof(T) == sizeof(half)) {
        params.stackSize = params.tmpBufferSize / TANH_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        params.stackSize = params.tmpBufferSize / TANH_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(params.stackSize, 0, tmpBufferSize);
    if constexpr (sizeof(T) == sizeof(half)) {
        params.tempTensorConv = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
    }
    params.tmpClip = params.sharedTmpBuffer[params.stackSize * (tmpUbIndex++)];
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    TanhParams<float> params;
    params.calCount = calCount;
    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    params.tmpBufferSize = tmpBufferSize / sizeof(float);
    CheckTmpBufferSize(params.tmpBufferSize, 0, tmpBufferSize);


    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    TanhFormulasTmpCalc<T>(params, tmpBufferSize);

    const uint32_t round = params.calCount / params.stackSize;
    const uint32_t tail = params.calCount % params.stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        TanhCompute(dstTensor[offset], srcTensor[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tanh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
# 97 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    TanhImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 64 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/atanh/atanh_common_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/atanh/atanh_common_impl.h"
namespace AscendC {
constexpr uint32_t ATANH_FLOAT_CALC_PROC = 1;
constexpr uint32_t ATANH_HALF_CALC_PROC = 4;




template <typename T>
[aicore] inline void AtanhCompute(const LocalTensor<T>& dstTensor,
    const LocalTensor<T>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Adds<float, false>(tmpBuffer, srcTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(dstTensor, srcTensor, static_cast<T>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(dstTensor, tmpBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, static_cast<T>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] inline void AtanhCompute(const LocalTensor<half>& dstTensor,
    const LocalTensor<half>& srcTensor,
    const LocalTensor<float> &tmpBuffer,
    uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];


    Cast<float, half, false>(tmpFloatBuffer1, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(-1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer1, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<float>(0.5), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dstTensor, tmpFloatBuffer1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                          ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / ATANH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ATANH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T>(0, splitCount);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    for (uint32_t i = 0; i < loopCount; i++) {
        AtanhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], tmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T>(0, calcTail);
        AtanhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], tmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanhImpl(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] inline void AtanhImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    AtanhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void Atanh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 75 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atanh(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/atanh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Atanh(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    Atanh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/deepnorm.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/deepnorm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/deepnorm/deepnorm_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/deepnorm/deepnorm_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/deepnorm/deepnorm_v220_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/deepnorm/deepnorm_v220_impl.h"
namespace AscendC {
namespace DeepNormAPI {
constexpr uint32_t BASIC_BLOCK_HLENGTH = 64;
constexpr uint32_t BASIC_BLOCK_BSLENGTH = 8;
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr float SQRT_EXPONENT = -0.5;



[aicore] inline void DeepNormBasicBlockVbrcb(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength)
{
    constexpr uint16_t brcbDstBlkStride = 8;
    constexpr uint16_t brcbDstRepStride = 64;
    constexpr uint16_t addSrcBlkStride = 0;
    const uint8_t repeatTimes = bsLength / 8;
    SetMaskNorm();
    ResetMask();

    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    Brcb<float>(dst, src, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);


    Adds<float, false>(dst, dst, 0, MASK_PLACEHOLDER, 1,
        {DEFAULT_BLK_STRIDE, addSrcBlkStride, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DeepNormVarianceBasicBlockByBrcb(const LocalTensor<float>& inputX,
    const LocalTensor<float>& inputMean, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;



    DeepNormBasicBlockVbrcb(params.tempTensorC, inputMean, tiling.bsCurLength);

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Sub<float, false>(params.tempTensorB[i * BASIC_BLOCK_HLENGTH], inputX[i * BASIC_BLOCK_HLENGTH],
            params.tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] inline void DeepNormOutputBasicBlockByBrcb(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const uint8_t num = tiling.hLength / BASIC_BLOCK_HLENGTH;
    const UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;



    DeepNormBasicBlockVbrcb(params.tempTensorC, params.tempTensorA, tiling.bsCurLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength / num);
    for (uint32_t i = 0; i < num; i++) {

        Mul<float, false>(params.tempTensorA[i * BASIC_BLOCK_HLENGTH], params.tempTensorC,
            xSubMean[i * BASIC_BLOCK_HLENGTH], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();


    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Adds<float, false>(params.tempTensorC, params.tempTensorA, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/deepnorm/deepnorm_common_impl.h" 2




namespace AscendC {
namespace DeepNormAPI {
template <typename T, bool isBasicBlock = false>
[aicore] inline bool IsDeepNormParamValid(DeepNormTiling& tiling)
{

                                                                                   ;

                                                                                                           ;

    const bool hDivBy64 = (tiling.hLength % BASIC_BLOCK_HLENGTH == 0) &&
        (tiling.originalHLength % BASIC_BLOCK_HLENGTH == 0);
    const bool bsDivBy8 = ((tiling.bLength * tiling.sLength) % BASIC_BLOCK_BSLENGTH == 0);
    if constexpr (isBasicBlock) {


                                                                                  ;
    }

    return true;
}

[aicore] inline void IsStackBufferValid(const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling)
{



      ;
}


[aicore] inline bool IsBasicBlockTmp8HBetter(const DeepNormTiling& tiling)
{
    bool bs8Check = (tiling.oneTmpSize % (tiling.hLength * BASIC_BLOCK_BSLENGTH)) == 0;

    bool bsWorse = tiling.bsCurLength > (tiling.hLength / BASIC_BLOCK_HLENGTH);
    return bs8Check && bsWorse;
}



template <bool isReuseSource = false>
[aicore] inline void GetDeepNormTensorInfo(const LocalTensor<half>& inputX, const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];




      ;

    IsStackBufferValid(stackBuffer, tiling);
}


template <bool isReuseSource = false>
[aicore] inline void GetDeepNormTensorInfo(const LocalTensor<float>& inputX, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const LocalTensor<float>& stackBuffer, const DeepNormTiling& tiling,
    DeepNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    if constexpr (isReuseSource) {
        params.tempTensorA = inputX;
        params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];



          ;
    } else {
        params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];



          ;
    }

    IsStackBufferValid(stackBuffer, tiling);
}


[aicore] inline void DeepNormExec(const LocalTensor<half>& inputX, const LocalTensor<half>& inputGx,
    const LocalTensor<half>& output, const half alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, half, false>(params.tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(params.tempTensorC, inputGx, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;

    Muls<float, false>(params.tempTensorA, params.tempTensorA, static_cast<float>(alpha), MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DeepNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& inputGx,
    const LocalTensor<float>& output, const float alpha, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);


    Muls<float, false>(params.tempTensorB, inputX, alpha, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tempTensorA, inputGx, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DeepNormBasicBlockReduceSum(const LocalTensor<float>& output, const LocalTensor<float>& tmp,
    const LocalTensor<float>& input, const UnaryRepeatParams& unaryParams, const uint32_t bsLength,
    const uint32_t hLength)
{
    const uint8_t num = hLength / BASIC_BLOCK_HLENGTH;

    BinaryRepeatParams binaryParams;
    binaryParams.dstRepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src0RepStride = num * DEFAULT_REPEAT_STRIDE;
    binaryParams.src1RepStride = num * DEFAULT_REPEAT_STRIDE;





    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
    for (uint32_t i = 1; i < num; i++) {
        Add<float, false>(input, input[i * BASIC_BLOCK_HLENGTH], input, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    RepeatReduceSum<float, false>(output, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    RepeatReduceSum<float, false>(tmp, input, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
        num * DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



[aicore] inline void DeepNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength, const uint32_t originalHLength)
{
    for (uint32_t i = 0; i < bsLength; i++) {
        uint32_t totalNum = originalHLength;
        LocalTensor<float> srcTmp = src[i * hLength];
        LocalTensor<float> dstTmp = dst[i * hLength];

        while (totalNum > 1) {
            SetVectorMask<float, MaskMode::COUNTER>(0, totalNum);


            if (totalNum <= ONE_REPEAT_FLOAT_SIZE) {
                RepeatReduceSum<float, false>(dstMVTmp[i], srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                dstTmp = dst[i];
            }

            RepeatReduceSum<float, false>(dstTmp, srcTmp, 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();

            totalNum = DivCeil(totalNum, ONE_REPEAT_FLOAT_SIZE);
            srcTmp = dstTmp;
        }
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, bsLength);
}



template <bool isBasicBlock = false, uint8_t mode = 0>
[aicore] inline void DeepNormBshHCalc(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    if constexpr(isBasicBlock) {
        const uint32_t loop = hLength / BASIC_BLOCK_HLENGTH;
        const uint16_t repStride = hLength / FLOAT_PER_BLOCK;
        BinaryRepeatParams binaryParams(1, 1, 1, repStride, repStride, 0);

        SetVectorMask<float, MaskMode::COUNTER>(0, bsLength * BASIC_BLOCK_HLENGTH);
        for (uint32_t i = 0; i < loop; i++) {
            uint32_t offset = i * BASIC_BLOCK_HLENGTH;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1[offset], MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, hLength);
    } else {
        BinaryRepeatParams binaryParams;
        for (uint32_t i = 0; i < bsLength; i++) {
            uint32_t offset = i * hLength;
            if constexpr(mode) {
                Mul<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            } else {
                Add<float, false>(dst[offset], src0[offset], src1, MASK_PLACEHOLDER, 1, binaryParams);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}




template <bool isBasicBlock = false>
[aicore] inline void GetDeepNormOutputMean(const LocalTensor<float>& tmpMean, const LocalTensor<float>& inputX,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const LocalTensor<float>& outputMean)
{
    const UnaryRepeatParams unaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);

    Muls<float, false>(params.tempTensorC, inputX, params.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputMean, tmpMean, params.tempTensorC, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    } else {
        DeepNormReduceSumImpl(outputMean, tmpMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


[aicore] inline void DeepNormVarianceByForLoop(const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean,
    const DeepNormTiling& tiling, const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Adds<float, false>(params.tempTensorB[i * tiling.hLength], inputX[i * tiling.hLength],
            (float)((inputMean.GetValue(i))*(-1)), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}


[aicore] inline void DeepNormOutputByForLoop(const LocalTensor<float>& xSubMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const UnaryRepeatParams& unaryParams)
{
    event_t eventIdVToS = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        Muls<float, false>(params.tempTensorC[i * tiling.hLength], xSubMean[i * tiling.hLength],
            (float)params.tempTensorA.GetValue(i), MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
}



template <bool isBasicBlock = false>
[aicore] inline void GetDeepNormOutputVariance(const LocalTensor<float>& tmpVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params, const LocalTensor<float>& outputVariance)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    if constexpr(isBasicBlock) {
        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormVarianceBasicBlockByBrcb(inputX, inputMean, tiling, params);
        } else {
            DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
        }
    } else {
        DeepNormVarianceByForLoop(inputX, inputMean, tiling, params, unaryParams);
    }


    Mul<float, false>(params.tempTensorC, params.tempTensorB, params.tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tempTensorA, params.tempTensorC, params.lastDimValueBack, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();


    if constexpr(isBasicBlock) {
        DeepNormBasicBlockReduceSum(outputVariance, tmpVariance, params.tempTensorA, unaryParams, tiling.bsCurLength,
            tiling.hLength);
    }
    else {
        DeepNormReduceSumImpl(outputVariance, tmpVariance, params.tempTensorA, tiling.bsCurLength, tiling.hLength,
            tiling.originalHLength);
    }
}


template <bool isBasicBlock = false>
[aicore] inline void GetDeepNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bsCurLength);

    Adds<float, false>(params.tempTensorA, inputVariance, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tempTensorC, static_cast<float>(1.0), 1, 1, 1, 8);
    PipeBarrier<PIPE_V>();


    Sqrt<float, false>(params.tempTensorA, params.tempTensorA, 1, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tempTensorA, params.tempTensorC, params.tempTensorA, 1, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr(isBasicBlock) {

        if ((IsBasicBlockTmp8HBetter(tiling))) {
            DeepNormOutputBasicBlockByBrcb(xSubMean, tiling, params);
            return;
        }
    }
    DeepNormOutputByForLoop(xSubMean, tiling, params, unaryParams);
}


template <bool isBasicBlock = false>
[aicore] inline void GetDeepNormOutput(const LocalTensor<half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<half>& gamm, const LocalTensor<half>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);

    unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
    Cast<float, half, false>(params.tempTensorA, gamm, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorB, inputY, params.tempTensorA, tiling.bsCurLength,
        tiling.hLength);


    Cast<float, half, false>(params.tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    DeepNormBshHCalc<isBasicBlock, 0>(params.tempTensorA, params.tempTensorB, params.tempTensorC, tiling.bsCurLength,
        tiling.hLength);

    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;


    Cast<half, float, false>(output, params.tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isBasicBlock = false>
[aicore] inline void GetDeepNormOutput(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamm, const LocalTensor<float>& beta, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 1>(params.tempTensorA, inputY, gamm, tiling.bsCurLength, tiling.hLength);
    DeepNormBshHCalc<isBasicBlock, 0>(output, params.tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}


[aicore] inline void GetDeepNormOutputMeanVariance(const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, tiling.meanVarSize);

    Cast<half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}



template <bool isBasicBlock = false>
[aicore] inline void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<half>& gamm,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const half epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, params.tempTensorA, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, params.tempTensorA, outputMean, tiling, params,
        outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, static_cast<float>(epsilon), tiling,
        params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <bool isBasicBlock = false>
[aicore] inline void DeepNormLayerNormExec(const LocalTensor<float>& inputX, const LocalTensor<float>& gamm,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const DeepNormTiling& tiling,
    const DeepNormParams<float>& params)
{
    GetDeepNormOutputMean<isBasicBlock>(params.tempTensorC, inputX, tiling, params, outputMean);
    GetDeepNormOutputVariance<isBasicBlock>(params.tempTensorC, inputX, outputMean, tiling, params, outputVariance);
    GetDeepNormOutputPre<isBasicBlock>(params.tempTensorB, params.tempTensorC, epsilon, tiling, params);
    GetDeepNormOutput<isBasicBlock>(output, params.tempTensorC, gamm, beta, tiling, params);
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void DeepNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& inputGx,
    const LocalTensor<T>& gamm, const LocalTensor<T>& beta, const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance, const T alpha, const T epsilon,
    DeepNormTiling& tiling, const DeepNormParams<float>& params)
{
    uint32_t BSHOffset = 0;
    uint32_t BSOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
        BSHOffset += tiling.inputRoundSize;
        BSOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        BSHOffset = tiling.inputTailPos;
        BSOffset = tiling.meanVarTailPos;
        DeepNormExec(inputX[BSHOffset], inputGx[BSHOffset], output, alpha, tiling, params);
        DeepNormLayerNormExec<isBasicBlock>(params.tempTensorA, gamm, beta, output[BSHOffset],
            params.meanTmpTensor[BSOffset], params.varianceTmpTensor[BSOffset], epsilon, tiling, params);
    }


    if constexpr(IsSameType<T, half>::value) {
        GetDeepNormOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] inline void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{

                                                 ;
    if (!DeepNormAPI::IsDeepNormParamValid<T, isBasicBlock>(tiling)) {
        return;
    }
                                                                                                                    ;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    DeepNormParams<float> deepnormParams;
    DeepNormAPI::GetDeepNormTensorInfo<isReuseSrc>(srcLocal, meanLocal, rstdLocal, stackBuffer, tiling, deepnormParams);
    deepnormParams.lastDimValueBack = tiling.lastDimValueBack;

    SetMaskCount();
    DeepNormAPI::DeepNormND<T, isBasicBlock>(srcLocal, gxLocal, gammaLocal, betaLocal, dstLocal, meanLocal, rstdLocal,
        alpha, epsilon, tiling, deepnormParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSrc, bool isBasicBlock>
[aicore] inline void DeepNormImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}

}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/deepnorm.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] inline void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const T alpha, const T epsilon, DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, sharedTmpBuffer, alpha, epsilon, tiling);
}
# 88 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/deepnorm.h"
template <typename T, bool isReuseSrc = false, bool isBasicBlock = false>
[aicore] inline void DeepNorm(const LocalTensor<T>& dstLocal, const LocalTensor<T>& meanLocal,
    const LocalTensor<T>& rstdLocal, const LocalTensor<T>& srcLocal, const LocalTensor<T>& gxLocal,
    const LocalTensor<T>& betaLocal, const LocalTensor<T>& gammaLocal, const T alpha, const T epsilon,
    DeepNormTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DeepNormAPI::DeepNormImpl<T, isReuseSrc, isBasicBlock>(dstLocal, meanLocal, rstdLocal, srcLocal, gxLocal, betaLocal,
        gammaLocal, alpha, epsilon, tiling);
}
#pragma end_pipe
}
# 66 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/exp.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/exp.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/exp/exp_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/exp/exp_common_impl.h"
namespace AscendC {
namespace ExpAPI {
constexpr uint8_t HALF_REPEAT_STRIDE = 4;
constexpr uint32_t EXP_TWO = 2;
constexpr uint32_t EXP_THREE = 3;
constexpr uint32_t EXP_FOUR = 4;


template <typename T, bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] inline void UpdataExpParams(const LocalTensor<T>& src, const uint32_t calCount,
    const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    uint32_t alignNum = ONE_BLK_SIZE / sizeof(T);



    bool isFloat = IsSameType<T, float>::value;
    uint32_t numberOfTmpBuf = EXP_FOUR;
    if (isFloat) {
        numberOfTmpBuf = isReuseSource ? EXP_TWO : EXP_THREE;
    }

    uint32_t inputSize = calCount;
    uint32_t stackBufferSize = stackBuffer.GetSize();
    uint32_t oneTmpSize = stackBufferSize / numberOfTmpBuf;
    oneTmpSize = oneTmpSize / alignNum * alignNum;
    uint32_t secondOffset = (isFloat && isReuseSource)? 0 : oneTmpSize;
    uint32_t fourthOffset = isFloat ? 0 : oneTmpSize;

    CheckTmpBufferSize(oneTmpSize, 0, stackBufferSize);

    params.inputSize = inputSize;
    params.oneTmpSize = oneTmpSize;
    params.firstTmpStartPos = 0;
    params.secondTmpStartPos = secondOffset;
    params.thirdTmpStartPos = params.secondTmpStartPos + oneTmpSize;
    params.fourthTmpStartPos = params.thirdTmpStartPos + fourthOffset;
    params.loopNum = inputSize / oneTmpSize;
    params.tailSize = inputSize % oneTmpSize;
    params.tailPos = inputSize - params.tailSize;
    params.curDataLength = oneTmpSize;
    params.expandLevel = expandLevel;
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] inline void GetExpTensorInfo(const LocalTensor<half>& src, const LocalTensor<half>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<half, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = stackBuffer[params.thirdTmpStartPos];
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}

template <bool isReuseSource = false, uint8_t expandLevel = 10>
[aicore] inline void GetExpTensorInfo(const LocalTensor<float>& src, const LocalTensor<float>& dst,
    const uint32_t calCount, const LocalTensor<float>& stackBuffer, ExpParams<float>& params)
{
    UpdataExpParams<float, isReuseSource, expandLevel>(src, calCount, stackBuffer, params);
    if constexpr(isReuseSource) {
        params.tempTensorFloorX = src;
    } else {
        params.tempTensorFloorX = stackBuffer[params.firstTmpStartPos];
    }
    params.tempTensorFloorXPow = stackBuffer[params.secondTmpStartPos];
    params.tempTensorRes = dst;
    params.tempTensorIntPart = stackBuffer[params.fourthTmpStartPos];
}


template <typename T>
[aicore] inline void GetExpInputInTmp(const LocalTensor<T>& src, const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    if constexpr (IsSameType<T, half>::value) {
        unaryParams.srcRepStride = HALF_REPEAT_STRIDE;
        Cast<float, half, false>(params.tempTensorFloorX, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    } else {
        Adds<float, false>(params.tempTensorFloorX, src, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}




[aicore] inline void GetExpFloorInput(const ExpParams<float>& params, uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);



    Cast<float, float, false>(params.tempTensorIntPart, params.tempTensorFloorX, RoundMode::CAST_FLOOR,
        MASK_PLACEHOLDER, 1, unaryParams);







    PipeBarrier<PIPE_V>();


    Sub<float, false>(params.tempTensorFloorX, params.tempTensorFloorX, params.tempTensorIntPart, MASK_PLACEHOLDER, 1,
        binaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(params.tempTensorIntPart, params.tempTensorIntPart, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] inline void ExpHighPrecisionExec(const ExpParams<float>& params, uint32_t maskLength, uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);


    Adds<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorFloorX, 0.0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], 1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    for (int32_t i = 2; i < params.expandLevel + 1; i++) {

        Mul<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorX, params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();



        Muls<float, false>(params.tempTensorFloorXPow, params.tempTensorFloorXPow,
            static_cast<float>(1.0) / static_cast<float>(i), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();


        Add<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorFloorXPow,
            MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(params.tempTensorRes[offset], params.tempTensorRes[offset], params.tempTensorIntPart,
        MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void GetExpCastedResult(const LocalTensor<half>& dst, const ExpParams<float>& params,
    uint32_t maskLength)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = HALF_REPEAT_STRIDE;
    SetVectorMask<float, MaskMode::COUNTER>(0, maskLength);
    Cast<half, float, false>(dst, params.tempTensorRes, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void ExpHighPrecisionND(const LocalTensor<T>& src, const LocalTensor<T>& dst,
    const ExpParams<float>& params, uint32_t offset, uint32_t maskLength)
{
    GetExpInputInTmp(src[offset], params, maskLength);
    GetExpFloorInput(params, maskLength);


    if constexpr(IsSameType<T, half>::value) {
        ExpHighPrecisionExec(params, maskLength, 0);
        GetExpCastedResult(dst[offset], params, maskLength);
    } else {
        ExpHighPrecisionExec(params, maskLength, offset);
    }
}


template <typename T>
[aicore] inline void ExpND(const LocalTensor<T>& src, const LocalTensor<T>& dst, const ExpParams<float>& params)
{
    SetMaskCount();

    uint32_t offset = 0;
    for (uint32_t index = 0; index < params.loopNum; index++) {
        ExpHighPrecisionND(src, dst, params, offset, params.curDataLength);
        offset += params.oneTmpSize;
    }

    if (params.tailSize > 0) {
        ExpHighPrecisionND(src, dst, params, offset, params.tailSize);
    }
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] inline void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                                         ;

    if (taylorExpandLevel == 0) {
        Exp<T>(dstLocal, srcLocal, calCount);
        return;
    }

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    CheckTmpBufferSize(bufferSize, 0, bufferSize);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    ExpParams<float> expParams;

    ExpAPI::GetExpTensorInfo<isReuseSource, taylorExpandLevel>(srcLocal, dstLocal, calCount, stackBuffer, expParams);
    ExpAPI::ExpND<T>(srcLocal, dstLocal, expParams);

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint8_t taylorExpandLevel, bool isReuseSource>
[aicore] inline void ExpImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/exp.h" 2

namespace AscendC {

#pragma begin_pipe(V)
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] inline void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, calCount);
}
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/exp.h"
template <typename T, uint8_t taylorExpandLevel, bool isReuseSource = false>
[aicore] inline void Exp(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value),
        "Failed to check the data types, current api support data types are half/float.");
    ExpAPI::ExpImpl<T, taylorExpandLevel, isReuseSource>(dstLocal, srcLocal, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/layernorm_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/layernorm_utils.h"
namespace AscendC {

struct LayerNormConfig {
    bool isNoBeta = false;
    bool isNoGamma = false;
    bool isOnlyOutput = false;
    bool isOutputRstd = true;
};

struct WelfordUpdateConfig {
    [aicore] constexpr WelfordUpdateConfig(const bool isInplaceIn): isInplace(isInplaceIn) {}
    bool isInplace = false;
};

constexpr WelfordUpdateConfig WFUPDATE_DEFAULT_CFG = {false};

struct LayerNormPara {
    uint32_t aLength;
    uint32_t rLength;
    uint32_t rLengthWithPadding;
};

struct WelfordUpdateParam {
    uint32_t rnLength;
    uint32_t abLength;
    uint32_t abComputeLength;
    float nRec;
};

};
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_normal_config.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_normal_config.h"
namespace AscendC {

[aicore] constexpr LayerNormConfig GetLayerNormNormalConfig()
{
    return {.isNoBeta = false, .isNoGamma = false, .isOnlyOutput = false, .isOutputRstd = true};
}

constexpr LayerNormConfig LNCFG_NORM = GetLayerNormNormalConfig();

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize_utils.h"
namespace AscendC {



enum class ReducePattern : uint32_t {
    AR = 0,
    RA = 1,
    R,
    ARA,
    ARAR,
    ARARA,
    ARARAR,
    ARARARA,
    ARARARAR,
    ARARARARA,
    RAR,
    RARA,
    RARAR,
    RARARA,
    RARARAR,
    RARARARA,
};


struct NormalizeConfig {
    ReducePattern reducePattern = ReducePattern::AR;
    int32_t aLength = -1;
    bool isNoBeta = false;
    bool isNoGamma = false;
    bool isOnlyOutput = false;
};

struct NormalizePara {
    uint32_t aLength;
    uint32_t rLength;
    uint32_t rLengthWithPadding;
};

};
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/normalize/normalize_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/normalize/normalize_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/normalize/normalize_config.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/normalize/normalize_config.h"
namespace AscendC {
[aicore] constexpr NormalizeConfig GetNormalizeConfig(bool isNoBeta, bool isNoGamma)
{
    return {.reducePattern = ReducePattern::AR,
        .aLength = -1,
        .isNoBeta = isNoBeta,
        .isNoGamma = isNoGamma,
        .isOnlyOutput = false};
}

constexpr NormalizeConfig NLCFG_NORM = GetNormalizeConfig(false, false);

constexpr NormalizeConfig NLCFG_NOBETA = GetNormalizeConfig(true, false);

constexpr NormalizeConfig NLCFG_NOGAMMA = GetNormalizeConfig(false, true);

constexpr NormalizeConfig NLCFG_NOOPT = GetNormalizeConfig(true, true);

};
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/normalize/normalize_common_impl.h" 2


namespace AscendC {
const float DEFAULT_EPSILON = 1e-5;

template <typename T>
struct NormalizeTmpTensor {
    [aicore] NormalizeTmpTensor(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> gammaTmpTensor;
    LocalTensor<T> betaTmpTensor;
};

template <typename U, typename T>
[aicore] inline constexpr bool IsDtypeValid()
{

    constexpr bool isValid1 = (IsSameType<T, float>::value) && (IsSameType<U, float>::value);
    constexpr bool isValid2 = (IsSameType<T, half>::value) && (IsSameType<U, half>::value);
    constexpr bool isValid3 = (IsSameType<T, half>::value) && (IsSameType<U, float>::value);
    return isValid1 || isValid2 || isValid3;
}

template <const NormalizeConfig& config>
[aicore] inline bool CheckParams(const NormalizePara& para)
{
    static_assert(config.isOnlyOutput == false, "isOnlyOutput must be set false for now.");
    static_assert(config.aLength != 1, "aLength in config must not be 1.");
    if constexpr(config.aLength != -1) {

                                                            ;
    }
    return true;
}

template <typename U, typename T>
[aicore] inline void GetNormalizeTensorInfo(const LocalTensor<float>& stackBuffer, const NormalizePara& para,
    NormalizeTmpTensor<float>& tempTensor, uint32_t& N)
{



                                                       ;
    uint32_t R2 = para.rLengthWithPadding * 2;
    if constexpr(IsSameType<U, float>::value) {
        N = stackBuffer.GetSize() / R2;
        N = (N >= para.aLength) ? para.aLength : N;
                                                                                              ;
        tempTensor.tempTensorA = stackBuffer[0];
        tempTensor.tempTensorB = stackBuffer[N * para.rLengthWithPadding];
    } else {
        N = (stackBuffer.GetSize() - R2) / R2;
        N = (N >= para.aLength) ? para.aLength : N;
                                                                                              ;
        tempTensor.tempTensorA = stackBuffer[0];
        tempTensor.tempTensorB = stackBuffer[N * para.rLengthWithPadding];
        tempTensor.gammaTmpTensor = stackBuffer[R2 * N];
        tempTensor.betaTmpTensor = stackBuffer[R2 * N + para.rLengthWithPadding];
    }
}

[aicore] inline void GetNormalizeOutputRstd(const LocalTensor<float>& dstRstd, const LocalTensor<float>& srcVar,
    const NormalizeTmpTensor<float>& tmpTensor, const UnaryRepeatParams& unaryParams,
    const BinaryRepeatParams& binaryParams, const NormalizePara& para, const float epsilon)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, para.aLength);


    Adds<float, false>(dstRstd, srcVar, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(tmpTensor.tempTensorA, static_cast<float>(1), 1, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Sqrt<float, false>(dstRstd, dstRstd, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dstRstd, tmpTensor.tempTensorA, dstRstd, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] inline void CastTensor(const LocalTensor<U>& src, const LocalTensor<float>& castRes)
{

    Cast<float, U, false>(castRes, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE,
        HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

template <typename U, const NormalizeConfig& config>
[aicore] inline void CastGammaBeta(const LocalTensor<U>& gamma, const LocalTensor<U>& beta,
    const NormalizeTmpTensor<float>& tmpTensor, const NormalizePara& para)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    if constexpr(!config.isNoGamma) {
        CastTensor<U>(gamma, tmpTensor.gammaTmpTensor);
    }
    if constexpr(!config.isNoBeta) {
        CastTensor<U>(beta, tmpTensor.betaTmpTensor);
    }
}

template <typename T>
[aicore] inline void CastSrc(const LocalTensor<T>& srcX, const NormalizeTmpTensor<float>& tmpTensor,
    const NormalizePara& para, const uint32_t N)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    if constexpr(IsSameType<T, float>::value) {
        Adds<float, false>(tmpTensor.tempTensorA, srcX, static_cast<float>(0), MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE,
            DEFAULT_REPEAT_STRIDE});
    } else {
        Cast<float, T, false>(tmpTensor.tempTensorA, srcX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1,
            DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}


[aicore] inline void GetNormalizeOutputPre(const LocalTensor<float>& srcX, const LocalTensor<float>& srcMean,
    const LocalTensor<float>& srcRstd, const LocalTensor<float>& brcbTmp, const LocalTensor<float>& dstVmuls,
    const NormalizePara& para, const BinaryRepeatParams& binaryParams, const uint32_t N, const uint32_t NBase)
{

    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    auto eventId = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventId);
    WaitFlag<HardEvent::V_S>(eventId);
    for (uint32_t i = 0; i < N; i++) {
        float value = srcMean.GetValue(i + NBase);
        eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Duplicate<float, false>(brcbTmp[i * para.rLengthWithPadding], value, 1, 1, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    Sub<float, false>(srcX, srcX, brcbTmp, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    for (uint32_t i = 0; i < N; i++) {
        float value = srcRstd.GetValue(i + NBase);
        eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Duplicate<float, false>(brcbTmp[i * para.rLengthWithPadding], value, 1, 1, DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    Mul<float, false>(dstVmuls, srcX, brcbTmp, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, const NormalizeConfig& config>
[aicore] inline void GetNormalizeOutput(const LocalTensor<float>& srcxFP32, const LocalTensor<float>& gammaFP32,
    const LocalTensor<float>& betaFP32, const LocalTensor<T>& output, const NormalizePara& para,
    const UnaryRepeatParams& unaryParams, const BinaryRepeatParams& binaryParams, const uint32_t N)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, para.rLength);
    for (uint32_t i = 0; i < N; i++) {
        if constexpr(!config.isNoGamma) {
            Mul<float, false>(srcxFP32[i * para.rLengthWithPadding], srcxFP32[i * para.rLengthWithPadding], gammaFP32,
                MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        if constexpr(!config.isNoBeta) {
            Add<float, false>(srcxFP32[i * para.rLengthWithPadding], srcxFP32[i * para.rLengthWithPadding], betaFP32,
                MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
    SetVectorMask<float, MaskMode::COUNTER>(0, N * para.rLengthWithPadding);
    if constexpr(IsSameType<T, float>::value) {
        Adds<float, false>(output, srcxFP32, (float)0, MASK_PLACEHOLDER, 1, unaryParams);
    } else {
        Cast<T, float, false>(output, srcxFP32, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, {1, 1,
            HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}

template <typename U, typename T, bool isReuseSource, const NormalizeConfig& config>
[aicore] inline void NormalizeImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const float epsilon, const NormalizePara& para)
{

                                        ;
    static_assert(IsDtypeValid<U, T>(), "Failed to check dtype in Normalize, current api support dtype combination is "
        "T: float, U: float; T: half, U: half / float.");
    bool res = CheckParams<config>(para);

    const UnaryRepeatParams unaryParam;
    const BinaryRepeatParams binaryParam;
    NormalizeTmpTensor<float> tmpTensor;
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t N = 0;
    GetNormalizeTensorInfo<U, T>(stackBuffer, para, tmpTensor, N);
    uint32_t mainRepeatTimes = para.aLength / N;
    uint32_t tailN = para.aLength % N;

    GetNormalizeOutputRstd(outputRstd, inputVariance, tmpTensor, unaryParam, binaryParam, para, epsilon);
    if constexpr(IsSameType<U, half>::value) {
        CastGammaBeta<U, config>(gamma, beta, tmpTensor, para);
    }

    for (uint32_t i = 0; i < mainRepeatTimes; i++) {
        uint32_t index = para.rLengthWithPadding * N * i;
        CastSrc<T>(inputX[index], tmpTensor, para, N);
        GetNormalizeOutputPre(tmpTensor.tempTensorA, inputMean, outputRstd, tmpTensor.tempTensorB,
            tmpTensor.tempTensorA, para, binaryParam, N, N * i);
        if constexpr(IsSameType<U, float>::value) {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, gamma, beta,
                output[index], para, unaryParam, binaryParam, N);
        } else {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, tmpTensor.gammaTmpTensor, tmpTensor.betaTmpTensor,
                output[index], para, unaryParam, binaryParam, N);
        }
    }

    if (tailN > 0) {
        uint32_t index = para.rLengthWithPadding * N * mainRepeatTimes;
        CastSrc<T>(inputX[index], tmpTensor, para, tailN);
        GetNormalizeOutputPre(tmpTensor.tempTensorA, inputMean, outputRstd, tmpTensor.tempTensorB,
            tmpTensor.tempTensorA, para, binaryParam, tailN, N * mainRepeatTimes);
        if constexpr(IsSameType<U, float>::value) {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, gamma, beta,
                output[index], para, unaryParam, binaryParam, tailN);
        } else {
            GetNormalizeOutput<T, config>(tmpTensor.tempTensorA, tmpTensor.gammaTmpTensor, tmpTensor.betaTmpTensor,
                output[index], para, unaryParam, binaryParam, tailN);
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename U, typename T, bool isReuseSource, const NormalizeConfig& config>
[aicore] inline void NormalizeImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const float epsilon, const NormalizePara& para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        sharedTmpBuffer, epsilon, para);
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 42 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h"
template <typename U, typename T, bool isReuseSource = false, const NormalizeConfig& config = NLCFG_NORM>
[aicore] inline void Normalize(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const float epsilon, const NormalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        sharedTmpBuffer, epsilon, para);
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/normalize.h"
template <typename U, typename T, bool isReuseSource = false, const NormalizeConfig& config = NLCFG_NORM>
[aicore] inline void Normalize(const LocalTensor<T>& output, const LocalTensor<float>& outputRstd,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<U>& gamma, const LocalTensor<U>& beta, const float epsilon, const NormalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    NormalizeImpl<U, T, isReuseSource, config>(output, outputRstd, inputMean, inputVariance, inputX, gamma, beta,
        epsilon, para);
}
#pragma end_pipe
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_basic_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_basic_impl.h"
namespace AscendC {
constexpr uint32_t MASK_LOW_6BITS = 0x3f;
constexpr uint32_t MASK_HIGH_26BITS = 0xFFFFFFC0;
template <typename T>
struct LayerNormRstdTmpTensorParams {
    [aicore] LayerNormRstdTmpTensorParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> varianceTmpTensor;
};

template <bool isRelocate = true, bool isTransposeDst = false>
[aicore] inline void LayerNormReduceSumImpl(const LocalTensor<float>& dstMVTmp, const LocalTensor<float>& dst,
    const LocalTensor<float>& src, const uint32_t bsLength, const uint32_t hLength)
{
    ResetMask();
    SetMaskNorm();

    constexpr uint32_t rightShiftSix = 6;
    if (hLength > ONE_REPEAT_FLOAT_SIZE) {
        uint32_t addRepeatTime = (hLength >> rightShiftSix) - 1;
        uint32_t addTailNumber = (hLength & MASK_LOW_6BITS);
        if ((hLength & MASK_LOW_6BITS) == 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else if (addRepeatTime > 0) {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTmp = src[i + ONE_REPEAT_FLOAT_SIZE];
                LocalTensor<float> srcTailTmp = src[i + (hLength & MASK_HIGH_26BITS)];
                Add(dstTmp, srcTmp, dstTmp, ONE_REPEAT_FLOAT_SIZE, addRepeatTime,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        } else {
            for (uint32_t i = 0; i < bsLength * hLength; i += hLength) {
                LocalTensor<float> dstTmp = src[i];
                LocalTensor<float> srcTailTmp = src[i + (hLength & MASK_HIGH_26BITS)];
                Add(dstTmp, srcTailTmp, dstTmp, addTailNumber, 1,
                    { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, 0, DEFAULT_REPEAT_STRIDE, 0 });
                PipeBarrier<PIPE_V>();
            }
        }
    }

    uint32_t repeatTime = bsLength;
    uint32_t cursorSrc = 0;
    uint32_t wholeReduceSumHLength = (hLength > ONE_REPEAT_FLOAT_SIZE) ? ONE_REPEAT_FLOAT_SIZE : hLength;
    constexpr uint32_t rightShiftThree = 3;
    const uint32_t reduceSumSrcRepeatStride = hLength >> rightShiftThree;

    while (repeatTime >= MAX_REPEAT_TIMES) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                MAX_REPEAT_TIMES, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, MAX_REPEAT_TIMES, hLength, DEFAULT_BLK_STRIDE,
            reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
        repeatTime -= MAX_REPEAT_TIMES;
        ++cursorSrc;
    }

    uint32_t reduceSumSrcRepeatTimeTail = bsLength - cursorSrc * MAX_REPEAT_TIMES;
    if (reduceSumSrcRepeatTimeTail > 0) {
        LocalTensor<float> srcTmp = src[cursorSrc * MAX_REPEAT_TIMES * hLength];
        LocalTensor<float> dstTmp = dst[cursorSrc * MAX_REPEAT_TIMES * hLength];
        if constexpr (isRelocate) {
            WholeReduceSum<float>(dstMVTmp[cursorSrc * MAX_REPEAT_TIMES], srcTmp, wholeReduceSumHLength,
                reduceSumSrcRepeatTimeTail, 1, DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        }
        WholeReduceSum<float>(dstTmp, srcTmp, wholeReduceSumHLength, reduceSumSrcRepeatTimeTail, hLength,
            DEFAULT_BLK_STRIDE, reduceSumSrcRepeatStride);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
}

[aicore] inline void BroadcastLastDim(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t bsLength, const uint32_t hLength)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, hLength);

    SetCmpMask<float>(src);
    PipeBarrier<PIPE_V>();

    LocalTensor<int16_t> maskLocal = src.ReinterpretCast<int16_t>();

    const UnaryRepeatParams unaryParams;
    Muls<int16_t, false>(maskLocal, maskLocal, 0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LocalTensor<uint16_t> maskLocalTmp = maskLocal.ReinterpretCast<uint16_t>();

    const BinaryRepeatParams binaryParams;
    Select<float, uint16_t>(dst, maskLocalTmp, dst, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 1; i < bsLength; i++) {
        SetCmpMask<float>(src[i * hLength]);
        PipeBarrier<PIPE_V>();

        Select<float, uint16_t>(dst[i * hLength], maskLocalTmp, dst, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void DuplicateMulImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Mul<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DuplicateAddImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src0,
    const LocalTensor<float>& src1, const uint32_t bsLength, const uint32_t hLength)
{
    const BinaryRepeatParams binaryParams;
    for (uint32_t i = 0; i < bsLength; i++) {
        Add<float, false>(dst[i * hLength], src0[i * hLength], src1, MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void GetLayerNormNDTensorInfo(const LocalTensor<T>& inputX, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<float>& stackBuffer, const LayerNormTiling& tiling,
    LayerNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];



      ;



      ;
}

template <>
[aicore] inline void GetLayerNormNDTensorInfo<float, false>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];




      ;




      ;
}

template <>
[aicore] inline void GetLayerNormNDTensorInfo<float, true>(const LocalTensor<float> &inputX,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &stackBuffer, const LayerNormTiling &tiling, LayerNormParams<float> &params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    params.tempTensorA = inputX;
    params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];




      ;




      ;
}

[aicore] inline void GetOutputMeanVariance(const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const LayerNormTiling& tiling, const LayerNormParams<float>& params)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.meanVarSize);

    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void GetLayerNormRstdTensorInfo(const LocalTensor<float>& stackBuffer,
    const LayerNormSeparateTiling& tiling, LayerNormRstdTmpTensorParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];



      ;



      ;
}

template <typename U, typename T, const LayerNormConfig& config = LNCFG_NORM>
[aicore] inline void CheckLayerNormRstd(const LocalTensor<float> stackBuffer, const LayerNormPara& para) {
    static_assert(SupportType<T, half, float>(), "current data type is not supported on current device!");
    if constexpr (IsSameType<T, half>::value) {
        static_assert(SupportType<U, half, float>(), "current data type is not supported on current device!");
    } else if constexpr (IsSameType<T, float>::value) {
        static_assert(SupportType<U, float>(), "current data type is not supported on current device!");
    }
    static_assert(config.isOnlyOutput == false, "current data type is not supported on current device!");

                                                                                          ;
}

template <typename T>
[aicore] inline void LayerNormPreProc(const LocalTensor<T>& inputX, const LocalTensor<float>& stackBuffer,
    const LayerNormPara& para)
{
    const LocalTensor<T> tempTensor = stackBuffer.ReinterpretCast<T>();
    Duplicate(tempTensor, (T)0, para.rLengthWithPadding);
    PipeBarrier<PIPE_V>();
    Adds(tempTensor, tempTensor, (T)1, para.rLength);
    PipeBarrier<PIPE_V>();
    for (int i = 0; i < para.aLength; i++) {
        Mul(inputX[i * para.rLengthWithPadding], inputX[i * para.rLengthWithPadding], tempTensor,
            para.rLengthWithPadding);
    }
    PipeBarrier<PIPE_V>();
}

[aicore] inline void WelfordUpdateComputeMean(const LocalTensor<float>& tmpVreg, const LocalTensor<float>& src,
    const LocalTensor<float>& inMean, const LocalTensor<float>& outVreg, const LocalTensor<float>& outMean,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams, const WelfordUpdateParam &para)
{
    PipeBarrier<PIPE_V>();
    Sub<float, false>(tmpVreg, src, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(outVreg, tmpVreg, static_cast<float>(para.nRec), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(outMean, outVreg, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void WelfordUpdateComputeVar(const LocalTensor<float>& tmpVreg, const LocalTensor<float>& inVar,
    const LocalTensor<float>& outVar, const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams,
    const WelfordUpdateParam &para)
{
    PipeBarrier<PIPE_V>();
    Add<float, false>(outVar, tmpVreg, inVar, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] inline constexpr uint32_t WelfordUpdateGetTmpSize()
{
    if constexpr (sizeof(T) == sizeof(half)) {
        return 0x3;
    }

    if constexpr (isReuseSource) {
        return 1;
    }
    return 0x2;
}

[aicore] inline void GetLayerNormOutputMean(const LocalTensor<float>& outputMean, const LocalTensor<float>& inputX,
    const LayerNormTiling& tiling, const LayerNormParams<float>& params, const LocalTensor<float>& tmpMean)
{
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(params.tempTensorC, inputX, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpMean, outputMean, params.tempTensorC, tiling.bsCurLength, tiling.hLength);
}

[aicore] inline void GetLayerNormOutputVariance(const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputX, const LocalTensor<float>& inputMean, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params, const LocalTensor<float>& tmpVariance)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorC, inputMean, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const BinaryRepeatParams binaryParams;
    Sub<float, false>(tempTensorB, inputX, tempTensorC, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempTensorC, tempTensorB, tempTensorB, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    const UnaryRepeatParams unaryParams;
    Muls<float, false>(tempTensorA, tempTensorC, tiling.lastDimValueBack, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(tmpVariance, outputVariance, tempTensorA, tiling.bsCurLength, tiling.hLength);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] inline void WelfordUpdateInplaceCompute(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<U>& inMean, const LocalTensor<U>& inVar, const WelfordUpdateParam &para, uint32_t alignNum)
{
    uint32_t inPlaceLength = AlignUp(para.abLength - para.abComputeLength, alignNum);
    uint32_t dstOffset = para.abLength - inPlaceLength;

    DataCopy(outMean[dstOffset], inMean[dstOffset], inPlaceLength);
    DataCopy(outVar[dstOffset], inVar[dstOffset], inPlaceLength);
    PipeBarrier<PIPE_V>();
}
[aicore] inline void WelfordUpdateInplace(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& inMean, const LocalTensor<float>& inVar, const WelfordUpdateParam &para)
{
    WelfordUpdateInplaceCompute(outMean, outVar, inMean, inVar, para, B32_DATA_NUM_PER_BLOCK);
}

[aicore] inline void WelfordUpdateInplace(const LocalTensor<half>& outMean, const LocalTensor<half>& outVar,
    const LocalTensor<half>& inMean, const LocalTensor<half>& inVar, const WelfordUpdateParam &para)
{
    WelfordUpdateInplaceCompute(outMean, outVar, inMean, inVar, para, B16_DATA_NUM_PER_BLOCK);
}

[aicore] inline void GetLayerNormOutputPre(const LocalTensor<float>& xSubMean,
    const LocalTensor<float>& inputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    const float exponent = -0.5;
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    BroadcastLastDim(tempTensorA, inputVariance, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    const UnaryRepeatParams unaryParams;
    Adds<float, false>(tempTensorC, tempTensorA, epsilon, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sqrt<float, false>(tempTensorA, tempTensorC, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_BLOCK);
    Duplicate<float, false>(tempTensorC, 1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    Div<float, false>(tempTensorA, tempTensorC, tempTensorA, MASK_PLACEHOLDER, 1,
        { 1, 0, 1, DEFAULT_REPEAT_STRIDE, 0, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Mul<float, false>(tempTensorC, tempTensorA, xSubMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h" 2


namespace AscendC {

template <typename T>
[aicore] inline void GetLayerNormOutput(const LocalTensor<T>& output, const LocalTensor<float>& inputY,
    const LocalTensor<T>& gamma, const LocalTensor<T>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] inline void GetLayerNormOutput<half>(const LocalTensor<half>& output, const LocalTensor<float>& inputY,
    const LocalTensor<half>& gamma, const LocalTensor<half>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(tempTensorA, gamma, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateMulImpl(tempTensorB, inputY, tempTensorA, tiling.bsCurLength, tiling.hLength);

    Cast<float, half, false>(tempTensorC, beta, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    DuplicateAddImpl(tempTensorA, tempTensorB, tempTensorC, tiling.bsCurLength, tiling.hLength);

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);

    Cast<half, float, false>(output, tempTensorA, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void GetLayerNormOutput<float>(const LocalTensor<float>& output, const LocalTensor<float>& inputY,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.hLength);

    DuplicateMulImpl(tempTensorA, inputY, gamma, tiling.bsCurLength, tiling.hLength);

    DuplicateAddImpl(output, tempTensorA, beta, tiling.bsCurLength, tiling.hLength);
}

template <typename T>
[aicore] inline void LayerNormExe(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const T epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{}

template <>
[aicore] inline void LayerNormExe<half>(const LocalTensor<half>& inputX, const LocalTensor<half>& gamma,
    const LocalTensor<half>& beta, const LocalTensor<half>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const half epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tiling.bshCurLength);

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(tempTensorA, inputX, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    GetLayerNormOutputMean(tempTensorB, tempTensorA, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, tempTensorA, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, static_cast<float>(epsilon), tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <>
[aicore] inline void LayerNormExe<float>(const LocalTensor<float>& inputX, const LocalTensor<float>& gamma,
    const LocalTensor<float>& beta, const LocalTensor<float>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputVariance, const float epsilon, const LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    GetLayerNormOutputMean(tempTensorB, inputX, tiling, params, outputMean);

    GetLayerNormOutputVariance(tempTensorC, inputX, tempTensorB, tiling, params, outputVariance);

    GetLayerNormOutputPre(tempTensorB, tempTensorC, epsilon, tiling, params);

    GetLayerNormOutput(output, tempTensorC, gamma, beta, tiling, params);
}

template <typename T>
[aicore] inline void LayerNormND(const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const T epsilon, LayerNormTiling& tiling,
    const LayerNormParams<float>& params)
{
    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);

        inputOffset += tiling.inputRoundSize;
        mvOffset += tiling.meanVarRoundSize;
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        LayerNormExe<T>(inputX[inputOffset], gamma, beta, output[inputOffset], params.meanTmpTensor[mvOffset],
            params.varianceTmpTensor[mvOffset], epsilon, tiling, params);
    }

    if constexpr (sizeof(T) == sizeof(half)) {
        GetOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
                                   ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
                                      ;
        return;
    }


                                          ;

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
                                                                                                                ;

    LayerNormParams<float> params;
    GetLayerNormNDTensorInfo<T, isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);

    SetMaskCount();
    LayerNormND<T>(inputX, gamma, beta, output, outputMean, outputVariance, epsilon, tiling, params);

    SetMaskNorm();
    ResetMask();
                                  ;
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}

template <typename T>
[aicore] inline void ComputeMeanVariance(const LocalTensor<float>& outputMean, const LocalTensor<float>& variance,
    const LocalTensor<T>& src, const LayerNormRstdTmpTensorParams<float>& params, const LayerNormPara& para,
    const LayerNormSeparateTiling& tiling)
{
    if constexpr (IsSameType<T, half>::value) {
        Cast<float, T>(params.tempTensorA, src, RoundMode::CAST_NONE, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    } else {
        DataCopy(params.tempTensorA, src, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    }

    Muls<float>(params.tempTensorA, params.tempTensorA, tiling.rValueBack, tiling.arCurLength);
    PipeBarrier<PIPE_V>();


    LayerNormReduceSumImpl(outputMean, params.tempTensorB, params.tempTensorA, tiling.aCurLength,
        para.rLengthWithPadding);
    auto eventId = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventId);
    WaitFlag<HardEvent::V_S>(eventId);

    if constexpr (IsSameType<T, half>::value) {
        PipeBarrier<PIPE_V>();
        Cast<float, T>(params.tempTensorA, src, RoundMode::CAST_NONE, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    } else {
        DataCopy(params.tempTensorA, src, tiling.arCurLength);
        PipeBarrier<PIPE_V>();
    }

    eventId = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    for (uint32_t j = 0; j < tiling.aCurLength; j++) {
        float scalar = static_cast<float>(-1) * outputMean.GetValue(j);
        SetFlag<HardEvent::S_V>(eventId);
        WaitFlag<HardEvent::S_V>(eventId);
        Adds<float>(params.tempTensorA[j * para.rLengthWithPadding], params.tempTensorA[j * para.rLengthWithPadding],
            scalar, tiling.rLength);
    }
    PipeBarrier<PIPE_V>();

    Mul<float>(params.tempTensorA, params.tempTensorA, params.tempTensorA, tiling.arCurLength);
    PipeBarrier<PIPE_V>();

    Muls<float>(params.tempTensorA, params.tempTensorA, tiling.rValueBack, tiling.arCurLength);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl(variance, params.tempTensorB, params.tempTensorA, tiling.aCurLength,
        para.rLengthWithPadding);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNormCalMeanVarImpl(const LocalTensor<float>& OutputMean, const LocalTensor<float>& variance,
    const LocalTensor<T>& inputX, const LocalTensor<float>& stackBuffer, const LayerNormPara& para,
    LayerNormSeparateTiling& tiling)
{
    if (para.rLength != para.rLengthWithPadding) {
        LayerNormPreProc<T>(inputX, stackBuffer, para);
    }

    LayerNormRstdTmpTensorParams<float> params;
    GetLayerNormRstdTensorInfo<isReuseSource>(stackBuffer, tiling, params);

    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    for (uint32_t index = 0; index < tiling.loopRound; index++) {
        ComputeMeanVariance<T>(OutputMean[mvOffset], variance[mvOffset], inputX[inputOffset], params, para, tiling);

        inputOffset += tiling.inputRoundSize;
        mvOffset += tiling.meanVarRoundSize;
    }
    if (tiling.inputTailSize > 0) {
        tiling.arCurLength = tiling.inputTailSize;
        tiling.aCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        ComputeMeanVariance<T>(OutputMean[mvOffset], variance[mvOffset], inputX[inputOffset], params, para, tiling);
    }
}

template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] inline void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LayerNormPara& para, const LayerNormSeparateTiling& tiling) {

                                           ;
        const LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        CheckLayerNormRstd<U, T, config>(stackBuffer, para);

        const LocalTensor<float> variance = stackBuffer[tiling.varianceTmpTensorPos];
        LayerNormSeparateTiling& Tiling = const_cast<LayerNormSeparateTiling&>(tiling);
        LayerNormCalMeanVarImpl<T, isReuseSource>(outputMean, variance, inputX, stackBuffer, para, Tiling);
        const LocalTensor<uint8_t> shareTmpBuffer = stackBuffer[tiling.firstTmpStartPos].ReinterpretCast<uint8_t>();

        NormalizePara normallizepara = {para.aLength, para.rLength, para.rLengthWithPadding};
        if (config.isNoBeta == false && config.isNoGamma == false) {
            Normalize<U, T, false, NLCFG_NORM>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else if (config.isNoBeta == true && config.isNoGamma == false) {
            Normalize<U, T, false, NLCFG_NOBETA>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else if (config.isNoBeta == false && config.isNoGamma == true) {
            Normalize<U, T, false, NLCFG_NOGAMMA>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        } else {
            Normalize<U, T, false, NLCFG_NOOPT>(output, outputRstd, outputMean, variance, inputX, gamma, beta,
                shareTmpBuffer, epsilon, normallizepara);
        }
    }

template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] inline void LayerNormImpl(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon,
        sharedTmpBuffer, para, tiling);
}

template <bool isReuseSource = false>
[aicore] inline void WelfordUpdateCompute(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<half>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> srcVreg = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t tmpIndex = B32_DATA_NUM_PER_REPEAT * tmpNum;
    LocalTensor<float> tmpVreg = srcVreg[tmpIndex];
    LocalTensor<float> outVreg = srcVreg[tmpIndex + tmpIndex];

    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(srcVreg, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});

    WelfordUpdateComputeMean(tmpVreg, srcVreg, inMean, outVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(outVreg, srcVreg, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, outVreg, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

[aicore] inline void WelfordUpdateComputeTo32Res(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> tmpVreg = sharedTmpBuffer.ReinterpretCast<float>();

    WelfordUpdateComputeMean(tmpVreg, src, inMean, tmpVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(tmpVreg, src, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(src, src, inMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, src, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

[aicore] inline void WelfordUpdateComputeTo32(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    LocalTensor<float> tmpVreg = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> outVreg = tmpVreg[B32_DATA_NUM_PER_REPEAT * tmpNum];

    WelfordUpdateComputeMean(tmpVreg, src, inMean, outVreg, outMean, unaryParams, binaryParams, para);

    Sub<float, false>(outVreg, src, outMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(tmpVreg, tmpVreg, outVreg, MASK_PLACEHOLDER, 1, binaryParams);

    WelfordUpdateComputeVar(tmpVreg, inVar, outVar, unaryParams, binaryParams, para);
}

template <bool isReuseSource = false>
[aicore] inline void WelfordUpdateCompute(const LocalTensor<float>& outMean, const LocalTensor<float>& outVar,
    const LocalTensor<float>& src, const LocalTensor<float>& inMean, const LocalTensor<float>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para, const uint32_t tmpNum,
    const UnaryRepeatParams unaryParams, const BinaryRepeatParams binaryParams)
{
    if (isReuseSource) {
        WelfordUpdateComputeTo32Res(outMean, outVar, src, inMean, inVar, sharedTmpBuffer, para, tmpNum, unaryParams,
            binaryParams);
    } else {
        WelfordUpdateComputeTo32(outMean, outVar, src, inMean, inVar, sharedTmpBuffer, para, tmpNum, unaryParams,
            binaryParams);
    }
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] inline void WelfordUpdateComputeImpl(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<T>& src, const LocalTensor<U>& inMean, const LocalTensor<U>& inVar,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam &para)
{
    constexpr uint32_t tmpBufNum = WelfordUpdateGetTmpSize<T, U, isReuseSource>();

    uint32_t tmpNum = sharedTmpBuffer.GetSize() / (ONE_REPEAT_BYTE_SIZE * tmpBufNum);
# 425 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h"
    const uint32_t round = para.abComputeLength / (B32_DATA_NUM_PER_REPEAT * tmpNum);
    const uint32_t tail = para.abComputeLength % (B32_DATA_NUM_PER_REPEAT * tmpNum);

    SetVectorMask<float, MaskMode::COUNTER>(0, B32_DATA_NUM_PER_REPEAT * tmpNum);
    uint32_t offset = 0;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    for (uint32_t i = 0; i < round; ++i) {
        WelfordUpdateCompute<isReuseSource>(outMean[offset], outVar[offset], src[offset], inMean[offset],
            inVar[offset], sharedTmpBuffer, para, tmpNum, unaryParams, binaryParams);
        offset = offset + B32_DATA_NUM_PER_REPEAT * tmpNum;
    }

    if (tail != 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, tail);
        WelfordUpdateCompute<isReuseSource>(outMean[offset], outVar[offset], src[offset], inMean[offset],
            inVar[offset], sharedTmpBuffer, para, tmpNum, unaryParams, binaryParams);
    }
}

template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig &config = WFUPDATE_DEFAULT_CFG>
[aicore] inline void WelfordUpdateImpl(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam& para)
{

                               ;
    static_assert((std::is_same<T, float>::value || std::is_same<T, half>::value),
        "Failed to check dtype of inputX, inputX support dtype is: half/float.");
    static_assert((std::is_same<U, float>::value),
        "Failed to check dtype of mean/var, mean/var support dtype is: float.");
# 489 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/layernorm/layernorm_common_impl.h"
    SetMaskCount();
    if (config.isInplace && (para.abComputeLength < para.abLength)) {
        WelfordUpdateInplace(outputMean, outputVariance, inputMean, inputVariance, para);
    }
    WelfordUpdateComputeImpl<T, U, isReuseSource>(outputMean, outputVariance, inputX, inputMean, inputVariance,
        sharedTmpBuffer, para);
    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig &config = WFUPDATE_DEFAULT_CFG>
[aicore] inline void WelfordUpdateImpl(const LocalTensor<U>& outMean, const LocalTensor<U>& outVar,
    const LocalTensor<U>& inMean, const LocalTensor<U>& inVar, const LocalTensor<T>& srcUb,
    const WelfordUpdateParam& para)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);
                                                                                 ;

    WelfordUpdateImpl<T, U, isReuseSource, config>(outMean, outVar, inMean, inVar, srcUb, stackTensor, para);
}

}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void LayerNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, LayerNormTiling& tiling)
{
    LayerNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, epsilon, tiling);
}
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] inline void LayerNorm(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon, para,
        tiling);
}
# 118 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename U, typename T, bool isReuseSource = false, const LayerNormConfig& config = LNCFG_NORM>
[aicore] inline void LayerNorm(const LocalTensor<T>& output, const LocalTensor<float>& outputMean,
    const LocalTensor<float>& outputRstd, const LocalTensor<T>& inputX, const LocalTensor<U>& gamma,
    const LocalTensor<U>& beta, const float epsilon, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LayerNormPara& para, const LayerNormSeparateTiling& tiling)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LayerNormImpl<U, T, isReuseSource, config>(output, outputMean, outputRstd, inputX, gamma, beta, epsilon,
        sharedTmpBuffer, para, tiling);
}
# 143 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig& config = WFUPDATE_DEFAULT_CFG>
[aicore] inline void WelfordUpdate(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const WelfordUpdateParam& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordUpdateImpl<T, U, isReuseSource, config>(outputMean, outputVariance, inputMean, inputVariance, inputX, para);
}
# 167 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/layernorm.h"
template <typename T, typename U, bool isReuseSource = false, const WelfordUpdateConfig& config = WFUPDATE_DEFAULT_CFG>
[aicore] inline void WelfordUpdate(const LocalTensor<U>& outputMean, const LocalTensor<U>& outputVariance,
    const LocalTensor<U>& inputMean, const LocalTensor<U>& inputVariance, const LocalTensor<T>& inputX,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const WelfordUpdateParam& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordUpdateImpl<T, U, isReuseSource, config>(outputMean, outputVariance, inputMean, inputVariance, inputX,
        sharedTmpBuffer, para);
}
#pragma end_pipe
}
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/welfordfinalize_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/normalization/welfordfinalize_utils.h"
namespace AscendC {
struct WelfordFinalizePara
{
    uint32_t rnLength;
    uint32_t abLength;
    uint32_t headCount;
    uint32_t headCountLength;
    uint32_t tailCount;
    uint32_t tailCountLength;
    float abRec;
    float rRec;
};

};
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/welfordfinalize/welfordfinalize_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/welfordfinalize/welfordfinalize_common_impl.h"
namespace AscendC
{
constexpr uint32_t OUTPUT_SIZE = 8;
constexpr uint32_t B32_LEN = 256 / sizeof(float);
constexpr uint32_t OUTPUT_MASK_B32 = 254;

struct WelfordFinalizeTilingData
{
    uint32_t computeLength;
    uint32_t round;
    uint32_t tail;
};

template <typename T>
struct WelfordFinalizeTmpTensors
{
    [aicore] WelfordFinalizeTmpTensors() {}
    LocalTensor<T> tempOutputMean;
    LocalTensor<T> tempOutputVariance;
    LocalTensor<T> tempMean;
    LocalTensor<T> tempVariance;
};

[aicore] inline void welfordFinalizeOutputPre(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
    const LocalTensor<float> &tempOutputMean, const LocalTensor<float> &tempOutputVariance)
{
    Adds(outputMean, tempOutputMean, static_cast<float>(0), 1);
    Adds(outputVariance, tempOutputVariance, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    SetVectorMask<float>(0, OUTPUT_MASK_B32);

    Duplicate<float, false>(outputMean, 0, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(outputVariance, 0, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeExeVariance(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputVariance, const LocalTensor<int32_t> &counts, const WelfordFinalizeTmpTensors<float> &tempTensors,
    const WelfordFinalizePara &para, const uint32_t computeLength)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    Adds(tempVariance, tempMean, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    BroadcastLastDim(tempMean, tempVariance, 1, computeLength);

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    Sub<float, false>(tempVariance, inputMean, tempMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, tempVariance, tempVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, int32_t, false>(tempVariance, counts, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, tempMean, tempVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tempVariance, tempMean, inputVariance, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempVariance, tempVariance, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempMean, outputVariance, tempVariance, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeExeMean(const LocalTensor<float> &inputMean, const LocalTensor<float> &outputMean, const LocalTensor<int32_t> &counts,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    Cast<float, int32_t, false>(tempMean, counts, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempMean, inputMean, tempMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempMean, tempMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempVariance, outputMean, tempMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetWelfordFinalizeOutputMeanWithTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &tempTensorCal,
    const LocalTensor<float> &tempoutputMean, const WelfordFinalizePara &para, const LocalTensor<float> &outputMean, const uint32_t computeLength, const uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t headCount = para.headCount;
    int32_t tailCount = para.tailCount;

    Muls<float, false>(tempTensorCal, inputMean, (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempoutputMean, tempTensorCal, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    int32_t headComputeLength = para.headCountLength - offset;
    if (headComputeLength > static_cast<int32_t>(computeLength)) {
        headComputeLength = computeLength;
    } else if (headComputeLength < 0) {
        headComputeLength = 0;
    }

    if (headComputeLength > 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, headComputeLength);
        Muls<float, false>(tempoutputMean, tempoutputMean, (float)headCount / (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    LayerNormReduceSumImpl<true, false>(tempTensorCal, outputMean, tempoutputMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetWelfordFinalizeOutputMeanNoTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &tempoutputMean,
    const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputMean, const uint32_t computeLength)
{
    SetMaskCount();
    const UnaryRepeatParams unaryParams;
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);

    Muls<float, false>(tempoutputMean, inputMean, para.abRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempTensorCal, outputMean, tempoutputMean, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetWelfordFinalizeOutputVarianceWithTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &tempoutputMean, const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputVariance,
    const uint32_t computeLength, const uint32_t offset)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t headCount = para.headCount;
    int32_t tailCount = para.tailCount;

    Sub<float, false>(tempTensorCal, inputMean, tempoutputMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempoutputMean, tempTensorCal, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    int32_t headComputeLength = para.headCountLength - offset;
    if (headComputeLength > static_cast<int32_t>(computeLength)) {
        headComputeLength = computeLength;
    } else if (headComputeLength < 0) {
        headComputeLength = 0;
    }

    if (headComputeLength > 0) {
        SetVectorMask<float, MaskMode::COUNTER>(0, headComputeLength);
        Muls<float, false>(tempTensorCal, tempTensorCal, (float)headCount / (float)tailCount, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    Add<float, false>(tempoutputMean, inputVariance, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempoutputMean, outputVariance, tempTensorCal, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetWelfordFinalizeOutputVarianceNoTail(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &tempoutputMean, const LocalTensor<float> &tempTensorCal, const WelfordFinalizePara &para, const LocalTensor<float> &outputVariance, const uint32_t computeLength)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, computeLength);
    int32_t rnLength = para.rnLength;

    Sub<float, false>(tempTensorCal, inputMean, tempoutputMean, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tempoutputMean, tempTensorCal, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, (float)rnLength, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tempoutputMean, inputVariance, tempTensorCal, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tempTensorCal, tempoutputMean, para.rRec, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LayerNormReduceSumImpl<true, false>(tempoutputMean, outputVariance, tempTensorCal, 1, computeLength);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeExeMean(const LocalTensor<float> &inputMean, const LocalTensor<float> &outputMean,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength, const uint32_t offset)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;

    if (para.tailCountLength == 0 || para.tailCount == 0) {
        GetWelfordFinalizeOutputMeanNoTail(inputMean, tempMean, tempVariance, para, outputMean, computeLength);
    } else {



              ;
        GetWelfordFinalizeOutputMeanWithTail(inputMean, tempVariance, tempMean, para, outputMean, computeLength, offset);
    }
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeExeVariance(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputVariance, WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, const uint32_t computeLength, const uint32_t offset)
{
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempVariance = tempTensors.tempVariance;
    Adds(tempVariance, tempMean, static_cast<float>(0), 1);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    BroadcastLastDim(tempMean, tempVariance, 1, computeLength);
    if (para.tailCountLength == 0 || para.tailCount == 0) {
        GetWelfordFinalizeOutputVarianceNoTail(inputMean, inputVariance, tempMean, tempVariance, para, outputVariance, computeLength);
    } else {



              ;
        GetWelfordFinalizeOutputVarianceWithTail(inputMean, inputVariance, tempMean, tempVariance, para, outputVariance, computeLength, offset);
    }
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeComputeImpl(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, WelfordFinalizeTmpTensors<float> &tempTensors,
    const WelfordFinalizePara &para, WelfordFinalizeTilingData &tiling)
{
    uint32_t offset = 0;
    uint32_t outOffset = 0;
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempOutputMean = tempTensors.tempOutputMean;
    LocalTensor<float> tempOutputVariance = tempTensors.tempOutputVariance;

    for (uint32_t i = 0; i < tiling.round; i++)
    {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset],
            tempTensors, para, tiling.computeLength, offset);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputMean, tempOutputMean, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset],
            tempTensors, para, tiling.tail, offset);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputMean, tempOutputMean, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();

    offset = 0;
    outOffset = 0;
    for (uint32_t i = 0; i < tiling.round; i++)
    {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset],
            tempTensors, para, tiling.computeLength, offset);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputVariance, tempOutputVariance, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset],
            tempTensors, para, tiling.tail, offset);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputVariance, tempOutputVariance, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();
    welfordFinalizeOutputPre(outputMean, outputVariance, tempOutputMean, tempOutputVariance);
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeComputeImpl(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
    const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const LocalTensor<int32_t> &counts,
    const WelfordFinalizeTmpTensors<float> &tempTensors, const WelfordFinalizePara &para, WelfordFinalizeTilingData &tiling)
{
    uint32_t offset = 0;
    uint32_t outOffset = 0;
    LocalTensor<float> tempMean = tempTensors.tempMean;
    LocalTensor<float> tempOutputMean = tempTensors.tempOutputMean;
    LocalTensor<float> tempOutputVariance = tempTensors.tempOutputVariance;

    for (uint32_t i = 0; i < tiling.round; i++)
    {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset], counts[offset],
            tempTensors, para, tiling.computeLength);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputMean, tempOutputMean, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        WelfordFinalizeExeMean<isReuseSource>(inputMean[offset], tempOutputMean[outOffset], counts[offset],
            tempTensors, para, tiling.tail);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputMean, tempOutputMean, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();

    offset = 0;
    outOffset = 0;
    for (uint32_t i = 0; i < tiling.round; i++)
    {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset], counts[offset],
            tempTensors, para, tiling.computeLength);
        offset += tiling.computeLength;
        outOffset++;
        if (outOffset == B32_LEN) {
            SetMaskNorm();
            WholeReduceSum(tempOutputVariance, tempOutputVariance, B32_LEN, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
            PipeBarrier<PIPE_V>();
            outOffset = 1;
        }
    }

    if (tiling.tail > 0) {
        Adds(tempMean, tempOutputMean, static_cast<float>(0), 1);
        PipeBarrier<PIPE_V>();
        WelfordFinalizeExeVariance<isReuseSource>(inputMean[offset], inputVariance[offset], tempOutputVariance[outOffset], counts[offset],
            tempTensors, para, tiling.tail);
        outOffset++;
    }
    SetMaskNorm();
    WholeReduceSum(tempOutputVariance, tempOutputVariance, outOffset, 1, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, outOffset >> 0x3);
    PipeBarrier<PIPE_V>();
    welfordFinalizeOutputPre(outputMean, outputVariance, tempOutputMean, tempOutputVariance);
}

template <bool isReuseSource = false>
[aicore] inline void GetWelfordFinalizeTensorInfo(const LocalTensor<float> &stackBuffer, const WelfordFinalizePara &para,
                                                    WelfordFinalizeTmpTensors<float> &tempTensors, WelfordFinalizeTilingData &tiling)
{

    uint32_t minTmpSize = B32_LEN * 0x2;

    const uint32_t minTmpOutSize = B32_LEN * 0x2;

                                                                                                                                       ;

    const uint32_t expFactor = (stackBuffer.GetSize() - minTmpOutSize) / minTmpSize;
    tiling.computeLength = expFactor * B32_LEN;
    tiling.round = para.abLength / tiling.computeLength;
    tiling.tail = para.abLength % tiling.computeLength;

    tempTensors.tempOutputMean = stackBuffer;
    tempTensors.tempOutputVariance = stackBuffer[B32_LEN];
    tempTensors.tempMean = stackBuffer[minTmpOutSize];
    tempTensors.tempVariance = stackBuffer[minTmpOutSize + tiling.computeLength];
}

[aicore] inline void welfordFinalizeCommonCheck(const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance,
                                                const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const WelfordFinalizePara &para)
{



      ;



      ;



      ;



      ;



      ;



      ;
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<uint8_t> &sharedTmpBuffer, const WelfordFinalizePara &para)
{

              ;
    welfordFinalizeCommonCheck(inputMean, inputVariance, outputMean, outputVariance, para);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    WelfordFinalizeTmpTensors<float> tempTensors;
    WelfordFinalizeTilingData tiling;
    GetWelfordFinalizeTensorInfo<isReuseSource>(stackBuffer, para, tempTensors, tiling);

    SetMaskCount();
    WelfordFinalizeComputeImpl<isReuseSource>(inputMean, inputVariance, outputMean, outputVariance, tempTensors, para, tiling);

    SetMaskNorm();
    ResetMask();
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, const LocalTensor<uint8_t> &sharedTmpBuffer, const WelfordFinalizePara &para)
{

              ;



      ;
    welfordFinalizeCommonCheck(inputMean, inputVariance, outputMean, outputVariance, para);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    WelfordFinalizeTmpTensors<float> tempTensors;
    WelfordFinalizeTilingData tiling;
    GetWelfordFinalizeTensorInfo<isReuseSource>(stackBuffer, para, tempTensors, tiling);

    SetMaskCount();
    WelfordFinalizeComputeImpl<isReuseSource>(inputMean, inputVariance, outputMean, outputVariance, counts, tempTensors, para, tiling);

    SetMaskNorm();
    ResetMask();
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance,
                                        const LocalTensor<float> &inputMean, const LocalTensor<float> &inputVariance, const WelfordFinalizePara &para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, sharedTmpBuffer, para);
}

template <bool isReuseSource = false>
[aicore] inline void WelfordFinalizeImpl(const LocalTensor<float> &outputMean, const LocalTensor<float> &outputVariance, const LocalTensor<float> &inputMean,
                                        const LocalTensor<float> &inputVariance, const LocalTensor<int32_t> &counts, const WelfordFinalizePara &para)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, counts, sharedTmpBuffer, para);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h" 2



namespace AscendC
{
#pragma begin_pipe(V)
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h"
template <bool isReuseSource = false>
[aicore] inline void WelfordFinalize(const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance,
    const LocalTensor<uint8_t>& sharedTmpBuffer, WelfordFinalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, sharedTmpBuffer, para);
}
# 61 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h"
template <bool isReuseSource = false>
[aicore] inline void WelfordFinalize(const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<int32_t>& counts,
    const LocalTensor<uint8_t>& sharedTmpBuffer, WelfordFinalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordFinalizeImpl<isReuseSource>(
        outputMean, outputVariance, inputMean, inputVariance, counts, sharedTmpBuffer, para);
}
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h"
template <bool isReuseSource = false>
[aicore] inline void WelfordFinalize(const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, WelfordFinalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, para);
}
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/welfordfinalize.h"
template <bool isReuseSource = false>
[aicore] inline void WelfordFinalize(const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& inputMean, const LocalTensor<float>& inputVariance, const LocalTensor<int32_t>& counts,
    WelfordFinalizePara& para)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    WelfordFinalizeImpl<isReuseSource>(outputMean, outputVariance, inputMean, inputVariance, counts, para);
}
#pragma end_pipe
}
# 69 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/reduce/sum_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/reduce/sum_utils.h"
namespace AscendC {
struct SumParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/sum/sum_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/sum/sum_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/sum/sum_common_impl.h" 2


namespace AscendC {

template <typename T>
[aicore] inline void SumForOneRepeatTime(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    SetVectorMask<T>(0, sumParams.n);
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SumCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                                  ;

    uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t firstRepeatTimes = (sumParams.n + elementNumPerRep - 1) / elementNumPerRep;
    SetMaskCount();
    if (firstRepeatTimes == 1) {
        return SumForOneRepeatTime(dstTensor, srcTensor, sumParams);
    }
    uint32_t totalCnt = 1;
    uint32_t dataSize = firstRepeatTimes;
    while (dataSize > 1) {
        ++totalCnt;
        dataSize = (dataSize + elementNumPerRep - 1) / elementNumPerRep;
    }
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < sumParams.outter; ++row) {
        uint32_t cnt = totalCnt;
        uint64_t lowMask = sumParams.n;
        SetVectorMask<T>(0, lowMask);
        RepeatReduceSum<T, false>(tmpTensor, srcTensor[row * sumParams.inner], 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

        PipeBarrier<PIPE_V>();
        lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
        --cnt;
        while (cnt != 0) {
            SetVectorMask<T>(0, lowMask);
            if (cnt == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(tmpTensor, tmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                    DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
            lowMask = (lowMask + elementNumPerRep - 1) / elementNumPerRep;
            --cnt;
        }
    }
    SetMaskNorm();

}

}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h" 2





namespace AscendC {
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h"
#pragma begin_pipe(V)
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SumCompute<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);
}
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/sum.h"
template <typename T, int32_t reduceDim = -1, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void Sum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const SumParams &sumParams)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Sum<T, reduceDim, isReuseSource, isBasicBlock>(dstTensor, srcTensor, sharedTmpBuffer, sumParams);

}
#pragma end_pipe
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/silu.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/silu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/silu/silu_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/silu/silu_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/silu/silu_common_impl.h" 2





namespace AscendC {
template <typename T>
[aicore] inline void SiluCalcSimplified(const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr,
    uint32_t repeatTimes)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Muls<T, false>(dstAddr, srcAddr, T(-1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, 1.0, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline __attribute__((inout_pipe("V"))) void SiluCompute(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{
                                                                                      ;
# 60 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/silu/silu_common_impl.h"
    SetMaskCount();
    SetVectorMask<T>(0, dataSize);
    SiluCalcSimplified<T>(dstLocal, srcLocal, 1);
    SetMaskNorm();
# 96 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/silu/silu_common_impl.h"
    ResetMask();
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/silu.h" 2

namespace AscendC {
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/silu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline __attribute__((inout_pipe("V"))) void Silu(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    uint32_t dataSize)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SiluCompute<T, isReuseSource>(dstLocal, srcLocal, dataSize);
}

}
# 72 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/gelu/gelu_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/gelu/gelu_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/gelu/gelu_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void GeluCalcTanhParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const T coefficientsA = 0.044715;
    const T coefficientsB = 1.5957691216057308;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Mul<T, false>(tempTensorA, srcLocal, srcLocal, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(tempTensorB, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorB, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void GeluCalcYGreaterThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;


    Mins<T, false>(tempTensorB, tempTensorA, 0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorB, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] inline void GeluCalcYLessThanZero(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& tempTensorB,
    const LocalTensor<T>& srcLocal, const GeluParams<T>& params)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, -1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, 1, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorA, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool highPerformance = false>
[aicore] inline void GeluCalcSimplifiedAvoid(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;



    GeluCalcTanhParams(tempTensorA, tempTensorB, srcLocal, params);


    GeluCalcYGreaterThanZero(tempTensorA, tempTensorB, params);


    GeluCalcYLessThanZero<T, highPerformance>(tempTensorA, tempTensorB, srcLocal, params);


    Mul<T, false>(dstLocal, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] inline void FastGeluCalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const LocalTensor<T>& stackBuffer = params.tempTensorA;


    const T coefficients = -1.702;


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(stackBuffer, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(stackBuffer, stackBuffer, 1.0, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();


    const BinaryRepeatParams binaryParams;
    if constexpr (highPerformance) {
        Reciprocal<T, false>(stackBuffer, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(dstLocal, srcLocal, stackBuffer, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] inline void FastGeluV2ClipParams(const LocalTensor<T>& tempTensorA, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficientsA = -0.1444;
    const T coefficientsB = -1.769;
    const T coefficientsBInv = 1.769;
    const T coefficientsC = 0.7071;
    const T coefficientsD = 0.5;

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    Muls<T, false>(tempTensorA, srcLocal, coefficientsC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mins<T, false>(tempTensorA, tempTensorA, coefficientsBInv, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(tempTensorA, tempTensorA, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Muls<T, false>(tempTensorA, tempTensorA, coefficientsA, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsD, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPerformance = false>
[aicore] inline void FastGeluV2CalcSimplified(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const GeluParams<T>& params)
{
    const T coefficients = 0.000000000001;
    const T coefficientsHalf = 0.5;
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;
    const LocalTensor<T>& tempTensorA = params.tempTensorA;
    const LocalTensor<T>& tempTensorB = params.tempTensorB;
    const LocalTensor<T>& tempTensorC = params.tempTensorC;


    FastGeluV2ClipParams(tempTensorA, srcLocal, params);


    Adds<T, false>(tempTensorB, srcLocal, coefficients, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Abs<T, false>(tempTensorC, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    if constexpr (highPerformance) {
        Reciprocal<T, false>(tempTensorC, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
        PipeBarrier<PIPE_V>();

        Mul<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        Div<T, false>(tempTensorB, tempTensorB, tempTensorC, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<T, false>(tempTensorA, tempTensorA, tempTensorB, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tempTensorA, tempTensorA, coefficientsHalf, MASK_PLACEHOLDER, params.repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<T, false>(dstLocal, srcLocal, tempTensorA, MASK_PLACEHOLDER, params.repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool highPrecision = false, uint32_t bufferNumber = 1>
[aicore] inline void GeluFormulasTmpCalc(GeluParams<T>& params)
{
    uint32_t needConvBuffer = bufferNumber;
    if constexpr (highPrecision) {
        needConvBuffer += 1;
    }

    params.tempTensorA = params.sharedTmpBuffer;
    params.stackSize = params.tmpBufferSize / needConvBuffer / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                       ;

    uint32_t nextTmpPos = params.stackSize;
    if constexpr (bufferNumber == TWO_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (bufferNumber >= THREE_OF_STACK_BUFFER) {
        params.tempTensorB = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
        params.tempTensorC = params.sharedTmpBuffer[nextTmpPos];
        nextTmpPos += params.stackSize;
    }

    if constexpr (highPrecision) {
        params.tempTensorConv = params.sharedTmpBuffer[nextTmpPos];
    }
}

[aicore] inline void GeluCastIntrinsicsImpl(const LocalTensor<float>& dstLocal, const LocalTensor<half>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<float, half, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GeluCastIntrinsicsImpl(const LocalTensor<half>& dstLocal, const LocalTensor<float>& srcLocal)
{
    UnaryRepeatParams unaryParams;
    unaryParams.dstRepStride = DEFAULT_REPEAT_STRIDE / sizeof(half);
    Cast<half, float, false>(dstLocal, srcLocal, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

template <uint32_t bufferNumber = 1>
[aicore] inline void GeluFormulasHighPrecision(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    GeluParams<float>& params,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluFormulasTmpCalc<float, true, bufferNumber>(params);

    const LocalTensor<float>& stackBufferConv = params.tempTensorConv;

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);

        GeluCastIntrinsicsImpl(stackBufferConv, srcLocal[offset]);

        func(stackBufferConv, stackBufferConv, params);

        GeluCastIntrinsicsImpl(dstLocal[offset], stackBufferConv);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] inline void GeluFormulas(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    GeluParams<T>& params, void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluFormulasTmpCalc<T, false, bufferNumber>(params);

    const uint32_t round = params.dataSize / params.stackSize;
    const uint32_t tail = params.dataSize % params.stackSize;

    SetMaskCount();
    SetVectorMask<uint8_t, MaskMode::COUNTER>(0, params.stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        func(dstLocal[offset], srcLocal[offset], params);
        offset = offset + params.stackSize;
    }

    if (tail != 0) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(0, tail);
        func(dstLocal[offset], srcLocal[offset], params);
    }

    SetMaskNorm();
    ResetMask();
}

template <uint32_t bufferNumber = 1>
[aicore] inline void GeluClass(const LocalTensor<half>& dstLocal, const LocalTensor<half>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<float>&, const LocalTensor<float>&, const GeluParams<float>&))
{
    GeluParams<float> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);

                                                                                                               ;
    GeluFormulasHighPrecision<bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, uint32_t bufferNumber = 1>
[aicore] inline void GeluClass(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize,
    void (*func)(const LocalTensor<T>&, const LocalTensor<T>&, const GeluParams<T>&))
{
    GeluParams<T> params;
    params.dataSize = dataSize;
    params.sharedTmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
    params.tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(T);

                                                                                                               ;
    GeluFormulas<T, bufferNumber>(dstLocal, srcLocal, params, func);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
                                                                                                                        ;
    if constexpr (highPrecision && (sizeof(T) == sizeof(half))) {
        GeluClass<TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<float, highPerformance>);
    } else {
        GeluClass<T, TWO_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            GeluCalcSimplifiedAvoid<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void GeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
                                                                                                                              ;
    if constexpr (highPrecision && (sizeof(T) == sizeof(half))) {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<float, highPerformance>);
    } else {
        GeluClass(dstLocal, srcLocal, sharedTmpBuffer, dataSize, FastGeluCalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluImpl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
                                                                                                                                ;
    if constexpr (highPrecision && (IsSameType<T, half>::value)) {
        GeluClass<THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<float, highPerformance>);
    } else {
        GeluClass<T, THREE_OF_STACK_BUFFER>(dstLocal, srcLocal, sharedTmpBuffer, dataSize,
            FastGeluV2CalcSimplified<T, highPerformance>);
    }
}

template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluV2Impl(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
#pragma end_pipe
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 31 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void Gelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    GeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 86 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGelu(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    FasterGeluImpl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
# 106 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, sharedTmpBuffer, dataSize);
}
# 127 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/gelu.h"
template <typename T, bool highPrecision = false, bool highPerformance = false>
[aicore] inline void FasterGeluV2(const LocalTensor<T>& dstLocal, const LocalTensor<T>& srcLocal,
    const uint32_t dataSize)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                         ;
    FasterGeluV2Impl<T, highPrecision, highPerformance>(dstLocal, srcLocal, dataSize);
}
#pragma end_pipe
}
# 73 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_quant_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_quant_utils.h"
namespace AscendC {
struct AscendQuantConfig {
    [aicore] constexpr AscendQuantConfig(const uint32_t calcCount, const uint32_t offsetCount,
        const uint32_t scaleCount, const uint32_t workLocalSize): calcCount(calcCount), offsetCount(offsetCount),
        scaleCount(scaleCount), workLocalSize(workLocalSize) {}
    uint32_t calcCount = 0;
    uint32_t offsetCount = 0;
    uint32_t scaleCount = 0;
    uint32_t workLocalSize = 0;
};

constexpr AscendQuantConfig ASCEND_QUANT_DEFAULT_CFG = {0, 0, 0, 0};
};
# 26 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_v220_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_pre_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_pre_impl.h"
namespace AscendC {

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void IsQuantValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{



      ;



      ;



      ;
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void IsQuantConfigValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor)
{



      ;



      ;




      ;




      ;



      ;
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void IsQuantConfigValid(const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor);




      ;




      ;
}


[aicore] inline void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackTensor, stackTensor, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] inline void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const half offset)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackTensor, stackTensor, static_cast<half>(offset), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] inline void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const LocalTensor<half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(stackTensor, stackTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}
[aicore] inline void AscendQuantPerChannelIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackTensor, const LocalTensor<half>& scaleTensor,
    const LocalTensor<half>& offsetTensor)
{
    BinaryRepeatParams binaryParam;
    UnaryRepeatParams f162s8Param;
    UnaryRepeatParams unaryParams;
    f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Mul<half, false>(stackTensor, srcTensor, scaleTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Add<half, false>(stackTensor, stackTensor, offsetTensor, MASK_PLACEHOLDER, 1, binaryParam);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Param);
    PipeBarrier<PIPE_V>();
}


template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<half>& scaleTensor, const LocalTensor<half>& offsetTensor, const uint32_t calCount)
{
    LocalTensor<half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<half>();
    if constexpr(config.workLocalSize != 0 && config.scaleCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        constexpr uint32_t loopCount = config.scaleCount / splitSize;
        constexpr uint32_t calcTail = config.scaleCount % splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
                tmpBuffer, scaleTensor[i * splitSize], offsetTensor[i * splitSize]);
        }
        if constexpr(calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
                tmpBuffer, scaleTensor[loopCount * splitSize], offsetTensor[loopCount * splitSize]);
        }
        return;
    }

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


      ;
    uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[i * splitSize],
            offsetTensor[i * splitSize]);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[loopCount * splitSize],
            offsetTensor[loopCount * splitSize]);
    }
}
template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantPerChannelImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LocalTensor<half>& scaleTensor, const half offset, const uint32_t calCount)
{
    LocalTensor<half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<half>();
    if constexpr(config.workLocalSize != 0 && config.scaleCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                       ;
        constexpr uint32_t calcTail = config.scaleCount % splitSize;
        constexpr uint32_t loopCount = config.scaleCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);

        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize], tmpBuffer,
                scaleTensor[i * splitSize], offset);
        }
        if constexpr(calcTail > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
            AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize],
                srcTensor[loopCount * splitSize], tmpBuffer, scaleTensor[loopCount * splitSize], offset);
        }
        return;
    }

    uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                                   ;
    SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);

    uint32_t calcTail = calCount % splitSize;
    uint32_t loopCount = calCount / splitSize;
    for (uint32_t i = 0; i < loopCount; ++i) {
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[i * splitSize], srcTensor[i * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[i * splitSize], offset);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        AscendQuantPerChannelIntrinsicsImpl(dstTensor[loopCount * splitSize], srcTensor[loopCount * splitSize],
            sharedTmpBuffer.ReinterpretCast<half>(), scaleTensor[loopCount * splitSize], offset);
    }
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImplStatic(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset)
{
    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        constexpr uint32_t N = config.calcCount / config.scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            SetVectorMask<half, MaskMode::COUNTER>(0, config.scaleCount);
            Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, halfScaleTensor, static_cast<half>(offset), config.scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, scaleTensor, static_cast<half>(offset), config.scaleCount);
            }
        }
    }
}

template <typename T, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImplStatic(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        constexpr uint32_t N = config.calcCount / config.scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            SetVectorMask<half, MaskMode::COUNTER>(0, config.scaleCount);
            LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            LocalTensor<half> halfOffsetTensor = offsetTensor.template ReinterpretCast<half>();
            Cast<half, float, false>(halfOffsetTensor, offsetTensor, RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, f162s8Param);

            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, halfScaleTensor, halfOffsetTensor, config.scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * config.scaleCount], srcTensor[i * config.scaleCount],
                    sharedTmpBuffer, scaleTensor, offsetTensor, config.scaleCount);
            }
        }
    }
}



[aicore] inline void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<half>& srcTensor, const LocalTensor<half>& stackBuffer, half scale, half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Muls<half, false>(stackBuffer, srcTensor, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}
[aicore] inline void AscendQuantIntrinsicsImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<half>& stackBuffer, half scale, half offset)
{
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams f162s8Params;
    f162s8Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    Cast<half, float, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
    Muls<half, false>(stackBuffer, stackBuffer, scale, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<half, false>(stackBuffer, stackBuffer, offset, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<int8_t, half, false>(dstTensor, stackBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, f162s8Params);
    PipeBarrier<PIPE_V>();
}


template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    IsQuantValid<T, config>(srcTensor, sharedTmpBuffer, calCount);

    SetMaskCount();
    LocalTensor<half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<half>();
    if constexpr(config.workLocalSize != 0 && config.calcCount != 0) {
        constexpr uint32_t splitSize = config.workLocalSize / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        constexpr uint32_t loopCount = config.calcCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * i], srcTensor[splitSize * i],
                tmpBuffer, static_cast<half>(scale), static_cast<half>(offset));
        }
        if constexpr(config.calcCount % splitSize > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, config.calcCount % splitSize);
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * loopCount], srcTensor[splitSize * loopCount],
                tmpBuffer, static_cast<half>(scale), static_cast<half>(offset));
        }
    } else {
        uint32_t splitSize = sharedTmpBuffer.GetSize() / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;


          ;
        uint32_t loopCount = calCount / splitSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, splitSize);
        for (uint32_t i = 0; i < loopCount; ++i) {
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * i], srcTensor[splitSize * i],
                sharedTmpBuffer.ReinterpretCast<half>(), static_cast<half>(scale), static_cast<half>(offset));
        }
        if (calCount % splitSize > 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, calCount % splitSize);
            AscendQuantIntrinsicsImpl(dstTensor[splitSize * loopCount], srcTensor[splitSize * loopCount],
                sharedTmpBuffer.ReinterpretCast<half>(),
                static_cast<half>(scale), static_cast<half>(offset));
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    IsQuantParamValid(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset, scaleCount, calCount);
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor);
    SetMaskCount();

    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        AscendQuantImplStatic<T, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset);
    } else {
        uint32_t N = calCount / scaleCount;
        if constexpr (IsSameType<T, float>::value) {

            LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
            SetVectorMask<half, MaskMode::COUNTER>(0, scaleCount);
            Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);
            PipeBarrier<PIPE_V>();
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, halfScaleTensor, static_cast<half>(offset), scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, scaleTensor, static_cast<half>(offset), scaleCount);
            }
        }
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantCalc(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    IsQuantParamValid(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
    IsQuantConfigValid<T, config>(srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor);
    SetMaskCount();

    if constexpr(config.scaleCount != 0 && config.calcCount != 0) {
        AscendQuantImplStatic<T, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor);
    } else {
        uint32_t N = calCount / scaleCount;
        if constexpr (IsSameType<T, float>::value) {
            SetVectorMask<half, MaskMode::COUNTER>(0, scaleCount);
            UnaryRepeatParams f162s8Param;
            f162s8Param.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;

            LocalTensor<half> halfScaleTensor = scaleTensor.template ReinterpretCast<half>();
            Cast<half, float, false>(halfScaleTensor, scaleTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                f162s8Param);

            LocalTensor<half> halfOffsetTensor = offsetTensor.template ReinterpretCast<half>();
            Cast<half, float, false>(halfOffsetTensor, offsetTensor, RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, 1, f162s8Param);

            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, halfScaleTensor, halfOffsetTensor, scaleCount);
            }
        } else {
            for (uint32_t i = 0; i < N; ++i) {
                AscendQuantPerChannelImpl<T, config>(dstTensor[i * scaleCount], srcTensor[i * scaleCount],
                    sharedTmpBuffer, scaleTensor, offsetTensor, scaleCount);
            }
        }
    }

    SetMaskNorm();
    ResetMask();
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_v220_impl.h" 2


namespace AscendC {

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                         ;
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                           ;
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor,
        offset, scaleCount, calCount);
}

template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                              ;
    AscendQuantCalc<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor,
        offsetTensor, scaleCount, offsetCount, calCount);
}

}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/quant/ascend_quant_common_impl.h" 2






namespace AscendC {
template<typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset, uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scale, offset, calCount);
}

template<typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scaleTensor, offset, scaleCount, calCount);
}

template<typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuantImpl(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    LocalTensor<uint8_t> stackTensor;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);

                                                                          ;
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, stackTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}

template <typename T>
[aicore] inline void IsQuantParamValid(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{



      ;



      ;



      ;



      ;



      ;
}
template <typename T>
[aicore] inline void IsQuantParamValid(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T& offset, const uint32_t scaleCount, const uint32_t calCount)
{



      ;



      ;



      ;



      ;
}
}
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, calCount);
}
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const float scale, const float offset, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scale, offset, calCount);
}
# 91 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scale, offset, srcTensor.GetSize());
}
# 109 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const float scale, const float offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scale, offset, srcTensor.GetSize());
}
# 134 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleCount, calCount);
}
# 156 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const T offset, const uint32_t scaleCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offset, scaleCount, calCount);
}
# 179 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 198 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const T offset)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offset,
        scaleTensor.GetSize(), srcTensor.GetSize());
}
# 225 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor, const uint32_t scaleCount, const uint32_t offsetCount,
    const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 249 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor, const uint32_t scaleCount,
    const uint32_t offsetCount, const uint32_t calCount)
{
    AscendQuantImpl<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleCount, offsetCount, calCount);
}
# 274 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const LocalTensor<T>& scaleTensor,
    const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, sharedTmpBuffer, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}
# 294 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_quant.h"
template <typename T, bool isReuseSource = false, const AscendQuantConfig& config = ASCEND_QUANT_DEFAULT_CFG>
[aicore] inline void AscendQuant(const LocalTensor<int8_t>& dstTensor,
    const LocalTensor<T>& srcTensor, const LocalTensor<T>& scaleTensor, const LocalTensor<T>& offsetTensor)
{
    AscendQuant<T, isReuseSource, config>(dstTensor, srcTensor, scaleTensor, offsetTensor,
        scaleTensor.GetSize(), offsetTensor.GetSize(), srcTensor.GetSize());
}
#pragma end_pipe
}
# 74 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_dequant_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_dequant_utils.h"
namespace AscendC {
struct DequantParams {
    uint32_t m;
    uint32_t n;
    uint32_t calCount;
};

};
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/dequant/ascend_dequant_common_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/dequant/ascend_dequant_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/dequant/ascend_dequant_common.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/dequant/ascend_dequant_common.h"
namespace AscendC {
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/dequant/ascend_dequant_common_impl.h" 2


namespace AscendC {
constexpr uint32_t FLOAT_PER_BLOCK = 8;
constexpr uint32_t FLOAT_PER_REPEAT = 64;

[aicore] inline bool IsCalCountValid(const LocalTensor<int32_t>& srcTensor, uint32_t calCount)
{



      ;
    return true;
}


template <typename scaleT>
[aicore] inline bool IsWithoutDequantParamsValid(const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{



                                                                            ;

    if constexpr(IsSameType<scaleT, uint64_t>::value) {


                                                       ;
    }
    return true;
}


template <typename dstT>
[aicore] inline bool IsDequantParamsValid(const LocalTensor<int32_t>& srcTensor, const LocalTensor<dstT>& dstTensor,
    DequantParams& params)
{


                                                           ;


                                                                                                          ;


                                                                                                          ;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t alignInner = (params.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;

                                                                                      ;

    return true;
}


template <typename scaleT>
[aicore] inline bool IsDeqscaleTensorValid(const LocalTensor<scaleT>& deqScale, DequantParams& params)
{


                                                                                      ;
    return true;
}


template <typename dstT, typename scaleT, bool isTensor>
[aicore] inline constexpr bool IsTemplateValid()
{
    if constexpr(isTensor) {


        constexpr bool isValid1 = (IsSameType<scaleT, uint64_t>::value) && (IsSameType<dstT, half>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);



        constexpr bool isValid3 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid5 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4 || isValid5;

    } else {






        constexpr bool isValid1 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, bfloat16_t>::value);
        constexpr bool isValid2 = (IsSameType<scaleT, bfloat16_t>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid3 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, float>::value);
        constexpr bool isValid4 = (IsSameType<scaleT, float>::value) && (IsSameType<dstT, bfloat16_t>::value);
        return isValid1 || isValid2 || isValid3 || isValid4;

    }
}


template <typename scaleT>
[aicore] inline void AscendDequantTmpCalc(const LocalTensor<float>& stackBuffer, DequantParams& dqParams,
    AscendDequantParams<float>& params, uint32_t srcSize, uint32_t deqScaleSize)
{
    uint32_t base = dqParams.n;

    deqScaleSize = (deqScaleSize + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    uint32_t tmpSrcSize = (stackBuffer.GetSize() - deqScaleSize) / base * base;
                                                                                                           ;
    tmpSrcSize = (tmpSrcSize > srcSize) ? srcSize : tmpSrcSize;
    params.tmpSize = tmpSrcSize;
    params.tmpAddrA = stackBuffer;
    params.tmpAddrB = stackBuffer[deqScaleSize];
}


template <typename scaleT>
[aicore] inline void AscendDequantTmpCalc(const LocalTensor<int32_t>& srcTensor, const scaleT deqScale,
    const LocalTensor<float>& stackBuffer, DequantParams& dqParams, AscendDequantParams<float>& params)
{
    uint32_t srcSize = dqParams.m * dqParams.n;
    uint32_t deqScaleSize = (dqParams.calCount + FLOAT_PER_BLOCK - 1) / FLOAT_PER_BLOCK * FLOAT_PER_BLOCK;

    AscendDequantTmpCalc<scaleT>(stackBuffer, dqParams, params, srcSize, deqScaleSize);

    if constexpr(IsSameType<scaleT, float>::value) {
        Duplicate<float>(params.tmpAddrA, deqScale, static_cast<int32_t>(dqParams.calCount));
    } else {
        Duplicate<float>(params.tmpAddrA, ToFloat(deqScale), static_cast<int32_t>(dqParams.calCount));
    }
    PipeBarrier<PIPE_V>();
}



template <typename dstT>
[aicore] inline RoundMode GetFP32CastMode()
{



    constexpr RoundMode castMode = IsSameType<dstT, bfloat16_t>::value ? RoundMode::CAST_RINT: RoundMode::CAST_NONE;
    return castMode;

}


template <typename dstT, DeQuantMode mode>
[aicore] inline void UpdateDequantParams(DequantParams& params)
{
    if constexpr(mode == DeQuantMode::DEQUANT_WITH_SINGLE_ROW) {
        constexpr uint32_t ONE_BLK_SIZE = 32;
        uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
        bool isCalCountAlign = (params.calCount % oneBlockNum == 0);
        bool isNDivisible = (params.n % params.calCount == 0);



        if (params.m == 1 && isCalCountAlign && isNDivisible) {
            params.m = params.n / params.calCount;
            params.n = params.calCount;
        }
    }
}



template <typename scaleT>
[aicore] inline void CastDeqscale(const LocalTensor<scaleT>& deqScale, AscendDequantParams<float>& params,
    uint32_t scaleSize)
{
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = IsSameType<scaleT, float>::value ? DEFAULT_REPEAT_STRIDE: HALF_DEFAULT_REPEAT_STRIDE;

    if constexpr(IsSameType<scaleT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Adds<float, false>(params.tmpAddrA, deqScale, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else if constexpr(IsSameType<scaleT, uint64_t>::value) {

        LocalTensor<float> deqScaleFP32 = deqScale.template ReinterpretCast<float>();


        GatherMaskParams reducev2Params;
        reducev2Params.repeatTimes = 1;
        uint64_t rsvdCnt = 0;
        GatherMask<float>(params.tmpAddrA, deqScaleFP32, 1, true, scaleSize * 2, reducev2Params, rsvdCnt);
        PipeBarrier<PIPE_V>();
        SetMaskCount();
    } else {
        SetVectorMask<float, MaskMode::COUNTER>(0, scaleSize);
        Cast<float, scaleT, false>(params.tmpAddrA, deqScale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void CastSrc(const LocalTensor<int32_t>& srcTensor, const LocalTensor<float>& dstTensor,
    UnaryRepeatParams& unaryParams, uint64_t counter)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, counter);
    Cast<float, int32_t, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}



[aicore] inline void DequantMul(const LocalTensor<float>& srcTensor, const LocalTensor<float>& deqScaleTensor,
    const LocalTensor<float>& dstTensor, BinaryRepeatParams& binaryParams, DequantParams& dqParams, uint32_t k,
    uint32_t loopCount, uint32_t tail)
{
    if (k == 0) {
        return;
    }


    if (dqParams.n > MAX_REPEAT_TIMES * FLOAT_PER_BLOCK) {
        BinaryRepeatParams binaryParamsDefault;
        SetVectorMask<float, MaskMode::COUNTER>(0, dqParams.calCount);
        for (uint32_t i = 0; i < k; i++) {
            Mul<float, false>(dstTensor[i * dqParams.n], srcTensor[i * dqParams.n], deqScaleTensor, MASK_PLACEHOLDER, 1,
                binaryParamsDefault);
        }
        PipeBarrier<PIPE_V>();
        return;
    }

    SetVectorMask<float, MaskMode::COUNTER>(0, FLOAT_PER_REPEAT * k);
    for (uint32_t i = 0; i < loopCount; i++) {
        Mul<float, false>(dstTensor[i * FLOAT_PER_REPEAT], srcTensor[i * FLOAT_PER_REPEAT],
            deqScaleTensor[i * FLOAT_PER_REPEAT], MASK_PLACEHOLDER, 1, binaryParams);
    }
    PipeBarrier<PIPE_V>();

    if (tail != 0) {
        SetMaskNorm();

        uint32_t kTimes = k / MAX_REPEAT_TIMES;
        uint32_t kRemains = k % MAX_REPEAT_TIMES;
        SetVectorMask<float, MaskMode::NORMAL>(0, ((uint64_t)1 << tail) - 1);

        uint32_t baseIndex = loopCount * FLOAT_PER_REPEAT;
        for (uint32_t i = 0; i < kTimes; i++) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * i * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER,
                MAX_REPEAT_TIMES, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        if (kRemains > 0) {
            uint32_t index = baseIndex + MAX_REPEAT_TIMES * kTimes * dqParams.n;
            Mul<float, false>(dstTensor[index], srcTensor[index], deqScaleTensor[baseIndex], MASK_PLACEHOLDER, kRemains,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT>
[aicore] inline void CastDst(const LocalTensor<dstT>& dstTensor, const LocalTensor<float>& srcFP32,
    UnaryRepeatParams& unaryParams, uint32_t srcInner, uint32_t dstInner, uint32_t dataNum)
{
    if constexpr(IsSameType<dstT, float>::value) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Adds<float, false>(dstTensor, srcFP32, 0, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        return;
    }

    RoundMode castMode = GetFP32CastMode<dstT>();
    if (srcInner == dstInner) {
        SetVectorMask<float, MaskMode::COUNTER>(0, dataNum);
        Cast<dstT, float, false>(dstTensor, srcFP32, castMode, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    } else {
        uint32_t loopNum = dataNum / srcInner;
        uint32_t tailPart = dataNum % srcInner;
        SetVectorMask<float, MaskMode::COUNTER>(0, srcInner);
        for (uint32_t i = 0; i < loopNum; i++) {
            Cast<dstT, float, false>(dstTensor[i * dstInner], srcFP32[i * srcInner], castMode, MASK_PLACEHOLDER, 1,
                unaryParams);
        }
        PipeBarrier<PIPE_V>();

        if (tailPart > 0) {
            SetVectorMask<float, MaskMode::COUNTER>(0, tailPart);
            Cast<dstT, float, false>(dstTensor[loopNum * dstInner], srcFP32[loopNum * srcInner], castMode,
                MASK_PLACEHOLDER, 1, unaryParams);
            PipeBarrier<PIPE_V>();
        }
    }
}


template <typename dstT, typename scaleT, bool isPureDqParams = false>
[aicore] inline void CalculateByInner(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams& dqParams, AscendDequantParams<float>& ascendDqParams,
    uint32_t calCount)
{
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;
    LocalTensor<float> srcFP32 = ascendDqParams.tmpAddrB;

    uint32_t oneBlockNum = ONE_BLK_SIZE / sizeof(dstT);
    uint32_t dstInner = (dqParams.n + oneBlockNum - 1) / oneBlockNum * oneBlockNum;
    uint32_t tmpSize = ascendDqParams.tmpSize;
    uint32_t loopCount = calCount / tmpSize;
    uint32_t tailSize = calCount % tmpSize;
    uint32_t k = tmpSize / dqParams.n;

    uint32_t mainBlockLoopCount = dqParams.calCount / FLOAT_PER_REPEAT;
    uint32_t mainBlockTail = dqParams.calCount % FLOAT_PER_REPEAT;
    BinaryRepeatParams binaryParams;
    BinaryRepeatParams binaryParamsMul(1, 1, 1, dqParams.n / FLOAT_PER_BLOCK, dqParams.n / FLOAT_PER_BLOCK, 0);
    UnaryRepeatParams unaryParams;
    UnaryRepeatParams unaryParamsDst;
    if constexpr(!IsSameType<dstT, float>::value) {
        unaryParamsDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    }

    CastDeqscale(deqScale, ascendDqParams, dqParams.calCount);


    uint32_t castDstIndex = (dqParams.n == dstInner) ? tmpSize : k * dstInner;
    for (uint32_t i = 0; i < loopCount; i++) {
        SetMaskCount();
        CastSrc(srcTensor[i * tmpSize], srcFP32, unaryParams, tmpSize);

        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        SetMaskCount();
        CastDst<dstT>(dstTensor[i * castDstIndex], srcFP32, unaryParamsDst, dqParams.n, dstInner, tmpSize);
    }


    if (tailSize > 0) {
        CastSrc(srcTensor[calCount - tailSize], srcFP32, unaryParams, tailSize);

        k = tailSize / dqParams.n;
        DequantMul(srcFP32, deqScaleFP32, srcFP32, binaryParamsMul, dqParams, k, mainBlockLoopCount, mainBlockTail);

        if constexpr(!isPureDqParams) {
            uint32_t tailK = tailSize % dqParams.n;
            if (tailK != 0) {
                SetMaskCount();
                SetVectorMask<float, MaskMode::COUNTER>(0, tailK);
                uint32_t idxMul = tailSize - tailK;
                Mul<float, false>(srcFP32[idxMul], srcFP32[idxMul], deqScaleFP32, MASK_PLACEHOLDER, 1, binaryParams);
                PipeBarrier<PIPE_V>();
            }
        }

        SetMaskCount();

        uint32_t index = (dqParams.n == dstInner) ? calCount - tailSize :
            (calCount - tailSize) / dqParams.n * dstInner;
        CastDst<dstT>(dstTensor[index], srcFP32, unaryParamsDst, dqParams.n, dstInner, tailSize);
    }
}



template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] inline void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams& params,
    uint32_t calCount)
{

                                                                            ;
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    static_assert(IsTemplateValid<dstT, scaleT, true>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");
    UpdateDequantParams<dstT, mode>(params);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(stackBuffer, params, ascendDqParams, params.m * params.n, params.calCount);

    SetMaskCount();
    CalculateByInner<dstT, scaleT, isPureDqParams>(dstTensor, srcTensor, deqScale, params, ascendDqParams, calCount);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantImpl(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (!IsCalCountValid(srcTensor, calCount) || !IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                ;
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    if (!IsWithoutDequantParamsValid<scaleT>(srcTensor, deqScale)) {
        return;
    }
    DequantParams params = {srcTensor.GetSize() / deqScale.GetSize(), deqScale.GetSize(), deqScale.GetSize()};
    AscendDequantImpl<dstT, scaleT, false, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        srcTensor.GetSize());
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantNoCalcountImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const LocalTensor<scaleT>& deqScale)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}


template <typename dstT, typename scaleT, bool isPureDqParams, DeQuantMode mode>
[aicore] inline void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer,
    DequantParams& params)
{

                                                                  ;

    static_assert(IsTemplateValid<dstT, scaleT, false>(),
        "current combination of deqScale dtype and dstTensor dtype is not supported, please check the document");

    UpdateDequantParams<dstT, mode>(params);

    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetMaskCount();
    AscendDequantParams<float> ascendDqParams;
    AscendDequantTmpCalc<scaleT>(srcTensor, deqScale, stackBuffer, params, ascendDqParams);
    LocalTensor<float> deqScaleFP32 = ascendDqParams.tmpAddrA;

    SetMaskCount();
    CalculateByInner<dstT, float, true>(dstTensor, srcTensor, deqScaleFP32, params, ascendDqParams,
        params.m * params.n);

    SetMaskNorm();
    ResetMask();
}

template <typename dstT, typename scaleT, DeQuantMode mode>
[aicore] inline void AscendDequantScalarImpl(const LocalTensor<dstT>& dstTensor,
    const LocalTensor<int32_t>& srcTensor, const scaleT deqScale, DequantParams& params)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    AscendDequantImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params,
        params.m * params.n);
}
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}
# 101 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, calCount);
}
# 123 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, calCount);
}
# 153 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer);
}
# 175 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const LocalTensor<scaleT>& deqScale)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantNoCalcountImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale);
}
# 206 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, const LocalTensor<uint8_t>& sharedTmpBuffer, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, true, mode>(dstTensor, srcTensor, deqScale, sharedTmpBuffer, params);
}
# 232 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_dequant.h"
template <typename dstT, typename scaleT, DeQuantMode mode = DeQuantMode::DEQUANT_WITH_SINGLE_ROW>
[aicore] inline void AscendDequant(const LocalTensor<dstT>& dstTensor, const LocalTensor<int32_t>& srcTensor,
    const scaleT deqScale, DequantParams params)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendDequantScalarImpl<dstT, scaleT, mode>(dstTensor, srcTensor, deqScale, params);
}

#pragma end_pipe
}
# 75 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_antiquant_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/quantization/ascend_antiquant_utils.h"
namespace AscendC {
struct AntiQuantShapeInfo {
    uint32_t offsetHeight{0};
    uint32_t offsetWidth{0};
    uint32_t scaleHeight{0};
    uint32_t scaleWidth{0};
};

};
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_common.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_common.h"
namespace AscendC {
constexpr uint32_t ANTIQUANT_TWO = 2;
constexpr uint32_t ANTIQUANT_FOUR = 4;
constexpr uint32_t ANTIQUANT_BRCB_BASE = 8;
constexpr uint32_t ANTIQUANT_MIN_METHOD2 = 80;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_BF16 = 64;
constexpr uint32_t ANTIQUANT_SINGLE_N_SIZE_FP16 = 128;
constexpr uint32_t ANTIQUANT_MAX_K = 255;
constexpr uint32_t MAX_K_FOR_FP16_BRCB = 4096;
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_impl.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_c220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_c220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_c220_impl.h" 2



namespace AscendC {

template <typename SrcType, typename OutType>
[aicore] inline void CheckApiDtypeValid()
{
    constexpr bool inputValid = (IsSameType<SrcType, int8_t>::value) || (IsSameType<SrcType, int4b_t>::value);
    constexpr bool outputValid = (IsSameType<OutType, half>::value) || (IsSameType<OutType, bfloat16_t>::value);

                                                                                                     ;
}

template <typename SrcType, bool withOffset = true>
[aicore] inline void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const UnaryRepeatParams &unaryParamsCastSrc,
    const UnaryRepeatParams &unaryParamsToFP32, const UnaryRepeatParams &unaryParamsFP32ToDst,
    const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    uint32_t offsetFp32Pos = calCount * sizeof(float);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<half>();
    auto offsetBuffer = sharedTmpBuffer[offsetFp32Pos].ReinterpretCast<float>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<half, SrcType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Cast<float, bfloat16_t, false>(offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            unaryParamsToFP32);
        PipeBarrier<PIPE_V>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Cast<float, bfloat16_t, false>(offsetBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParamsFP32ToDst);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] inline void AntiQuantInnerLoop(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const bfloat16_t offset, const bfloat16_t scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const UnaryRepeatParams &unaryParamsToFP32,
    const UnaryRepeatParams &unaryParamsFP32ToDst, const UnaryRepeatParams &unaryParamsScalar, const uint32_t calCount)
{
    uint32_t srcFp16Pos = calCount * sizeof(bfloat16_t);
    auto fp16TmpBuffer = sharedTmpBuffer[srcFp16Pos].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<half, SrcType, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsToFP32);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<float, false>(resultBuffer, resultBuffer, ToFloat(offset), MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<float, false>(resultBuffer, resultBuffer, ToFloat(scale), MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1, unaryParamsFP32ToDst);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType>
[aicore] inline void AscendAntiQuantNoTransposePerformance(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    uint32_t posOffsetScale = N * sizeof(float) * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    s42f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;

    for (uint32_t i = 0; i < N / ANTIQUANT_SINGLE_N_SIZE_BF16; i++) {
        SetMaskNorm();
        SetVectorMask<half, MaskMode::NORMAL>(ANTIQUANT_SINGLE_N_SIZE_BF16);
        if constexpr (IsSameType<SrcType, int4b_t>::value) {

            Cast<half, int4b_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s42f16unaryParams);
        } else {

            Cast<half, int8_t, false>(fp16TmpBuffer, src[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], RoundMode::CAST_NONE,
                MASK_PLACEHOLDER, K, s82f16unaryParams);
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_BF16 * K);
        Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, K, f162f32unaryParams);
        PipeBarrier<PIPE_V>();

        auto offsetBuffer = sharedTmpBuffer[ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        auto scaleBuffer = sharedTmpBuffer[N * sizeof(float) +
            ANTIQUANT_SINGLE_N_SIZE_BF16 * i * sizeof(float)].ReinterpretCast<float>();
        Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Cast<bfloat16_t, float, false>(dst[ANTIQUANT_SINGLE_N_SIZE_BF16 * i], resultBuffer,
            RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType>
[aicore] inline void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<bfloat16_t> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    uint32_t index = N / ANTIQUANT_SINGLE_N_SIZE_BF16 * ANTIQUANT_SINGLE_N_SIZE_BF16;
    uint32_t posOffset = N * sizeof(float);
    uint32_t posOffsetScale = posOffset * ANTIQUANT_TWO;
    uint32_t posCast = posOffsetScale + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(half);
    auto fp16TmpBuffer = sharedTmpBuffer[posCast].ReinterpretCast<half>();
    auto resultBuffer = sharedTmpBuffer[posOffsetScale].ReinterpretCast<float>();
    auto offsetBuffer = sharedTmpBuffer[index * sizeof(float)].ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset + index * sizeof(float)].ReinterpretCast<float>();

    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = N / ANTIQUANT_TWO / ONE_BLK_SIZE;
    UnaryRepeatParams s82f16unaryParams;
    s82f16unaryParams.srcRepStride = N * sizeof(int8_t) / ONE_BLK_SIZE;
    s82f16unaryParams.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f162f32unaryParams;
    f162f32unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    BinaryRepeatParams binaryParams;
    binaryParams.src1RepStride = 0;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = N * sizeof(bfloat16_t) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(mask);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        Cast<half, int4b_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s42f16unaryParams);
    } else {
        Cast<half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K, s82f16unaryParams);
    }
    PipeBarrier<PIPE_V>();


    Cast<float, half, false>(resultBuffer, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, K,
        f162f32unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(resultBuffer, resultBuffer, offsetBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(resultBuffer, resultBuffer, scaleBuffer, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<bfloat16_t, float, false>(dst, resultBuffer, RoundMode::CAST_RINT, MASK_PLACEHOLDER, K, f322f16Params);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename SrcType>
[aicore] inline void PreCast(const LocalTensor<bfloat16_t> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<bfloat16_t> &offset, const LocalTensor<bfloat16_t> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    uint32_t posOffset = offset.GetSize() * sizeof(float);
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE / sizeof(bfloat16_t);
    uint32_t repeatTimes =
        offset.GetSize() % repeatEle == 0 ? offset.GetSize() / repeatEle : offset.GetSize() / repeatEle + 1;
    auto offsetBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    auto scaleBuffer = sharedTmpBuffer[posOffset].ReinterpretCast<float>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, offset.GetSize());
    Cast<float, bfloat16_t, false>(offsetBuffer, offset, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();
    Cast<float, bfloat16_t, false>(scaleBuffer, scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
        unaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename DstType>
[aicore] inline bool AntiQuantCheckPerformanceMode(const LocalTensor<DstType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K)
{
    if constexpr (IsSameType<DstType, bfloat16_t>::value) {
        uint32_t maxTmpBufferSize =
            scale.GetSize() * ANTIQUANT_TWO * sizeof(float) + ANTIQUANT_SINGLE_N_SIZE_BF16 * K * sizeof(float);
        return sharedTmpBuffer.GetSize() >= maxTmpBufferSize;
    }
    return true;
}



template <typename SrcType, typename DstType, bool isOffset>
[aicore] inline void CalculationMax(const LocalTensor<SrcType> &src, const LocalTensor<DstType> &dst,
    AntiquantParams<float> &params, const uint32_t calCount, const uint32_t N, const uint32_t K, const uint32_t NOffset)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    auto fp16TmpBuffer = params.tempTensorInput[srcFp16Pos].ReinterpretCast<half>();

    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams f322f16Params;
    f322f16Params.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t count = K / ANTIQUANT_SINGLE_N_SIZE;



    BinaryRepeatParams binaryParams(1, 1, 0, count * DEFAULT_REPEAT_STRIDE, count * DEFAULT_REPEAT_STRIDE, 1);

    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);

    Cast<half, int8_t, false>(fp16TmpBuffer, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(params.tempTensorInput, fp16TmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);
    for (uint32_t i = 0; i < count; i++) {

        uint32_t curOffset = i * ANTIQUANT_SINGLE_N_SIZE;

        if constexpr (isOffset) {
            Add<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
                params.tempTensorOffset[NOffset], MASK_PLACEHOLDER, N, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<float, false>(params.tempTensorInput[curOffset], params.tempTensorInput[curOffset],
            params.tempTensorScale[NOffset], MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }


    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<bfloat16_t, float, false>(dst, params.tempTensorInput, RoundMode::CAST_RINT,
        MASK_PLACEHOLDER, 1, f322f16Params);
    PipeBarrier<PIPE_V>();
}


template <typename DstType>
[aicore] inline void GetAntiquantTensorInfo(const LocalTensor<DstType> &scale, const LocalTensor<float> &stackBuffer,
    AntiquantParams<float> &params)
{
    uint32_t N = scale.GetSize();
    params.tempTensorOffset = stackBuffer[0];
    params.tempTensorScale = stackBuffer[ANTIQUANT_BRCB_BASE * N];
    params.tempTensorInput = stackBuffer[ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO * N];
}



template <typename DstType, bool withOffset = true>
[aicore] inline void CastAndBrcb(const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale,
    AntiquantParams<float> &params, const UnaryRepeatParams &unaryParams, const uint32_t scaleEleNum)
{
    uint32_t offsetEleNum = offset.GetSize();
    const uint32_t alignBase = (ONE_BLK_SIZE / sizeof(float));
    uint32_t scaleAlign = (scaleEleNum + alignBase - 1) / alignBase * alignBase;
    uint32_t tensorIndex = ANTIQUANT_BRCB_BASE * offsetEleNum - offsetEleNum;


    SetVectorMask<half, MaskMode::COUNTER>(0, scaleEleNum);
    if constexpr (withOffset) {
        Cast<float, DstType, false>(params.tempTensorOffset[tensorIndex], offset, RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
    Cast<float, DstType, false>(params.tempTensorScale[tensorIndex], scale, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        unaryParams);
    PipeBarrier<PIPE_V>();

    constexpr uint16_t brcbDstBlkStride = 1;
    constexpr uint16_t brcbDstRepStride = ANTIQUANT_BRCB_BASE;
    uint8_t repeatTimes = (scaleEleNum + ANTIQUANT_BRCB_BASE - 1 ) / ANTIQUANT_BRCB_BASE;
    BrcbRepeatParams brcbParams(brcbDstBlkStride, brcbDstRepStride);

    SetMaskNorm();
    ResetMask();

    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, params.tempTensorOffset[tensorIndex], repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
    Brcb(params.tempTensorScale, params.tempTensorScale[tensorIndex], repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
}


template <typename SrcType>
[aicore] inline void CastSrcProcess(const LocalTensor<SrcType> &src, const LocalTensor<float> &srcFP32,
    const UnaryRepeatParams &src2Fp16Params, const UnaryRepeatParams &fp162Fp32Params, const uint32_t calCount,
    const uint32_t scaleEleNum, const uint32_t n)
{

    uint32_t srcFp16Pos = calCount / ANTIQUANT_TWO;
    auto fp16TmpBuf = srcFP32[srcFp16Pos].ReinterpretCast<half>();
    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(0, FULL_MASK);
    Cast<half, SrcType, false>(fp16TmpBuf, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, scaleEleNum, src2Fp16Params);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(srcFP32, fp16TmpBuf, RoundMode::CAST_NONE, MASK_PLACEHOLDER, scaleEleNum, fp162Fp32Params);
    PipeBarrier<PIPE_V>();
}


template <bool withOffset>
[aicore] inline void AddMulProcess(const LocalTensor<float> &srcFP32, const LocalTensor<float> &offsetFP32,
    const LocalTensor<float> &scaleFP32, const BinaryRepeatParams &binaryParams, const uint32_t N)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE * N);

    if constexpr (withOffset) {
        Add<float, false>(srcFP32, srcFP32, offsetFP32, MASK_PLACEHOLDER, N, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(srcFP32, srcFP32, scaleFP32, MASK_PLACEHOLDER, N, binaryParams);
    PipeBarrier<PIPE_V>();
}


template <typename dstT>
[aicore] inline void CastDstProcess(const LocalTensor<float> &resFP32, const LocalTensor<dstT> &dst,
    const UnaryRepeatParams &unaryParamsFp322Dst, const uint32_t n, const uint32_t srcN)
{
    SetMaskNorm();
    SetVectorMask<float, MaskMode::NORMAL>(0, FULL_MASK);
    constexpr RoundMode castMode = IsSameType<dstT, bfloat16_t>::value ? RoundMode::CAST_RINT : RoundMode::CAST_NONE;
    Cast<dstT, float, false>(dst, resFP32, castMode, MASK_PLACEHOLDER, srcN, unaryParamsFp322Dst);
    PipeBarrier<PIPE_V>();
}


template <typename SrcType, typename DstType, bool withOffset>
[aicore] inline void CalcN64ByBrcb(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<float> &stackBuffer,
    const uint32_t scaleEleNum, const uint32_t K, const uint32_t shapeN)
{
    AntiquantParams<float> params;
    GetAntiquantTensorInfo<DstType>(scale, stackBuffer, params);

    uint32_t n = K / ANTIQUANT_SINGLE_N_SIZE;
    uint32_t numPerLoop = ANTIQUANT_SINGLE_N_SIZE * scaleEleNum;

    UnaryRepeatParams unaryParamsSrc2Fp16;
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        unaryParamsSrc2Fp16.srcRepStride = n;
    } else {
        unaryParamsSrc2Fp16.srcRepStride = ANTIQUANT_TWO * n;
    }
    unaryParamsSrc2Fp16.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParamsB16Fp32;
    unaryParamsB16Fp32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

    BinaryRepeatParams binaryParams;
    binaryParams.src1BlkStride = 0;
    binaryParams.src1RepStride = 1;

    UnaryRepeatParams unaryParamsFp322Dst;
    unaryParamsFp322Dst.dstRepStride = ANTIQUANT_SINGLE_N_SIZE * n / (ONE_BLK_SIZE / sizeof(DstType));

    SetMaskCount();
    CastAndBrcb<DstType, withOffset>(offset, scale, params, unaryParamsB16Fp32, scaleEleNum);

    uint32_t curNKOffset = 0;
    for (uint32_t i = 0; i < n; i++) {
        curNKOffset = ANTIQUANT_SINGLE_N_SIZE * i;
        CastSrcProcess<SrcType>(src[curNKOffset], params.tempTensorInput, unaryParamsSrc2Fp16, unaryParamsB16Fp32,
            numPerLoop, scaleEleNum, n);
        AddMulProcess<withOffset>(params.tempTensorInput, params.tempTensorOffset, params.tempTensorScale, binaryParams,
            scaleEleNum);
        CastDstProcess<DstType>(params.tempTensorInput, dst[curNKOffset], unaryParamsFp322Dst, n, shapeN);
    }
}

template <bool withOffset = true>
[aicore] inline void AntiQuantFp16Brcb(const LocalTensor<half> &scale, const LocalTensor<half> &offset,
    AntiquantParams<half> &params, const uint32_t scaleN)
{
    const uint8_t repeatTimes = scaleN / BRCB_BROADCAST_NUMBER;
    BrcbRepeatParams brcbParams(DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    SetMaskNorm();
    ResetMask();
    Brcb(params.tempTensorScale, scale, repeatTimes, brcbParams);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Brcb(params.tempTensorOffset, offset, repeatTimes, brcbParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename SrcType, typename DstType>
[aicore] inline void AscendAntiQuantBF16Transpose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t srcEleNum = src.GetSize();
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0)) {
        return AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, srcEleNum, K, shapeInfo);
    }

    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();

    uint32_t scaleEleNum = scale.GetSize();
    uint32_t shapeN = src.GetSize() / K;
    uint32_t stackBufferSize = scaleEleNum * (ANTIQUANT_SINGLE_N_SIZE + ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO);
    stackBuffer.SetSize(stackBufferSize);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        scaleEleNum = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
        shapeN = scaleEleNum;
    }
    CalcN64ByBrcb<SrcType, DstType, true>(dst, src, offset, scale, stackBuffer, scaleEleNum, K, shapeN);
}

template <typename SrcType, typename DstType>
[aicore] inline void AscendAntiQuantBF16Transpose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    uint32_t srcEleNum = src.GetSize();
    if (K > ANTIQUANT_MAX_K * ANTIQUANT_BRCB_BASE || (K % ANTIQUANT_SINGLE_N_SIZE != 0)) {
        return AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, srcEleNum, K, shapeInfo);
    }


    uint32_t scaleEleNum = scale.GetSize();
    uint32_t shapeN = src.GetSize() / K;
    auto stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t stackBufferSize = scaleEleNum * (ANTIQUANT_SINGLE_N_SIZE + ANTIQUANT_BRCB_BASE * ANTIQUANT_TWO);
    stackBuffer.SetSize(stackBufferSize);
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        scaleEleNum = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
        shapeN = scaleEleNum;
    }
    CalcN64ByBrcb<SrcType, DstType, false>(dst, src, scale, scale, stackBuffer, scaleEleNum, K, shapeN);
}

}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/../../../impl/adv_api/detail/quantization/antiquant/ascend_antiquant_impl.h" 2




namespace AscendC {
[aicore] inline void AntiQuantInnerLoopF16(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Add<half, false>(dst, offset, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] inline void AntiQuantInnerLoop(const LocalTensor<half> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const BinaryRepeatParams &binaryParams, const uint32_t calCount)
{
    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Cast<half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Add<half, false>(dst, offset, dst, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<half, false>(dst, scale, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, bool withOffset = true>
[aicore] inline void AntiQuantInnerLoop(const LocalTensor<half> &dst, const LocalTensor<SrcType> &src,
    const half offset, const half scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const UnaryRepeatParams &unaryParamsCastSrc, const UnaryRepeatParams &unaryParamsScalar, const uint32_t calCount)
{
    SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
    Cast<half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParamsCastSrc);
    PipeBarrier<PIPE_V>();
    if constexpr (withOffset) {
        Adds<half, false>(dst, dst, offset, MASK_PLACEHOLDER, 1, unaryParamsScalar);
        PipeBarrier<PIPE_V>();
    }
    Muls<half, false>(dst, dst, scale, MASK_PLACEHOLDER, 1, unaryParamsScalar);
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] inline void AntiQuantOuterLoop(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount)
{
    UnaryRepeatParams unaryParamsCastSrc;
    if constexpr(IsSameType<SrcType, int8_t>::value) {
        unaryParamsCastSrc.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParamsCastSrc.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    BinaryRepeatParams binaryParams;
    if constexpr (IsSameType<DstType, half>::value) {
        AntiQuantInnerLoop<SrcType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, unaryParamsCastSrc,
            binaryParams, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(DstType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        UnaryRepeatParams unaryParamsFP32ToDst;
        unaryParamsFP32ToDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        UnaryRepeatParams unaryParamsToFP32;
        unaryParamsToFP32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer,
                unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, binaryParams, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale,
                sharedTmpBuffer, unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, binaryParams, tailSize);
        }
    }
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] inline void AntiQuantOuterLoop(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    UnaryRepeatParams unaryParamsCastSrc;
    if constexpr(IsSameType<SrcType, int8_t>::value) {
        unaryParamsCastSrc.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    } else {
        unaryParamsCastSrc.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    }
    UnaryRepeatParams unaryParamsScalar;
    if constexpr (IsSameType<DstType, half>::value) {
        AntiQuantInnerLoop<SrcType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, unaryParamsCastSrc,
            unaryParamsScalar, calCount);
    } else {
        uint32_t tmpSize = sharedTmpBuffer.GetSize() / sizeof(DstType) / ANTIQUANT_FOUR;
        uint32_t loopCount = calCount / tmpSize;
        uint32_t tailSize = calCount % tmpSize;

        UnaryRepeatParams unaryParamsToFP32;
        unaryParamsToFP32.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        UnaryRepeatParams unaryParamsFP32ToDst;
        unaryParamsFP32ToDst.dstRepStride = HALF_DEFAULT_REPEAT_STRIDE;

        for (uint32_t i = 0; i < loopCount; i++) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[i * tmpSize], src[i * tmpSize], offset, scale, sharedTmpBuffer,
                unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, unaryParamsScalar, tmpSize);
        }
        if (tailSize > 0) {
            AntiQuantInnerLoop<SrcType, withOffset>(dst[loopCount * tmpSize], src[loopCount * tmpSize], offset, scale,
                sharedTmpBuffer, unaryParamsCastSrc, unaryParamsToFP32, unaryParamsFP32ToDst, unaryParamsScalar,
                tailSize);
        }
    }
}

template <typename SrcType>
[aicore] inline void AscendAntiQuantNoTransposePerformance(const LocalTensor<half> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<half> &offset, const LocalTensor<half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, ANTIQUANT_SINGLE_N_SIZE_FP16 * K);
    uint32_t loopN = N / ANTIQUANT_SINGLE_N_SIZE_FP16;
    for (uint32_t i = 0; i < loopN; i++) {
        uint32_t loopOffset = ANTIQUANT_SINGLE_N_SIZE_FP16 * i;

        Add<half, false>(dst[loopOffset], dst[loopOffset], offset[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();

        Mul<half, false>(dst[loopOffset], dst[loopOffset], scale[loopOffset], MASK_PLACEHOLDER, K, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType>
[aicore] inline void AscendAntiQuantNoTransposePerformanceTail(const LocalTensor<half> &dst,
    const LocalTensor<SrcType> &src, const LocalTensor<half> &offset, const LocalTensor<half> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K, const uint32_t N, const uint32_t mask)
{
    BinaryRepeatParams binaryParams;
    binaryParams.src0RepStride = N * sizeof(half) / ONE_BLK_SIZE;
    binaryParams.src1RepStride = 0;
    binaryParams.dstRepStride = N * sizeof(half) / ONE_BLK_SIZE;


    SetMaskNorm();
    SetVectorMask<half, MaskMode::NORMAL>(mask);

    Add<half, false>(dst, dst, offset, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();

    Mul<half, false>(dst, dst, scale, MASK_PLACEHOLDER, K, binaryParams);
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <typename SrcType>
[aicore] inline void PreCast(const LocalTensor<half> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<half> &offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K)
{
    UnaryRepeatParams s42f16unaryParams;
    s42f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
    UnaryRepeatParams unaryParams;
    unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
    uint32_t repeatEle = ONE_REPEAT_BYTE_SIZE;
    uint32_t repeatTimes = src.GetSize() % repeatEle == 0 ? src.GetSize() / repeatEle : src.GetSize() / repeatEle + 1;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, src.GetSize());
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        Cast<half, int4b_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, s42f16unaryParams);
    } else {
        Cast<half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename SrcType, typename DstType>
[aicore] inline void AntiQuantNoTransposeImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const uint32_t N, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    SetMaskCount();
    if constexpr (IsSameType<DstType, half>::value && IsSameType<SrcType, int8_t>::value) {
        SetVectorMask<half, MaskMode::COUNTER>(0, calCount);
        UnaryRepeatParams unaryParams;
        unaryParams.srcRepStride = HALF_DEFAULT_REPEAT_STRIDE;
        BinaryRepeatParams binaryParams;

        Cast<half, int8_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < groupSize; j++) {
            AntiQuantInnerLoopF16(dst[j * N], dst[j * N], offset, scale, sharedTmpBuffer, binaryParams, N);
        }
        return;
    }
    for (uint32_t j = 0; j < groupSize; j++) {
        AntiQuantOuterLoop<SrcType, DstType, true>(dst[j * N], src[j * N], offset, scale, sharedTmpBuffer, N);
    }
}

template <typename SrcType, typename DstType>
[aicore] inline void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    bool isPerformance = AntiQuantCheckPerformanceMode(scale, sharedTmpBuffer, K);
    if (isPerformance) {

        PreCast(dst, src, offset, scale, sharedTmpBuffer, K);
        uint32_t kTail = K % ANTIQUANT_MAX_K, loopK = K / ANTIQUANT_MAX_K, nTail;
        if constexpr (IsSameType<DstType, half>::value) {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_FP16;
        } else {
            nTail = N % ANTIQUANT_SINGLE_N_SIZE_BF16;
        }
        uint32_t NAlign = N - nTail;
        for (int i = 0; i < K / ANTIQUANT_MAX_K; i++) {
            uint32_t offsetSrc = i * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer,
                ANTIQUANT_MAX_K, N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, ANTIQUANT_MAX_K, N, nTail);
            }
        }
        if (kTail > 0) {
            uint32_t offsetSrc = K / ANTIQUANT_MAX_K * ANTIQUANT_MAX_K * N;
            AscendAntiQuantNoTransposePerformance(dst[offsetSrc], src[offsetSrc], offset, scale, sharedTmpBuffer, kTail,
                N);
            if (nTail > 0) {
                AscendAntiQuantNoTransposePerformanceTail(dst[offsetSrc + NAlign], src[offsetSrc + NAlign],
                    offset[NAlign], scale[NAlign], sharedTmpBuffer, kTail, N, nTail);
            }
        }
        return;
    }
    AntiQuantNoTransposeImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, N, shapeInfo);
}

template <typename SrcType, typename DstType>
[aicore] inline void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t groupCount = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t groupSize = K / groupCount;
    uint32_t N = (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);

    SetMaskCount();
    for (uint32_t i = 0; i < groupCount; i++) {
        for (uint32_t j = 0; j < groupSize; j++) {

            AntiQuantOuterLoop<SrcType, DstType, false>(dst[(i * groupSize + j) * N], src[(i * groupSize + j) * N],
                scale, scale[i * N], sharedTmpBuffer, N);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] inline void AscendAntiQuantNoTranspose(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<SrcType, DstType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
}

template <typename SrcType, typename DstType>
[aicore] inline void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t calCount, const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.offsetWidth == 0 ? offset.GetShapeInfo().shape[1] : shapeInfo.offsetWidth);
    uint32_t offsetLength = K / groupSize;
    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < offsetLength; j++) {
            auto offsetValue = offset.GetValue(i * offsetLength + j);
            auto scaleValue = scale.GetValue(i * offsetLength + j);
            AntiQuantOuterLoop<SrcType, DstType, true>(dst[i * K + j * groupSize], src[i * K + j * groupSize],
                offsetValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename SrcType, typename DstType>
[aicore] inline void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t N = src.GetSize() / K;
    uint32_t groupSize = K / (shapeInfo.scaleWidth == 0 ? scale.GetShapeInfo().shape[1] : shapeInfo.scaleWidth);
    uint32_t scaleLength = K / groupSize;

    SetMaskCount();
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < scaleLength; j++) {
            auto scaleValue = scale.GetValue(i * scaleLength + j);
            AntiQuantOuterLoop<SrcType, DstType, false>(dst[i * K + j * groupSize], src[i * K + j * groupSize],
                scaleValue, scaleValue, sharedTmpBuffer, groupSize);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename SrcType, typename DstType, bool withOffset = true>
[aicore] inline void AntiQuantImplScalar(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    SetMaskCount();
    AntiQuantOuterLoop<SrcType, DstType, withOffset>(dst, src, offset, scale, sharedTmpBuffer, calCount);
}

template <bool withOffset = true>
[aicore] inline void AntiQuantFp16TransposeMainImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &scale, const LocalTensor<half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskCount();



    uint32_t repStride = K * sizeof(half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    SetVectorMask<half, MaskMode::COUNTER>(0, srcN * B16_DATA_NUM_PER_REPEAT);
    const uint32_t loop = K / B16_DATA_NUM_PER_REPEAT;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t tmpOffset = i * B16_DATA_NUM_PER_REPEAT;
        if constexpr (withOffset) {
            Add<half, false>(dst[tmpOffset], src[tmpOffset], offset, MASK_PLACEHOLDER, srcN, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[tmpOffset], dst[tmpOffset], scale, MASK_PLACEHOLDER, srcN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}

template <bool withOffset = true>
[aicore] inline void AntiQuantFp16TransposeTailImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
    const LocalTensor<half> &scale, const LocalTensor<half> &offset, const uint32_t srcN, const uint32_t K)
{
    SetMaskNorm();
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    SetVectorMask<half, MaskMode::NORMAL>(tailK);



    const uint32_t repStride = K * sizeof(half) / ONE_BLK_SIZE;
    BinaryRepeatParams binaryParams(1, 1, 0, repStride, repStride, 1);
    const uint32_t loop = srcN / MAX_REPEAT_TIMES;
    for (uint32_t i = 0; i < loop; ++i) {
        const uint32_t srcOffset = MAX_REPEAT_TIMES * K * i;
        const uint32_t scaleOffset = MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK * i;
        if constexpr (withOffset) {
            Add<half, false>(dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, MAX_REPEAT_TIMES,
            binaryParams);
        PipeBarrier<PIPE_V>();
    }
    const uint32_t tailN = srcN % MAX_REPEAT_TIMES;
    if (tailN != 0) {
        const uint32_t srcOffset = loop * MAX_REPEAT_TIMES * K;
        const uint32_t scaleOffset = loop * MAX_REPEAT_TIMES * B16_DATA_NUM_PER_BLOCK;
        if constexpr (withOffset) {
            Add<half, false>(dst[srcOffset], dst[srcOffset], offset[scaleOffset], MASK_PLACEHOLDER, tailN,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        Mul<half, false>(dst[srcOffset], dst[srcOffset], scale[scaleOffset], MASK_PLACEHOLDER, tailN, binaryParams);
        PipeBarrier<PIPE_V>();
    }
}




template <typename SrcType, bool withOffset = true>
[aicore] inline void AscendAntiQuantFP16Transpose(const LocalTensor<half> &dst, const LocalTensor<SrcType> &src,
    LocalTensor<half> offset, const LocalTensor<half> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo)
{
    uint32_t calCount = src.GetSize();
    uint32_t scaleN = (shapeInfo.scaleHeight == 0 ? scale.GetShapeInfo().shape[0] : shapeInfo.scaleHeight);
    uint32_t scaleBrcbSize = ONE_BLK_SIZE / sizeof(half) * scaleN;
    uint32_t stackBufferSize = sharedTmpBuffer.GetSize() / sizeof(half);
    constexpr uint32_t tmpBufferCoeff = withOffset ? ANTIQUANT_TWO : 1;
    if (stackBufferSize < scaleBrcbSize * tmpBufferCoeff || K >= MAX_K_FOR_FP16_BRCB) {
        return withOffset ? AntiQuantImplScalar(dst, src, offset, scale, sharedTmpBuffer, calCount, K, shapeInfo) :
            AntiQuantImplScalar(dst, src, scale, sharedTmpBuffer, calCount, K, shapeInfo);
    }


    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, src.GetSize());
    if constexpr (IsSameType<SrcType, int4b_t>::value) {
        UnaryRepeatParams s42f16unaryParams;
        s42f16unaryParams.srcRepStride = ONE_FOURTH_DEFAULT_REPEAT_STRIDE;
        Cast<half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, s42f16unaryParams);
    } else {
        UnaryRepeatParams unaryParams(1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE);
        Cast<half, SrcType, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, unaryParams);
    }
    PipeBarrier<PIPE_V>();

    LocalTensor<half> stackBuffer = sharedTmpBuffer.ReinterpretCast<half>();
    AntiquantParams<half> params;
    params.tempTensorScale = stackBuffer[0];
    if constexpr (withOffset) {
        params.tempTensorOffset = stackBuffer[B16_DATA_NUM_PER_BLOCK * scaleN];
    }
    AntiQuantFp16Brcb<withOffset>(scale, offset, params, scaleN);
    uint32_t srcN = src.GetSize() / K;
    if (K < B16_DATA_NUM_PER_REPEAT) {
        return AntiQuantFp16TransposeTailImpl<withOffset>(dst, dst, params.tempTensorScale, params.tempTensorOffset,
            srcN, K);
    }

    AntiQuantFp16TransposeMainImpl<withOffset>(dst, dst, params.tempTensorScale, params.tempTensorOffset, srcN, K);
    const uint32_t tailK = K % B16_DATA_NUM_PER_REPEAT;
    if (tailK != 0) {
        const uint32_t srcOffset = K - tailK;
        AntiQuantFp16TransposeTailImpl<withOffset>(dst[srcOffset], dst[srcOffset], params.tempTensorScale,
            params.tempTensorOffset, srcN, K);
    }
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{

                                                         ;

    uint32_t calCount = src.GetSize();
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose(dst, src, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else if constexpr (IsSameType<DstType, half>::value) {
        AscendAntiQuantFP16Transpose<SrcType, false>(dst, src, scale, scale, sharedTmpBuffer, K, shapeInfo);
    } else {
        AscendAntiQuantBF16Transpose(dst, src, scale, sharedTmpBuffer, K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{

                                                         ;

    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<SrcType, DstType, false>(dst, src, scale, scale, sharedTmpBuffer, src.GetSize(), K,
            shapeInfo);
    } else {
        AntiQuantImplScalar<SrcType, DstType, false>(dst, src, scale, scale, sharedTmpBuffer, src.GetSize(), K,
            shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImplCommon(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose(dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else if constexpr (IsSameType<DstType, half>::value) {
        AscendAntiQuantFP16Transpose<SrcType, true>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
    } else {
        AscendAntiQuantBF16Transpose(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImplCommon(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr (!isTranspose) {
        AscendAntiQuantNoTranspose<SrcType, DstType, true>(
            dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    } else {
        AntiQuantImplScalar<SrcType, DstType, true>(
            dst, src, offset, scale, sharedTmpBuffer, src.GetSize(), K, shapeInfo);
    }
    SetMaskNorm();
    ResetMask();
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{

                                                                 ;

    AscendAntiQuantImplCommon<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const LocalTensor<DstType> &offset, const LocalTensor<DstType> &scale, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}


template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t K,
    const AntiQuantShapeInfo& shapeInfo = {})
{

                                                                 ;

    AscendAntiQuantImplCommon<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}

template <typename SrcType, typename DstType, bool isTranspose>
[aicore] inline void AscendAntiQuantImpl(const LocalTensor<DstType> &dst, const LocalTensor<SrcType> &src,
    const DstType offset, const DstType scale, const uint32_t K, const AntiQuantShapeInfo& shapeInfo = {})
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                           ;
    AscendAntiQuantImpl<SrcType, DstType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, K, shapeInfo);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t k, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, k,
        shapeInfo);
}
# 55 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t k,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, k, shapeInfo);
}
# 76 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const LocalTensor<OutputDataType> &offset, const LocalTensor<OutputDataType> &scale, const uint32_t k,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, k, shapeInfo);
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const uint32_t k, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, sharedTmpBuffer, k,
        shapeInfo);
}
# 120 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType scale, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t k,
    const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, scale, sharedTmpBuffer, k, shapeInfo);
}
# 141 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/quantization/ascend_antiquant.h"
template <typename InputDataType, typename OutputDataType, bool isTranspose>
[aicore] inline void AscendAntiQuant(const LocalTensor<OutputDataType> &dst, const LocalTensor<InputDataType> &src,
    const OutputDataType offset, const OutputDataType scale, const uint32_t k, const AntiQuantShapeInfo& shapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AscendAntiQuantImpl<InputDataType, OutputDataType, isTranspose>(dst, src, offset, scale, k, shapeInfo);
}
#pragma end_pipe
}
# 76 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/logsoftmax.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/logsoftmax.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/logsoftmax_base_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/logsoftmax_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/common/logsoftmax_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/membase/common/logsoftmax_common_impl.h"
namespace AscendC {
constexpr float SCALAR_NATURE_LOG_10 = 0.4342944819;
 [aicore] inline void GenericLogNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
     const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10),
                MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void LogSoftMaxGenericNZImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    const UnaryRepeatParams unaryParams;
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t copyBlockCount = splitCount / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    GenericLogNZImpl(dst, dst, reduceParam.originalSrcM, tiling.srcK);
}

[aicore] inline void LogSoftMaxGenericNZReduceMaxImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<half>& maxTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceMaxLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(maxTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

[aicore] inline void LogSoftMaxGenericNZSubImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2],
    const uint32_t& lastBlockMaskLen, const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Sub<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Sub<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] inline void LogSoftMaxGenericNZReduceSumImpl(const LocalTensor<float>& tmpBuffer0,
    const LocalTensor<float>& tmpBuffer1, const LocalTensor<half>& sumTensor, const uint32_t& offset2,
    const uint32_t& splitCount, uint64_t mask[2], const ReduceLastND& reduceParam)
{
    ReduceSumLastNZImpl(tmpBuffer1, tmpBuffer0, mask, reduceParam);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    Cast<half, float, false>(sumTensor[offset2], tmpBuffer1, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LogSoftMaxGenericNZDivImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Div<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], tmpBuffer1, MASK_PLACEHOLDER, 1,
            { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    BinaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer1,
        mask, lastBlockMaskLen, splitCount, Div<float>);
}

[aicore] inline void LogSoftMaxGenericNZLogImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<half>& dst, const uint32_t& splitOffset,
    const uint32_t& splitCount, const SoftMaxTiling& tiling, const uint32_t& offset1)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    PipeBarrier<PIPE_V>();

    for (uint32_t j = 0; j < splitNZBlockCount; j++) {
        Ln<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Muls<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j],
            static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT],
            tmpBuffer0[splitOffset * j], FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, 1,
            { 1, 1, HALF_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void LogSoftMaxGenericNZExpImpl(const uint32_t& splitNZBlockCount,
    const LocalTensor<float>& tmpBuffer0, const LocalTensor<float>& tmpBuffer1, const uint32_t& splitOffset,
    const uint32_t& lastSplitNZBlockOffset, uint64_t mask[2], const uint32_t& lastBlockMaskLen,
    const uint32_t& splitCount)
{
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);

    for (uint32_t j = 0; j < splitNZBlockCount - 1; j++) {
        Exp<float, false>(tmpBuffer0[splitOffset * j], tmpBuffer0[splitOffset * j], MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();
    UnaryComputeWithSpecialMask(tmpBuffer0[lastSplitNZBlockOffset], tmpBuffer0[lastSplitNZBlockOffset], mask,
        lastBlockMaskLen, splitCount, Exp<float>);

    PipeBarrier<PIPE_V>();
}

[aicore] inline void LogSoftMaxGenericNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, uint64_t mask[2], const uint32_t& offset1, const uint32_t& offset2,
    const uint32_t& splitCount, const ReduceLastND& reduceParam)
{
    LocalTensor<float> tmpBuffer0 = workLocal;
    LocalTensor<float> tmpBuffer1 = workLocal[tiling.splitSize];
    const uint64_t splitOffset = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t splitNZBlockCount = tiling.srcK / SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    const uint64_t lastSplitNZBlockOffset = splitOffset * (splitNZBlockCount - 1);
    const uint64_t lastBlockMaskLen = reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        reduceParam.originalSrcK % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(0, splitCount);
    for (uint64_t j = 0; j < splitNZBlockCount; j++) {
        Cast<float, half, false>(tmpBuffer0[splitOffset * j],
            src[offset1 + j * tiling.srcM * SOFTMAX_SHAPE_NZ_BASIC_COUNT], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
    }
    SetMaskNorm();
    ResetMask();

    PipeBarrier<PIPE_V>();
    LogSoftMaxGenericNZReduceMaxImpl(tmpBuffer0, tmpBuffer1, maxTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZSubImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZExpImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZReduceSumImpl(tmpBuffer0, tmpBuffer1, sumTensor, offset2, splitCount, mask, reduceParam);

    LogSoftMaxGenericNZDivImpl(splitNZBlockCount, tmpBuffer0, tmpBuffer1, splitOffset, lastSplitNZBlockOffset,
        mask, lastBlockMaskLen, splitCount);

    LogSoftMaxGenericNZLogImpl(splitNZBlockCount, tmpBuffer0, dst, splitCount, splitOffset, tiling, offset1);
}

[aicore] inline bool LogSoftMaxTilingFunc(const uint32_t workLocalSize, const LastAxisShapeND& ndinfo,
    AscendC::tiling::LogSoftMaxTiling& softmaxTiling, const uint32_t dataTypeSize1, const uint32_t dataTypeSize2,
    bool isDataFormatNZ = false)
{

                                                                                                 ;
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / dataTypeSize2;
    softmaxTiling.srcM = ndinfo.m;
    softmaxTiling.srcK = ndinfo.k;
    softmaxTiling.srcSize = ndinfo.m * ndinfo.k;
    softmaxTiling.outMaxM = ndinfo.m;
    softmaxTiling.outMaxK = elementNumPerBlk;
    softmaxTiling.outMaxSize = ndinfo.m * elementNumPerBlk;
    if (isDataFormatNZ) {
        softmaxTiling.reduceM = workLocalSize / (SOFTMAX_SHAPE_NZ_BASIC_COUNT + ndinfo.k);
    } else {
        softmaxTiling.reduceM = CalculateNDSplitM(workLocalSize, dataTypeSize1, elementNumPerBlk, ndinfo);
    }

    if (softmaxTiling.reduceM < ndinfo.m && softmaxTiling.reduceM > SOFTMAX_BASIC_TILE_NUM) {
        softmaxTiling.reduceM = softmaxTiling.reduceM / SOFTMAX_BASIC_TILE_NUM * SOFTMAX_BASIC_TILE_NUM;
    }
    softmaxTiling.reduceM = softmaxTiling.reduceM < ndinfo.m ? softmaxTiling.reduceM : ndinfo.m;
    softmaxTiling.reduceK = elementNumPerBlk;
    softmaxTiling.reduceSize = softmaxTiling.reduceM * elementNumPerBlk;

    softmaxTiling.splitM = softmaxTiling.reduceM;
    softmaxTiling.splitK = ndinfo.k;
    softmaxTiling.splitSize = softmaxTiling.reduceM * ndinfo.k;

                                                                                              ;
    softmaxTiling.rangeM = ndinfo.m / softmaxTiling.reduceM;
    softmaxTiling.tailM = ndinfo.m % softmaxTiling.reduceM;

    softmaxTiling.tailSplitSize = softmaxTiling.tailM * ndinfo.k;
    softmaxTiling.tailReduceSize = softmaxTiling.tailM * elementNumPerBlk;
    return true;
}

[aicore] inline void GenericLogNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& src,
    const uint32_t originalSrcM, const uint32_t srcK)
{
    if (srcK < SOFTMAX_SUB_DIV_ROW_COLUMN_SIZE) {
        const uint8_t blockStride = srcK / FLOAT_NUM_PER_BLK;
        SetMaskCount();
        SetVectorMask<float>(0, originalSrcM * FLOAT_NUM_PER_BLK);
        for (uint8_t j = 0; j < blockStride; j++) {
            Ln<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK], MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * FLOAT_NUM_PER_BLK], src[j * FLOAT_NUM_PER_BLK],
                static_cast<float>(SCALAR_NATURE_LOG_10), MASK_PLACEHOLDER, 1,
                { blockStride, blockStride, static_cast<uint8_t>(srcK), static_cast<uint8_t>(srcK)});
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetMaskCount();
        SetVectorMask<float>(0, srcK);
        for (int j = 0; j < originalSrcM; j++) {
            Ln<float, false>(dst[j * srcK], src[j * srcK], MASK_PLACEHOLDER, 1,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
            Muls<float, false>(dst[j * srcK], src[j * srcK], static_cast<float>(SCALAR_NATURE_LOG_10), 1,
                MASK_PLACEHOLDER, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
            PipeBarrier<PIPE_V>();
        }
    }
    SetMaskNorm();
    ResetMask();
}

[aicore] inline void LogSoftMaxGenericNDImpl(const LocalTensor<float>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<float>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const UnaryRepeatParams unaryParams;

    NewReduceMaxLastNDImpl(maxTensor[offset2], src[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(dst[offset1], src[offset1], maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(dst[offset1], dst[offset1], splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], dst[offset1], tmpBuffer0, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(dst[offset1], dst[offset1], sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK,
        tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(dst[offset1], dst[offset1], reduceParam.originalSrcM, tiling.srcK);
}

[aicore] inline void LogSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const uint32_t& reduceSize, const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float>& tmpBuffer3 = workLocal[tiling.splitSize + tiling.reduceSize];
    const UnaryRepeatParams unaryParams;
    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);

    PipeBarrier<PIPE_V>();
    Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);
    PipeBarrier<PIPE_V>();

    NewReduceSumLastNDImpl(tmpBuffer2, tmpBuffer0, tmpBuffer3, reduceParam);
    PipeBarrier<PIPE_V>();

    Cast(sumTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, reduceSize);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, tmpBuffer2, reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);
    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

[aicore] inline void LogSoftMaxGenericNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const SoftMaxTiling& tiling, const uint32_t& offset1, const uint32_t& offset2, const uint32_t& splitSize,
    const ReduceLastND& reduceParam)
{
    const LocalTensor<float>& tmpBuffer0 = workLocal;
    const LocalTensor<float>& tmpBuffer1 = workLocal[tiling.splitSize];
    const UnaryRepeatParams unaryParams;

    Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, splitSize);
    PipeBarrier<PIPE_V>();
    NewReduceMaxLastNDImpl(maxTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericSubNDImpl(tmpBuffer0, tmpBuffer0, maxTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();
    Exp(tmpBuffer0, tmpBuffer0, splitSize);

    PipeBarrier<PIPE_V>();
    NewReduceSumLastNDImpl(sumTensor[offset2], tmpBuffer0, tmpBuffer1, reduceParam);
    PipeBarrier<PIPE_V>();

    GenericDivNDImpl(tmpBuffer0, tmpBuffer0, sumTensor[offset2], reduceParam.originalSrcM, tiling.srcK, tiling.reduceK);

    PipeBarrier<PIPE_V>();

    GenericLogNDImpl(tmpBuffer0, tmpBuffer0, reduceParam.originalSrcM, tiling.srcK);

    Cast(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, splitSize);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LogSoftMaxNZImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2,
            splitSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

[aicore] inline void LogSoftMaxNZImpl(const LocalTensor<half>& dst, const LocalTensor<half>& sumTensor,
    const LocalTensor<half>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    const ReduceLastND& mainReduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    const ReduceLastND& tailReduceParam = { tiling.tailM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.splitM, SOFTMAX_SHAPE_NZ_BASIC_COUNT };
    uint32_t lastBlockMaskLen = originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT != 0 ?
        originalSrcShape.k % SOFTMAX_SHAPE_NZ_BASIC_COUNT :
        SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    uint64_t mask[2] = { 0, 0 };
    CreateSpecialFormatMask(mask[0], lastBlockMaskLen, FLOAT_REPEAT_SIZE / SOFTMAX_SHAPE_NZ_BASIC_COUNT);

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitCount = tiling.splitM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset1 = i * splitCount;
        offset2 = i * tiling.reduceSize;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            mainReduceParam);
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset1 = tiling.rangeM * splitCount;
        offset2 = tiling.rangeM * tiling.reduceSize;
        splitCount = tiling.tailM * SOFTMAX_SHAPE_NZ_BASIC_COUNT;
        LogSoftMaxGenericNZImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, mask, offset1, offset2, splitCount,
            tailReduceParam);
    }
}

[aicore] inline void LogSoftMaxNDImpl(const LocalTensor<half>& dst, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    PipeBarrier<PIPE_V>();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LogSoftMaxNDImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<float>& workLocal,
    const LastAxisShapeND& originalSrcShape, const SoftMaxTiling& tiling)
{
    SetMaskNorm();
    ResetMask();
    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint32_t splitSize = tiling.splitSize;
    uint32_t reduceSize = tiling.reduceSize;
    PipeBarrier<PIPE_V>();
    for (uint32_t i = 0; i <= tiling.rangeM; i++) {
        LogSoftMaxGenericNDImpl(dst, sumTensor, maxTensor, src, workLocal, tiling, offset1, offset2, splitSize,
            reduceSize, reduceParam);
        offset1 += tiling.splitSize;
        offset2 += tiling.reduceSize;
        if (i == (tiling.rangeM - 1)) {
            if (tiling.tailM == 0) {
                break;
            }
            offset2 = tiling.rangeM * tiling.reduceSize;
            offset1 = tiling.rangeM * tiling.splitSize;
            splitSize = tiling.tailSplitSize;
            reduceSize = tiling.tailReduceSize;
            reduceParam.originalSrcM = tiling.tailM;
            reduceParam.srcM = tiling.tailM;
            reduceParam.dstM = tiling.tailM;
            PipeBarrier<PIPE_V>();
        }
    }
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/logsoftmax_base_impl.h" 2



namespace AscendC {
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void LogSoftMaxImpl(const LocalTensor<T>& dst, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{

                                                                       ;

    LocalTensor<float> tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tempBuffer.SetSize(sharedTmpBuffer.GetSize() / B32_BYTE_SIZE);
    ShapeInfo srcShape = src.GetShapeInfo();
    uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);




      ;
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    SoftMaxTiling newTiling;
    SoftMaxTilingFunc(tempBuffer.GetSize(), { srcNDinfo.m, srcNDinfo.k, originalSrcShape.m, srcNDinfo.k },
        newTiling, sizeof(T), sizeof(T), isDataFormatNZ);
    if constexpr (isDataFormatNZ) {
        LogSoftMaxNZImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape, newTiling);
    } else {
        LogSoftMaxNDImpl(dst, sumTensor, maxTensor, src, tempBuffer, originalSrcShape, newTiling);
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/logsoftmax.h" 2
#pragma begin_pipe(V)

namespace AscendC {
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/logsoftmax.h"
template <typename T, bool isReuseSource = false, bool isDataFormatNZ = false>
[aicore] inline void LogSoftMax(const LocalTensor<T>& dst, const LocalTensor<T>& sum,
    const LocalTensor<T>& max, const LocalTensor<T>& src, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const LogSoftMaxTiling& tiling, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                    ;
    LogSoftMaxImpl<T, isReuseSource, isDataFormatNZ>(dst, sum, max, src, sharedTmpBuffer,
        tiling, softmaxShapeInfo);
                                   ;
}
}

#pragma end_pipe
# 77 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flash_base_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flash_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_flash.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_flash.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/../softmax_flash_base_impl/softmax_flash_nd_process_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/../softmax_flash_base_impl/softmax_flash_nd_process_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/../softmax_flash_base_impl/softmax_flash_basic_block_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/../softmax_flash_base_impl/softmax_flash_basic_block_impl.h"
namespace AscendC {

template <typename T>
[aicore] inline void SoftmaxFlashBasicBlock(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[halfSplitSize + DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM,
            { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM),
                { 1, 1, 0, offset, offset, B16_BYTE_SIZE });
        }
        PipeBarrier<PIPE_V>();

        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t i = 2; i < splitBlock; ++i) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * i], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();

        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_BLK_NUM);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        Brcb(tmpBuffer1[DEFAULT_BLK_NUM], tmpBuffer3, splitCeilM, { HALF_FACTOR, DEFAULT_REPEAT_STRIDE * HALF_FACTOR });
        PipeBarrier<PIPE_V>();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 2 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Cast<float, half, false>(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Cast<half, float, false>(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_BLK_NUM, DEFAULT_BLK_NUM });

        Cast<float, half, false>(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, DEFAULT_BLK_NUM, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER,
            reduceCeilValue, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Cast<half, float, false>(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
        for (uint32_t i = 0; i < splitBlock; ++i) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer0[FLOAT_REPEAT_SIZE * i], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, HALF_FACTOR });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_NUM });
    }
}


[aicore] inline void SoftmaxFlashBasicBlockFloat(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer1 = workLocal[0];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &inSumTmp = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / HALF_FACTOR;
    const uint32_t halfSplitSize = tiling.splitSize / HALF_FACTOR;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        __attribute__((cce_unif_buff)) float *tmpBufferAddr0 = (__attribute__((cce_unif_buff)) float *)src[offset1].GetPhyAddr();
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Max<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Max<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer2[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j],
                tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(src[offset1], src[offset1], MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();

        if (splitBlock == 1) {
            Copy<float, false>(tmpBuffer1, src[offset1], MASK_PLACEHOLDER, repeatTimes,
                { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        } else {
            Add<float, false>(tmpBuffer1, src[offset1], src[offset1 + FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
            for (uint32_t j = 2; j < splitBlock; ++j) {
                PipeBarrier<PIPE_V>();
                Add<float, false>(tmpBuffer1, tmpBuffer1, src[offset1 + FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                    (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
            }
        }

        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();

        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer2[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer2[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(expMaxTensor[offset2], inMaxTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(src[offset1 + FLOAT_REPEAT_SIZE * j], src[offset1 + FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
    }
}



[aicore] inline void SoftmaxFlashBasicBlock(const LocalTensor<half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &src, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &tmpBuffer3 = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    uint8_t repeatTimes = (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE);
    uint8_t offset = (uint8_t)(FLOAT_NUM_PER_BLK * (tiling.splitK / FLOAT_REPEAT_SIZE));
    const uint8_t splitCeilM = (uint8_t)(DivCeil(tiling.splitM, FLOAT_NUM_PER_BLK));
    const uint8_t reduceCeilValue = (uint8_t)(DivCeil(tiling.reduceSize, FLOAT_REPEAT_SIZE));
    const uint32_t splitBlock = tiling.splitK / FLOAT_REPEAT_SIZE;
    const uint32_t halfRepeatNum = DEFAULT_REPEAT_STRIDE / B16_BYTE_SIZE;
    const uint32_t halfSplitSize = tiling.splitSize / B16_BYTE_SIZE;
    BinaryRepeatParams binaryRepeatParams;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        SetMaskNorm();
        ResetMask();
        PipeBarrier<PIPE_V>();
        Cast<float, half, false>(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, DEFAULT_REPEAT_STRIDE, halfRepeatNum });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Max<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceMax<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();

        Brcb(tmpBuffer1[halfSplitSize], tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Sub<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j],
                tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer0, tmpBuffer0, MASK_PLACEHOLDER, (uint8_t)(tiling.splitSize / FLOAT_REPEAT_SIZE),
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Add<float, false>(tmpBuffer1, tmpBuffer0, tmpBuffer0[FLOAT_REPEAT_SIZE], MASK_PLACEHOLDER,
            (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, offset, offset });
        for (uint32_t j = 2; j < splitBlock; ++j) {
            PipeBarrier<PIPE_V>();
            Add<float, false>(tmpBuffer1, tmpBuffer1, tmpBuffer0[FLOAT_REPEAT_SIZE * j], MASK_PLACEHOLDER,
                (uint8_t)(tiling.splitM), { 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, offset });
        }
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer1, tmpBuffer1, (uint8_t)(tiling.splitM), MASK_PLACEHOLDER, 1, 1,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        BlockReduceSum<float, false>(tmpBuffer3, tmpBuffer1, splitCeilM, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        Brcb(tmpBuffer1, tmpBuffer3, splitCeilM, { 1, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Div<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer1,
                MASK_PLACEHOLDER, (uint8_t)(tiling.splitM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(inMaxTmp, inMaxTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        Max<float, false>(tmpBuffer2, inMaxTmp, tmpBuffer1[halfSplitSize], MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();

        Copy<float, false>(maxTensor[offset2], tmpBuffer2, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Sub<float, false>(tmpBuffer3, tmpBuffer1[halfSplitSize], tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue,
            binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(tmpBuffer3, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        Sub<float, false>(inMaxTmp, inMaxTmp, tmpBuffer2, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Exp<float, false>(inMaxTmp, inMaxTmp, MASK_PLACEHOLDER, reduceCeilValue,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });

        Copy<float, false>(inSumTmp, inSumTensor[offset2], MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });

        PipeBarrier<PIPE_V>();
        Mul<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        Mul<float, false>(tmpBuffer3, tmpBuffer3, tmpBuffer1, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Add<float, false>(inSumTmp, inMaxTmp, tmpBuffer3, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        Div<float, false>(inMaxTmp, inMaxTmp, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();


        Copy<float, false>(tmpBuffer1, inMaxTmp, MASK_PLACEHOLDER, B16_BYTE_SIZE, { B16_BYTE_SIZE, 1, 1, 0 });
        PipeBarrier<PIPE_V>();
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize * B16_BYTE_SIZE);
        Cast<half, float, false>(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer1, FLOAT2HALF_ROUND_MODE,
            MASK_PLACEHOLDER, reduceCeilValue, { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
        SetVectorMask<float, MaskMode::COUNTER>(0, tiling.reduceSize);

        Copy<float, false>(sumTensor[offset2], inSumTmp, MASK_PLACEHOLDER, 1, { 1, 1, 1, 0 });
        Div<float, false>(tmpBuffer3, tmpBuffer3, inSumTmp, MASK_PLACEHOLDER, reduceCeilValue, binaryRepeatParams);
        PipeBarrier<PIPE_V>();
        SetMaskNorm();

        ResetMask();
        for (uint32_t j = 0; j < splitBlock; ++j) {
            Mul<float, false>(tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer0[FLOAT_REPEAT_SIZE * j], tmpBuffer3,
                MASK_PLACEHOLDER, (uint8_t)(tiling.reduceM), { 1, 1, 0, offset, offset, 1 });
        }
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dst[offset1], tmpBuffer0, FLOAT2HALF_ROUND_MODE, MASK_PLACEHOLDER, repeatTimes,
            { 1, 1, halfRepeatNum, DEFAULT_REPEAT_STRIDE });
    }
}


}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/../softmax_flash_base_impl/softmax_flash_nd_process_impl.h" 2

namespace AscendC {

template <typename T, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashNDImpl(const LocalTensor<T> &dst, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &src, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LastAxisShapeND &originalSrcShape,
    const SoftMaxTiling &tiling)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();

    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock<T>(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
            PipeBarrier<PIPE_V>();

            Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);
            Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.reduceSize);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceParam);
        PipeBarrier<PIPE_V>();

        Cast(inMaxTmp, inMaxTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(maxTensor[offset2], tmpBuffer2, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        Cast(inSumTmp, inSumTensor[offset2], RoundMode::CAST_NONE, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2], inMaxTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);
        Cast(sumTensor[offset2], inSumTmp, FLOAT2HALF_ROUND_MODE, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}


[aicore] inline void SoftmaxFlashNDImpl(const LocalTensor<float> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<float> &src, const LocalTensor<float> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &tmpBuffer2 = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inMaxTmp = workLocal[0];

    const ReduceLastND reduceMainParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
                                           tiling.splitK, tiling.reduceM, tiling.reduceK };
    const ReduceLastND reduceTailParam = { tiling.tailM, originalSrcShape.k, tiling.tailM,
                                           tiling.splitK, tiling.tailM, tiling.reduceK };
    const BroadCastLastND mainBrcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    const BroadCastLastND tailBrcParam = { tiling.tailM, tiling.splitK, tiling.tailM, tiling.reduceK };

    uint32_t offset1 = 0;
    uint32_t offset2 = 0;
    for (uint32_t i = 0; i < tiling.rangeM; i++) {
        offset2 = i * tiling.reduceSize;
        offset1 = i * tiling.splitSize;
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceMainParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceMainParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

        DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(sumTensor[offset2], sumTensor[offset2], tiling.reduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.reduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, mainBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.splitSize);
    }

    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, src[offset1], reduceSumBuffer, reduceTailParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, src[offset1], tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, tmpBuffer2, reduceTailParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(tmpBuffer2, inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        DataCopy(maxTensor[offset2], tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Sub(tmpBuffer4, tmpBuffer4, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, tmpBuffer2, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(sumTensor[offset2], inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(expMaxTensor[offset2], inMaxTmp, sumTensor[offset2], tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, sumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, tailBrcParam);
        PipeBarrier<PIPE_V>();
        Mul(dst[offset1], tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
    }
}

template <typename T, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashPostProcess(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling, bool isUpdate = false,
    const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t workLocalSize = workLocal.GetSize();
    if constexpr (sizeof(T) == sizeof(half)) {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {
            SoftmaxFlashNDImpl<T, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, originalSrcShape, tiling);
        }
    } else {
        if (!isUpdate) {
            SoftMaxNDImpl<T, T>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
        } else {

            if constexpr (isBasicBlock) {
                SoftmaxFlashBasicBlockFloat(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, workLocal, tiling);
            } else

            {
                SoftmaxFlashNDImpl(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor, inMaxTensor,
                    workLocal, originalSrcShape, tiling);
            }
        }
    }
}

template <bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashNDImpl(const LocalTensor<half> &dst, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &src, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const LocalTensor<float> &workLocal,
    const LastAxisShapeND &originalSrcShape, const SoftMaxTiling &tiling)
{
    const LocalTensor<float> &tmpBuffer0 = workLocal[0];
    const LocalTensor<float> &tmpBuffer1 = workLocal[tiling.splitSize];
    const LocalTensor<float> &inMaxTmp = workLocal[tiling.splitSize + tiling.splitSize];
    const LocalTensor<float> &reduceSumBuffer = workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize];
    const LocalTensor<float> &tmpBuffer4 =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize];
    const LocalTensor<float> &inSumTmp =
        workLocal[tiling.splitSize + tiling.splitSize + tiling.reduceSize + tiling.reduceSize + tiling.reduceSize];

    ReduceLastND reduceParam = { tiling.splitM, originalSrcShape.k, tiling.splitM,
        tiling.splitK, tiling.reduceM, tiling.reduceK };
    BroadCastLastND brcParam = { tiling.splitM, tiling.splitK, tiling.reduceM, tiling.reduceK };
    uint32_t offset1 = 0;
    uint32_t offset2 = 0;


    if constexpr (isBasicBlock) {
        SoftmaxFlashBasicBlock(dst, sumTensor, maxTensor, src, expMaxTensor, inSumTensor, inMaxTensor, workLocal,
            tiling);
    } else

    {
        for (uint32_t i = 0; i < tiling.rangeM; i++) {
            offset2 = i * tiling.reduceSize;
            offset1 = i * tiling.splitSize;
            Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer1, tmpBuffer1, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
            PipeBarrier<PIPE_V>();

            DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.reduceSize);
            PipeBarrier<PIPE_V>();

            Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(tmpBuffer4, tmpBuffer4, tiling.reduceSize);

            Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Exp(inMaxTmp, inMaxTmp, tiling.reduceSize);

            DataCopy(inSumTmp, inSumTensor[offset2], tiling.reduceSize);

            PipeBarrier<PIPE_V>();
            Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            DataCopy(sumTensor[offset2], inSumTmp, tiling.reduceSize);


            BroadCastLastImpl(tmpBuffer0, inMaxTmp,
                { tiling.reduceM, HALF_NUM_PER_BLK, tiling.reduceM, tiling.reduceK });
            PipeBarrier<PIPE_V>();
            Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
                tiling.reduceSize * HALF_FACTOR);

            Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.reduceSize);
            PipeBarrier<PIPE_V>();
            BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
            PipeBarrier<PIPE_V>();
            Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.splitSize);
            PipeBarrier<PIPE_V>();
            Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.splitSize);
        }
    }
    PipeBarrier<PIPE_V>();
    if (tiling.tailM != 0) {
        offset2 = tiling.rangeM * tiling.reduceSize;
        offset1 = tiling.rangeM * tiling.splitSize;

        Cast(tmpBuffer0, src[offset1], RoundMode::CAST_NONE, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceMaxLastNDImpl(tmpBuffer4, tmpBuffer0, reduceSumBuffer, reduceParam);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer1, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer1, tmpBuffer0, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer1, tmpBuffer1, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        ReduceSumLastNDImpl(reduceSumBuffer, tmpBuffer1, inMaxTmp, reduceParam);
        PipeBarrier<PIPE_V>();

        DataCopy(inMaxTmp, inMaxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Max(maxTensor[offset2], inMaxTmp, tmpBuffer4, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();

        Sub(tmpBuffer4, tmpBuffer4, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(tmpBuffer4, tmpBuffer4, tiling.tailReduceSize);

        Sub(inMaxTmp, inMaxTmp, maxTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Exp(inMaxTmp, inMaxTmp, tiling.tailReduceSize);
        DataCopy(inSumTmp, inSumTensor[offset2], tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Mul(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        Mul(reduceSumBuffer, tmpBuffer4, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Add(inSumTmp, inMaxTmp, reduceSumBuffer, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        Div(inMaxTmp, inMaxTmp, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();


        BroadCastLastImpl(tmpBuffer0, inMaxTmp,
            { tiling.reduceM, FLOAT_NUM_PER_BLK * B16_BYTE_SIZE, tiling.reduceM, tiling.reduceK });
        PipeBarrier<PIPE_V>();
        Cast(expMaxTensor[offset2 * HALF_FACTOR], tmpBuffer0, FLOAT2HALF_ROUND_MODE,
            tiling.tailReduceSize * B16_BYTE_SIZE);
        DataCopy(sumTensor[offset2], inSumTmp, tiling.tailReduceSize);

        Div(tmpBuffer4, tmpBuffer4, inSumTmp, tiling.tailReduceSize);
        PipeBarrier<PIPE_V>();
        BroadCastLastImpl(tmpBuffer0, tmpBuffer4, brcParam);
        PipeBarrier<PIPE_V>();
        Mul(tmpBuffer1, tmpBuffer1, tmpBuffer0, tiling.tailSplitSize);
        PipeBarrier<PIPE_V>();
        Cast(dst[offset1], tmpBuffer1, FLOAT2HALF_ROUND_MODE, tiling.tailSplitSize);
    }
}

}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_common/softmax_common_flash.h" 2

namespace AscendC {

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashCommonImpl(const LocalTensor<T1> &dstTensor, const LocalTensor<T2> &sumTensor,
    const LocalTensor<T2> &maxTensor, const LocalTensor<T1> &srcTensor, const LocalTensor<T1> &expMaxTensor,
    const LocalTensor<T2> &inSumTensor, const LocalTensor<T2> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    LocalTensor<float> workLocal;
    PopStackBuffer<float, TPosition::LCM>(workLocal);
    uint32_t workLocalSize = workLocal.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }
    if constexpr (Std::is_same<T1, half>::value && Std::is_same<T2, float>::value) {
        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
            if (!isUpdate) {
                SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape,
                    newTiling);
            } else {
                SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, workLocal, originalSrcShape, newTiling);
            }
        } else {
            if (!isUpdate) {
                SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, workLocal, originalSrcShape, tiling);
            } else {
                SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, workLocal, originalSrcShape, tiling);
            }
        }
    } else if constexpr (Std::is_same<T1, T2>::value){
        const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T1);
        const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T1);
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, elementNumPerBlk, isUpdate, isBasicBlock);
            SoftmaxFlashPostProcess<T1, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, newTiling, isUpdate);
        } else {
            SoftmaxFlashPostProcess<T1, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, workLocal, originalSrcShape, tiling, isUpdate);
        }
    }
}

template <typename T1, typename T2, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashTmpBufCommonImpl(const LocalTensor<T1> &dstTensor, const LocalTensor<T2> &sumTensor,
    const LocalTensor<T2> &maxTensor, const LocalTensor<T1> &srcTensor, const LocalTensor<T1> &expMaxTensor,
    const LocalTensor<T2> &inSumTensor, const LocalTensor<T2> &inMaxTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SoftMaxTiling &tiling, bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{
    auto tempBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t workLocalSize = tempBuffer.GetSize();
    LastAxisShapeND srcNDinfo;
    LastAxisShapeND originalSrcShape;
    if (softmaxShapeInfo.srcM == 0 || softmaxShapeInfo.srcK == 0) {
        ShapeInfo srcShape = srcTensor.GetShapeInfo();
        srcNDinfo = GetLastAxisShapeND(srcShape);
        originalSrcShape = GetLastAxisOriginShapeND(srcShape);
    } else {
        srcNDinfo = { softmaxShapeInfo.srcM, softmaxShapeInfo.srcK };
        originalSrcShape = { softmaxShapeInfo.oriSrcM, softmaxShapeInfo.oriSrcK };
    }

    if constexpr (Std::is_same<T1, half>::value && Std::is_same<T2, float>::value) {
        if (srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, FLOAT_NUM_PER_BLK, isUpdate, isBasicBlock);
            if (!isUpdate) {
                SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                    newTiling);
            } else {
                SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, tempBuffer, originalSrcShape, newTiling);
            }
        } else {
            if (!isUpdate) {
                SoftMaxNDImpl<half, float>(dstTensor, sumTensor, maxTensor, srcTensor, tempBuffer, originalSrcShape,
                    tiling);
            } else {
                SoftmaxFlashNDImpl<isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                    inMaxTensor, tempBuffer, originalSrcShape, tiling);
            }
        }
    } else if constexpr (Std::is_same<T1, T2>::value){
        const uint32_t elementNumPerBlk = ONE_BLK_SIZE / sizeof(T1);
        const uint32_t elementNumPerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T1);
        if (__builtin_expect(!!(srcNDinfo.k != tiling.srcK || srcNDinfo.m != tiling.srcM), 0)) {
            SoftMaxTiling newTiling = tiling;
            SoftMaxFlashTilingFunc(workLocalSize, srcNDinfo, newTiling, elementNumPerBlk, isUpdate, isBasicBlock);
            SoftmaxFlashPostProcess<T1, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, newTiling, isUpdate);
        } else {
            SoftmaxFlashPostProcess<T1, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
                inMaxTensor, tempBuffer, originalSrcShape, tiling, isUpdate);
        }
    }
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/softmax/softmax_flash_base_impl.h" 2


namespace AscendC {
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{

                                                                                    ;
    SoftmaxFlashCommonImpl<T, T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
        expMaxTensor, inSumTensor, inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashImpl(const LocalTensor<half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &srcTensor, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{

                                                                                               ;
    SoftmaxFlashCommonImpl<half, float, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor,
        expMaxTensor, inSumTensor, inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SoftMaxTiling &tiling, bool isUpdate, const SoftMaxShapeInfo &softmaxShapeInfo)
{

                                                                                                                ;
    SoftmaxFlashTmpBufCommonImpl<T, T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
}

template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[aicore] inline void SoftmaxFlashImpl(const LocalTensor<half> &dstTensor, const LocalTensor<float> &sumTensor,
    const LocalTensor<float> &maxTensor, const LocalTensor<half> &srcTensor, const LocalTensor<half> &expMaxTensor,
    const LocalTensor<float> &inSumTensor, const LocalTensor<float> &inMaxTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SoftMaxTiling &tiling, bool isUpdate,
    const SoftMaxShapeInfo &softmaxShapeInfo)
{

                                                                                                                ;
    SoftmaxFlashTmpBufCommonImpl<half, float, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor,
        srcTensor, expMaxTensor, inSumTensor, inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 51 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[[deprecated("/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" " is deprecated, please use softmaxflashv2.h instead!")]]
[aicore] inline void SoftmaxFlash(const LocalTensor<T> &dstTensor, const LocalTensor<T> &sumTensor,
    const LocalTensor<T> &maxTensor, const LocalTensor<T> &srcTensor, const LocalTensor<T> &expMaxTensor,
    const LocalTensor<T> &inSumTensor, const LocalTensor<T> &inMaxTensor, const SoftMaxTiling &tiling,
    bool isUpdate = false, const SoftMaxShapeInfo &softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[[deprecated("/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" " is deprecated, please use softmaxflashv2.h instead!")]]
[aicore] inline void SoftmaxFlash(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor, const SoftMaxTiling& tiling,
    bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 132 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[[deprecated("/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" " is deprecated, please use softmaxflashv2.h instead!")]]
[aicore] inline void SoftmaxFlash(const LocalTensor<T>& dstTensor, const LocalTensor<T>& sumTensor,
    const LocalTensor<T>& maxTensor, const LocalTensor<T>& srcTensor, const LocalTensor<T>& expMaxTensor,
    const LocalTensor<T>& inSumTensor, const LocalTensor<T>& inMaxTensor, const LocalTensor<uint8_t>& sharedTmpBuffer,
    const SoftMaxTiling& tiling, bool isUpdate = false, const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
# 173 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h"
template <typename T, bool isReuseSource = false, bool isBasicBlock = false>
[[deprecated("/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/softmaxflash.h" " is deprecated, please use softmaxflashv2.h instead!")]]
[aicore] inline void SoftmaxFlash(const LocalTensor<half>& dstTensor, const LocalTensor<float>& sumTensor,
    const LocalTensor<float>& maxTensor, const LocalTensor<half>& srcTensor, const LocalTensor<half>& expMaxTensor,
    const LocalTensor<float>& inSumTensor, const LocalTensor<float>& inMaxTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const SoftMaxTiling& tiling, bool isUpdate = false,
    const SoftMaxShapeInfo& softmaxShapeInfo = {})
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                      ;
    SoftmaxFlashImpl<T, isReuseSource, isBasicBlock>(dstTensor, sumTensor, maxTensor, srcTensor, expMaxTensor, inSumTensor,
        inMaxTensor, sharedTmpBuffer, tiling, isUpdate, softmaxShapeInfo);
                                     ;
}
}
#pragma end_pipe
# 78 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nd012.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nd012.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nz012.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nz012.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_0213.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_0213.h"
namespace AscendC {
const uint32_t CUBE_HALF_SIZE = CUBE_MAX_SIZE / 2;

template <typename T> struct ConfusionTranspose0213Params {
    [aicore] ConfusionTranspose0213Params(){};

    int32_t i;
    int32_t j;
    int32_t k;
    int32_t m;

    TransposeType transposeType;

    TransDataTo5HDParams mainTransDataParams;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams tailTransDataParams;
    TransDataTo5HDParams transDataParams2;


    uint64_t mainDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t mainSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t tailDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t tailSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] inline void InitConfusionTranspose0213TransParams(ConfusionTranspose0213Tiling& tiling,
    ConfusionTranspose0213Params<T>& params, TransposeType transposeTypeIn)
{
    params.transposeType = transposeTypeIn;

    params.mainTransDataParams.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.mainTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.mainTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.mainTransDataParams.repeatTimes == 1) {
        params.mainTransDataParams.dstRepStride = 0;
        params.mainTransDataParams.srcRepStride = 0;
    }

    params.transDataParams1.repeatTimes = tiling.newPopSize / CUBE_MAX_SIZE;
    params.transDataParams1.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams1.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams1.repeatTimes == 1) {
        params.transDataParams1.dstRepStride = 0;
        params.transDataParams1.srcRepStride = 0;
    }

    params.tailTransDataParams.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.tailTransDataParams.dstRepStride = BLOCK_CUBE / tiling.blockSize;
    params.tailTransDataParams.srcRepStride = CUBE_MAX_SIZE / tiling.blockSize;
    if (params.tailTransDataParams.repeatTimes == 1) {
        params.tailTransDataParams.dstRepStride = 0;
        params.tailTransDataParams.srcRepStride = 0;
    }

    params.transDataParams2.repeatTimes = tiling.tailSize / CUBE_MAX_SIZE;
    params.transDataParams2.dstRepStride = tiling.alignA3MulA1 * BLOCK_CUBE / tiling.blockSize;
    params.transDataParams2.srcRepStride = BLOCK_CUBE / tiling.blockSize;
    if (params.transDataParams2.repeatTimes == 1) {
        params.transDataParams2.dstRepStride = 0;
        params.transDataParams2.srcRepStride = 0;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose0213MainHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainDstLocalList[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.mainSrcLocalList[n] = (uint64_t)srcTensor[(tiling.newPopSize * params.m + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] =
                (uint64_t)
                    dstTensor[(tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList1[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
}

template <typename T>
[aicore] inline void ConfusionTranspose0213MainFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.mainDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.mainDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.mainSrcLocalList[n] = (uint64_t)
                srcTensor[(p * tiling.blockSize + tiling.newPopSize * params.m + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.mainDstLocalList, params.mainSrcLocalList, params.mainTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * BLOCK_CUBE +
                    params.i * tiling.alignA3 + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList1[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)]
                    .GetPhyAddr();
                params.dstLocalList1[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.newPopH * params.m * tiling.alignA3MulA1 + params.j * tiling.shapeA1BlockCube +
                    params.i * BLOCK_CUBE + tiling.alignA3MulA1 * (n / 2) + tiling.blockSize +
                    params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList1[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose0213TailHalf(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{

    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailDstLocalList[n] = (uint64_t)tmp1[tiling.newPopH * n].GetPhyAddr();
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.tailSrcLocalList[n] =
            (uint64_t)srcTensor[(tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
            params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)]
            .GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);

    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * BLOCK_CUBE +
                params.i * tiling.alignA3 + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[(tiling.mainOffset + params.j * tiling.shapeA1BlockCube +
                params.i * BLOCK_CUBE + tiling.alignA3MulA1 * n + params.k * tiling.batchOffset)]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp1[(tiling.newPopH * n)].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] inline void ConfusionTranspose0213TailFloat(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTranspose0213Params<T>& params, ConfusionTranspose0213Tiling& tiling, const LocalTensor<T>& tmp1)
{
    for (int16_t p = 0; p < 2; p++) {

        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
            params.tailDstLocalList[n] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2))].GetPhyAddr();
            params.tailDstLocalList[n + 1] =
                (uint64_t)tmp1[(p * tiling.blockSize * tiling.newPopH + tiling.newPopH * (n / 2)) + tiling.blockSize]
                .GetPhyAddr();
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.tailSrcLocalList[n] = (uint64_t)srcTensor[(p * tiling.blockSize +
                tiling.newPopSize * tiling.mainBlocks + params.j * tiling.needSize +
                params.i * tiling.alignA2MulAlignA3 + BLOCK_CUBE * n + params.k * tiling.batchOffset)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.tailDstLocalList, params.tailSrcLocalList, params.tailTransDataParams);
    }
    for (int16_t p = 0; p < 2; p++) {

        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * BLOCK_CUBE + params.i * tiling.alignA3 +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + params.k * tiling.batchOffset)].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[(p * tiling.blockSize * tiling.alignA3MulA1 +
                    tiling.mainOffset + params.j * tiling.shapeA1BlockCube + params.i * BLOCK_CUBE +
                    tiling.alignA3MulA1 * (n / 2) + tiling.blockSize + params.k * tiling.batchOffset)].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp1[(p * tiling.blockSize + tiling.newPopH * n)].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nz012.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose2NZ012NParams {
    [aicore] ConfusionTranspose2NZ012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2Count;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];

    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];

    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] inline void InitConfusionTranspose2NZ012N(ConfusionTranspose2NZ012NParams<T> &params,
    ConfusionTranspose2NZ012NTiling &tiling)
{

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }

    params.tmp1RemainRowCount = 0;
    params.tmp2Count = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.dstPrehnCount = 0;
    params.dstAllCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NTmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsBlockCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NTmp2RemainRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList2[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset]
                        .GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
            }
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params)
{

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NTmp2NeedRowCount(ConfusionTranspose2NZ012NTiling& tiling,
    ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;


            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
            if (params.dstPrehnCount == tiling.hnDiv) {
                params.dstPrehnCount = 0;
            }
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NTmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2NZ012NTiling& tiling, ConfusionTranspose2NZ012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsBlockCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList3[n] =
                        (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                        tiling.alignsBlockCube +
                        params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                        params.k * tiling.dstBatchOffset].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
            }
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_2nd012.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose2ND012NParams {
    [aicore] ConfusionTranspose2ND012NParams(){};

    int32_t i;
    int32_t j;
    int32_t k;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t dstPrehnCount;
    uint32_t dstAllCount;
    uint32_t dstPrehnCountBefore;
    uint32_t PrehnCount;

    TransposeType transposeType;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams1;


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams2;


    uint64_t dstLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList3[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams3;
};

template <typename T>
[aicore] inline void InitConfusionTranspose2ND012N(ConfusionTranspose2ND012NParams<T> &params,
    ConfusionTranspose2ND012NTiling &tiling)
{
    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = BLOCK_CUBE;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.dstPrehnCount = 0;

    params.dstAllCount = 0;
    params.dstPrehnCountBefore = 0;
    params.PrehnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;

    params.transDataParams3.repeatTimes = 1;
    params.transDataParams3.dstRepStride = 0;
    params.transDataParams3.srcRepStride = 0;

    if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp1RemainRowCount(const LocalTensor<T>& srcTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                    .GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (uint16_t m = 0; m < 2; m++) {
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
                }
                for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                    params.srcLocalList1[n] =
                        (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset]
                        .GetPhyAddr();
                }
                PipeBarrier<PIPE_V>();
                TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
            }
        }

        params.tmp1RemainRowCount = BLOCK_CUBE;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp1RemainRowCountFirst(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{


    if (((params.i * BLOCK_CUBE) <= tiling.hnDiv) && (((params.i + 1) * BLOCK_CUBE) > tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE - tiling.gap;
    }

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams1.dstStride = 0;
            dataCopyParams1.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);

            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp2NeedRowCountHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp2NeedRowCountFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList2[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp2NeedRowCount(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012NTmp2NeedRowCountFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;



        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {


            params.tmp1RemainRowCount = 0;
        }

        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }
    }

    if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
        params.tmp2NeedRowCount = BLOCK_CUBE;
    } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
        params.tmp2NeedRowCount =
            (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ? BLOCK_CUBE : (tiling.hnDiv - params.dstPrehnCount);
    } else if (tiling.hnDiv < BLOCK_CUBE) {
        params.tmp2NeedRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(ConfusionTranspose2ND012NTiling& tiling,
    ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp1, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount != 0) {
        if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.tmp2Count = params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        } else {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            dataCopyParams2.dstStride = 0;
            dataCopyParams2.srcStride = 0;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2, tmp1[((BLOCK_CUBE - params.tmp1RemainRowCount) * BLOCK_CUBE)], dataCopyParams2);
            params.dstPrehnCount += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    if (tiling.hnDiv <= BLOCK_CUBE) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) * tiling.alignsMulAlignHnDiv +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    } else {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList3[n] =
                (uint64_t)
                    dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                tiling.alignsMulAlignHnDiv +
                ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                params.j * tiling.alignHnDivCube + tiling.alignHnDiv * n + params.k * tiling.dstBatchOffset]
                .GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList3[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (tiling.hnDiv <= BLOCK_CUBE) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList3[n] =
                    (uint64_t)dstTensor[(params.transdataRepeat - params.j * tiling.prehBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    params.j * tiling.alignHnDivCube + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
            }
        } else {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList3[n] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    params.k * tiling.dstBatchOffset]
                    .GetPhyAddr();
                params.dstLocalList3[n + 1] =
                    (uint64_t)
                        dstTensor[((params.transdataRepeat - params.j * tiling.prehBlockNum) / tiling.hnDivBlockNum) *
                    tiling.alignsMulAlignHnDiv +
                    ((params.transdataRepeat - params.j * tiling.prehBlockNum) % tiling.hnDivBlockNum) * BLOCK_CUBE +
                    params.j * tiling.alignHnDivCube + m * tiling.alignHnDivBlockSize + tiling.alignHnDiv * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList3[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList3, params.srcLocalList3, params.transDataParams3);
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose2ND012Ntmp2NeedRowCountZero(const LocalTensor<T>& dstTensor,
    ConfusionTranspose2ND012NTiling& tiling, ConfusionTranspose2ND012NParams<T>& params, const LocalTensor<T>& tmp2)
{

    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose2ND012Ntmp2NeedRowCountZeroFloat(dstTensor, tiling, params, tmp2);
        }
        params.transdataRepeat += 1;
        if ((params.transdataRepeat % tiling.shapeN) == 0 && (tiling.hnDiv < BLOCK_CUBE)) {
            params.tmp1RemainRowCount = 0;
        } else if ((params.dstAllCount % tiling.shapeH) == 0) {
            params.tmp1RemainRowCount = 0;
        }
        params.tmp2Count = 0;
        if (params.dstPrehnCount == tiling.hnDiv) {
            params.dstPrehnCount = 0;
        }

        if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount == tiling.hnDiv)) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else if ((tiling.hnDiv >= BLOCK_CUBE) && (params.dstPrehnCount != tiling.hnDiv)) {
            params.tmp2NeedRowCount = (tiling.hnDiv - params.dstPrehnCount) >= BLOCK_CUBE ?
                BLOCK_CUBE :
                (tiling.hnDiv - params.dstPrehnCount);
        } else if (tiling.hnDiv < BLOCK_CUBE) {
            params.tmp2NeedRowCount = tiling.hnDiv;
        }
    }
}

}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_base_impl.h" 2

namespace AscendC {
template <typename T> struct ConfusionTranspose012Params {
    [aicore] ConfusionTranspose012Params(){};

    int32_t i;
    int32_t j;
    int32_t k;

    TransposeType transposeType;
    TransDataTo5HDParams transDataParams1;
    TransDataTo5HDParams transDataParams2;

    uint32_t tmp1RemainRowCount;
    uint32_t tmp2NeedRowCount;
    uint32_t transdataRepeat;
    uint32_t tmp2Count;
    uint32_t tmp1Count;
    uint32_t dstAllCount;
    uint32_t dstPreHnCount;


    uint64_t dstLocalList1[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList1[NCHW_CONV_ADDR_LIST_SIZE];


    uint64_t dstLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList2[NCHW_CONV_ADDR_LIST_SIZE];
};

template <typename T>
[aicore] inline void InitConfusionTranspose012TransParams(ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, TransposeType transposeTypeIn)
{
    if (tiling.shapeH < 16) {
        params.tmp2NeedRowCount = tiling.shapeH;
    }
    params.transposeType = transposeTypeIn;

    params.tmp1RemainRowCount = 0;
    params.tmp2NeedRowCount = 16;
    params.transdataRepeat = 0;
    params.tmp2Count = 0;
    params.tmp1Count = 0;
    params.dstAllCount = 0;
    params.dstPreHnCount = 0;

    params.transDataParams1.repeatTimes = 1;
    params.transDataParams1.dstRepStride = 0;
    params.transDataParams1.srcRepStride = 0;

    params.transDataParams2.repeatTimes = 1;
    params.transDataParams2.dstRepStride = 0;
    params.transDataParams2.srcRepStride = 0;
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp1RemainRowCountZero(const LocalTensor<T>& srcTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount != 0) {
        return;
    }
    if constexpr (sizeof(T) == sizeof(half)) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList1[n] = (uint64_t)tmp1[BLOCK_CUBE * n].GetPhyAddr();
            params.srcLocalList1[n] = (uint64_t)srcTensor[params.i * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
    } else if constexpr (sizeof(T) == sizeof(float)) {
        for (uint16_t m = 0; m < 2; m++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList1[n] = (uint64_t)tmp1[m * CUBE_HALF_SIZE + tiling.blockSize * n].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.srcLocalList1[n] =
                    (uint64_t)srcTensor[params.i * tiling.alignsCube + params.j * CUBE_MAX_SIZE +
                        m * tiling.blockSize + BLOCK_CUBE * n + params.k * tiling.srcBatchOffset].GetPhyAddr();
            }
            PipeBarrier<PIPE_V>();
            TransDataTo5HD<T>(params.dstLocalList1, params.srcLocalList1, params.transDataParams1);
        }
    }

    if (tiling.hnDiv > BLOCK_CUBE) {
        if ((params.dstPreHnCount < tiling.hnDiv) && (params.dstPreHnCount + BLOCK_CUBE) > tiling.hnDiv) {
            params.tmp1RemainRowCount = tiling.hnDiv - params.dstPreHnCount;
        } else {
            params.tmp1RemainRowCount = BLOCK_CUBE;
        }
    } else if (tiling.hnDiv <= BLOCK_CUBE) {
        params.tmp1RemainRowCount = tiling.hnDiv;
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp1ToTmp2(const LocalTensor<T>& tmp2, ConfusionTranspose012Tiling& tiling,
    ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp2NeedRowCount <= params.tmp1RemainRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp2NeedRowCount;
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else if (params.tmp2NeedRowCount > params.tmp1RemainRowCount) {
        if (params.tmp1RemainRowCount != 0) {
            DataCopyParams dataCopyParams1;
            dataCopyParams1.blockCount = 1;
            dataCopyParams1.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1, dataCopyParams1);
            params.tmp1Count += params.tmp1RemainRowCount;
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count += params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp2ToDstHalf(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                params.j * tiling.alignhBlockCube + tiling.alignH * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                params.j * CUBE_MAX_SIZE + BLOCK_CUBE * n + params.k * tiling.dstBatchOffset].GetPhyAddr();
        }
    }
    for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
        params.srcLocalList2[n] = (uint64_t)tmp2[BLOCK_CUBE * n].GetPhyAddr();
    }
    PipeBarrier<PIPE_V>();
    TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp2ToDstFloat(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    for (uint16_t m = 0; m < 2; m++) {
        if (params.transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n = n + 2) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH +
                    tiling.alignH * (n / 2) + params.k * tiling.dstBatchOffset].GetPhyAddr();
                params.dstLocalList2[n + 1] = (uint64_t)dstTensor[params.transdataRepeat * BLOCK_CUBE +
                    params.j * tiling.alignhBlockCube + m * tiling.blockSizeMulAlignH + tiling.alignH * (n / 2) +
                    tiling.blockSize + params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        } else if (params.transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                params.dstLocalList2[n] = (uint64_t)dstTensor[params.transdataRepeat * tiling.alignsCube +
                    params.j * CUBE_MAX_SIZE + m * CUBE_HALF_SIZE + tiling.blockSize * n +
                    params.k * tiling.dstBatchOffset].GetPhyAddr();
            }
        }
        for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
            params.srcLocalList2[n] = (uint64_t)tmp2[m * tiling.blockSize + BLOCK_CUBE * n].GetPhyAddr();
        }
        PipeBarrier<PIPE_V>();
        TransDataTo5HD<T>(params.dstLocalList2, params.srcLocalList2, params.transDataParams2);
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp2ToDst(const LocalTensor<T>& dstTensor,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp2)
{
    if (params.tmp2NeedRowCount == 0) {
        if constexpr (sizeof(T) == sizeof(half)) {
            ConfusionTranspose012Tmp2ToDstHalf(dstTensor, tiling, params, tmp2);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            ConfusionTranspose012Tmp2ToDstFloat(dstTensor, tiling, params, tmp2);
        }

        params.tmp2Count = 0;
        params.transdataRepeat += 1;
        if (params.dstAllCount == tiling.shapeH) {
            params.tmp1RemainRowCount = 0;
            params.dstAllCount = 0;
        }

        if ((params.transdataRepeat + 1) != tiling.hBlockNum) {
            params.tmp2NeedRowCount = BLOCK_CUBE;
        } else {
            params.tmp2NeedRowCount = tiling.shapeH - params.transdataRepeat * BLOCK_CUBE;
        }
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose012Tmp1ToTmp2Remain(const LocalTensor<T>& tmp2,
    ConfusionTranspose012Tiling& tiling, ConfusionTranspose012Params<T>& params, const LocalTensor<T>& tmp1)
{

    if (params.tmp1RemainRowCount >= params.tmp2NeedRowCount) {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp2NeedRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp2NeedRowCount;
            params.dstPreHnCount += params.tmp2NeedRowCount;
            params.tmp2Count += params.tmp2NeedRowCount;
            params.tmp1RemainRowCount -= params.tmp2NeedRowCount;
            params.tmp2NeedRowCount = 0;
        }
    } else {
        if (params.tmp2NeedRowCount != 0) {
            DataCopyParams dataCopyParams2;
            dataCopyParams2.blockCount = 1;
            dataCopyParams2.blockLen = params.tmp1RemainRowCount * tiling.blockNum;
            PipeBarrier<PIPE_V>();
            DataCopy(tmp2[(params.tmp2Count * BLOCK_CUBE)], tmp1[(params.tmp1Count * BLOCK_CUBE)], dataCopyParams2);
            params.dstAllCount += params.tmp1RemainRowCount;
            params.dstPreHnCount += params.tmp1RemainRowCount;
            params.tmp2Count = params.tmp1RemainRowCount;
            params.tmp2NeedRowCount -= params.tmp1RemainRowCount;
            params.tmp1RemainRowCount = 0;
        }
    }
}






template <typename T>
[aicore] inline void ConfusionTranspose0213Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling &tiling)
{
    ConfusionTranspose0213Params<T> params;
    InitConfusionTranspose0213TransParams<T>(tiling, params, transposeTypeIn);

    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        for (params.i = 0; params.i < tiling.shapeA1; params.i++) {
            for (params.j = 0; params.j < tiling.widthTiling; params.j++) {
                for (params.m = 0; params.m < tiling.mainBlocks; params.m++) {

                    if constexpr (sizeof(T) == sizeof(half)) {
                        ConfusionTranspose0213MainHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213MainFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
                if (tiling.tailSize) {
                    if constexpr (sizeof(T) == sizeof(half)) {
                        ConfusionTranspose0213TailHalf(dstTensor, srcTensor, params, tiling, tmp1);
                    } else if constexpr (sizeof(T) == sizeof(float)) {
                        ConfusionTranspose0213TailFloat(dstTensor, srcTensor, params, tiling, tmp1);
                    } else {
                                     ;
                    }
                }
            }
        }
    }
}





template <typename T>
[aicore] inline void ConfusionTranspose2NZ012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2NZ012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2NZ012NParams<T> params;
    InitConfusionTranspose2NZ012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2NZ012NTmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2NZ012NTmp2RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2NZ012NTmp2RemainRowCountZero(dstTensor, tiling, params, tmp2);
                ConfusionTranspose2NZ012NCalcTmp2NeedRowCount(tiling, params);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2NZ012NTmp2NeedRowCount(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2NZ012NTmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}





template <typename T>
[aicore] inline void ConfusionTranspose2ND012NCompute(const LocalTensor<T> &dstTensor,
    const LocalTensor<T> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer,
    ConfusionTranspose2ND012NTiling &tiling)
{
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    ConfusionTranspose2ND012NParams<T> params;
    InitConfusionTranspose2ND012N(params, tiling);

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            for (params.i = 0; params.i < tiling.hBlockNum; params.i++) {
                ConfusionTranspose2ND012NTmp1RemainRowCount(srcTensor, tiling, params, tmp1);
                ConfusionTranspose2ND012NTmp1RemainRowCountFirst(tiling, params, tmp1, tmp2);
                ConfusionTranspose2ND012NTmp2NeedRowCount(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {
                    ConfusionTranspose2ND012NTmp2NeedRowCountNotZero(tiling, params, tmp1, tmp2);
                    ConfusionTranspose2ND012Ntmp2NeedRowCountZero(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}






template <typename T>
[aicore] inline void ConfusionTranspose012Compute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling &tiling)
{
    ConfusionTranspose012Params<T> params;
    InitConfusionTranspose012TransParams<T>(tiling, params, transposeTypeIn);
    LocalTensor<T> tmp1 = sharedTmpBuffer.ReinterpretCast<T>();
    LocalTensor<T> tmp2 = tmp1[CUBE_MAX_SIZE];

    for (params.k = 0; params.k < tiling.shapeB; params.k++) {
        params.transdataRepeat = 0;
        for (params.j = 0; params.j < tiling.sBlockNum; params.j++) {
            params.transdataRepeat = 0;
            for (params.i = 0; params.i < (tiling.hnDivBlockNum * tiling.shapeN); params.i++) {

                params.tmp1Count = 0;
                ConfusionTranspose012Tmp1RemainRowCountZero(srcTensor, tiling, params, tmp1);

                if (params.dstPreHnCount == tiling.hnDiv) {
                    params.dstPreHnCount = 0;
                }
                ConfusionTranspose012Tmp1ToTmp2(tmp2, tiling, params, tmp1);
                ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);


                while (params.tmp1RemainRowCount) {

                    ConfusionTranspose012Tmp1ToTmp2Remain(tmp2, tiling, params, tmp1);

                    ConfusionTranspose012Tmp2ToDst(dstTensor, tiling, params, tmp2);
                }
            }
        }
    }
}




template <typename T>
[aicore] inline void ConfusionTransposeOnlyCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    ConfusionTransposeOnlyTiling &tiling)
{
    uint64_t dstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t srcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    TransDataTo5HDParams transDataParams;
    transDataParams.repeatTimes = tiling.repeat;
    transDataParams.dstRepStride = transDataParams.repeatTimes > 1 ? tiling.stride : 0;
    transDataParams.srcRepStride = transDataParams.repeatTimes > 1 ? 1 : 0;
    for (int32_t i = 0; i < tiling.highBlock; i++) {
        if constexpr (sizeof(T) == sizeof(half)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m++) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * m].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        } else if constexpr (sizeof(T) == sizeof(float)) {
            for (int32_t m = 0; m < NCHW_CONV_ADDR_LIST_SIZE; m = m + 2) {
                dstLocalList[m] = (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2)].GetPhyAddr();
                dstLocalList[m + 1] =
                    (uint64_t)dstTensor[i * BLOCK_CUBE + tiling.height * (m / 2) + tiling.blockSize].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                srcLocalList[n] =
                    (uint64_t)srcTensor[i * tiling.width * BLOCK_CUBE + tiling.width * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(dstLocalList, srcLocalList, transDataParams);
        }
    }
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_v220_impl.h" 2

namespace AscendC {





template <typename T>
[aicore] inline void ConfusionTranspose0213(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose0213Tiling& tiling)
{
    ConfusionTranspose0213Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}





template <typename T>
[aicore] inline void ConfusionTranspose2NZ012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2NZ012NTiling& tiling)
{
    ConfusionTranspose2NZ012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}





template <typename T>
[aicore] inline void ConfusionTranspose2ND012N(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, ConfusionTranspose2ND012NTiling& tiling)
{
    ConfusionTranspose2ND012NCompute(dstTensor, srcTensor, sharedTmpBuffer, tiling);
}






template <typename T>
[aicore] inline void ConfusionTranspose012(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeTypeIn, ConfusionTranspose012Tiling& tiling)
{
    ConfusionTranspose012Compute(dstTensor, srcTensor, sharedTmpBuffer, transposeTypeIn, tiling);
}




template <typename T>
[aicore] inline void ConfusionTransposeOnly(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    ConfusionTransposeOnlyTiling& tiling)
{
    ConfusionTransposeOnlyCompute(dstTensor, srcTensor, tiling);
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/confusion_transpose/confusion_transpose_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void ConfusionTransposeImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
                                                                                                                     ;





    if (transposeType == TransposeType::TRANSPOSE_NZ2ND_0213 || transposeType == TransposeType::TRANSPOSE_NZ2NZ_0213) {
        ConfusionTranspose0213(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose0213Tiling&>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITH_N) {
        ConfusionTranspose2NZ012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2NZ012NTiling &>(tiling));
    }




    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITH_N) {
        ConfusionTranspose2ND012N(dstTensor, srcTensor, sharedTmpBuffer,
            reinterpret_cast<ConfusionTranspose2ND012NTiling &>(tiling));
    }





    else if (transposeType == TransposeType::TRANSPOSE_NZ2ND_012_WITHOUT_N ||
        transposeType == TransposeType::TRANSPOSE_NZ2NZ_012_WITHOUT_N) {
        ConfusionTranspose012(dstTensor, srcTensor, sharedTmpBuffer, transposeType,
            reinterpret_cast<ConfusionTranspose012Tiling &>(tiling));
    }



    else if (transposeType == TransposeType::TRANSPOSE_ND2ND_ONLY) {
        ConfusionTransposeOnly(dstTensor, srcTensor, reinterpret_cast<ConfusionTransposeOnlyTiling &>(tiling));
    }
}

template <typename T>
[aicore] inline void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    ConfusionTransposeImpl<T>(dstTensor, srcTensor, sharedTmpBuffer, transposeType, tiling);
}

template <typename T>
[aicore] inline void ConfusionTranspose(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    ConfusionTransposeImpl<T>(dstTensor, srcTensor, tmpBuffer, transposeType, tiling);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h"
template <typename T>
[aicore] inline void Transpose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t> &sharedTmpBuffer, TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    ConfusionTransposeImpl<T>(dst, src, sharedTmpBuffer, transposeType, tiling);
}
# 57 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/confusion_transpose.h"
template <typename T>
[aicore] inline void Transpose(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    TransposeType transposeType, ConfusionTransposeTiling& tiling)
{
    LocalTensor<uint8_t> tmpBuffer;
    bool res = PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
                                                                               ;

    ConfusionTransposeImpl<T>(dst, src, tmpBuffer, transposeType, tiling);
}
#pragma end_pipe
}
# 79 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/selectwithbytesmask.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/selectwithbytesmask.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_impl.h" 1
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_common_impl.h"
namespace AscendC {

template <typename T> [aicore] inline void InitScalarSelectMask(const LocalTensor<T> &tmpMask, T scalar)
{
    SetVectorMask<half, MaskMode::COUNTER>(0, ONE_REPEAT_BYTE_SIZE / sizeof(T));
    Duplicate<T, false>(tmpMask, static_cast<T>(scalar), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask(tmpMask);
    PipeBarrier<PIPE_V>();
}

template <typename U>
[aicore] inline void CastMaskToHalfImpl(const LocalTensor<half> &localMaskTmp, const LocalTensor<U> &mask)
{
    if constexpr (sizeof(U) == 4) {
        LocalTensor<float> tmpTensor = mask.template ReinterpretCast<float>();
        Cast<half, float, false>(localMaskTmp, tmpTensor, RoundMode::CAST_ODD, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE / 2, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 2) {
        LocalTensor<int16_t> tmpTensor = mask.template ReinterpretCast<int16_t>();
        Cast<half, int16_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else if constexpr (sizeof(U) == 1) {
        LocalTensor<uint8_t> tmpTensor = mask.template ReinterpretCast<uint8_t>();
        Cast<half, uint8_t, false>(localMaskTmp, tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / 2 });
    }
}

template <typename T, typename U, bool reverse = false>
[aicore] inline void SelectWithBytesMaskPerAxisImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcAxisLen,
    const uint32_t bucketSize)
{
    const auto paddingLen = AlignUp(bucketSize / ONE_BYTE_BIT_SIZE, ONE_BLK_SIZE);
    const auto localMaskTmpOffset = paddingLen;
    const auto localScalarOffset = sizeof(half) * bucketSize;
    LocalTensor<uint8_t> localMask = sharedTmpBuffer;
    LocalTensor<half> localMaskTmp = sharedTmpBuffer.ReinterpretCast<half>();
    SetVectorMask<half, MaskMode::COUNTER>(0, srcAxisLen);
    CastMaskToHalfImpl<U>(localMaskTmp, mask);
    PipeBarrier<PIPE_V>();

    BinaryRepeatParams binaryParams;
    UnaryRepeatParams unaryParams;
    constexpr auto loopSize = ONE_REPEAT_BYTE_SIZE / sizeof(half);
    const auto repeatTime = DivCeil(srcAxisLen, loopSize);

    if constexpr (!reverse) {
        CompareScalar<half, uint8_t, false>(localMask, localMaskTmp, static_cast<half>(0), CMPMODE::EQ,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    } else {
        CompareScalar<half, uint8_t, false>(localMask, localMaskTmp, static_cast<half>(0), CMPMODE::NE,
            MASK_PLACEHOLDER, repeatTime, unaryParams);
    }
    PipeBarrier<PIPE_V>();
    Select(dst, localMask, src0, 1, binaryParams);
}


template <typename U>
[aicore] inline void RemoveRedundantMask(const LocalTensor<U> &dst, const LocalTensor<U> &mask,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    LocalTensor<uint16_t> tmpDst = dst.template ReinterpretCast<uint16_t>();
    LocalTensor<uint16_t> tmpMask = mask.template ReinterpretCast<uint16_t>();

    uint64_t rsvdCnt;

    GatherMask<uint16_t>(tmpDst, tmpMask, REDUCEV2_MODE_SEVEN, true, info.srcLastAxis * sizeof(U) / sizeof(uint16_t),
        { DEFAULT_BLK_STRIDE, static_cast<uint16_t>(info.firstAxis),
        static_cast<uint16_t>(info.maskLastAxis * sizeof(U) / ONE_BLK_SIZE), 0 },
        rsvdCnt);
    SetMaskCount();
}

template <typename T, typename U, bool reverse = false>
[aicore] inline void SelectWithBytesMaskLoopImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t loopSize,
    const uint32_t totalLen, const uint32_t srcOriginOffset, const uint32_t maskOriginOffset)
{
    for (uint32_t offset = 0; offset < totalLen; offset += loopSize) {
        auto calSize = offset + loopSize > totalLen ? totalLen - offset : loopSize;
        SelectWithBytesMaskPerAxisImpl<T, U, reverse>(dst[srcOriginOffset + offset], src0[srcOriginOffset + offset],
            src1, mask[maskOriginOffset + offset], sharedTmpBuffer, calSize, loopSize);
        PipeBarrier<PIPE_V>();
    }
}
}
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_v220_impl.h"
namespace AscendC {
[aicore] inline uint32_t ComputeMaskExtraBufSize(const uint32_t srcSize, const uint32_t typeSize)
{
    return AlignUp(srcSize * typeSize, ONE_BLK_SIZE);
}

template <typename T, typename U, bool reverse = false>
[aicore] inline void SelectWithBytesMaskProcess(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<U> &tmpMask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info, const uint32_t tmpBufferOffset, const uint32_t loopSize)
{
    if (info.srcLastAxis != info.maskLastAxis) {
        RemoveRedundantMask(tmpMask, mask, sharedTmpBuffer, info);
        PipeBarrier<PIPE_V>();
    }

    SelectWithBytesMaskLoopImpl<T, U, reverse>(dst, src0, src1, tmpMask, sharedTmpBuffer[tmpBufferOffset], loopSize,
        src0.GetSize(), 0, 0);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/../../../impl/adv_api/detail/select/selectwithbytesmask/selectwithbytesmask_impl.h" 2




namespace AscendC {


template <typename T, typename U, bool isReuseMask, bool reverse = false>
[aicore] inline __attribute__((inout_pipe("V"))) void SelectWithBytesMaskImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0,
    T src1, const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer,
    const SelectWithBytesMaskShapeInfo &info)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                               ;
    PipeBarrier<PIPE_V>();
    constexpr uint32_t MIN_REQUIRED_BUFFER = 1024;
    constexpr uint32_t RESERVED_BUFFER = 256;
    constexpr uint32_t MAX_CALC_BYTE_PER_LOOP = 255 * ONE_REPEAT_BYTE_SIZE;
    const uint32_t firstAxis = info.firstAxis;
    const uint32_t srcLastAxis = info.srcLastAxis;
    const uint32_t maskLastAxis = info.maskLastAxis;
    const uint32_t srcSize = src0.GetSize();

    uint32_t bufferSize = sharedTmpBuffer.GetSize();

                                                                                                                 ;
    LocalTensor<U> tmpMask = mask;
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    uint32_t tmpBufferOffset = 0;
    if constexpr (!isReuseMask) {
        if (srcLastAxis != maskLastAxis) {
            const uint32_t tmpMaskRequiredBuffer = ComputeMaskExtraBufSize(srcSize, sizeof(U));




              ;
            tmpMask = sharedTmpBuffer.template ReinterpretCast<U>();
            tmpMask.SetSize(tmpMaskRequiredBuffer / sizeof(U));
            bufferSize -= tmpMaskRequiredBuffer;
            tmpBufferOffset = tmpMaskRequiredBuffer;
        }
    }

    bufferSize -= RESERVED_BUFFER;
    uint32_t loopSize = bufferSize / sizeof(half) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    if (loopSize > MAX_CALC_BYTE_PER_LOOP / sizeof(half)) {
        loopSize = MAX_CALC_BYTE_PER_LOOP / sizeof(half);
    }

    SetMaskCount();
    InitScalarSelectMask(tmpTensor, src1);

    SelectWithBytesMaskProcess<T, U, reverse>(dst, src0, src1, mask, tmpMask, sharedTmpBuffer, info, tmpBufferOffset,
        loopSize);
    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseMask = true>
[aicore] inline void SelectWithBytesMask(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, false>(dst, src0, src1, mask, sharedTmpBuffer, info);
}

template <typename T, typename U, bool isReuseMask = true>
[aicore] inline void SelectWithBytesMask(const LocalTensor<T> &dst, T src0, const LocalTensor<T> &src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, true>(dst, src1, src0, mask, sharedTmpBuffer, info);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/selectwithbytesmask.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 44 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] inline void Select(const LocalTensor<T> &dst, const LocalTensor<T> &src0, T src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, false>(dst, src0, src1, mask, sharedTmpBuffer, info);
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/select/selectwithbytesmask.h"
template <typename T, typename U, bool isReuseMask = true>
[aicore] inline void Select(const LocalTensor<T> &dst, T src0, const LocalTensor<T> &src1,
    const LocalTensor<U> &mask, const LocalTensor<uint8_t> &sharedTmpBuffer, const SelectWithBytesMaskShapeInfo &info)
{
    SelectWithBytesMaskImpl<T, U, isReuseMask, true>(dst, src1, src0, mask, sharedTmpBuffer, info);
}
#pragma end_pipe
}
# 80 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sinh/sinh_common_impl.h" 1
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sinh/sinh_common_impl.h"
namespace AscendC {
constexpr float SINH_NEG_LN_TWO = -0.69314718055994530941723212145818;
constexpr float SINH_NEG_ONE = -1.0;
constexpr float SINH_POINT_FIVE = 0.5;
constexpr uint32_t SINH_HALF_CALC_PROC = 4;
constexpr uint32_t SINH_FLOAT_CALC_PROC = 1;


template <typename T>
[aicore] inline void SinhCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<T>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<T>();


    Muls<T, false>(tmpFloatBuffer1, src, static_cast<T>(SINH_NEG_ONE), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, src, static_cast<T>(SINH_NEG_LN_TWO), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<T, false>(dst, dst, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}




template <>
[aicore] inline void SinhCompute(const LocalTensor<half>& dst, const LocalTensor<half>& src,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t offset)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    const LocalTensor<float>& tmpFloatBuffer1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpFloatBuffer2 = tmpFloatBuffer1[offset];

    Cast<float, half, false>(tmpFloatBuffer1, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_ONE, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, SINH_NEG_LN_TWO, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);

    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / SINH_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / SINH_FLOAT_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        SinhCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        SinhCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, splitCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SinhImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    SinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SinhImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 81 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{
    SinhImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sinh.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sinh(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Sinh<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 81 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swiglu/swiglu_common_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swiglu/swiglu_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swiglu/swiglu_common_impl.h" 2



namespace AscendC {
constexpr float NUMBER_ONE = 1.0;
constexpr uint32_t REPEAT_TIME_SWIGLU = 1;
constexpr uint32_t SWIGLU_HALF_BUFFER_SIZE = 3;
constexpr uint32_t SWIGLU_FLOAT_TMP_BUFFER_SIZE = 0;
constexpr uint32_t SWIGLU_STRIDE_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}


template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLUImpl(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0, const LocalTensor<T>& srcTensor1,
    const float& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer)
{

                                                                         ;
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, srcTensor0.GetSize());
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLUImpl(LocalTensor<T> &dstTensor, LocalTensor<T> &srcTensor0, LocalTensor<T> &srcTensor1,
                              const float &scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLUImpl(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
                              const LocalTensor<T> &srcTensor1, const float &scalarValue,
                              const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                   ;


                                                                                             ;

                                                                                                                    ;

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    uint32_t stackSize = calCount;

    if (sizeof(T) == sizeof(half)) {


        stackSize = sharedTmpBuffer.GetSize() / sizeof(float) / SWIGLU_HALF_BUFFER_SIZE;
    }

    stackSize = ((stackSize * sizeof(T)) / ONE_BLK_SIZE * ONE_BLK_SIZE) / sizeof(T);

    if (stackSize <= 0) {
        stackSize = ONE_BLK_SIZE / sizeof(T);
    }

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }
    if (tail != 0) {

        bool isTail32BAligned = (tail * sizeof(T) % ONE_BLK_SIZE == 0);
        auto tail32BAligned = (tail * sizeof(T) / ONE_BLK_SIZE + (isTail32BAligned ? 0 : 1)) *
                              ONE_BLK_SIZE / sizeof(T);
        SetVectorMask<T>(0, tail);
        SwiGLUImpl(dstTensor[offset], srcTensor0[offset], srcTensor1[offset],
                   scalarValue, tmpBuffer, tail32BAligned);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] inline void SwishCalcSimplified(
   const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const float &scalarValue)
{


    const UnaryRepeatParams unaryParams;

    Muls<float, false>(dstTensor, srcTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<float, false>(dstTensor, dstTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, static_cast<T>(1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<float, false>(dstTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void SwiGLUImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
                                  const float &beta, const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{

    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(dst, src1, scalar);

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <>
[aicore] inline void SwiGLUImpl<half>(const LocalTensor<half> &dst, const LocalTensor<half> &src0,
                                        const LocalTensor<half> &src1, const float &beta,
                                        const LocalTensor<float> &sharedTmpBuffer, uint32_t calCount)
{
    LocalTensor<float> tmpSrc1FloatBuffer1 = sharedTmpBuffer;
    LocalTensor<float> tmpSrc1FloatBuffer2 = sharedTmpBuffer[calCount];
    LocalTensor<float> tmpSrc0FloatBuffer = sharedTmpBuffer[2 * calCount];


    Cast<float, half, false>(tmpSrc1FloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();


    float scalar = static_cast<float>(static_cast<float>(-1.0) * static_cast<float>(beta));
    SwishCalcSimplified(tmpSrc1FloatBuffer2, tmpSrc1FloatBuffer1, scalar);


    Cast<float, half, false>(tmpSrc0FloatBuffer, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS });
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;

    Mul<float, false>(tmpSrc1FloatBuffer2, tmpSrc0FloatBuffer, tmpSrc1FloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dst, tmpSrc1FloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, { 1, 1, DEFAULT_REPEAT_STRIDE / SWIGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 34 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLU(LocalTensor<T>& dstTensor, LocalTensor<T>& srcTensor0, LocalTensor<T>& srcTensor1,
                              const float& scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue);
}
# 55 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer);
}
# 71 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const float& scalarValue, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, calCount);
}
# 93 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swiglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void SwiGLU(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
                              const LocalTensor<T>& srcTensor1, const float& scalarValue,
                              const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    SwiGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, scalarValue, sharedTmpBuffer, calCount);
}
#pragma end_pipe
}
# 82 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/reglu.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/reglu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/reglu/reglu_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/reglu/reglu_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/reglu/reglu_v220_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/reglu/reglu_v220_impl.h"
namespace AscendC {
template <typename T>
[aicore] inline void ReGluCast(const LocalTensor<T> &dstTensor, const LocalTensor<float> &srcTensor)
{
    if constexpr (IsSameType<T, half>::value) {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    } else {
        Cast<T, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    }
    PipeBarrier<PIPE_V>();
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/reglu/reglu_common_impl.h" 2




namespace AscendC {
const uint8_t REGLU_HALF_CALC_PROCEDURE = 3;
const uint32_t REGLU_TEMP_BUFFER_OFFSET = 2U;

[aicore] inline void Compute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor0,
    const LocalTensor<float>& srcTensor1)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Maxs<float, false>(dstTensor, srcTensor1, static_cast<float>(0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, srcTensor0, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void ReGluCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<float>& tmpTensor, const uint32_t splitSize)
{
    const LocalTensor<float>& x0CastBuffer = tmpTensor;
    const LocalTensor<float>& x1CastBuffer = tmpTensor[splitSize];
    const LocalTensor<float>& yCastBuffer = tmpTensor[splitSize * REGLU_TEMP_BUFFER_OFFSET];

    Cast<float, T, false>(x0CastBuffer, srcTensor0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    Cast<float, T, false>(x1CastBuffer, srcTensor1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Compute(yCastBuffer, x0CastBuffer, x1CastBuffer);
    ReGluCast(dstTensor, yCastBuffer);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                                       ;
                                                                                                                  ;




                                                                                        ;

    SetMaskCount();
    if constexpr (IsSameType<T, float>::value) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calCount);
        Compute(dstTensor, srcTensor0, srcTensor1);
    } else {
        uint32_t tmpBufferSize = sharedTmpBuffer.GetSize() / sizeof(float);
                                                                                                     ;
        LocalTensor<float> tmpBuffer;
        tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        uint32_t stackSize = 0;

        stackSize = tmpBufferSize / REGLU_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
                                                                                             ;

        const uint32_t round = calCount / stackSize;
        const uint32_t tail = calCount % stackSize;
        SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);
        uint32_t offset = 0;

        for (uint32_t i = 0; i < round; i++) {
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
            offset = offset + stackSize;
        }

        if (tail != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, tail);
            ReGluCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, stackSize);
        }
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void ReGluImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool hasStackBuffer = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                          ;
    ReGluImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/reglu.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/reglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}







template <typename T, bool isReuseSource = false>
[aicore] inline void ReGlu(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor0,
    const LocalTensor<T>& srcTensor1, const uint32_t calCount)
{
    if (g_coreType == AIC) {
        return;
    }
    ReGluImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, calCount);
}

#pragma end_pipe
}
# 83 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h" 1
# 48 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_v220_impl.h"
namespace AscendC {
[aicore] inline void TanCast(
    const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor, RoundMode castType)
{
    Cast<float, float, false>(dstTensor, srcTensor, castType, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_common_impl.h" 2




namespace AscendC {
constexpr uint32_t TAN_HALF_CALC_PROCEDURE = 10;
constexpr uint32_t TAN_FLOAT_CALC_PROCEDURE = 4;

constexpr float PI_FOR_X_TODIV = 0.3183098733425140380859375;
constexpr float KPI_FIRS_PI_MULS = 0.0009670257568359375;

constexpr float PI_V2 = 3.140625;

constexpr float PI_DOWN = 1.57079637050628662109375;
constexpr float PI_DOWN_NEG = -1.57079637050628662109375;

constexpr float KPI_TWI_PI_MULS = 6.2771141529083251953125e-7;
constexpr float PI_RESDOWN_ADDS = 0.00000004371139000189375;
constexpr float PI_RESDOWN_ADDS_NEG = -0.00000004371139000189375;

constexpr float KPI_THIR_PI_MULS = 1.21644916362129151821136474609375e-10;

constexpr float KPI_FOR_PI_MULS = -1.0291767438275201129727065563201904296875e-13;

constexpr float TAN_RES_MULIT_SCA = 0.0698520831551998762793;
constexpr float TAN_RES_ADDICT_UP = -6.8711573651634203789;
constexpr float TAN_2ADDS = 61.20362572811089435388;
constexpr float TAN_3ADDS = -24.8048928861126769186219;

[aicore] inline void KPI_0(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, PI_V2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void KPI_1(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FIRS_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, srcTensor, PI_DOWN, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, srcTensor, PI_DOWN_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void KPI_2(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_TWI_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor1, resTensor1, PI_RESDOWN_ADDS_NEG, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(resTensor2, resTensor2, PI_RESDOWN_ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void KPI_3(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_THIR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void KPI_4(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(dstTensor, roundTensor, KPI_FOR_PI_MULS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(srcTensor, srcTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor1, resTensor1, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Sub<float, false>(resTensor2, resTensor2, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void TanRound(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<float>& roundTensor, const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 171 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Muls<float, false>(roundTensor, srcTensor, PI_FOR_X_TODIV, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanCast(roundTensor, roundTensor, RoundMode::CAST_RINT);

    KPI_0(dstTensor, srcTensor, roundTensor);
    KPI_1(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_2(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_3(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
    KPI_4(dstTensor, srcTensor, roundTensor, resTensor1, resTensor2);
}

[aicore] inline void TanPolynomialApproximation(const LocalTensor<float>& dstTensor,
    const LocalTensor<float>& srcTensor, const LocalTensor<float>& roundTensor,
    const LocalTensor<float>& resTensor1, const LocalTensor<float>& resTensor2)
{
# 199 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/tan/tan_common_impl.h"
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;


    Mul<float, false>(roundTensor, srcTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, roundTensor, TAN_RES_MULIT_SCA, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_RES_ADDICT_UP, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(dstTensor, dstTensor, TAN_2ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(dstTensor, dstTensor, srcTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(roundTensor, roundTensor, TAN_3ADDS, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor1, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(roundTensor, roundTensor, resTensor2, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Div<float, false>(dstTensor, dstTensor, roundTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TanCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const UnaryRepeatParams unaryParams;
    const LocalTensor<T>& tmpTensor1 = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<T>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<T>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<T>& tmpTensor4 = tmpTensor3[calSize];

    Adds<T, false>(tmpTensor4, srcTensor, static_cast<float>(0.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    TanRound(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
    TanPolynomialApproximation(dstTensor, tmpTensor4, tmpTensor1, tmpTensor2, tmpTensor3);
}

template <>
[aicore] inline void TanCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpBuffer, uint32_t calSize)
{
    const LocalTensor<float>& tempTensorConv = tmpBuffer.ReinterpretCast<float>();
    const LocalTensor<float>& tmpTensor1 = tempTensorConv[calSize];
    const LocalTensor<float>& tmpTensor2 = tmpTensor1[calSize];
    const LocalTensor<float>& tmpTensor3 = tmpTensor2[calSize];
    const LocalTensor<float>& tmpTensor4 = tmpTensor3[calSize];

    Cast<float, half, false>(tmpTensor1, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TanRound(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);
    TanPolynomialApproximation(tempTensorConv, tmpTensor1, tmpTensor2, tmpTensor3, tmpTensor4);

    Cast<half, float, false>(dstTensor, tempTensorConv,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                        ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / TAN_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / TAN_FLOAT_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    const uint32_t loopCount = calCount / splitCount;
    const uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < loopCount; ++i) {
        TanCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer, splitCount);
    }

    if (calcTail > 0) {
        uint32_t tailCount = calcTail / ONE_BLK_SIZE * ONE_BLK_SIZE;
        tailCount = (calcTail % ONE_BLK_SIZE == 0) ? tailCount : (tailCount + ONE_BLK_SIZE);
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        TanCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer, tailCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TanImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 49 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h"
 template <typename T, bool isReuseSource = false>
[aicore] inline void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TanImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 111 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TanImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 128 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/tan.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Tan(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Tan<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 84 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/round.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/round.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/round/round_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/round/round_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/round/round_v220_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/round/round_v220_impl.h"
namespace AscendC {
constexpr uint32_t STRIDE_OF_DIFFERENT_DIGITS = 2;

template <typename T, bool isReuseSource = false>
[aicore] inline void RoundComputeCount(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{}

template <>
[aicore] inline void RoundComputeCount<float, false>(const LocalTensor<float> &dstTensor,
    const LocalTensor<float> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SetVectorMask<float, MaskMode::COUNTER>(0, calCount);
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
}

template <>
[aicore] inline void RoundComputeCount<half, false>(const LocalTensor<half> &dstTensor,
    const LocalTensor<half> &srcTensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = sharedTmpBufferSize / sizeof(float) / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitCount, 0, sharedTmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    const LocalTensor<float> &tmpTensor = sharedTmpBuffer.ReinterpretCast<float>();

    SetVectorMask<half, MaskMode::COUNTER>(0, splitCount);


    for (uint32_t i = 0; i < loopCount; ++i) {
        Cast<float, half, false>(tmpTensor, srcTensor[i * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();

        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dstTensor[i * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
    if (calcTail > 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, calcTail);
        Cast<float, half, false>(tmpTensor, srcTensor[loopCount * splitCount], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS });
        PipeBarrier<PIPE_V>();
        Cast<float, float, false>(tmpTensor, tmpTensor, RoundMode::CAST_RINT, MASK_PLACEHOLDER, (uint8_t)1,
            { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(dstTensor[loopCount * splitCount], tmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            (uint8_t)1, { 1, 1, DEFAULT_REPEAT_STRIDE / STRIDE_OF_DIFFERENT_DIGITS, DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
    }
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/round/round_common_impl.h" 2





namespace AscendC {

template <typename T, bool isReuseSource = false>
[aicore] inline void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                          ;

    SetMaskCount();
    if constexpr (sizeof(T) == sizeof(half)) {
        RoundComputeCount<half, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    } else {
        RoundComputeCount<float, false>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void RoundImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    RoundImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/round.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 53 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/round.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Round(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    RoundImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
#pragma end_pipe
}
# 85 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/trunc/trunc_common_impl.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/trunc/trunc_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/trunc/trunc_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/trunc/trunc_v220_impl.h"
namespace AscendC {
[aicore] inline void TruncCastForTrunc(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_TRUNC, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/trunc/trunc_common_impl.h" 2


namespace AscendC {
[aicore] inline void TruncCompute(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();

    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();


    TruncCastForTrunc(floatTmpTensor, floatTmpTensor, tmpTensor);

    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

[aicore] inline void TruncCompute(const LocalTensor<float>& dstTensor, const LocalTensor<float>& srcTensor,
    const LocalTensor<uint8_t>& tmpTensor)
{

    TruncCastForTrunc(dstTensor, srcTensor, tmpTensor);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                          ;

    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    constexpr uint32_t TRUNC_HALF_CALC_PROCEDURE = 2;
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / TRUNC_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        TruncCompute(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<half>(0, calcTail);
        TruncCompute(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void TruncImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    TruncImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    TruncImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 56 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{
    TruncImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}
# 76 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/trunc.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Trunc(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor)
{
    Trunc<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
#pragma end_pipe
}
# 86 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swish.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swish.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swish/swish_common_impl.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swish/swish_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swish/swish_common_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void SwishCalcSimplified(
    const LocalTensor<T> &dstAddr, const LocalTensor<T> &srcAddr, T &scalarValue, uint32_t repeatTimes)
{


    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstAddr, srcAddr, scalarValue, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Exp<T, false>(dstAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<T, false>(dstAddr, dstAddr, static_cast<T>(1), MASK_PLACEHOLDER, repeatTimes, unaryParams);
    PipeBarrier<PIPE_V>();

    const BinaryRepeatParams binaryParams;
    Div<T, false>(dstAddr, srcAddr, dstAddr, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline __attribute__((inout_pipe("V"))) void SwishCompute(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                    ;
    T scalar = static_cast<T>(static_cast<float>(-1) * static_cast<float>(scalarValue));

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, dataSize);
    SwishCalcSimplified(dstLocal, srcLocal, scalar, 1);
    SetMaskNorm();
# 96 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/swish/swish_common_impl.h"
    ResetMask();
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swish.h" 2

namespace AscendC {
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/swish.h"
template <typename T, bool isReuseSource = false>
[aicore] inline __attribute__((inout_pipe("V"))) void Swish(
    const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, uint32_t dataSize, const T scalarValue)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    SwishCompute<T, isReuseSource>(dstLocal, srcLocal, dataSize, scalarValue);
}
}
# 87 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/sort/topk_utils.h" 1
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/sort/topk_utils.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk_utils_constants.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk_utils_constants.h"
namespace AscendC {
enum class TopKMode {
    TOPK_NORMAL,
    TOPK_NSMALL,
};

};
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/sort/topk_utils.h" 2

namespace AscendC {
struct TopKInfo {
    int32_t outter = 1;
    int32_t inner;
    int32_t n;
};
};
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h" 2





# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_common_utils.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_common_utils.h"
namespace {
constexpr uint16_t MIN_SORT32_SIZE = 32;
constexpr uint16_t MIN_RPSORT16_SIZE = 16;
constexpr uint32_t BUF_LIST_SIZE = 2;
constexpr uint32_t MRG_MAX_ARRAY_SIZE = 15;
constexpr uint32_t MRGSORT_VALID_QUEUE = 4;
constexpr uint32_t MRGSORT_VALID_TWO = 2;
constexpr uint32_t MRGSORT_VALID_TWO_OFFSET = 769;
constexpr uint32_t TWO = 2;
constexpr uint32_t THREE = 3;
constexpr uint32_t FOUR = 4;
constexpr uint32_t FIVE = 5;
constexpr uint32_t SIX = 6;
constexpr uint32_t SEVEN = 7;
constexpr uint32_t EIGHT = 8;
constexpr uint32_t NINE = 9;
constexpr uint32_t TWELVE = 12;
constexpr uint32_t SIXTEEN = 16;
constexpr uint32_t THIRTY_TWO = 32;
constexpr uint32_t FORTYEIGHT = 48;
constexpr uint32_t VREDUCEV2_HALF_MASK = 128;
constexpr uint32_t VREDUCEV2_FOUR_BYTE_MASK = 64;
constexpr uint32_t SRC1_STACK_TENSORSIZE = 10;
constexpr uint32_t SRC1_STACK_VAL_OFFSET = 16;
constexpr uint32_t TOPK_INNER_ALIGN_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_HALF_LEN = 2048;
constexpr uint32_t TOPK_NSMALL_INNER_LEN = 32;
constexpr uint32_t TOPK_NORMAL_INNER_MAX_LEN = 4096;
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_common_impl.h" 1
# 29 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_v220_impl.h" 1
# 27 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_v220_impl.h"
namespace AscendC {
# 64 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_v220_impl.h"
template <typename T>
[aicore] inline void MrgFourQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, uint16_t &z, int32_t &mrgFourQueueCount, int32_t &dstIdx)
{
    uint16_t innerU16Type = static_cast<uint16_t>(topKInfo.inner);
    for (; z * MRGSORT_VALID_QUEUE <= innerU16Type; z *= MRGSORT_VALID_QUEUE) {
        auto src = (mrgFourQueueCount % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        dstIdx = (mrgFourQueueCount + 1) % TWO;
        auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
        mrgFourQueueCount += 1;
        uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {z, z, z, z};
        struct MrgSort4Info srcInfo(elementLengths, false, 0b1111, tilling.mrgSortRepeat / z);
        struct MrgSortSrcList<T> srcList(src,
            src[z * tilling.mrgSortSrc1offset],
            src[z * tilling.mrgSortSrc2offset],
            src[z * tilling.mrgSortSrc3offset]);
        MrgSort<T>(dst, srcList, srcInfo);
        PipeBarrier<PIPE_V>();
        const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
        DataCopy(src, dst, intriParams);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T>
[aicore] inline void MrgTwoQueueSort(const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo, const uint16_t z, const int32_t mrgFourQueueCount, int32_t &dstIdx, const int32_t k)
{
    int32_t arrayCount = 0;
    if (z < topKInfo.inner) {

        int32_t mrgArray[MRG_MAX_ARRAY_SIZE] = {0};
        int32_t tmpInner = topKInfo.inner;
        for (int32_t i = z; i >= MIN_SORT32_SIZE; i /= MRGSORT_VALID_QUEUE) {
            int32_t count;
            for (count = 0; count < tmpInner / i; ++count) {
                mrgArray[arrayCount++] = i;
            }
            tmpInner -= count * i;
        }
        uint16_t mrgSortedLen = 0;
        for (int32_t i = 0; i < arrayCount - 1; ++i) {
            auto src = ((mrgFourQueueCount + i) % TWO == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            dstIdx = (mrgFourQueueCount + 1 + i) % TWO;
            auto dst = (dstIdx == 1) ? tmpLocal[tilling.innerDataSize] : tmpLocal[0];
            mrgSortedLen += static_cast<uint16_t>(mrgArray[i]);
            uint64_t tmpMrgSortedLen = mrgSortedLen;
            uint64_t tmpMrgArray = mrgArray[i + 1];
            if (mrgSortedLen > k) {
                tmpMrgSortedLen = k;
            }
            if (mrgArray[i + 1] > k) {
                tmpMrgArray = k;
            }
            uint16_t elementLengths[MRG_SORT_ELEMENT_LEN] = {
                static_cast<uint16_t>(tmpMrgSortedLen), static_cast<uint16_t>(tmpMrgArray), 0, 0};
            struct MrgSort4Info srcInfo(elementLengths, false, 0b0011, 1);
            struct MrgSortSrcList<T> srcList(src, src[mrgSortedLen * tilling.mrgSortTwoQueueSrc1Offset], src, src);
            MrgSort<T>(dst, srcList, srcInfo);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T>
[aicore] inline void GatherDstValAndDstIdx(const LocalTensor<T> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<T> &tmpLocal, const TopkTiling &tilling,
    const int32_t dstIdx, const int32_t dstOffsetFourBytes, const int outterIdx)
{
    uint64_t rsvdCnt = 0;
    int32_t tmpLocalDstOffset = tilling.innerDataSize * dstIdx;
    if constexpr (sizeof(T) == sizeof(float)) {

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetFourBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_ONE,
                      true, tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    } else {
        int32_t dstOffsetTwoBytes = outterIdx * tilling.kAlignTwoBytes;

        struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
        GatherMask<T>(dstValueLocal[dstOffsetTwoBytes], tmpLocal[tmpLocalDstOffset], REDUCEV2_MODE_THREE,
                      true, tilling.maskVreducev2TwoBytes, reducev2Params, rsvdCnt);
    }
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBuffer = dstIndexLocal[dstOffsetFourBytes].template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal[tmpLocalDstOffset].template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_BLK_STRIDE);
    GatherMask<float>(tempBuffer, tempBufferLocal, REDUCEV2_MODE_TWO, true,
                      tilling.maskVreducev2FourBytes, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();
    SetMaskCount();
}

template <typename T, bool isInitIndex>
[aicore] inline void TmpLocalSort32(const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal,
    const LocalTensor<T> &tmpLocal, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest,
    const int outterIdx, const UnaryRepeatParams unaryParams)
{


    int offset = outterIdx * topKInfo.inner;
    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, topKInfo.inner);
        Muls<T, false>(tmpLocal[tilling.innerDataSize], srcLocal[offset], T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, tilling.sortRepeat);
        }
    } else {
        if constexpr (!isInitIndex) {
            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.srcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        } else {
            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal[offset], tempBufferUint32, tilling.sortRepeat);
        }
    }
    PipeBarrier<PIPE_V>();
    const DataCopyParams intriParams = {static_cast<uint16_t>(tilling.copyUbToUbBlockCount), 1, 0, 0};
    DataCopy(tmpLocal[tilling.innerDataSize], tmpLocal, intriParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] inline void TopKCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    const UnaryRepeatParams unaryParams;
    for (int j = 0; j < topKInfo.outter; ++j) {
        int32_t dstOffsetFourBytes = j * tilling.kAlignFourBytes;
        TmpLocalSort32<T, isInitIndex>(srcLocal, srcIndexLocal, tmpLocal, tilling, topKInfo, isLargest, j, unaryParams);

        int32_t mrgFourQueueCount = 0;
        uint16_t z = MIN_SORT32_SIZE;
        int32_t dstIdx = 0;
        MrgFourQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx);
        MrgTwoQueueSort<T>(tmpLocal, tilling, topKInfo, z, mrgFourQueueCount, dstIdx, k);
        GatherDstValAndDstIdx(dstValueLocal, dstIndexLocal, tmpLocal, tilling, dstIdx, dstOffsetFourBytes, j);

        if constexpr (isHasfinish) {
            bool finishValue = finishLocal.GetValue(j);
            auto eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
            SetFlag<HardEvent::S_V>(eventID);
            WaitFlag<HardEvent::S_V>(eventID);

            if (finishValue) {
                SetVectorMask<T, MaskMode::COUNTER>(0, k);
                Duplicate<int32_t, false>(dstIndexLocal[dstOffsetFourBytes],
                    static_cast<int32_t>(topKInfo.n),
                    MASK_PLACEHOLDER,
                    1,
                    1,
                    DEFAULT_REPEAT_STRIDE);
            }
        }
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void TopKNSmallGetFloatTopKValue(const LocalTensor<float> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<float> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint32_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint32_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(SRC1_STACK_TENSORSIZE);

    src1stackTensor.SetValue(0, tilling.vreduceValMask0);

    src1stackTensor.SetValue(1, tilling.vreduceValMask1);

    src1stackTensor.SetValue(EIGHT, tilling.vreduceIdxMask0);
    src1stackTensor.SetValue(NINE, tilling.vreduceIdxMask1);

    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<float, uint32_t>(dstValueLocal, tmpLocal, src1stackTensor,
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params, rsvdCnt);


    LocalTensor<float> tempBuffer = dstIndexLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBuffer, tmpLocal,
                                tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + EIGHT].ReinterpretCast<uint32_t>(),
                                true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void TopKNSmallGetHalfTopKValue(const LocalTensor<half> &dstValueLocal,
    const LocalTensor<int32_t> &dstIndexLocal, const LocalTensor<half> &tmpLocal, const TopkTiling &tilling,
    const TopKInfo &topKInfo)
{
    LocalTensor<uint16_t> src1stackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset].template
                                            ReinterpretCast<uint16_t>();
    auto eventID = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventID);
    WaitFlag<HardEvent::V_S>(eventID);
    src1stackTensor.SetSize(EIGHT);
    src1stackTensor.SetValue(0, tilling.vreducehalfValMask0);
    src1stackTensor.SetValue(1, tilling.vreducehalfValMask1);
    src1stackTensor.SetValue(TWO, tilling.vreducehalfValMask2);
    src1stackTensor.SetValue(THREE, tilling.vreducehalfValMask3);
    src1stackTensor.SetValue(FOUR, tilling.vreducehalfValMask4);
    src1stackTensor.SetValue(FIVE, tilling.vreducehalfValMask5);
    src1stackTensor.SetValue(SIX, tilling.vreducehalfValMask6);
    src1stackTensor.SetValue(SEVEN, tilling.vreducehalfValMask7);
    LocalTensor<uint32_t> indexstackTensor = tmpLocal[tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET].
                                             template ReinterpretCast<uint32_t>();;
    indexstackTensor.SetSize(TWO);
    indexstackTensor.SetValue(0, tilling.vreduceIdxMask0);
    indexstackTensor.SetValue(1, tilling.vreduceIdxMask1);
    eventID = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::S_V>(eventID);
    WaitFlag<HardEvent::S_V>(eventID);

    struct GatherMaskParams reducev2Params(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    uint64_t rsvdCnt = 0;
    GatherMask<half, uint16_t>(dstValueLocal, tmpLocal, src1stackTensor,
                               true, VREDUCEV2_HALF_MASK, reducev2Params, rsvdCnt);
    PipeBarrier<PIPE_V>();

    LocalTensor<float> tempBufferIndex = dstIndexLocal.template ReinterpretCast<float>();
    LocalTensor<float> tempBufferLocal = tmpLocal.template ReinterpretCast<float>();
    struct GatherMaskParams reducev2Params2(DEFAULT_BLK_STRIDE, topKInfo.outter, DEFAULT_REPEAT_STRIDE, 0);
    GatherMask<float, uint32_t>(tempBufferIndex, tempBufferLocal,
        tempBufferLocal[(tilling.topkMrgSrc1MaskSizeOffset + SRC1_STACK_VAL_OFFSET) / TWO].ReinterpretCast<uint32_t>(),
        true, VREDUCEV2_FOUR_BYTE_MASK, reducev2Params2, rsvdCnt);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isInitIndex, bool isHasfinish>
[aicore] inline void TopKNSmallCompute(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<T> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest)
{
    if (!isLargest) {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, tmpLocal[tilling.innerDataSize], tempBufferUint32, topKInfo.outter);
        }
    } else {
        if (!isInitIndex) {

            LocalTensor<uint32_t> tempBufferUint32 = tmpLocal[tilling.topkNSmallSrcIndexOffset].template
                                                     ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        } else {

            LocalTensor<uint32_t> tempBufferUint32 = srcIndexLocal.template ReinterpretCast<uint32_t>();
            Sort32<T>(tmpLocal, srcLocal, tempBufferUint32, topKInfo.outter);
        }
    }
    PipeBarrier<PIPE_V>();

    if constexpr (sizeof(T) == sizeof(float)) {
        TopKNSmallGetFloatTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
    } else {
        TopKNSmallGetHalfTopKValue(dstValueLocal, dstIndexLocal, tmpLocal, tilling, topKInfo);
    }
}

[aicore] inline void CopyData(LocalTensor<int32_t> indexLocalTmp, const TopKInfo &topKInfo)
{
    Copy(indexLocalTmp[topKInfo.inner], indexLocalTmp, THIRTY_TWO, topKInfo.outter - 1, {1, 1, FOUR, 0});
    PipeBarrier<PIPE_V>();
}

}
# 30 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/../../../impl/adv_api/detail/sort/topk/topk_common_impl.h" 2





namespace AscendC {
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] inline void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.srcIndexOffset].template ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
    }

    SetMaskCount();
    TopKCompute<T, isInitIndex, isHasfinish>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal,
        tempBuffer, k, tilling, topKInfo, isLargest);

    if (!isLargest) {
        const UnaryRepeatParams unaryParams;
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] inline void TopKNormal(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<uint8_t> stackTensor;
    PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);


                                             ;
    stackTensor.SetSize(tilling.tmpLocalSize * sizeof(T));
    TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
        finishLocal, stackTensor, k, tilling, topKInfo, isLargest);
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] inline void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{
    LocalTensor<T> tempBuffer = tmpLocal.template ReinterpretCast<T>();

    if constexpr (!isInitIndex) {
        LocalTensor<int32_t> indexLocalTmp = tempBuffer[tilling.topkNSmallSrcIndexOffset].template
                                             ReinterpretCast<int32_t>();
        ArithProgression(indexLocalTmp, static_cast<int32_t>(0), static_cast<int32_t>(1), topKInfo.inner);
        PipeBarrier<PIPE_V>();
        if (topKInfo.outter > 1) {
            CopyData(indexLocalTmp, topKInfo);
        }
    }

    SetMaskCount();
    const UnaryRepeatParams unaryParams;

    if (!isLargest) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.allDataSize);
        Muls<T, false>(tempBuffer[tilling.innerDataSize], srcLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }
    TopKNSmallCompute<T, isInitIndex, isHasfinish>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal,
        tempBuffer, k, tilling, topKInfo, isLargest);

    if (!isLargest) {
        PipeBarrier<PIPE_V>();
        SetMaskCount();
        SetVectorMask<T, MaskMode::COUNTER>(0, tilling.maskOffset);
        Muls<T, false>(dstValueLocal, dstValueLocal, T(-1), MASK_PLACEHOLDER, 1, unaryParams);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false>
[aicore] inline void TopKNSmall(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{
    LocalTensor<uint8_t> stackTensor;
    PopStackBuffer<uint8_t, TPosition::LCM>(stackTensor);


                                             ;
    stackTensor.SetSize(tilling.tmpLocalSize * sizeof(T));

    TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
        finishLocal, stackTensor, k, tilling, topKInfo, isLargest);
}

}
# 28 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 64 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] inline void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const LocalTensor<uint8_t> &tmpLocal, const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo,
    const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                                                                       ;

    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
            finishLocal, tmpLocal, k, tilling, topKInfo, isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal,
            finishLocal, tmpLocal, k, tilling, topKInfo, isLargest);
    }
}
# 114 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/sort/topk.h"
template <typename T, bool isInitIndex = false, bool isHasfinish = false, bool isReuseSrc = false,
    enum TopKMode topkMode = TopKMode::TOPK_NORMAL>
[aicore] inline void TopK(const LocalTensor<T> &dstValueLocal, const LocalTensor<int32_t> &dstIndexLocal,
    const LocalTensor<T> &srcLocal, const LocalTensor<int32_t> &srcIndexLocal, const LocalTensor<bool> &finishLocal,
    const int32_t k, const TopkTiling &tilling, const TopKInfo &topKInfo, const bool isLargest = true)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


                                                                                                             ;

    if constexpr (topkMode == TopKMode::TOPK_NORMAL) {
        TopKNormal<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
    if constexpr (topkMode == TopKMode::TOPK_NSMALL) {
        TopKNSmall<T, isInitIndex, isHasfinish, isReuseSrc>(
            dstValueLocal, dstIndexLocal, srcLocal, srcIndexLocal, finishLocal, k, tilling, topKInfo, isLargest);
    }
}
#pragma end_pipe
}
# 88 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/geglu/geglu_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/geglu/geglu_common_impl.h"
namespace AscendC {
constexpr float COEFF0 = -0.0713548162726;
constexpr float COEFF1 = 2.2363860002236e1;
constexpr uint32_t GEGLU_HALF_BUFFER_SIZE = 8;
constexpr uint32_t GEGLU_FLOAT_BUFFER_SIZE = 0;
constexpr uint32_t GEGLU_STRIDE_DIGITS = 2;
constexpr uint32_t GEGLU_ALGINED = 31;

template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }
    LocalTensor<uint8_t> tmpBuffer;
    PopStackBuffer<uint8_t, TPosition::LCM>(tmpBuffer);
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLUImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{
                                                                                                                       ;

    if (g_coreType == AIC) {
        return;
    }
# 62 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/../../../impl/adv_api/detail/activation/geglu/geglu_common_impl.h"
    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    tmpBuffer.SetSize(sharedTmpBuffer.GetSize() / sizeof(float));
    SetMaskCount();

    if (sizeof(T) == sizeof(half)) {
        auto tmpBufCount = sharedTmpBuffer.GetSize() / GEGLU_HALF_BUFFER_SIZE;
        tmpBufCount = tmpBufCount * sizeof(T) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
        for (uint32_t offset = 0; offset < calCount; offset += tmpBufCount) {
            auto splitSize = (calCount - offset) > tmpBufCount ? tmpBufCount : (calCount - offset);
            SetVectorMask<T>(0, splitSize);
            splitSize = (splitSize * sizeof(T) + GEGLU_ALGINED) / ONE_BLK_SIZE * ONE_BLK_SIZE / sizeof(T);
            GeGLUCompute(dstTensor[offset], srcTensor0[offset], srcTensor1[offset], tmpBuffer, splitSize);
        }
    } else {
        SetVectorMask<T>(0, calCount);
        GeGLUCompute(dstTensor, srcTensor0, srcTensor1, tmpBuffer, calCount);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] inline void GeGLUCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src0, const LocalTensor<T> &src1,
    const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;


    Mul<T, false>(dst, src1, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(COEFF1), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, dst, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<T, false>(dst, dst, static_cast<T>(COEFF0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<T, false>(dst, dst, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<T, false>(dst, dst, static_cast<T>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Div<T, false>(dst, src1, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<T, false>(dst, src0, dst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}



template <>
[aicore] inline void GeGLUCompute(const LocalTensor<half> &dst, const LocalTensor<half> &src0,
    const LocalTensor<half> &src1, const LocalTensor<float> &tmpBuffer, uint32_t calSize)
{
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;

    LocalTensor<float> tmpFloatBuffer1 = tmpBuffer;
    LocalTensor<float> tmpFloatBuffer2 = tmpBuffer[calSize];

    Cast<float, half, false>(tmpFloatBuffer1, src1, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, tmpFloatBuffer1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, COEFF0, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Exp<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmpFloatBuffer2, tmpFloatBuffer2, static_cast<float>(1.0), MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpFloatBuffer1, src0, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS});
    PipeBarrier<PIPE_V>();


    Mul<float, false>(tmpFloatBuffer2, tmpFloatBuffer1, tmpFloatBuffer2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dst, tmpFloatBuffer2, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE / GEGLU_STRIDE_DIGITS, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h" 2
namespace AscendC {
#pragma begin_pipe(V)
# 32 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer, uint32_t calCount)
{
    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, calCount);
}
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, uint32_t calCount)
{

    if (g_coreType == AIC) {
        return;
    }

    GeGLUImpl<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, calCount);
}
# 67 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/activation/geglu.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1, const LocalTensor<uint8_t> &sharedTmpBuffer)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, sharedTmpBuffer, srcTensor0.GetSize());
}







template <typename T, bool isReuseSource = false>
[aicore] inline void GeGLU(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor0,
    const LocalTensor<T> &srcTensor1)
{
    GeGLU<T, isReuseSource>(dstTensor, srcTensor0, srcTensor1, srcTensor0.GetSize());
}
#pragma end_pipe
}
# 89 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/lgamma.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/lgamma.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_utils.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_utils.h"
namespace AscendC {
namespace {
constexpr float f05 = 0.5;
constexpr float f07 = 0.7;
constexpr float f15 = 1.5;
constexpr float f58 = 5.8;
constexpr float f1 = 1.0;
constexpr float f2 = 2.0;
constexpr float f3 = 3.0;
constexpr float fn05 = -0.5;
constexpr float fn1 = -1.0;
constexpr float fn2 = -2.0;
constexpr float fn3 = -3.0;
constexpr float fPi = 3.1415927410125732421875;
constexpr uint32_t FLOAT_NOREUSE_CALC_PROC = 8;
constexpr uint32_t FLOAT_REUSE_CALC_PROC = 7;
constexpr uint32_t LGAMMA_HALF_CALC_PROCEDURE = 13;
constexpr uint32_t i2 = 2;
constexpr uint32_t i4 = 4;
constexpr uint32_t i6 = 6;
constexpr uint32_t i7 = 7;
constexpr uint32_t i16 = 16;
constexpr float PI = 3.14159265358979323846264338327950288;
constexpr float t0 = 0.0f;
constexpr float t4 = 4.0f;
constexpr float t5 = 5.0f;
constexpr float t01 = 0.1f;
constexpr float t12 = 12.0f;
constexpr float N01 = -0.1f;

constexpr size_t params007Len = 7U;
constexpr float params007[params007Len] = {0.00358751555905,
    -0.00547128543258,
    -0.0446271263063,
    0.167317703366,
    -0.0421359799802,
    -0.655867278576,
    0.577215373516};

constexpr size_t params0715Len = 11U;
constexpr float params0715[params0715Len] = {0.0458826646209,
    0.103739671409,
    0.122803635895,
    0.127524212003,
    0.143216684461,
    0.169343575835,
    0.207407936454,
    0.27058750391,
    0.400685429573,
    0.82246696949,
    0.577215671539};

constexpr size_t params153Len = 10U;
constexpr float params153[params153Len] = {4.95984932058e-05,
    -0.000220894842641,
    0.000541314249858,
    -0.00120451697148,
    0.00288425176404,
    -0.00738275796175,
    0.0205813199282,
    -0.067352488637,
    0.322467029095,
    0.422784328461};

constexpr size_t params378X1Len = 4U;
constexpr size_t params378X2Len = 3U;
constexpr float params378X1[params378X1Len] = {-748.890319824, -12349.7421875, -41061.375, -48310.6640625};
constexpr float params378X2[params378X2Len] = {-259.250976562, -10777.1796875, -92685.046875};

constexpr size_t params58Len = 2U;
constexpr float params58[params58Len] = {0.000777830660809, -0.00277765537612};

constexpr size_t negParamsOddLen = 4U;
constexpr float negParamsOdd[negParamsOddLen] = {0.00002427957952022552490234375,
    -0.001388786011375486850738525390625,
    0.0416667275130748748779296875,
    -0.4999999701976776123046875};
constexpr size_t negParamsEvenLen = 3U;
constexpr float negParamsEven[negParamsEvenLen] = {
    -0.000195746586541645228862762451171875, 0.0083327032625675201416015625, -0.16666662693023681640625};
}

struct LGammaParams {
    [aicore] LGammaParams()
    {}
    LocalTensor<float> tmp1;
    LocalTensor<float> tmp2;
    LocalTensor<float> tmp3;
    LocalTensor<float> tmp4;
    LocalTensor<float> tmp5;
    LocalTensor<float> tmp6;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> tmpMask1;
    LocalTensor<uint8_t> tmpMask2;
    LocalTensor<uint8_t> tmpMask3;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_basic_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_basic_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_v220_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_v220_impl.h"
namespace AscendC {
[aicore] inline void LGammaFloor(const LocalTensor<float> &dst, const LocalTensor<float> &src)
{
    Cast<float, float, false>(
        dst, src, RoundMode::CAST_FLOOR, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_basic_impl.h" 2




namespace AscendC {
[aicore] inline void LGammaCalcMulAdd(const LocalTensor<float> &tmp, const LocalTensor<float> &src,
    const UnaryRepeatParams &unaryParams, const BinaryRepeatParams binaryParams, const float params[],
    const size_t paramLen)
{

    Muls<float, false>(tmp, src, params[0], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(tmp, tmp, params[1], MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    for (size_t i = 2U; i < paramLen && i < params0715Len - 1U; ++i) {

        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[i], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }

    if (paramLen == params0715Len) {
        Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
        Adds<float, false>(tmp, tmp, params[params0715Len - 1U], MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
    }


    Mul<float, false>(tmp, tmp, src, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGamma007(const LocalTensor<float> &src, const LGammaParams &params)
{

    LGammaCalcMulAdd(params.tmp1, src, params.unaryParams, params.binaryParams, params007, params007Len);


    Mul<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGamma0715(const LocalTensor<float> &src, const LGammaParams &params)
{

    Muls<float, false>(params.tmp1, src, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params0715, params0715Len);
}


[aicore] inline void LGamma153(const LocalTensor<float> &src, const LGammaParams &params)
{

    Adds<float, false>(params.tmp1, src, fn2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params153, params153Len);
}


[aicore] inline void LGamma358(const LocalTensor<float> &src, const LGammaParams &params)
{

    Adds<float, false>(params.tmp1, src, fn3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp2, params.tmp1, params.unaryParams, params.binaryParams, params378X1, params378X1Len);

    constexpr float ftmp2 = -143033.40625;
    Adds<float, false>(params.tmp2, params.tmp2, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp3, params.tmp1, params.unaryParams, params.binaryParams, params378X2, params378X2Len);

    constexpr float ftmp3 = -206353.578125;
    Adds<float, false>(params.tmp3, params.tmp3, ftmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Div<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp3, params.tmp3, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGamma58(const LocalTensor<float> &src, const LGammaParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(params.tmp2, src, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp2, params.tmp2, params.tmp1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Reciprocal<float, false>(params.tmp3, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmp4, params.tmp3, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(params.tmp1, params.tmp4, params.unaryParams, params.binaryParams, params58, params58Len);
    constexpr float ftmp1 = 0.0833332762122;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp2 = 0.91893851757;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp1, params.tmp1, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaGenLTMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaGenGEMask(
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src, const LGammaParams &params, const float scalar)
{
    Duplicate<float, false>(params.tmp1, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmp1, CMPMODE::GE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaGenRangeMask(
    const LocalTensor<float> &src, const LGammaParams &params, const float min, const float max)
{
    LGammaGenLTMask(params.mask, src, params, max);
    LGammaGenGEMask(params.tmpMask1, src, params, min);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &mask, const LGammaParams &params)
{
    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(params.tmp1, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dst, params.tmp1, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaPositive(const LGammaParams &params)
{
    Duplicate<float, false>(params.tmp5, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaGenLTMask(params.mask, params.tmp6, params, f07);
    LGamma007(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f07, f15);
    LGamma0715(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f15, f3);
    LGamma153(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp2, params.mask, params);


    LGammaGenRangeMask(params.tmp6, params, f3, f58);
    LGamma358(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp3, params.mask, params);


    NotNumUnion notNum;
    notNum.i = F32_INF;
    LGammaGenRangeMask(params.tmp6, params, f58, notNum.f);
    LGamma58(params.tmp6, params);
    LGammaSelect(params.tmp5, params.tmp1, params.mask, params);


    LGammaGenGEMask(params.mask, params.tmp6, params, notNum.f);
    LGammaSelect(params.tmp5, params.tmp6, params.mask, params);
}


[aicore] inline void LGammaCalNegTmp1(const LGammaParams &params)
{

    Add<float, false>(params.tmp2, params.tmp6, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmp2, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp2, params.tmp2);


    Muls<float, false>(params.tmp3, params.tmp2, f05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaFloor(params.tmp3, params.tmp3);
    Muls<float, false>(params.tmp3, params.tmp3, f2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmp3, params.tmp2, params.tmp3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaGenGEMask(params.mask, params.tmp3, params, f05);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));

    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    SetVectorMask<float>(0, params.splitSize);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmp2, params.tmp2, fn05, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp2, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp2, fPi, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaCalNegTmp2(const LGammaParams &params)
{

    Mul<float, false>(params.tmp3, params.tmp2, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    LGammaCalcMulAdd(
        params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsEven, negParamsEvenLen);
    Mul<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp1, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmp2, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.tmpMask1, params);


    LGammaCalcMulAdd(params.tmp1, params.tmp3, params.unaryParams, params.binaryParams, negParamsOdd, negParamsOddLen);
    Adds<float, false>(params.tmp1, params.tmp1, f1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp2, params.tmp1, params.mask, params);
}


[aicore] inline void LGammaCalNegTmp3(const LGammaParams &params)
{

    Abs<float, false>(params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(params.tmp3, params.tmp1, params.tmp6, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(params.tmp1, params.tmp3, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    constexpr float ftmp = 1.1447298526763916015625;
    Adds<float, false>(params.tmp1, params.tmp1, ftmp, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp2, params.tmp5, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmp2, params.tmp1, params.tmp2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaCalMinNeg(const LocalTensor<float> &src, const LGammaParams &params)
{

    Ln<float, false>(params.tmp1, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmp1, params.tmp1, fn1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaNegative(const LGammaParams &params)
{
    Duplicate<float, false>(params.tmp4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    constexpr float minf = 9.99999968266e-20;
    LGammaGenLTMask(params.mask, params.tmp6, params, minf);
    LGammaCalMinNeg(params.tmp6, params);
    LGammaSelect(params.tmp4, params.tmp1, params.mask, params);


    LGammaCalNegTmp1(params);

    LGammaCalNegTmp2(params);

    LGammaCalNegTmp3(params);


    LGammaFloor(params.tmp1, params.tmp6);


    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(
        params.mask, params.tmp1, params.tmp6, CMPMODE::NE, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    NotNumUnion notNum;
    notNum.i = F32_INF;
    Duplicate<float, false>(params.tmp1, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Compare<float, uint8_t, false>(
        params.tmpMask1, params.tmp3, params.tmp1, CMPMODE::LT, MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.tmpMask1, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(params.tmp4, params.tmp2, params.mask, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    LGammaGenGEMask(params.mask, params.tmp6, params, minf);
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    Duplicate<float, false>(params.tmp2, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(params.tmp4, params.tmp2, params.tmpMask1, params);
}

template <bool isReuseSource = false>
[aicore] inline void LGammaInitFParams(
    const LocalTensor<float> &tmp, const uint32_t splitSize, const LocalTensor<float> &src, LGammaParams &params)
{
    params.tmp1 = tmp;
    params.tmp2 = tmp[splitSize];
    params.tmp3 = params.tmp2[splitSize];
    params.tmp4 = params.tmp3[splitSize];
    params.tmp5 = params.tmp4[splitSize];
    params.tmp6 = params.tmp5[splitSize];
    if constexpr (isReuseSource) {
        params.mask = params.tmp6[splitSize].ReinterpretCast<uint8_t>();
        params.tmpScalar = src.ReinterpretCast<float>();
    } else {
        params.tmpScalar = params.tmp6[splitSize];
        params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    }
    params.tmpMask1 = params.mask[splitSize];
    params.tmpMask2 = params.tmpMask1[splitSize];
    params.tmpMask3 = params.tmpMask2[splitSize];

    params.tmp1.SetSize(splitSize);
    params.tmp2.SetSize(splitSize);
    params.tmp3.SetSize(splitSize);
    params.tmp4.SetSize(splitSize);
    params.tmp5.SetSize(splitSize);
    params.tmp6.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.tmpMask1.SetSize(splitSize);
    params.tmpMask2.SetSize(splitSize);
    params.tmpMask3.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);

    params.splitSize = splitSize;
}

template <bool isReuseSource = false>
[aicore] inline void LGammaInitHParams(
    const LocalTensor<float> &tmp, const uint32_t splitSize, const LocalTensor<half> &src, LGammaParams &params)
{
    params.tmp1 = tmp;
    params.tmp2 = tmp[splitSize];
    params.tmp3 = params.tmp2[splitSize];
    params.tmp4 = params.tmp3[splitSize];
    params.tmp5 = params.tmp4[splitSize];
    params.tmpScalar = params.tmp5[splitSize];
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.tmpMask1 = params.mask[splitSize];
    params.tmpMask2 = params.tmpMask1[splitSize];
    params.tmpMask3 = params.tmpMask2[splitSize];
    params.tmp6 = params.tmpScalar[splitSize * i2];

    params.tmp1.SetSize(splitSize);
    params.tmp2.SetSize(splitSize);
    params.tmp3.SetSize(splitSize);
    params.tmp4.SetSize(splitSize);
    params.tmp5.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.tmpMask1.SetSize(splitSize);
    params.tmpMask2.SetSize(splitSize);
    params.tmpMask3.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.tmp6.SetSize(splitSize * i6);

    params.splitSize = splitSize;
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/lgamma/lgamma_common_impl.h" 2




namespace AscendC {
[aicore] inline void Lgamma1Compute(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);

    Adds<float, false>(tmp1Tensor, srcTensor, t4, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dstTensor, dstTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp2Tensor, dstTensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(tmp2Tensor, tmp2Tensor, f05, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();



    Muls<float, false>(tmp3Tensor, dstTensor, N01, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp4Tensor, tmp1Tensor, t12, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(dstTensor, f1, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(tmp4Tensor, dstTensor, tmp4Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp4Tensor, tmp4Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp4Tensor, tmp4Tensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Mul<float, false>(tmp4Tensor, tmp4Tensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Add<float, false>(dstTensor, tmp4Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LgammaComputePosHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmpTensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmpTensor[splitSize * 2];
    LocalTensor<float> tmp4Tensor = tmpTensor[splitSize * 3];

    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);


    Lgamma1Compute(dstTensor, srcTensor, tmpTensor, splitSize);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(tmp3Tensor, srcTensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f2, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Adds<float, false>(tmp2Tensor, srcTensor, f3, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(tmp3Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dstTensor, dstTensor, tmp3Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LgammaComputeNegHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<float> &tmpTensor, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    LocalTensor<float> tmp1Tensor = tmpTensor;
    LocalTensor<float> tmp2Tensor = tmp1Tensor[splitSize];
    LocalTensor<float> tmp3Tensor = tmp2Tensor[splitSize];
    LocalTensor<float> tmp4Tensor = tmp3Tensor[splitSize];
    LocalTensor<float> tmp5Tensor = tmp4Tensor[splitSize];
    LocalTensor<float> tmp6Tensor = tmp5Tensor[splitSize];
    LocalTensor<float> tmp7Tensor = tmpTensor[splitSize * i2];
    tmp1Tensor.SetSize(splitSize);
    tmp2Tensor.SetSize(splitSize);
    tmp3Tensor.SetSize(splitSize);
    tmp4Tensor.SetSize(splitSize);
    tmp5Tensor.SetSize(splitSize);
    tmp6Tensor.SetSize(splitSize);
    tmp7Tensor.SetSize(splitSize * i4);


    Muls<float, false>(tmp1Tensor, srcTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Adds<float, false>(tmp1Tensor, tmp1Tensor, f1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    LgammaComputePosHalf(dstTensor, tmp1Tensor, tmp7Tensor, splitSize);
    PipeBarrier<PIPE_V>();

    Muls<float, false>(dstTensor, dstTensor, fn1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    LGammaFloor(tmp1Tensor, srcTensor);


    Sub<float, false>(tmp1Tensor, srcTensor, tmp1Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(tmp1Tensor, tmp1Tensor, PI, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();


    SinCompute(tmp2Tensor, tmp1Tensor, tmp7Tensor, splitSize, false);
    PipeBarrier<PIPE_V>();

    Abs<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(tmp3Tensor, PI, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    Div<float, false>(tmp2Tensor, tmp3Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();

    Ln<float, false>(tmp2Tensor, tmp2Tensor, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, dstTensor, tmp2Tensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaGenLTMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::LT, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void LGammaGenGEMaskHalf(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    const LocalTensor<float> &tmptensor, const float scalar, const uint32_t splitSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binParams;

    Duplicate<float, false>(tmptensor, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, tmptensor, CMPMODE::GE, MASK_PLACEHOLDER, repeat, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LGammaSelectHalf(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmpTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(dstTensor, tmpTensor, dstTensor, MASK_PLACEHOLDER, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LGammaSelectINF(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmpTensor, const LocalTensor<float> &tmpScalar)
{
    const BinaryRepeatParams binParams;
    Duplicate<float, false>(tmpScalar, 655040.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetCmpMask<float>(tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(dstTensor, mask, srcTensor, 1, binParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void LgammaComputeImpl(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    LGammaParams &params, const uint32_t splitSize)
{

    Duplicate<float, false>(params.tmp1, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Cast<float, half, false>(params.tmp2, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LgammaComputePosHalf(params.tmp3, params.tmp2, params.tmp6, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(params.mask, params.tmp2, params.tmp5, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(params.tmp1, params.tmp3, params.mask, params.tmp5, params.tmpScalar);
    PipeBarrier<PIPE_V>();


    LgammaComputeNegHalf(params.tmp4, params.tmp2, params.tmp6, splitSize);
    PipeBarrier<PIPE_V>();

    LGammaGenLTMaskHalf(params.tmpMask1, params.tmp2, params.tmp5, 0.0f, splitSize);
    PipeBarrier<PIPE_V>();
    LGammaSelectHalf(params.tmp1, params.tmp4, params.tmpMask1, params.tmp5, params.tmpScalar);
    PipeBarrier<PIPE_V>();


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.tmpMask2.ReinterpretCast<uint16_t>(), params.mask.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, params.unaryParams);
    Not<uint16_t, false>(params.tmpMask3.ReinterpretCast<uint16_t>(), params.tmpMask1.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(params.tmpMask2.ReinterpretCast<uint16_t>(), params.tmpMask2.ReinterpretCast<uint16_t>(),
        params.tmpMask3.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);

    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelectHalf(params.tmp1, params.tmp2, params.tmpMask2, params.tmp4, params.tmpScalar);
    PipeBarrier<PIPE_V>();


    Abs<float, false>(params.tmp2, params.tmp2, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();

    LGammaGenGEMaskHalf(params.tmpMask2, params.tmp2, params.tmp4, 65504.0f, splitSize);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.tmpMask3.ReinterpretCast<uint16_t>(), params.tmpMask2.ReinterpretCast<uint16_t>(),
                         MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelectINF(params.tmp1, params.tmp1, params.tmpMask3, params.tmp4, params.tmpScalar);
    PipeBarrier<PIPE_V>();


    Cast<half, float, false>(dstTensor, params.tmp1, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
}

[aicore] inline void LgammaComputeImpl(
    const LocalTensor<float> &dst, const LocalTensor<float> &src, LGammaParams &params)
{

    LGammaGenGEMask(params.tmpMask2, src, params, 0.0f);
    LGammaGenLTMask(params.tmpMask3, src, params, 0.0f);



    Abs<float, false>(params.tmp6, src, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();


    LGammaPositive(params);
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    LGammaSelect(dst, params.tmp5, params.tmpMask2, params);


    LGammaNegative(params);
    LGammaSelect(dst, params.tmp4, params.tmpMask3, params);


    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.tmpMask3.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(params.mask.ReinterpretCast<uint16_t>(),
        params.tmpMask1.ReinterpretCast<uint16_t>(),
        params.mask.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
    LGammaSelect(dst, params.tmp6, params.mask, params);
}

template <bool isReuseSource = false>
[aicore] inline void LgammaCompute(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                              ;

    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t tmpBufferSize = bufferSize / sizeof(float);
    CheckTmpBufferSize(tmpBufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t splitSize = 0;

    splitSize = tmpBufferSize / LGAMMA_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(splitSize, 0, bufferSize);


    LGammaParams params;
    LGammaInitHParams<isReuseSource>(tmpBuffer, splitSize, srcTensor, params);

    const uint32_t round = calCount / splitSize;
    const uint32_t tail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(0, splitSize);
    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        LgammaComputeImpl(dstTensor[offset], srcTensor[offset], params, splitSize);
        offset = offset + splitSize;
    }

    if (tail > 0) {
        SetVectorMask<half, MaskMode::COUNTER>(0, tail);
        params.splitSize = tail;
        LgammaComputeImpl(
            dstTensor[round * splitSize], srcTensor[round * splitSize], params, splitSize);
    }
    SetMaskNorm();
    AscendCUtils::ResetMask();
}

template <bool isReuseSource = false>
[aicore] inline void LgammaCompute(const LocalTensor<float> &dst, const LocalTensor<float> &src,
    const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{
                                                                                       ;

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();
    uint32_t splitSize = tmpBufferSize;
    if constexpr (isReuseSource) {
        splitSize = splitSize / FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitSize = splitSize / FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);


    LGammaParams params;
    LGammaInitFParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<float>(0, splitSize);
    for (uint32_t i = 0U; i < loopCount; ++i) {
        LgammaComputeImpl(dst[i * splitSize], src[i * splitSize], params);
    }
    if (calcTail > 0) {
        SetVectorMask<float>(0, calcTail);
        params.splitSize = calcTail;
        LgammaComputeImpl(dst[loopCount * splitSize], src[loopCount * splitSize], params);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void LgammaImpl(const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{
    LgammaCompute<isReuseSource>(dst, src, tmp, calCount);
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/lgamma.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 33 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LgammaImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/lgamma.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Lgamma(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    LgammaImpl<T, isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 90 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/digamma.h" 1
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/digamma.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_basic_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_basic_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_v220_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_v220_impl.h"
namespace AscendC {
[aicore] inline void DigammaCast(const LocalTensor<float> &dst, const LocalTensor<float> &src, RoundMode castType)
{
    Cast<float, float, false>(dst, src, castType, MASK_PLACEHOLDER, 1,
                              {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_basic_impl.h" 2





namespace AscendC {
namespace {
constexpr float MIN_NEG_WITH_FLOAT = -8388608.0;
constexpr float DIGAMMA_PI = 3.141592653589793238f;
constexpr float DIGAMMA_NEG_PI = -3.141592653589793238f;
constexpr uint32_t DIGAMMA_FLOAT_NOREUSE_CALC_PROC = 7;
constexpr uint32_t DIGAMMA_FLOAT_REUSE_CALC_PROC = 6;
constexpr uint32_t DIGAMMA_HALF_CALC_PROC = 8;
constexpr size_t DIGAMMA_MAX_LOOP = 5;

constexpr float posCalcConst[] = {2.10927960927960927961e-2, 7.57575757575757575758e-3, 4.16666666666666666667e-3,
                                  3.96825396825396825397e-3, 8.33333333333333333333e-3, 8.33333333333333333333e-2};
constexpr float tmp1CalcConst[] = {1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0};
constexpr float tmp1HalfCalcConst[] = {1.0, 2.0};
constexpr float picotCalcConst[] = {0.00326538085938f, 0.0242919921875f, 0.053466796875f,
                                    0.133377909660f, 0.333332300186f};
}

struct DigammaParams {
    [aicore] DigammaParams() {}
    LocalTensor<float> result;
    LocalTensor<float> tmpCal1;
    LocalTensor<float> tmpCal2;
    LocalTensor<float> tmpCal3;
    LocalTensor<float> tmpCal4;
    LocalTensor<float> tmpCal5;
    LocalTensor<float> tmpScalar;
    LocalTensor<uint8_t> mask;
    LocalTensor<uint8_t> mask1;
    LocalTensor<uint8_t> mask2;
    UnaryRepeatParams unaryParams;
    BinaryRepeatParams binaryParams;
    uint32_t splitSize;
};
#pragma begin_pipe(V)

[aicore] inline void DigammaGenCompareMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                             DigammaParams &params, const float scalar, CMPMODE cmpMode)
{
    Duplicate<float, false>(params.tmpScalar, scalar, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(mask, src, params.tmpScalar, cmpMode,
                                   MASK_PLACEHOLDER, repeat, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DigammaGenNegIntMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    DigammaParams &params, const float scalar)
{

    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::GT);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);

    DigammaCast(params.tmpCal1, src, RoundMode::CAST_ROUND);
    uint8_t repeat = DivCeil(params.splitSize * sizeof(float), ONE_REPEAT_BYTE_SIZE);
    Compare<float, uint8_t, false>(params.mask2, src, params.tmpCal1, CMPMODE::EQ, MASK_PLACEHOLDER, repeat,
        params.binaryParams);
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] inline void DigammaGenRangeMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
    DigammaParams &params, const float min, const float max)
{

    DigammaGenCompareMask(params.mask1, src, params, max, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, min, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(), params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] inline void DigammaGenNanMask(const LocalTensor<uint8_t> &mask, const LocalTensor<float> &src,
                                         DigammaParams &params)
{
    DigammaGenCompareMask(params.mask1, src, params, 0.0f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, src, params, 0.0f, CMPMODE::GE);

    SetVectorMask<float>(0, ConstCeil(params.splitSize, sizeof(uint16_t) * ONE_BYTE_BIT_SIZE));
    Not<uint16_t, false>(params.mask1.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    Not<uint16_t, false>(params.mask2.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.unaryParams);
    PipeBarrier<PIPE_V>();
    And<uint16_t, false>(mask.ReinterpretCast<uint16_t>(),
        params.mask1.ReinterpretCast<uint16_t>(),
        params.mask2.ReinterpretCast<uint16_t>(),
        MASK_PLACEHOLDER,
        1,
        params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);
}


[aicore] inline void DigammaSelect(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                     const LocalTensor<uint8_t> &mask, const LocalTensor<float> &tmp,
                                     DigammaParams &params)
{
    Duplicate<float, false>(params.tmpScalar, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    SetCmpMask<float>(params.tmpScalar);
    PipeBarrier<PIPE_V>();
    Select<float, uint8_t>(tmp, mask, src, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dst, tmp, dst, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DigammaNegativeRange(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                            DigammaParams &params)
{

    DigammaCast(params.tmpScalar, src, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpScalar, src, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpScalar, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    CosCompute<float>(params.tmpCal3, params.tmpScalar, params.result, params.splitSize, true);


    Muls<float, false>(src, src, DIGAMMA_NEG_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    SinCompute<float>(params.tmpScalar, src, params.result, params.splitSize, true);


    Muls<float, false>(params.tmpCal3, params.tmpCal3, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal3, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, params.tmpCal2, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

template <bool isReuseSource = false>
[aicore] inline void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<half> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = params.result[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    params.tmpCal4 = params.tmpCal3[splitSize];
    params.tmpCal5 = params.tmpCal4[splitSize];
    params.tmpScalar = params.tmpCal5[splitSize];
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];


    params.result.SetSize(splitSize * 4);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpCal5.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}

template <bool isReuseSource = false>
[aicore] inline void DigammaInitParams(const LocalTensor<float> &tmp, const uint32_t &splitSize,
                                         const LocalTensor<float> &src, DigammaParams &params)
{
    params.result = tmp;
    params.tmpCal1 = tmp[splitSize];
    params.tmpCal2 = params.tmpCal1[splitSize];
    params.tmpCal3 = params.tmpCal2[splitSize];
    if constexpr (isReuseSource) {
        params.tmpCal4 = src;
        params.tmpScalar = params.tmpCal3[splitSize];
    } else {
        params.tmpCal4 = params.tmpCal3[splitSize];
        params.tmpScalar = params.tmpCal4[splitSize];
    }
    params.mask = params.tmpScalar[splitSize].ReinterpretCast<uint8_t>();
    params.mask1 = params.mask[splitSize];
    params.mask2 = params.mask1[splitSize];

    params.result.SetSize(splitSize);
    params.tmpCal1.SetSize(splitSize);
    params.tmpCal2.SetSize(splitSize);
    params.tmpCal3.SetSize(splitSize);
    params.tmpCal4.SetSize(splitSize);
    params.tmpScalar.SetSize(splitSize);
    params.mask.SetSize(splitSize);
    params.mask1.SetSize(splitSize);
    params.mask2.SetSize(splitSize);

    params.splitSize = splitSize;
}
#pragma end_pipe
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_impl.h" 2





namespace AscendC {
#pragma begin_pipe(V)


[aicore] inline void DigammaPositiveHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 3.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal1, 0.0833333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.0083333333333333f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal2, params.tmpCal2, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpScalar, params.tmpCal2, 0.003968253968254f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    constexpr size_t calcSize = 2;
    for (size_t i = 0U; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1HalfCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DigammaNegativeHalf(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(params.tmpCal5, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaPositiveHalf(dst, params.tmpCal5, params);


    Adds<float, false>(params.tmpCal2, dst, 0.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal3, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DigammaComputeImpl(const LocalTensor<half> &dst, const LocalTensor<half> &src,
                                          DigammaParams &params)
{

    Cast<float, half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal4, params.tmpCal5, RoundMode::CAST_FLOOR);
    Sub<float, false>(params.tmpCal4, params.tmpCal5, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal4, params.tmpCal4, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    TanCompute<float>(params.tmpScalar, params.tmpCal4, params.result.ReinterpretCast<uint8_t>(), params.splitSize);

    Duplicate<float, false>(params.tmpCal1, DIGAMMA_PI, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal3, params.tmpCal1, params.tmpScalar, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<float, false>(params.tmpCal4, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenNegIntMask(params.mask1, params.tmpCal5, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(params.tmpCal4, params.result, params.mask1, params.tmpCal1, params);


    DigammaGenNanMask(params.mask, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.tmpCal5, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, 0.0f, CMPMODE::GE);
    DigammaPositiveHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);


    DigammaGenCompareMask(params.mask, params.tmpCal5, params, -0.0001f, CMPMODE::LT);
    DigammaNegativeHalf(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<float, half, false>(params.tmpCal5, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    DigammaGenRangeMask(params.mask, params.tmpCal5, params, -0.0001f, 0.0f);
    DigammaNegativeRange(params.result, params.tmpCal5, params);
    DigammaSelect(params.tmpCal4, params.result, params.mask, params.tmpCal1, params);

    Cast<half, float, false>(dst, params.tmpCal4, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                             {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}
# 197 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/digamma/digamma_common_impl.h"
[aicore] inline void DigammaPositiveTmp0(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Adds<float, false>(params.tmpCal1, src, 10.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Ln<float, false>(dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Muls<float, false>(params.tmpCal2, params.tmpCal1, 0.5f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Mul<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);


    Duplicate<float, false>(params.tmpCal2, 8.33333333333333333333e-2, MASK_PLACEHOLDER, 1,
                            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 6;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {
        Duplicate<float, false>(params.tmpScalar, posCalcConst[i], MASK_PLACEHOLDER, 1,
                                DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


        Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
        Sub<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Sub<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}




[aicore] inline void DigammaPositiveTmp1(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                           DigammaParams &params)
{

    Duplicate<float, false>(params.tmpScalar, 1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(dst, params.tmpScalar, src, MASK_PLACEHOLDER, 1, params.binaryParams);

    for (size_t i = 0U; i < DIGAMMA_MAX_LOOP; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
    constexpr size_t calcSize = 9;
    for (size_t i = DIGAMMA_MAX_LOOP; i < calcSize; ++i) {

        Adds<float, false>(params.tmpCal2, src, tmp1CalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();

        Div<float, false>(params.tmpCal2, params.tmpScalar, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Add<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();
    }
}


[aicore] inline void DigammaPositive(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    DigammaPositiveTmp0(dst, src, params);


    DigammaPositiveTmp1(params.tmpCal1, src, params);


    Sub<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DigammaNegPicotPix(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{

    Add<float, false>(params.tmpCal1, src, src, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    DigammaCast(params.tmpCal2, params.tmpCal1, RoundMode::CAST_ROUND);


    Sub<float, false>(params.tmpCal1, params.tmpCal1, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Muls<float, false>(params.tmpCal1, params.tmpCal1, 1.5707963267948966f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Cast<int32_t, float, false>(params.tmpCal2.ReinterpretCast<int32_t>(), params.tmpCal2, RoundMode::CAST_ROUND,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<int32_t, false>(params.tmpCal3.ReinterpretCast<int32_t>(), 1, MASK_PLACEHOLDER, 1,
                              DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize * (sizeof(float) / sizeof(uint16_t)));
    And<uint16_t, false>(params.tmpCal2.ReinterpretCast<uint16_t>(), params.tmpCal2.ReinterpretCast<uint16_t>(),
            params.tmpCal3.ReinterpretCast<uint16_t>(), MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    SetVectorMask<float>(0, params.splitSize);


    Cast<float, int32_t, false>(params.tmpCal2, params.tmpCal2.ReinterpretCast<int32_t>(), RoundMode::CAST_NONE,
                                MASK_PLACEHOLDER, 1, params.unaryParams);
    DigammaGenCompareMask(params.mask1, params.tmpCal2, params, 0.5f, CMPMODE::LT);
    DigammaGenCompareMask(params.mask2, params.tmpCal2, params, 0.5f, CMPMODE::GE);


    Mul<float, false>(params.tmpCal2, params.tmpCal1, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0093383789065f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    constexpr size_t calcSize = 5;
    for (size_t i = 0U; i < calcSize; ++i) {
        Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
        PipeBarrier<PIPE_V>();

        Adds<float, false>(dst, dst, picotCalcConst[i], MASK_PLACEHOLDER, 1, params.unaryParams);
        PipeBarrier<PIPE_V>();
    }

    Mul<float, false>(dst, dst, params.tmpCal2, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Mul<float, false>(dst, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
    Add<float, false>(params.tmpCal1, dst, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();


    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    DigammaSelect(dst, params.tmpCal1, params.mask2, params.tmpCal3, params);


    Duplicate<float, false>(params.tmpScalar, -1.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    Div<float, false>(params.tmpCal1, params.tmpScalar, params.tmpCal1, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();

    DigammaSelect(dst, params.tmpCal1, params.mask1, params.tmpCal3, params);


    Muls<float, false>(dst, dst, DIGAMMA_PI, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void DigammaNegative(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                       DigammaParams &params)
{

    Muls<float, false>(params.tmpCal3, src, -1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    Adds<float, false>(params.tmpCal3, params.tmpCal3, 1.0f, MASK_PLACEHOLDER, 1, params.unaryParams);
    PipeBarrier<PIPE_V>();
    DigammaPositive(dst, params.tmpCal3, params);


    DigammaNegPicotPix(params.tmpCal4, src, params);


    Add<float, false>(dst, dst, params.tmpCal4, MASK_PLACEHOLDER, 1, params.binaryParams);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void DigammaComputeImpl(const LocalTensor<float> &dst, const LocalTensor<float> &src,
                                          DigammaParams &params)
{
    Duplicate<float, false>(dst, 0.0f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    NotNumUnion notNum;
    notNum.i = F32_NAN;
    Duplicate<float, false>(params.result, notNum.f, MASK_PLACEHOLDER, 1, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);


    DigammaGenCompareMask(params.mask, src, params, MIN_NEG_WITH_FLOAT, CMPMODE::LE);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenNegIntMask(params.mask1, src, params, MIN_NEG_WITH_FLOAT);
    DigammaSelect(dst, params.result, params.mask1, params.tmpCal3, params);


    DigammaGenNanMask(params.mask, src, params);
    DigammaSelect(dst, src, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::GE);
    DigammaPositive(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);


    DigammaGenCompareMask(params.mask, src, params, 0.0f, CMPMODE::LT);
    DigammaNegative(params.result, src, params);
    DigammaSelect(dst, params.result, params.mask, params.tmpCal3, params);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void DigammaCompute(const LocalTensor<T> &dst, const LocalTensor<T> &src,
                                      const LocalTensor<uint8_t> &tmp, const uint32_t calCount)
{
                                                                                    ;

    LocalTensor<float> tmpBuffer = tmp.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();
    uint32_t splitSize = tmpBufferSize;

    if (sizeof(T) == sizeof(float)) {
        if constexpr (isReuseSource) {
            splitSize = splitSize / DIGAMMA_FLOAT_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        } else {
            splitSize = splitSize / DIGAMMA_FLOAT_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        }
    } else {
        splitSize = splitSize / DIGAMMA_HALF_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }

    CheckTmpBufferSize(splitSize, 0, tmpBufferSize);


    DigammaParams params;
    DigammaInitParams<isReuseSource>(tmpBuffer, splitSize, src, params);

    const uint32_t loopCount = calCount / splitSize;
    uint32_t calcTail = calCount % splitSize;
    SetMaskCount();
    SetVectorMask<T>(0, splitSize);
    uint32_t offset = 0;
    for (uint32_t i = 0U; i < loopCount; ++i) {
        DigammaComputeImpl(dst[offset], src[offset], params);
        offset += splitSize;
    }

    if (calcTail > 0) {
        calcTail = (calcTail + ONE_BYTE_BIT_SIZE - 1U) / ONE_BYTE_BIT_SIZE * ONE_BYTE_BIT_SIZE;
        SetVectorMask<T>(0, calcTail);
        params.splitSize = calcTail;
        DigammaComputeImpl(dst[offset], src[offset], params);
    }
    SetMaskNorm();
    ResetMask();
}

#pragma end_pipe
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/digamma.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 37 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/digamma.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Digamma(LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                               LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 57 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/digamma.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Digamma(LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;
    DigammaCompute<T, isReuseSource>(dstTensor, srcTensor, tmp, calCount);
}
#pragma end_pipe
}
# 91 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sign/sign_common_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sign/sign_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/sign/sign_common_impl.h" 2



#pragma begin_pipe(V)
namespace AscendC {
constexpr uint32_t SIGN_CALC_PROC = 3;
constexpr uint32_t SIGN_BIT = 8;

template <typename T>
[aicore] inline void SignComputeImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &tmpBuffer1, const LocalTensor<uint8_t> &tmpBuffer2, const LocalTensor<T> &tmpBuffer3,
    const LocalTensor<T> &tmpBuffer4, uint32_t calCount, uint32_t repeatTimes)
{
    BinaryRepeatParams binaryParams;

    Duplicate<T, false>(dstTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();





    Compare<T, uint8_t, false>(
        tmpBuffer1, srcTensor, dstTensor, CMPMODE::LT, MASK_PLACEHOLDER, repeatTimes, binaryParams);
    Compare<T, uint8_t, false>(
        tmpBuffer2, srcTensor, dstTensor, CMPMODE::GT, MASK_PLACEHOLDER, repeatTimes, binaryParams);





    Duplicate<T, false>(tmpBuffer3, static_cast<T>(1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(tmpBuffer4, tmpBuffer1, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Duplicate<T, false>(tmpBuffer3, static_cast<T>(-1), MASK_PLACEHOLDER, 1, 1, 8);
    PipeBarrier<PIPE_V>();

    SetCmpMask<T>(tmpBuffer3);
    Select<T, uint8_t>(dstTensor, tmpBuffer2, dstTensor, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Add<T, false>(dstTensor, tmpBuffer4, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void SignCompute(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                         ;
    uint32_t sharedTmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = sharedTmpBufferSize / sizeof(T) / SIGN_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;

    CheckTmpBufferSize(splitCount, 0, sharedTmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer1 = (__attribute__((cce_unif_buff)) uint8_t *)sharedTmpBuffer.GetPhyAddr();
    uint32_t tmpLen = AlignUp(splitCount / SIGN_BIT, ONE_BLK_SIZE);
    __attribute__((cce_unif_buff)) uint8_t *tmpBuffer2 = tmpBuffer1 + tmpLen;
    LocalTensor<T> stackTensor = sharedTmpBuffer[tmpLen * 2].ReinterpretCast<T>();
    __attribute__((cce_unif_buff)) T *tmpBuffer3 = (__attribute__((cce_unif_buff)) T *)stackTensor.GetPhyAddr();
    __attribute__((cce_unif_buff)) T *tmpBuffer4 = tmpBuffer3 + splitCount;

    uint32_t offset = 0;
    uint32_t repeatTimes = (splitCount * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
    for (uint32_t i = 0; i < loopCount; ++i) {
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], splitCount, repeatTimes);
        offset = offset + splitCount;
    }
    if (calcTail > 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, calcTail);
        repeatTimes = (calcTail * sizeof(T) + ONE_REPEAT_BYTE_SIZE - 1) / ONE_REPEAT_BYTE_SIZE;
        SignComputeImpl(dstTensor[offset], srcTensor[offset],
            sharedTmpBuffer, sharedTmpBuffer[tmpLen], stackTensor, stackTensor[splitCount], calcTail, repeatTimes);
    }
    SetMaskNorm();
    ResetMask();
}
}
#pragma end_pipe
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h" 2

#pragma begin_pipe(V)
namespace AscendC {
# 36 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    SignCompute<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sign(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcTensor.GetSize());
}
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }


    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Sign<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 91 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/sign.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Sign(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor)
{
    Sign<T, isReuseSource>(dstTensor, srcTensor, srcTensor.GetSize());
}
}
#pragma end_pipe
# 92 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/reduce/mean_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/reduce/mean_utils.h"
namespace AscendC {
struct MeanParams {
    uint32_t outter = 1;
    uint32_t inner;
    uint32_t n;
};

};
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/mean/mean_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/mean/mean_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/mean/mean_common_impl.h" 2



namespace AscendC {
constexpr uint32_t HALF_NUM_PER = 128;
constexpr uint32_t FLOAT_NUM_PER = 64;

[aicore] inline void MeanCast(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    const UnaryRepeatParams unaryParams;
    float scalarValue = static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n));
    LocalTensor<float> TmpTensor = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<half> castTensor = sharedTmpBuffer.ReinterpretCast<half>();
    SetMaskCount();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        SetVectorMask<half>(0, meanParams.n);
        Cast<float, half, false>(TmpTensor, srcTensor[row * meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor, 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        uint32_t reduceNums = repeateTimes;
        while (reduceNums > 1) {
            SetVectorMask<half>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            RepeatReduceSum<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner], 1,
                                      MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                      DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);

            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<half>(0, 1);
        Muls<float, false>(TmpTensor[meanParams.inner], TmpTensor[meanParams.inner],
                       scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
        PipeBarrier<PIPE_V>();
        Cast<half, float, false>(castTensor, TmpTensor[meanParams.inner], RoundMode::CAST_NONE, MASK_PLACEHOLDER,
                                 1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        RepeatReduceSum<half, false>(dstTensor[row], castTensor, 1, MASK_PLACEHOLDER,
                                     DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] inline void MeanForOneRepeatTime(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                            const MeanParams& meanParams, T scalarValue)
{
    SetVectorMask<T>(0, meanParams.n);
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        RepeatReduceSum<T, false>(dstTensor[row], srcTensor[row * meanParams.inner], 1, MASK_PLACEHOLDER,
                                  DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
    }
    PipeBarrier<PIPE_V>();
    SetVectorMask<T>(0, meanParams.outter);
    const UnaryRepeatParams unaryParams;
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
    ResetMask();
}

template <typename T, typename accType, bool isReuseSource>
[aicore] inline void MeanCommon(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams)
{
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    if constexpr (sizeof(T) == sizeof(half)) {
        elementNumPerRep = HALF_NUM_PER;
    }
    uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
    T scalarValue = static_cast<T>(static_cast<float>(1) / static_cast<float>(static_cast<int32_t>(meanParams.n)));
    SetMaskCount();
    if (repeateTimes == 1) {
        return MeanForOneRepeatTime(dstTensor, srcTensor, meanParams, scalarValue);
    }
    const UnaryRepeatParams unaryParams;
    LocalTensor<T> TmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
    for (uint32_t row = 0; row < meanParams.outter; ++row) {
        uint32_t reduceNums = repeateTimes;
        SetVectorMask<T>(0, meanParams.n);
        RepeatReduceSum<T, false>(TmpTensor,
            srcTensor[row * meanParams.inner],
            1,
            MASK_PLACEHOLDER,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_BLK_STRIDE,
            DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        while (reduceNums > 1) {
            SetVectorMask<T>(0, reduceNums);
            reduceNums = (reduceNums + elementNumPerRep - 1) / elementNumPerRep;
            if (reduceNums == 1) {
                RepeatReduceSum<T, false>(dstTensor[row], TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            } else {
                RepeatReduceSum<T, false>(TmpTensor, TmpTensor, 1, MASK_PLACEHOLDER, DEFAULT_BLK_STRIDE,
                                          DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
            }
            PipeBarrier<PIPE_V>();
        }
    }
    SetVectorMask<T>(0, meanParams.outter);
    Muls<T, false>(dstTensor, dstTensor, scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    SetMaskNorm();
}

template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] inline void MeanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const MeanParams& meanParams) {
    uint32_t elementNumPerRep = FLOAT_NUM_PER;
    if constexpr (sizeof(T) == sizeof(half) && sizeof(accType) == sizeof(float))
    {
        uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
        uint32_t finalWorkSize = meanParams.inner * sizeof(float) + (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;

                                                        ;
        MeanCast(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    } else {

        if constexpr (sizeof(T) == sizeof(half)) {
            elementNumPerRep = HALF_NUM_PER;
        }
        uint32_t repeateTimes = (meanParams.n + elementNumPerRep - 1) / elementNumPerRep;
        uint32_t finalWorkSize = (repeateTimes + ONE_BLK_SIZE - 1) / ONE_BLK_SIZE * ONE_BLK_SIZE;


                                                        ;
        MeanCommon<T, accType, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
    }
}

#pragma end_pipe
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h" 2







namespace AscendC {
#pragma begin_pipe(V)
# 43 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] inline void Mean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                            const LocalTensor<uint8_t> &sharedTmpBuffer, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }
    MeanImpl<T, accType, isReuseSource, isBasicBlock, reduceDim>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
}
# 66 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/mean.h"
template <typename T, typename accType = T, bool isReuseSource = false, bool isBasicBlock = false,
          int32_t reduceDim = -1>
[aicore] inline void Mean(
    const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const MeanParams &meanParams)
{
    if constexpr(g_coreType == AscendC::AIC)
    {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    Mean<T, accType, isReuseSource, isBasicBlock, reduceDim>(dstTensor, srcTensor, sharedTmpBuffer, meanParams);
}
#pragma end_pipe
}
# 93 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/axpy.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/axpy.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/axpy/axpy_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/axpy/axpy_common_impl.h"
namespace AscendC {
 template <typename T, typename U>
[aicore] inline void AxpyIntrinsicsImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor,
    const U& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{


                                                                             ;
}





 template <>
[aicore] inline void AxpyIntrinsicsImpl(const LocalTensor<half>& dstTensor, const LocalTensor<half>& srcTensor,
    const half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    LocalTensor<float> tmpSrc = stackBuffer[0];
    LocalTensor<float> tmpDst = stackBuffer[stackSize];

    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, half, false>(tmpSrc, srcTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Cast<float, half, false>(tmpDst, dstTensor,
        RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1, { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(tmpSrc, tmpSrc, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(tmpDst, tmpSrc, tmpDst, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Cast<half, float, false>(dstTensor, tmpDst, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}




 template <>
[aicore] inline void AxpyIntrinsicsImpl(const LocalTensor<float>& dstTensor, const LocalTensor<half>& srcTensor,
    const half& scalarValue, LocalTensor<float> stackBuffer, uint32_t stackSize)
{
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;

    Cast<float, half, false>(stackBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();

    Muls<float, false>(stackBuffer, stackBuffer, (float)scalarValue, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    Add<float, false>(dstTensor, stackBuffer, dstTensor, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline uint32_t axpyTmpCalc(uint32_t tmpBufferSize)
{
    uint32_t stackSize = tmpBufferSize;
    if constexpr (sizeof(T) == sizeof(half)) {
        stackSize = tmpBufferSize / 2 / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        stackSize = tmpBufferSize / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(stackSize, 0, tmpBufferSize);
    return stackSize;
}

template <typename T, typename U, bool isReuseSource = false>
[aicore] inline void AxpySub(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U& scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    CheckTmpBufferSize(bufferSize, 0, bufferSize);

    LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
    uint32_t tmpBufferSize = tmpBuffer.GetSize();

    uint32_t stackSize = axpyTmpCalc<T>(tmpBufferSize);

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, stackSize);

    uint32_t offset = 0;
    for (uint32_t i = 0; i < round; i++) {
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
        offset = offset + stackSize;
    }

    if (tail != 0) {
        SetVectorMask<T, MaskMode::COUNTER>(0, tail);
        AxpyIntrinsicsImpl(dstTensor[offset], srcTensor[offset], scalarValue, tmpBuffer, stackSize);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, typename U, bool isReuseSource>
[aicore] inline void AxpyImpl(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                                         ;

    if constexpr (sizeof(U) == sizeof(float)) {
        Axpy<T, U>(dstTensor, srcTensor, scalarValue, calCount);
    } else {
        AxpySub<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
    }
}

}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/axpy.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 39 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/axpy.h"
template <typename T, typename U, bool isReuseSource = false>
[aicore] inline void Axpy(const LocalTensor<T>& dstTensor, const LocalTensor<U>& srcTensor, const U scalarValue,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    AxpyImpl<T, U, isReuseSource>(dstTensor, srcTensor, scalarValue, sharedTmpBuffer, calCount);
}

#pragma end_pipe
}
# 94 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/ceil.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/ceil.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/ceil/ceil_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/ceil/ceil_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/ceil/ceil_v220_impl.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/ceil/ceil_v220_impl.h"
namespace AscendC {
[aicore] inline void CeilProcess(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    (void)tmpTensor;
    Cast<float, float, false>(dstTensor, srcTensor, RoundMode::CAST_CEIL, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/ceil/ceil_common_impl.h" 2





namespace AscendC {

constexpr uint32_t CEIL_HALF_CALC_PROCEDURE = 2;

[aicore] inline void CeilProcess(const LocalTensor<half> &dstTensor, const LocalTensor<half> &srcTensor,
    const LocalTensor<uint8_t> &tmpTensor)
{
    const LocalTensor<float> floatTmpTensor = tmpTensor.ReinterpretCast<float>();


    Cast<float, half, false>(floatTmpTensor, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    CeilProcess(floatTmpTensor, floatTmpTensor, tmpTensor);
    Cast<half, float, false>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        { 1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
                                                                                                         ;



    uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();
    uint32_t splitCount = tmpBufferSize / sizeof(T);
    if constexpr (sizeof(T) == sizeof(half)) {
        splitCount = splitCount / CEIL_HALF_CALC_PROCEDURE / ONE_BLK_SIZE * ONE_BLK_SIZE;
    } else {
        splitCount = splitCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    }
    CheckTmpBufferSize(splitCount, 0, tmpBufferSize);

    uint32_t loopCount = calCount / splitCount;
    uint32_t calcTail = calCount % splitCount;

    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, splitCount);
    for (uint32_t i = 0; i < loopCount; ++i) {
        CeilProcess(dstTensor[i * splitCount], srcTensor[i * splitCount], sharedTmpBuffer);
    }
    if (calcTail > 0) {
        SetVectorMask<T>(0, calcTail);
        CeilProcess(dstTensor[loopCount * splitCount], srcTensor[loopCount * splitCount], sharedTmpBuffer);
    }

    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void CeilImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t calCount)
{

    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CeilImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/ceil.h" 2

namespace AscendC {
#pragma begin_pipe(V)
# 38 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl<T, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, calCount);
}
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/ceil.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Ceil(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor, const uint32_t calCount)
{


    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

    CeilImpl<T, isReuseSource>(dstTensor, srcTensor, calCount);
}

#pragma end_pipe
}
# 95 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_impl.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_utils.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_utils.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_utils.h" 2

namespace AscendC {

constexpr uint32_t ONE_VOR_BLOCK_DIM = 8;
constexpr uint32_t ELEMENT_NUM_FOR_UINT16 = 16;
constexpr int32_t FLOAT_ELEMENT_NUM = 2;
constexpr uint32_t REPEAT_STRIDE_NUM = 8;
constexpr uint32_t MAX_REPEAT_NUM = 255;

template <typename T, bool isReuseSource = false>
[aicore] inline void TwoDimBroadCastDimAlign(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    uint32_t orCounts = firstDim / ONE_VOR_BLOCK_DIM;
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint8_t repeateTimes = blockDim / oneBlockElementNum;
    SetMaskNorm();
    SetVectorMask<uint16_t, MaskMode::NORMAL>(ONE_VOR_BLOCK_DIM * ELEMENT_NUM_FOR_UINT16);
    uint8_t dstBlkStride = blockDim * dtypeCount / ELEMENT_NUM_FOR_UINT16;
    BinaryRepeatParams binaryParams(dstBlkStride, 0, 0, 1, 1, 0);
    uint32_t transTmpBufferOffset = 0;
    for (uint32_t i = 0; i < orCounts; i++) {
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            repeateTimes,
            binaryParams);
        transTmpBufferOffset += ONE_VOR_BLOCK_DIM * blockDim;
    }
    uint32_t orCountsTail = firstDim - orCounts * ONE_VOR_BLOCK_DIM;
    if (orCountsTail > 0) {
        SetMaskNorm();
        SetVectorMask<uint16_t, MaskMode::NORMAL>(orCountsTail * ELEMENT_NUM_FOR_UINT16);
        Or<uint16_t, false>(dstLocal[transTmpBufferOffset].template ReinterpretCast<uint16_t>(),
                            srcLocal.template ReinterpretCast<uint16_t>(),
                            zeroTemp.template ReinterpretCast<uint16_t>(),
                            MASK_PLACEHOLDER,
                            repeateTimes,
                            binaryParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void LoopBroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const LocalTensor<T> &zeroTemp, const uint32_t firstDim, const uint32_t blockDim)
{
    int32_t dtypeCount = 1;
    if constexpr (sizeof(T) == sizeof(float)) {
        dtypeCount = FLOAT_ELEMENT_NUM;
    }
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(firstDim * dtypeCount);
    BinaryRepeatParams binaryParams(1, 1, 0, REPEAT_STRIDE_NUM, REPEAT_STRIDE_NUM, 0);
    uint32_t temBufferOffset = 0;
    for (uint32_t i = 0; i < blockDim; i++) {
        Or<uint16_t, false>(dstLocal[temBufferOffset].template ReinterpretCast<uint16_t>(),
            srcLocal.template ReinterpretCast<uint16_t>(),
            zeroTemp.template ReinterpretCast<uint16_t>(),
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        temBufferOffset += firstDim;
    }
    PipeBarrier<PIPE_V>();
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_impl.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_v220_impl.h" 2

namespace AscendC {
constexpr uint32_t BRCB_ONE_SIZE = 8;
constexpr uint32_t BRCB_HALF_MAX_REPEATE_TIMES = 254;
constexpr uint32_t BRCB_FLOAT_MAX_REPEATE_TIMES = 255;
constexpr uint8_t GATHER_MASK_PATTERN = 7;

template <typename T>
[aicore] inline void BrcbToOneBlock(const LocalTensor<T> &srcLocal, const uint32_t firstDim,
    uint32_t oneBlockElementNum, LocalTensor<T> &brcbOneBlockTempBuffer)
{
    const uint32_t brcbRepeatTime = (firstDim + BRCB_ONE_SIZE - 1) / BRCB_ONE_SIZE;
    uint32_t brcbMaxRepeateTimes = BRCB_HALF_MAX_REPEATE_TIMES;
    if constexpr (sizeof(T) == sizeof(float)) {
        brcbMaxRepeateTimes = BRCB_FLOAT_MAX_REPEATE_TIMES;
    }
    const uint32_t brcbCount = brcbRepeatTime / brcbMaxRepeateTimes;
    const uint32_t tailBrcbRepeateTime = brcbRepeatTime % brcbMaxRepeateTimes;
    uint32_t brcbSrcOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < brcbCount; i++) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            brcbMaxRepeateTimes,
            {1, DEFAULT_REPEAT_STRIDE});
        brcbOneBlockTempBufferOffset += brcbMaxRepeateTimes * oneBlockElementNum * BRCB_ONE_SIZE;
        brcbSrcOffset += brcbMaxRepeateTimes * BRCB_ONE_SIZE;
    }
    if (tailBrcbRepeateTime != 0) {
        Brcb(brcbOneBlockTempBuffer[brcbOneBlockTempBufferOffset],
            srcLocal[brcbSrcOffset],
            tailBrcbRepeateTime,
            {1, DEFAULT_REPEAT_STRIDE});
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] inline void TwoDimBroadCastLastDimAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    SetVectorMask<T, MaskMode::COUNTER>(blockDim);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)(blockDim / oneBlockElementNum), 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDim;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(dstLocal[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource>
[aicore] inline void TwoDimBroadCastLastDimNotAlign220(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    LocalTensor<T> &tmpBuffer, const uint32_t firstDim, const uint32_t blockDim)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    BrcbToOneBlock(srcLocal, firstDim, oneBlockElementNum, tmpBuffer);
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    SetVectorMask<T, MaskMode::COUNTER>(blockDimAlign);
    const CopyRepeatParams copyRepeatParams = {1, 0, (uint16_t)blockDimAlignBlockNum, 1};
    uint32_t CopyCounts = firstDim / MAX_REPEAT_TIMES;
    uint32_t dstOffset = 0;
    uint32_t brcbOneBlockTempBufferOffset = 0;
    auto copyTempBuffer = tmpBuffer[firstDim * oneBlockElementNum];
    for (uint32_t i = 0; i < CopyCounts; i++) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            MAX_REPEAT_TIMES,
            copyRepeatParams);
        dstOffset += MAX_REPEAT_TIMES * blockDimAlign;
        brcbOneBlockTempBufferOffset += MAX_REPEAT_TIMES * oneBlockElementNum;
    }
    uint32_t tailsCopyRepeateTimes = firstDim % MAX_REPEAT_TIMES;
    if (tailsCopyRepeateTimes != 0) {
        Copy<T, false>(copyTempBuffer[dstOffset],
            tmpBuffer[brcbOneBlockTempBufferOffset],
            MASK_PLACEHOLDER,
            tailsCopyRepeateTimes,
            copyRepeatParams);
    }
    PipeBarrier<PIPE_V>();
    const GatherMaskParams gatherMaskParams = {
        1, (uint16_t)firstDim, (uint16_t)blockDimAlignBlockNum, 0};
    uint64_t rsvdCnt = 0;
    GatherMask(dstLocal, copyTempBuffer, GATHER_MASK_PATTERN, true, blockDim, gatherMaskParams, rsvdCnt);
    SetMaskCount();
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void GetAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    constexpr uint32_t minTmpBufferSize = minBrcbTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T>
[aicore] inline void GetNotAlignLoopNumbers(const uint32_t firstDim, const uint32_t blockDim,
    const uint32_t tmpBufferSize, uint32_t &oneRepeateSize, uint32_t &rangeM, uint32_t &tailM)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t minBrcbTempBufferSize = oneBlockElementNum * oneBlockElementNum;
    const uint32_t blockDimAlignBlockNum = (blockDim + oneBlockElementNum - 1) / oneBlockElementNum;
    const uint32_t blockDimAlign = blockDimAlignBlockNum * oneBlockElementNum;
    const uint32_t minCopyTempBufferSize = oneBlockElementNum * blockDimAlign;
    const uint32_t minTmpBufferSize = minBrcbTempBufferSize + minCopyTempBufferSize;





      ;
    oneRepeateSize = tmpBufferSize / minTmpBufferSize * oneBlockElementNum;
    rangeM = firstDim / oneRepeateSize;
    tailM = firstDim - oneRepeateSize * rangeM;
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void TwoDimBroadCastLastDim(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const auto firstDim = dstShape[0];
    const auto blockDim = dstShape[axis];
    uint32_t oneRepeateSize = 0;
    uint32_t rangeM = 0;
    uint32_t tailM = 0;
    uint32_t dstLocalOffset = 0;
    uint32_t srcLocalOffset = 0;
    if (blockDim * sizeof(T) % ONE_BLK_SIZE == 0) {
        GetAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }

        if (tailM != 0) {
            TwoDimBroadCastLastDimAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    } else {
        GetNotAlignLoopNumbers<T>(firstDim, blockDim, tmpBuffer.GetSize(), oneRepeateSize, rangeM, tailM);
        for (uint32_t i = 0; i < rangeM; i++) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, oneRepeateSize, blockDim);
            dstLocalOffset += oneRepeateSize * blockDim;
            srcLocalOffset += oneRepeateSize;
        }
        if (tailM != 0) {
            TwoDimBroadCastLastDimNotAlign220<T, isReuseSource>(
                dstLocal[dstLocalOffset], srcLocal[srcLocalOffset], tmpBuffer, tailM, blockDim);
        }
    }
}

template <typename T>
[aicore] inline void NoBroad(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal, const uint32_t size)
{
    SetVectorMask<T, MaskMode::COUNTER>(size);
    Copy<T, false>(dstLocal, srcLocal, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
}

}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/../../../impl/adv_api/detail/pad/broadcast/broadcast_common_impl.h" 2





namespace AscendC {
constexpr uint32_t TWO_DIM = 2;
constexpr uint32_t HALF_ONE_BLK_SIZE = 16;

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void TwoDimBroadCastFirstDim(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    const uint32_t firstDim = dstShape[0];
    const uint32_t blockDim = dstShape[1];

                                                                                                                     ;

    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t FIRST_DIM_LOOP_LIMITE = MAX_REPEAT_NUM * oneBlockElementNum;

    auto zeroTemp = tmpBuffer;
    Duplicate(zeroTemp.template ReinterpretCast<uint16_t>(), (uint16_t)0, ONE_BLK_SIZE / sizeof(uint16_t));
    PipeBarrier<PIPE_V>();

    if (blockDim >= FIRST_DIM_LOOP_LIMITE) {
        LoopBroadCast<T>(dstLocal, srcLocal, zeroTemp, blockDim, firstDim);
        return;
    }

    TwoDimBroadCastDimAlign<T, isReuseSource>(dstLocal, srcLocal, zeroTemp, firstDim, blockDim);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void BroadCastCompute(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<T> &tmpBuffer)
{
    uint32_t srcSize = 1;
    uint32_t dstSize = 1;
    for (uint32_t i = 0; i < dim; i++) {
        srcSize *= srcShape[i];
        dstSize *= dstShape[i];
    }

    if (srcSize == dstSize) {
        NoBroad(dstLocal, srcLocal, dstSize);
    } else if (srcSize == 1) {
        TEventID event1 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_S));
        SetFlag<HardEvent::V_S>(event1);
        WaitFlag<HardEvent::V_S>(event1);
        auto scalar = srcLocal.GetValue(0);
        TEventID event2 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::S_V));
        SetFlag<HardEvent::S_V>(event2);
        WaitFlag<HardEvent::S_V>(event2);
        Duplicate(dstLocal, scalar, dstSize);
        PipeBarrier<PIPE_V>();
    } else {
        if constexpr (dim == TWO_DIM) {
            if constexpr (axis == 1) {
                TwoDimBroadCastLastDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            } else {
                TwoDimBroadCastFirstDim<T, dim, axis, false>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
            }
        } else {
                                                                                                   ;
        }
    }
    SetMaskCount();
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim]);

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer);

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void BroadCastCommon(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim])
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void BroadCastCommon(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
                                   ;
    if constexpr (sizeof(T) == 1) {
        LocalTensor<half> tmpBuffer = sharedTmpBuffer.ReinterpretCast<half>();
        uint32_t srcSize = 1;
        uint32_t dstSize = 1;
        for (uint32_t i = 0; i < dim; i++) {
            srcSize *= srcShape[i];
            dstSize *= dstShape[i];
        }
        auto srcTempBuffer = tmpBuffer;
        const uint32_t alignSrcSize = ((srcSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        const uint32_t alignDstSize = ((dstSize + HALF_ONE_BLK_SIZE - 1) / HALF_ONE_BLK_SIZE) * HALF_ONE_BLK_SIZE;
        auto dstTempBuffer = tmpBuffer[alignSrcSize];
        auto tempTempBuffer = dstTempBuffer[alignDstSize];
        SetMaskCount();
        SetVectorMask<T, MaskMode::COUNTER>(srcSize);
        Cast<half, T, false>(srcTempBuffer,
            srcLocal,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        BroadCastCompute<half, dim, axis, isReuseSource>(
            dstTempBuffer, srcTempBuffer, dstShape, srcShape, tempTempBuffer);
        SetVectorMask<T, MaskMode::COUNTER>(dstSize);
        Cast<T, half, false>(dstLocal,
            dstTempBuffer,
            RoundMode::CAST_NONE,
            MASK_PLACEHOLDER,
            1,
            {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    } else {
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        SetMaskCount();
        BroadCastCompute<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, tmpBuffer);
        SetMaskNorm();
        ResetMask();
    }
                                  ;
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource>
[aicore] inline void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim])
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    BroadCastCommon<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape);
}

template <typename T, int32_t dim, int32_t axis, bool isReuseSource>
[aicore] inline void BroadCast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }

                                                                  ;
    BroadCastCommon<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h" 2



namespace AscendC {
#pragma begin_pipe(V)
# 35 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void Broadcast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim], LocalTensor<uint8_t> &sharedTmpBuffer)
{
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape, sharedTmpBuffer);
}
# 50 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/pad/broadcast.h"
template <typename T, int32_t dim, int32_t axis, bool isReuseSource = false>
[aicore] inline void Broadcast(const LocalTensor<T> &dstLocal, const LocalTensor<T> &srcLocal,
    const uint32_t dstShape[dim], const uint32_t srcShape[dim])
{
    BroadCast<T, dim, axis, isReuseSource>(dstLocal, srcLocal, dstShape, srcShape);
}
#pragma end_pipe
}
# 96 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 2




# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_v220_impl.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_v220_impl.h"
namespace AscendC {
[aicore] inline void CastInt162Float(const LocalTensor<float>& dst, const LocalTensor<int16_t>& src)
{
    Cast<float, int16_t, false>(dst, src, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
                          {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
}

[aicore] inline void CastFloat2Int16(const LocalTensor<int16_t>& dst, const LocalTensor<float>& src)
{
    Cast<int16_t, float, false>(dst, src, RoundMode::CAST_ROUND, MASK_PLACEHOLDER, 1,
                          {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_xor_sum/reduce_xor_sum_common_impl.h" 2






namespace AscendC {
namespace {
constexpr uint32_t REDUCE_XOR_SUM_REUSE_CALC_PROC = 2U;
constexpr uint32_t REDUCE_XOR_SUM_NOREUSE_CALC_PROC = 3U;
}

struct ReduceXorSumParam {
    [aicore] ReduceXorSumParam() {};
    LocalTensor<int16_t> tmpTensor1;
    LocalTensor<int16_t> tmpTensor2;
    LocalTensor<int16_t> tmpTensor3;
};

#pragma begin_pipe(V)
template <typename T, bool isReuseSource = false>
[aicore] inline void ReduceXorSumCompute(LocalTensor<T>& dst, const LocalTensor<T>& src0,
    const LocalTensor<T>& src1, LocalTensor<uint8_t>& tmp, const uint32_t calCount)
{
    static_assert(std::is_same<T, int16_t>::value, "ReduceXorSum only support int16_t data type on current device!");

                       ;

    uint32_t splitSize = 0;
    ReduceXorSumParam param;

    if constexpr (isReuseSource) {
        splitSize = tmp.GetSize() / sizeof(T) / REDUCE_XOR_SUM_REUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        param.tmpTensor1 = tmp.ReinterpretCast<int16_t>();
        param.tmpTensor2 = param.tmpTensor1[splitSize];
        param.tmpTensor3 = src1;
    } else {
        splitSize = tmp.GetSize() / sizeof(T) / REDUCE_XOR_SUM_NOREUSE_CALC_PROC / ONE_BLK_SIZE * ONE_BLK_SIZE;
        param.tmpTensor1 = tmp.ReinterpretCast<int16_t>();
        param.tmpTensor2 = param.tmpTensor1[splitSize];
        param.tmpTensor3 = param.tmpTensor2[splitSize];
    }



      ;

    SetMaskCount();
    SetVectorMask<T>(0, calCount);
    const UnaryRepeatParams unaryParams;
    const BinaryRepeatParams binaryParams;


    And<T, false>(param.tmpTensor1, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);

    Or<T, false>(param.tmpTensor2, src0, src1, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    Not<T, false>(param.tmpTensor1, param.tmpTensor1, MASK_PLACEHOLDER, 1, unaryParams);
    PipeBarrier<PIPE_V>();

    And<T, false>(param.tmpTensor2, param.tmpTensor1, param.tmpTensor2, MASK_PLACEHOLDER, 1, binaryParams);
    PipeBarrier<PIPE_V>();

    CastInt162Float(param.tmpTensor1.ReinterpretCast<float>(), param.tmpTensor2);
    PipeBarrier<PIPE_V>();

    SetMaskNorm();
    ResetMask();

    ReduceSum<float>(param.tmpTensor1.ReinterpretCast<float>(), param.tmpTensor1.ReinterpretCast<float>(),
        param.tmpTensor3.ReinterpretCast<float>(), calCount);
    PipeBarrier<PIPE_V>();

    SetMaskCount();
    SetVectorMask<T>(0, 1);
    CastFloat2Int16(dst, param.tmpTensor1.ReinterpretCast<float>());
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}
#pragma end_pipe
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h" 2








namespace AscendC {
#pragma begin_pipe(V)
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 68 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_xor_sum.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void ReduceXorSum(LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
                                    const LocalTensor<T>&src1Tensor, const uint32_t calCount)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;

    ReduceXorSumCompute<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, tmp, calCount);
}
#pragma end_pipe
}
# 97 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_common.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_common.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_common_util_impl.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_common_util_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_common_util_impl.h" 2



namespace AscendC {
namespace Pattern {
namespace Detail {
constexpr int32_t DIM_TWO = 2;
constexpr int32_t DIM_THREE = 3;
constexpr int32_t DIM_FOUR = 4;
constexpr int32_t DIM_FIVE = 5;
constexpr int32_t DIM_SIX = 6;
constexpr int32_t DIM_SEVEN = 7;
constexpr int32_t DIM_EIGHT = 8;
constexpr int32_t DIM_NINE = 9;
constexpr int32_t PATTERN_R = 0;
constexpr int32_t PATTERN_RA = 1;
constexpr int32_t PATTERN_AR = 2;
constexpr int32_t PATTERN_ARA = 3;
constexpr int32_t PATTERN_ARAR = 4;
constexpr int32_t PATTERN_ARARA = 5;
constexpr int32_t PATTERN_ARARAR = 6;
constexpr int32_t PATTERN_ARARARA = 7;
constexpr int32_t PATTERN_ARARARAR = 8;
constexpr int32_t PATTERN_ARARARARA = 9;
constexpr int32_t PATTERN_RAR = 10;
constexpr int32_t PATTERN_RARA = 11;
constexpr int32_t PATTERN_RARAR = 12;
constexpr int32_t PATTERN_RARARA = 13;
constexpr int32_t PATTERN_RARARAR = 14;
constexpr int32_t PATTERN_RARARARA = 15;

template <int32_t id, bool firstA, bool tailA, int32_t dim>
struct PatternConstInfo {
constexpr static int32_t ID = id;
constexpr static bool FirstA = firstA;
constexpr static bool TailA = tailA;
constexpr static int32_t Dim = dim;
};
}
}

namespace Internal {

enum class ApiMode : uint8_t {
    API_MODE_SUM = 0,
    API_MODE_MIN,
    API_MODE_MAX,
    API_MODE_ANY,
    API_MODE_ALL
};


[aicore] inline uint32_t FindClosestPowerOfTwo(uint32_t n)
{
                                                                                      ;
    constexpr uint32_t totalShiftBits = 63;
    return totalShiftBits - ScalarCountLeadingZero(n);
}

template <class T>
[aicore] inline void ComputeMaskBit(uint32_t oneBlkMask, uint32_t oneBlkElems, uint32_t blkNum,
    uint64_t& maskLow, uint64_t& maskHigh)
{


    if constexpr (sizeof(T) == sizeof(half)) {
        uint32_t maskLBlkNum = blkNum > HALF_DEFAULT_REPEAT_STRIDE ? HALF_DEFAULT_REPEAT_STRIDE : blkNum;
        uint32_t maskHBlkNum = blkNum - maskLBlkNum;
        for (int32_t k = 0; k < maskLBlkNum; k++) {
            maskLow += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
        for (int32_t k = 0; k < maskHBlkNum; k++) {
            maskHigh += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
    } else if constexpr (sizeof(T) == sizeof(float)) {
        for (int32_t k = 0; k < blkNum; k++) {
            maskLow += (((1ULL << oneBlkMask) - 1ULL) << (k * oneBlkElems));
        }
    }
}

template <class T, ApiMode apiMode, MaskMode maskMode = MaskMode::NORMAL>
[aicore] inline void BlockReduceCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const int32_t repeat, const uint64_t mask[], const int32_t blkStride, const int32_t repStride)
{
    if constexpr (maskMode == MaskMode::NORMAL) {
        if constexpr (apiMode == ApiMode::API_MODE_SUM) {
            BlockReduceSum(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        } else if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
            BlockReduceMin(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
            BlockReduceMax(dstTensor, srcTensor, repeat, mask, 1, 1, DEFAULT_REPEAT_STRIDE);
        }
    } else {
        if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
            BlockReduceMin<T, false>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER, 1, blkStride, repStride);
        } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
            BlockReduceMax<T, false>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER, 1, blkStride, repStride);
        }
    }
}

template <class T, ApiMode apiMode>
[aicore] inline void WholeReduceCompute(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const int32_t repeat, const int32_t mask, const int32_t repStride)
{
    if constexpr (apiMode == ApiMode::API_MODE_MIN || apiMode == ApiMode::API_MODE_ALL) {
        WholeReduceMin(dstTensor, srcTensor, mask, repeat, 1, 1, repStride, ReduceOrder::ORDER_ONLY_VALUE);
    } else if constexpr (apiMode == ApiMode::API_MODE_MAX || apiMode == ApiMode::API_MODE_ANY) {
        WholeReduceMax(dstTensor, srcTensor, mask, repeat, 1, 1, repStride, ReduceOrder::ORDER_ONLY_VALUE);
    }
}
}
}
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce_common.h" 2

namespace AscendC {
namespace Pattern {
namespace Reduce {





struct R : private Detail::PatternConstInfo<Detail::PATTERN_R, true, false, 1> {};
struct RA : private Detail::PatternConstInfo<Detail::PATTERN_RA, false, true, Detail::DIM_TWO> {};
struct AR : private Detail::PatternConstInfo<Detail::PATTERN_AR, true, false, Detail::DIM_TWO> {};
struct ARA : private Detail::PatternConstInfo<Detail::PATTERN_ARA, true, true, Detail::DIM_THREE> {};
struct ARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARAR, true, false, Detail::DIM_FOUR> {};
struct ARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARA, true, true, Detail::DIM_FIVE> {};
struct ARARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARARAR, true, false, Detail::DIM_SIX> {};
struct ARARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARARA, true, true, Detail::DIM_SEVEN> {};
struct ARARARAR : private Detail::PatternConstInfo<Detail::PATTERN_ARARARAR, true, false, Detail::DIM_EIGHT> {};
struct ARARARARA : private Detail::PatternConstInfo<Detail::PATTERN_ARARARARA, true, true, Detail::DIM_NINE> {};
struct RAR : private Detail::PatternConstInfo<Detail::PATTERN_RAR, false, false, Detail::DIM_THREE> {};
struct RARA : private Detail::PatternConstInfo<Detail::PATTERN_RARA, false, true, Detail::DIM_FOUR> {};
struct RARAR : private Detail::PatternConstInfo<Detail::PATTERN_RARAR, false, false, Detail::DIM_FIVE> {};
struct RARARA : private Detail::PatternConstInfo<Detail::PATTERN_RARARA, false, true, Detail::DIM_SIX> {};
struct RARARAR : private Detail::PatternConstInfo<Detail::PATTERN_RARARAR, false, false, Detail::DIM_SEVEN> {};
struct RARARARA : private Detail::PatternConstInfo<Detail::PATTERN_RARARARA, false, true, Detail::DIM_EIGHT> {};
}
}
}
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2


# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/reduce_prod_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/reduce_prod_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/reduce_prod_v220_impl.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/../reduce_common_util_v220_impl.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/../reduce_common_util_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/../reduce_common_util_v220_impl.h" 2



namespace AscendC {
namespace Internal {

template <typename T, ApiMode apiMode>
[aicore] inline void DoReduceLessThanBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t firstAxis, uint32_t lastAxis)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstBlkRepeat = DivCeil(firstAxis, DEFAULT_BLK_NUM);
    uint32_t blkMaxRepeat = DivCeil(firstBlkRepeat, MAX_REPEAT_TIMES);
    uint32_t blkRepeatTail =
        firstBlkRepeat % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstBlkRepeat % MAX_REPEAT_TIMES;
    uint32_t mainBlkNum = firstAxis < DEFAULT_BLK_NUM ? firstAxis : DEFAULT_BLK_NUM;
    uint64_t mainMaskLow = 0;
    uint64_t mainMaskHigh = 0;
    ComputeMaskBit<T>(lastAxis, elePerBlk, mainBlkNum, mainMaskLow, mainMaskHigh);
    uint64_t mainMask[] = { mainMaskLow, mainMaskHigh };
    uint32_t tailBlkNum = firstAxis % DEFAULT_BLK_NUM;
    if (tailBlkNum == 0 || firstAxis < DEFAULT_BLK_NUM) {
        uint32_t blkMainRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < blkMaxRepeat; i++) {
            blkMainRepeat = i == blkMaxRepeat - 1 ? blkRepeatTail : MAX_REPEAT_TIMES;
            BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                srcTensor[i * MAX_REPEAT_TIMES * elePerRep], blkMainRepeat, mainMask, 1,
                DEFAULT_REPEAT_STRIDE);
            PipeBarrier<PIPE_V>();
        }
    } else {
        uint64_t tailMaskLow = 0;
        uint64_t tailMaskHigh = 0;
        ComputeMaskBit<T>(lastAxis, elePerBlk, tailBlkNum, tailMaskLow, tailMaskHigh);
        uint64_t tailMask[] = { tailMaskLow, tailMaskHigh };
        for (int32_t i = 0; i < blkMaxRepeat; i++) {
            if (i == blkMaxRepeat - 1) {
                BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                    srcTensor[i * MAX_REPEAT_TIMES * elePerRep], blkRepeatTail - 1, mainMask, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
                BlockReduceCompute<T, apiMode>(dstTensor[(i * MAX_REPEAT_TIMES + blkRepeatTail - 1) * DEFAULT_BLK_NUM],
                    srcTensor[(i * MAX_REPEAT_TIMES + blkRepeatTail - 1) * elePerRep], 1, tailMask, 1,
                    DEFAULT_REPEAT_STRIDE);
            } else {
                BlockReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM],
                    srcTensor[i * MAX_REPEAT_TIMES * elePerRep], MAX_REPEAT_TIMES, mainMask, 1,
                    DEFAULT_REPEAT_STRIDE);
                PipeBarrier<PIPE_V>();
            }
        }
    }
}

template <typename T, ApiMode apiMode>
[aicore] inline void DoReduceOneBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    uint32_t firstAxis, uint32_t lastAxis)
{
    SetMaskCount();
    SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * lastAxis);
    BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER_LIST, 1,
        DEFAULT_REPEAT_STRIDE);
}

template <typename T, void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t,
    const uint8_t, const BinaryRepeatParams &)>
[aicore] inline void AccValOnBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, const BinaryRepeatParams& mainParams, const BinaryRepeatParams& tailParams,
    uint32_t firstAxis, uint32_t lastAxis, uint32_t tmpOffset, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    uint32_t blkCount = lastAxis / elePerBlk;
    uint32_t blkTail = lastAxis % elePerBlk;
    for (int32_t i = 1; i < blkCount; i++) {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        func(tmpTensor, tmpTensor, srcTensor[i * elePerBlk], MASK_PLACEHOLDER, 1, mainParams);
        PipeBarrier<PIPE_V>();
    }
    if (blkTail != 0) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(blkTail);
        uint32_t blkRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            blkRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            func(tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                srcTensor[i * MAX_REPEAT_TIMES * padLast + blkCount * elePerBlk], blkTail, blkRepeat, tailParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskCount();
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] inline void DoReduceByBlk(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint8_t blkStridePerRow = padLast / elePerBlk;
    uint8_t blkStridePerRep = (padLast / elePerBlk) * DEFAULT_BLK_NUM;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        UnaryRepeatParams blockUnaryParams{ 1, blkStridePerRow, DEFAULT_REPEAT_STRIDE, blkStridePerRep };
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, blockUnaryParams);
        PipeBarrier<PIPE_V>();
        BinaryRepeatParams blockMainParams{ 1, 1, blkStridePerRow, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, blkStridePerRep};
        BinaryRepeatParams blockTailParams{ 1, 1, 1, 1, 1, blkStridePerRow };
        AccValOnBlk<T, func>(dstTensor, srcTensor, tmpTensor, blockMainParams, blockTailParams, firstAxis, lastAxis, elePerBlk, padLast);
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            DEFAULT_REPEAT_STRIDE);
    } else {
        BinaryRepeatParams blockMainParams{ blkStridePerRow, blkStridePerRow, blkStridePerRow, blkStridePerRep,
            blkStridePerRep, blkStridePerRep};
        BinaryRepeatParams blockTailParams{ 1, 1, 1, blkStridePerRow, blkStridePerRow, blkStridePerRow };
        AccValOnBlk<T, func>(dstTensor, srcTensor, srcTensor, blockMainParams, blockTailParams, firstAxis, lastAxis, padLast, padLast);
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerBlk);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, srcTensor, 1, MASK_PLACEHOLDER_LIST, blkStridePerRow,
            blkStridePerRep);
    }
}

template <typename T, ApiMode apiMode>
[aicore] inline void GetReduceValOnRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t tmpOffset, uint32_t repStride)
{
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    if constexpr (IsSameType<T, half>::value) {
        SetMaskNorm();
        uint32_t blockRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            blockRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            WholeReduceCompute<T, apiMode>(dstTensor[i * MAX_REPEAT_TIMES], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                blockRepeat, elePerRep, repStride);
            PipeBarrier<PIPE_V>();
        }
    } else {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(tmpTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            repStride);
        PipeBarrier<PIPE_V>();
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * DEFAULT_BLK_NUM);
        BlockReduceCompute<T, apiMode, MaskMode::COUNTER>(dstTensor, tmpTensor, 1, MASK_PLACEHOLDER_LIST, 1,
            DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] inline void AccValOnRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, const BinaryRepeatParams& binaryParams, uint32_t firstAxis, uint32_t lastAxis,
    uint32_t tmpOffset, uint32_t repStride, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t firstRepeat = DivCeil(firstAxis, MAX_REPEAT_TIMES);
    uint32_t firstRepeatTail = firstAxis % MAX_REPEAT_TIMES == 0 ? MAX_REPEAT_TIMES : firstAxis % MAX_REPEAT_TIMES;
    uint32_t repCount = lastAxis / elePerRep;
    uint32_t repTail = lastAxis % elePerRep;
    for (int32_t i = 1; i < repCount; i++) {
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        func(tmpTensor, tmpTensor, srcTensor[i * elePerRep], MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }
    if (repTail != 0) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(repTail);
        uint32_t repRepeat = MAX_REPEAT_TIMES;
        for (int32_t i = 0; i < firstRepeat; i++) {
            repRepeat = i == firstRepeat - 1 ? firstRepeatTail : MAX_REPEAT_TIMES;
            func(tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset], tmpTensor[i * MAX_REPEAT_TIMES * tmpOffset],
                srcTensor[i * MAX_REPEAT_TIMES * padLast + repCount * elePerRep], repTail, repRepeat, binaryParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskCount();
    }
    GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, tmpTensor, firstAxis, tmpOffset, repStride);
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] inline void DoReduceByRep(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint8_t repStridePerRow = padLast / elePerBlk;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        UnaryRepeatParams repeatUnaryParams{ 1, 1, DEFAULT_REPEAT_STRIDE, repStridePerRow };
        SetVectorMask<T, MaskMode::COUNTER>(0, firstAxis * elePerRep);
        Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, repeatUnaryParams);
        PipeBarrier<PIPE_V>();
        BinaryRepeatParams binaryParams{ 1, 1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, repStridePerRow};
        AccValOnRep<T, apiMode, func>(dstTensor, srcTensor, tmpTensor, binaryParams, firstAxis, lastAxis, elePerRep,
            DEFAULT_REPEAT_STRIDE, padLast);
    } else {
        BinaryRepeatParams binaryParams{ 1, 1, 1, repStridePerRow, repStridePerRow, repStridePerRow};
        AccValOnRep<T, apiMode, func>(dstTensor, srcTensor, srcTensor, binaryParams, firstAxis, lastAxis, padLast,
            repStridePerRow, padLast);
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
        const BinaryRepeatParams &)>
[aicore] inline void DoLongLastReduce(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    uint32_t repCount = DivCeil(lastAxis, elePerRep);
    uint32_t repTail = lastAxis % elePerRep == 0 ? elePerRep : lastAxis % elePerRep;
    BinaryRepeatParams defaultParams;
    UnaryRepeatParams defaultUnaryParams;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        SetVectorMask<T, MaskMode::COUNTER>(0, elePerRep);
        for (int32_t i = 0; i < firstAxis; i++) {
            Adds<T, false>(tmpTensor[i * elePerRep], srcTensor[i * padLast], static_cast<T>(0), MASK_PLACEHOLDER,
                1, defaultUnaryParams);
            PipeBarrier<PIPE_V>();
        }
        uint32_t mask = elePerRep;
        for (int32_t i = 1; i < repCount; i++) {
            mask = i == repCount - 1 ? repTail : elePerRep;
            SetVectorMask<T, MaskMode::COUNTER>(0, mask);
            for (int32_t j = 0; j < firstAxis; j++) {
                func(tmpTensor[j * elePerRep], tmpTensor[j * elePerRep],
                    srcTensor[j * padLast + i * elePerRep], MASK_PLACEHOLDER, 1, defaultParams);
                PipeBarrier<PIPE_V>();
            }
        }
        GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, tmpTensor, firstAxis, elePerRep, DEFAULT_REPEAT_STRIDE);
    } else {
        uint32_t mask = elePerRep;
        for (int32_t i = 0; i < firstAxis; i++) {
            for (int32_t j = 1; j < repCount; j++) {
                mask = j == repCount - 1 ? repTail : elePerRep;
                SetVectorMask<T, MaskMode::COUNTER>(0, mask);
                if (j == 1) {
                    func(srcTensor[i * elePerRep], srcTensor[i * padLast], srcTensor[i * padLast + j * elePerRep],
                        MASK_PLACEHOLDER, 1, defaultParams);
                    PipeBarrier<PIPE_V>();
                } else {
                    func(srcTensor[i * elePerRep], srcTensor[i * elePerRep], srcTensor[i * padLast + j * elePerRep],
                        MASK_PLACEHOLDER, 1, defaultParams);
                    PipeBarrier<PIPE_V>();
                }
            }
        }
        GetReduceValOnRep<T, apiMode>(dstTensor, srcTensor, srcTensor, firstAxis, elePerRep, DEFAULT_REPEAT_STRIDE);
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] inline void BlockReduceByLastAxis(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{



      ;
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    constexpr uint32_t elePerRep = ONE_REPEAT_BYTE_SIZE / sizeof(T);
    if (lastAxis < elePerBlk) {
        DoReduceLessThanBlk<T, apiMode>(dstTensor, srcTensor, firstAxis, lastAxis);
    } else if (lastAxis == elePerBlk) {
        DoReduceOneBlk<T, apiMode>(dstTensor, srcTensor, firstAxis, lastAxis);
    } else if (lastAxis > elePerBlk && lastAxis < elePerRep) {
        DoReduceByBlk<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis, padLast);
    } else if (lastAxis >= elePerRep && lastAxis <= MAX_REPEAT_TIMES * elePerBlk) {
        DoReduceByRep<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis, padLast);
    } else {
        DoLongLastReduce<T, isReuseSource, apiMode, func>(dstTensor, srcTensor, tmpTensor, firstAxis, lastAxis,
            padLast);
    }
}

struct ReduceParams {
public:
    [aicore] ReduceParams() {}
    [aicore] ReduceParams(uint32_t first, uint32_t last,
        uint32_t padLast, uint32_t splitK, uint32_t tail, uint32_t elePerBlk)
    {
        this->first = first;
        this->last = last;
        this->padLast = padLast;
        this->splitK = splitK;
        this->tail = tail;
        this->elePerBlk = elePerBlk;
    }
    uint32_t first = 0;
    uint32_t last = 0;
    uint32_t padLast = 0;
    uint32_t splitK = 0;
    uint32_t tail = 0;
    uint32_t elePerBlk = 0;
    BinaryRepeatParams defaultParam = { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE };
    UnaryRepeatParams defaultUnaryParam = { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE,
                DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE };
};

template <class T, ApiMode apiMode>
[aicore] inline void BlkReduceForLoop(const LocalTensor<T>& dst,
    const LocalTensor<T>& tmp, uint32_t srcOffset, uint32_t first, uint32_t last) {
    constexpr uint32_t blkReduceDstStride = 8;
    uint32_t srcPerBlkElements = ONE_BLK_SIZE/sizeof(T);
    uint64_t maskHigh = 0;
    uint32_t oneRepElements = srcPerBlkElements * DEFAULT_BLK_NUM;
    uint32_t nMaxRepBlkNum = first / (MAX_REPEAT_TIMES * DEFAULT_BLK_NUM);
    uint32_t tailMaxRepBlkNum = first % (MAX_REPEAT_TIMES * DEFAULT_BLK_NUM);
    uint32_t tailNBlkNum = tailMaxRepBlkNum / DEFAULT_BLK_NUM;
    uint32_t tailRemainOfBlkNum = tailMaxRepBlkNum % DEFAULT_BLK_NUM;
    uint32_t dstOffset = 0;
    uint32_t blkReduceSrcOffset = 0;
    uint32_t oneBlkMask = last > srcPerBlkElements ? srcPerBlkElements : last;
    uint64_t maskLow = 0;
    ComputeMaskBit<T>(oneBlkMask, srcPerBlkElements, DEFAULT_BLK_NUM, maskLow, maskHigh);

    uint64_t blkReduceMask[] = { maskLow, maskHigh };
    for (int k = 0; k < nMaxRepBlkNum; k++) {
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[srcOffset], MAX_REPEAT_TIMES, blkReduceMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
    if (tailNBlkNum > 0) {
        dstOffset = nMaxRepBlkNum * MAX_REPEAT_TIMES * blkReduceDstStride;
        blkReduceSrcOffset = srcOffset + nMaxRepBlkNum * MAX_REPEAT_TIMES * DEFAULT_BLK_NUM * srcPerBlkElements;
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[blkReduceSrcOffset], tailNBlkNum, blkReduceMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
    if (tailRemainOfBlkNum > 0) {
        maskLow = 0;
        maskHigh = 0;
        uint32_t tailBlkReduceRep = 1;
        ComputeMaskBit<T>(oneBlkMask, srcPerBlkElements, tailRemainOfBlkNum, maskLow, maskHigh);
        uint64_t tailMask[] = { maskLow, maskHigh };
        dstOffset = tailNBlkNum * blkReduceDstStride + (nMaxRepBlkNum * MAX_REPEAT_TIMES * blkReduceDstStride);
        blkReduceSrcOffset = srcOffset + tailNBlkNum * oneRepElements + (nMaxRepBlkNum * MAX_REPEAT_TIMES * oneRepElements);
        BlockReduceCompute<T, apiMode>(dst[dstOffset], tmp[blkReduceSrcOffset], tailBlkReduceRep, tailMask,
            DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, bool isReuseSource, ApiMode apiMode,
            void (*func)(const LocalTensor<half> &, const LocalTensor<half> &,
                       const LocalTensor<half> &, uint64_t, const uint8_t,
                       const BinaryRepeatParams &)>
[aicore] inline void BinaryReduceAnyAllCompute(
    const LocalTensor<T> &dst, const LocalTensor<T> &src,
    const LocalTensor<T> &tmp, const ReduceParams &params)
{
    half halfZero = 0.0;
    LocalTensor<half> tmpBuf = tmp.template ReinterpretCast<half>();
    uint32_t tmpK;
    constexpr uint32_t halfBlkElements = 16;
    SetMaskCount();
    for (int i = 0; i < params.first; i++) {
        SetVectorMask<uint8_t, MaskMode::COUNTER>(params.padLast);
        Cast<half, uint8_t, false>(tmpBuf, src[i*params.padLast], RoundMode::CAST_NONE,
            MASK_PLACEHOLDER, 1,
            { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE });
        PipeBarrier<PIPE_V>();
        if (params.tail > 0 && params.splitK > 0) {
            SetVectorMask<half, MaskMode::COUNTER>(params.tail);
            func(tmpBuf, tmpBuf, tmpBuf[params.splitK], MASK_PLACEHOLDER, 1, params.defaultParam);
            PipeBarrier<PIPE_V>();
        }
        tmpK = params.splitK;
        while (tmpK > halfBlkElements) {
            tmpK >>= 1;
            SetVectorMask<half, MaskMode::COUNTER>(tmpK);
            func(tmpBuf, tmpBuf, tmpBuf[tmpK], MASK_PLACEHOLDER, 1, params.defaultParam);
            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<half, MaskMode::COUNTER>(halfBlkElements);
        Adds<half, false>(tmpBuf[params.padLast + i * halfBlkElements], tmpBuf, halfZero,
            MASK_PLACEHOLDER, 1, params.defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
    BlkReduceForLoop<half, apiMode>(tmpBuf, tmpBuf, params.padLast, params.first, params.last);
    SetMaskCount();
    SetVectorMask<half, MaskMode::COUNTER>(params.first);
    Cast<uint8_t, half, false>(dst, tmpBuf, RoundMode::CAST_NONE,
        MASK_PLACEHOLDER, 1,
        { DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE });
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <typename T, bool isReuseSource,
    void (*func)(const LocalTensor<T> &, const LocalTensor<T> &, const LocalTensor<T> &, uint64_t, const uint8_t,
    const BinaryRepeatParams &)>
[aicore] inline void BinaryReduceByFirstAxis(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<T>& tmpTensor, uint32_t firstAxis, uint32_t lastAxis, uint32_t padLast)
{



      ;
    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    uint32_t k = FindClosestPowerOfTwo(firstAxis);
    uint32_t splitK = 1 << k;
    uint32_t remain = firstAxis - splitK;
    SetMaskCount();
    if constexpr (isReuseSource) {

        if (remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * remain);
            func(srcTensor, srcTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
    } else {
        CheckTmpBufferSize(tmpTensor.GetSize(), 0, tmpTensor.GetSize());

        if (remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK * padLast);
            Adds<T, false>(tmpTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1,
                           defaultUnaryParam);
            PipeBarrier<PIPE_V>();
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * remain);
            func(tmpTensor, tmpTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else if (splitK > 1) {
            splitK >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, padLast * splitK);
            func(tmpTensor, srcTensor, srcTensor[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else {
            SetVectorMask<T, MaskMode::COUNTER>(0, lastAxis);
            Adds<T, false>(dstTensor, srcTensor, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();
            return;
        }
    }

    LocalTensor<T> currBuff = isReuseSource ? srcTensor : tmpTensor;
    while (splitK > 1) {
        splitK >>= 1;
        SetVectorMask<T, MaskMode::COUNTER>(0, padLast * splitK);
        func(currBuff, currBuff, currBuff[splitK * padLast], MASK_PLACEHOLDER, 1, defaultParam);
        PipeBarrier<PIPE_V>();
    }
    SetVectorMask<T, MaskMode::COUNTER>(0, lastAxis);
    Adds<T, false>(dstTensor, currBuff, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
    PipeBarrier<PIPE_V>();
}
}
}
# 17 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_prod/reduce_prod_v220_impl.h" 2



namespace AscendC {
namespace Internal {

template <typename T, bool isReuseSource>
[aicore] inline void PreProcessReduceForAR(const LocalTensor<T>& src, const LocalTensor<T>& currBuff,
    uint32_t row, uint32_t last, uint32_t padLast, uint32_t remain, uint32_t& splitK)
{
    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    constexpr uint32_t bytePerBlk = 32;
    constexpr uint32_t elePerBlk = bytePerBlk / sizeof(T);
    if constexpr (isReuseSource) {
        if (last >= elePerBlk && remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, remain);
            Mul<T, false>(currBuff, currBuff, currBuff[splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
    } else {
        if (last >= elePerBlk && remain != 0) {
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK);
            Adds<T, false>(currBuff, src[row * padLast], static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();

            SetVectorMask<T, MaskMode::COUNTER>(0, remain);
            Mul<T, false>(currBuff, currBuff, src[row * padLast + splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else if (splitK > elePerBlk) {
            splitK >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, splitK);

            Mul<T, false>(currBuff, src[row * padLast], src[row * padLast + splitK], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        } else {
            SetVectorMask<T, MaskMode::COUNTER>(0, last);
            Adds<T, false>(currBuff, src[row * padLast], static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
            PipeBarrier<PIPE_V>();
        }
    }
}

template <typename T, bool isReuseSource>
[aicore] inline void ReduceProdByLastAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmp, uint32_t first, uint32_t last, uint32_t padLast)
{
    constexpr uint32_t bytePerBlk = 32;
    constexpr uint32_t elePerBlk = bytePerBlk / sizeof(T);
    constexpr uint32_t bytePerRep = 256;
    constexpr uint32_t elePerRep = bytePerRep / sizeof(T);

    BinaryRepeatParams defaultParam;
    UnaryRepeatParams defaultUnaryParam;
    BrcbRepeatParams defaultBrcbParam;;
    LocalTensor<T> resBeforeGather = tmp[elePerRep];
    LocalTensor<T> finalResStored = isReuseSource ? src : resBeforeGather;

    uint32_t k = FindClosestPowerOfTwo(last);
    uint32_t splitK = 1 << k;
    uint32_t remain = last - splitK;
    SetMaskCount();
    if constexpr (!isReuseSource) {
        CheckTmpBufferSize(tmp.GetSize(), 0, tmp.GetSize());
    }
    for (uint32_t j = 0; j < first; j++) {
        uint32_t splitKCopy = splitK;
        LocalTensor<T> tmpRowRes = isReuseSource ? src[j * padLast] : resBeforeGather[j * elePerBlk];
        LocalTensor<T> tmpDst = isReuseSource ? src[j * elePerBlk] : resBeforeGather[j * elePerBlk];

        PreProcessReduceForAR<T, isReuseSource>(src, tmpRowRes, j, last, padLast, remain, splitKCopy);

        while (splitKCopy > elePerBlk) {
            splitKCopy >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, splitKCopy);

            Mul<T, false>(tmpRowRes, tmpRowRes, tmpRowRes[splitKCopy], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }

        Brcb(tmp, tmpRowRes, 1, defaultBrcbParam);
        PipeBarrier<PIPE_V>();
        uint32_t finalTail = last < elePerBlk ? splitK : elePerBlk;
        if (splitK != last && finalTail < elePerBlk) {
            SetVectorMask<T, MaskMode::COUNTER>(0, remain * elePerBlk);
            Mul<T, false>(tmp, tmp, tmp[splitK * elePerBlk], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
        while (finalTail > 1) {
            finalTail >>= 1;
            SetVectorMask<T, MaskMode::COUNTER>(0, finalTail * elePerBlk);

            Mul<T, false>(tmp, tmp, tmp[finalTail * elePerBlk], MASK_PLACEHOLDER, 1, defaultParam);
            PipeBarrier<PIPE_V>();
        }
        SetVectorMask<T, MaskMode::COUNTER>(0, elePerBlk);
        Adds<T, false>(tmpDst, tmp, static_cast<T>(0), MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    LocalTensor<uint32_t> tmpInt = tmp.template ReinterpretCast<uint32_t>();
    Duplicate(tmpInt, 1u, elePerRep);
    PipeBarrier<PIPE_V>();
    GatherMaskParams gatherMaskParam = {1, static_cast<uint16_t>(first), 1, 0};
    uint64_t rsvdCnt;
    GatherMask(dst, finalResStored, tmpInt, true, elePerBlk, gatherMaskParam, rsvdCnt);
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceProdImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float>(), "failed to check the data type, current api supports data type is float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                               ;
    LocalTensor<T> tmpDst = sharedTmpBuffer.ReinterpretCast<T>();
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        ReduceProdByLastAxis<T, isReuseSource>(dstTensor, srcTensor,tmpDst, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Mul<T, false>>(
            dstTensor, srcTensor, tmpDst, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_max/reduce_max_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_max/reduce_max_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_max/reduce_max_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMaxImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_MAX, Max<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Max<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_min/reduce_min_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_min/reduce_min_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_min/reduce_min_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMinImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;

    LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_MIN, Min<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    } else {
        BinaryReduceByFirstAxis<T, isReuseSource, Min<T, false>>(
            dstTensor, srcTensor, tmpTensor, first, last, padLast);
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_sum/reduce_sum_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_sum/reduce_sum_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_sum/reduce_sum_v220_impl.h" 2





namespace AscendC {
namespace Internal {

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumComputeSliceAAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const uint32_t tailLen, const ReduceParams &params)
{
    uint32_t curOffset;
    if constexpr (isReuseSource) {
        SetVectorMask<T, MaskMode::COUNTER>(tailLen);
        for (uint32_t i = 0; i < repeat; i++) {
            curOffset = i * params.padLast;
            Add<T, false>(tmpBuf[curOffset], tmpBuf[curOffset],
                src[curOffset + perRowReduceSize], MASK_PLACEHOLDER, 1, params.defaultParam);
        }
        PipeBarrier<PIPE_V>();

        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, tmpBuf, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    } else {
        for (uint32_t i = 0; i < repeat; i++) {
            curOffset = i * params.padLast;
            SetVectorMask<T, MaskMode::COUNTER>(tailLen);
            Add<T, false>(tmpBuf[i * perRowReduceSize], tmpBuf[i * perRowReduceSize],
                src[curOffset + perRowReduceSize], MASK_PLACEHOLDER, 1, params.defaultParam);
        }
        PipeBarrier<PIPE_V>();
    }
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumComputeSliceRAxis(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const uint32_t tailLenN, const uint32_t tailLenRemain, const ReduceParams &params)
{
    SetMaskNorm();
    uint32_t srcOffset;
    uint32_t tmpOffset;
    uint32_t perRowSize = perRowReduceSize;
    if constexpr (isReuseSource) {
        perRowSize = params.padLast;
    }
    uint8_t dstRepStride = static_cast<uint8_t>(perRowSize / B32_DATA_NUM_PER_BLOCK);
    uint8_t src0RepStride = dstRepStride;
    uint8_t src1RepStride = static_cast<uint8_t>(params.padLast / B32_DATA_NUM_PER_BLOCK);
    SetVectorMask<T, MaskMode::NORMAL>(B32_DATA_NUM_PER_REPEAT);
    for (uint32_t i = 0; i < tailLenN; i++) {
        srcOffset = perRowReduceSize + i * B32_DATA_NUM_PER_REPEAT;
        tmpOffset = i * B32_DATA_NUM_PER_REPEAT;
        Add<T, false>(tmpBuf[tmpOffset], tmpBuf[tmpOffset], src[srcOffset], MASK_PLACEHOLDER, repeat,
            {1, 1, 1, dstRepStride, src0RepStride, src1RepStride});
    }
    if (tailLenRemain > 0) {
        tmpOffset = tailLenN * B32_DATA_NUM_PER_REPEAT;
        srcOffset = perRowReduceSize + tailLenN * B32_DATA_NUM_PER_REPEAT;
        SetVectorMask<T, MaskMode::NORMAL>(tailLenRemain);
        Add<T, false>(tmpBuf[tmpOffset], tmpBuf[tmpOffset], src[srcOffset], MASK_PLACEHOLDER, repeat,
            {1, 1, 1, dstRepStride, src0RepStride, src1RepStride});
    }
    PipeBarrier<PIPE_V>();
    if constexpr (isReuseSource) {
        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, tmpBuf, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    }
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumComputeTail(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const ReduceParams &params)
{
    uint32_t tailLen = params.tail;
    if (params.tail == 0 && perRowReduceSize > 0) {
        tailLen = params.padLast - perRowReduceSize;
    }
    uint32_t tailLenN = tailLen / B32_DATA_NUM_PER_REPEAT;
    uint32_t tailLenRemain = tailLen % B32_DATA_NUM_PER_REPEAT;
    bool isLastAxisSliceLarge = (tailLenN + (tailLenRemain > 0)) < repeat;
    uint32_t vaddMaxRepStrideVal = 255;

    bool isLeMaxRepStride = (perRowReduceSize / B32_DATA_NUM_PER_BLOCK) <= vaddMaxRepStrideVal;
    bool performanceSlice = isLastAxisSliceLarge && params.last > B32_DATA_NUM_PER_REPEAT && isLeMaxRepStride;
    if (performanceSlice) {
        ReduceSumComputeSliceRAxis<T, isReuseSource>(dst, src, tmpBuf,
            repeat, perRowReduceSize, tailLenN, tailLenRemain, params);
    } else {
        ReduceSumComputeSliceAAxis<T, isReuseSource>(dst, src, tmpBuf, repeat, perRowReduceSize, tailLen, params);
    }
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumInLargeLast(const LocalTensor<T>& dst, const LocalTensor<T>& src,
        const LocalTensor<T>& tmpBuf, const uint32_t repeat,
        const uint32_t perRowReduceSize, const ReduceParams &params)
{
    if constexpr (!isReuseSource) {
        uint16_t blockCount = repeat;
        uint16_t blockLen = perRowReduceSize / B32_DATA_NUM_PER_BLOCK;
        uint16_t srcStride = (params.padLast - perRowReduceSize) / B32_DATA_NUM_PER_BLOCK;
        uint16_t dstStride = 0;
        DataCopy(tmpBuf, src, { blockCount, blockLen, srcStride, dstStride });
        PipeBarrier<PIPE_V>();
    }
    ReduceSumComputeTail<T, isReuseSource>(dst, src, tmpBuf, repeat, perRowReduceSize, params);
    ResetMask();
    SetMaskCount();
    uint32_t tmpK = perRowReduceSize;
    while (tmpK > B32_DATA_NUM_PER_REPEAT) {
        SetVectorMask<T, MaskMode::COUNTER>(repeat * tmpK);
        PairReduceSum<T, false>(tmpBuf, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
        PipeBarrier<PIPE_V>();
        tmpK >>= 1;
    }
    SetVectorMask<T, MaskMode::COUNTER>(repeat * B32_DATA_NUM_PER_REPEAT);
    BlockReduceSum<T, false>(tmpBuf, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
    PipeBarrier<PIPE_V>();
    SetVectorMask<T, MaskMode::COUNTER>(repeat * B32_DATA_NUM_PER_BLOCK);
    BlockReduceSum<T, false>(dst, tmpBuf, 1, MASK_PLACEHOLDER, 1, 1, DEFAULT_REPEAT_STRIDE);
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumArCompute(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuf, const uint32_t perRowReduceSize, const ReduceParams &params)
{
    constexpr uint32_t maxRepeatTimes = 248;
    uint32_t maxRepOffsetN = params.first / maxRepeatTimes;
    uint32_t maxRepeatOffsetTail = params.first % maxRepeatTimes;
    uint32_t srcMaxRepNOffset;
    uint32_t dstMaxRepNOffset;
    uint32_t srcMaxRepTailOffset;
    uint32_t dstMaxRepTailOffset;
    if (params.last <= B32_DATA_NUM_PER_REPEAT) {
        SetMaskNorm();
        SetVectorMask<T, MaskMode::NORMAL>(params.last);
        for (uint32_t i = 0; i < maxRepOffsetN; i++) {
            srcMaxRepNOffset = maxRepeatTimes * i * params.padLast;
            dstMaxRepNOffset = maxRepeatTimes * i;
            WholeReduceSum<T, false>(dst[dstMaxRepNOffset], src[srcMaxRepNOffset],
                MASK_PLACEHOLDER, maxRepeatTimes, 1, 1, params.padLast / params.elePerBlk);
        }
        if (maxRepeatOffsetTail > 0) {
            srcMaxRepTailOffset = (params.first - maxRepeatOffsetTail) * params.padLast;
            dstMaxRepTailOffset = params.first - maxRepeatOffsetTail;
            SetVectorMask<T, MaskMode::NORMAL>(params.last);
            WholeReduceSum<T, false>(dst[dstMaxRepTailOffset], src[srcMaxRepTailOffset],
                MASK_PLACEHOLDER, maxRepeatOffsetTail, 1, 1, params.padLast / params.elePerBlk);
        }
    } else {
        SetMaskCount();
        uint32_t tmpBufMaxRepNOffset;

        for (uint32_t i = 0; i < maxRepOffsetN; i++) {
            srcMaxRepNOffset = maxRepeatTimes * i * params.padLast;
            dstMaxRepNOffset = maxRepeatTimes * i;
            tmpBufMaxRepNOffset = maxRepeatTimes * i * B32_DATA_NUM_PER_REPEAT;
            ReduceSumInLargeLast<T, isReuseSource>(dst[dstMaxRepNOffset], src[srcMaxRepNOffset],
                tmpBuf[tmpBufMaxRepNOffset], maxRepeatTimes, perRowReduceSize, params);
        }
        if (maxRepeatOffsetTail > 0) {
            srcMaxRepTailOffset = (params.first - maxRepeatOffsetTail) * params.padLast;
            dstMaxRepTailOffset = params.first - maxRepeatOffsetTail;
            tmpBufMaxRepNOffset = (params.first - maxRepeatOffsetTail) * B32_DATA_NUM_PER_REPEAT;
            ReduceSumInLargeLast<T, isReuseSource>(dst[dstMaxRepTailOffset],
                src[srcMaxRepTailOffset], tmpBuf[tmpBufMaxRepNOffset],
                maxRepeatOffsetTail, perRowReduceSize, params);
        }
        SetMaskNorm();
    }
    PipeBarrier<PIPE_V>();
    ResetMask();
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumArReusedSrc(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    ReduceParams params)
{
    if (params.last <= B32_DATA_NUM_PER_BLOCK) {
        BlkReduceForLoop<T, ApiMode::API_MODE_SUM>(dst, src, 0, params.first, params.last);
    } else {
        uint32_t perRowReduceSize = params.elePerBlk > params.splitK ? params.elePerBlk : params.splitK;
        if (params.last == params.splitK) {
            perRowReduceSize >>= 1;
        }
        ReduceSumArCompute<T, isReuseSource>(dst, src, src, perRowReduceSize, params);
    }
}

template <class T, bool isReuseSource = false>
[aicore] inline void ReduceSumArUnReusedSrc(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmpBuf, const ReduceParams &params)
{
    uint32_t perRowReduceSize = params.elePerBlk > params.splitK ? params.elePerBlk : params.splitK;
    if (params.last == params.splitK) {
        perRowReduceSize >>= 1;
    }

    if (params.last <= B32_DATA_NUM_PER_BLOCK) {
        BlkReduceForLoop<T, ApiMode::API_MODE_SUM>(dst, src, 0, params.first, params.last);
    } else {
        ReduceSumArCompute<T, isReuseSource>(dst, src, tmpBuf, perRowReduceSize, params);
    }
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceSumCommon(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad, const ReduceParams &reduceParams)
{
    uint32_t first = reduceParams.first;
    uint32_t last = reduceParams.last;
    uint32_t padLast = reduceParams.padLast;
    uint32_t elePerBlk = reduceParams.elePerBlk;
    LocalTensor<T> tmpBuf = sharedTmpBuffer.ReinterpretCast<T>();
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {





          ;
        uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
        if (last <= elePerBlk) {
            splitK = 0;
        }
        uint32_t tail = last - splitK;
        if constexpr (isReuseSource) {
            ReduceSumArReusedSrc<T, true>(dstTensor, srcTensor, ReduceParams(
                first, last, padLast, splitK, tail, elePerBlk));
        } else {
            ReduceSumArUnReusedSrc<T, false>(dstTensor, srcTensor, tmpBuf,
                ReduceParams(first, last, padLast, splitK, tail, elePerBlk));
        }
    } else {



          ;
        BinaryReduceByFirstAxis<T, isReuseSource, Add<T, false>>(
                dstTensor, srcTensor, tmpBuf, first, last, padLast);
    }
}

template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceSumImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad) {
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float>(), "failed to check the data type, current api supports data type is float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    ReduceParams reduceParams = ReduceParams(first, last, padLast, 0, 0, elePerBlk);
    ReduceSumCommon<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad, reduceParams);
    SetMaskNorm();
    ResetMask();
}

}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_mean/reduce_mean_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_mean/reduce_mean_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_mean/reduce_mean_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMeanImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, half, float>(), "failed to check the data type, current api supports data type is half/float!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");

                                                         ;
    ReduceParams reduceParams = ReduceParams(first, last, padLast, 0, 0, elePerBlk);
    ReduceSumCommon<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad, reduceParams);
    SetMaskCount();
    UnaryRepeatParams defaultUnaryParam;
    if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
        float lastAxisValReciprocal = 1.0f/static_cast<int32_t>(last);
        SetVectorMask<T, MaskMode::COUNTER>(first);
        Muls<T, false>(dstTensor, dstTensor, lastAxisValReciprocal, MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    } else {
        float firstAxisValReciprocal = 1.0f/static_cast<int32_t>(first);
        SetVectorMask<T, MaskMode::COUNTER>(last);
        Muls<T, false>(dstTensor, dstTensor, firstAxisValReciprocal, MASK_PLACEHOLDER, 1, defaultUnaryParam);
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 23 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_any/reduce_any_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_any/reduce_any_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_any/reduce_any_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAnyImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float, uint8_t>(), "failed to check the data type, current api supports data type is float/uint8_t!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    if constexpr (SupportType<T, uint8_t>()) {
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
            if (last < elePerBlk) {
                splitK = 0;
            }
            uint32_t tail = last - splitK;
            BinaryReduceAnyAllCompute<T, isReuseSource, ApiMode::API_MODE_MAX, Max<half, false>>(
                    dstTensor, srcTensor, sharedTmpBuffer, ReduceParams(first, last, padLast, splitK, tail, elePerBlk));
        } else {

            padLast >>= 1;
            last = (last + 1) >> 1;
            LocalTensor<int16_t> srcTmpBuff = srcTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> dstTmpBuff = dstTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> tmpBuf = sharedTmpBuffer.template ReinterpretCast<int16_t>();
            BinaryReduceByFirstAxis<int16_t, isReuseSource, Or<int16_t, false>>(
                dstTmpBuff, srcTmpBuff, tmpBuf, first, last, padLast);
        }
    } else {
        LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();

        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_ANY, Max<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        } else {
            BinaryReduceByFirstAxis<T, isReuseSource, Max<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        }
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_all/reduce_all_v220_impl.h" 1
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_all/reduce_all_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/../../../impl/adv_api/detail/reduce/reduce_all/reduce_all_v220_impl.h" 2





namespace AscendC {
namespace Internal {
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAllImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
                                      const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t srcShape[],
                                      bool srcInnerPad)
{
    uint32_t last = srcShape[1];
    uint32_t first = srcShape[0];
    constexpr uint32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    uint32_t padLast = AlignUp(last, elePerBlk);
    static_assert(SupportType<T, float, uint8_t>(), "failed to check the data type, current api supports data type is float/uint8_t!");
    static_assert(SupportType<pattern, Pattern::Reduce::AR, Pattern::Reduce::RA>(),
        "failed to check the reduce pattern, it only supports AR/RA pattern!");
                                                                                                                              ;
    if constexpr (SupportType<T, uint8_t>()) {
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            uint32_t splitK = 1 << FindClosestPowerOfTwo(last);
            if (last < elePerBlk) {
                splitK = 0;
            }
            uint32_t tail = last - splitK;
            BinaryReduceAnyAllCompute<T, isReuseSource, ApiMode::API_MODE_MIN, Min<half, false>>(
                dstTensor, srcTensor, sharedTmpBuffer, ReduceParams(first, last, padLast, splitK, tail, elePerBlk));

        } else {

            padLast >>= 1;
            last = (last + 1) >> 1;
            LocalTensor<int16_t> srcTmpBuff = srcTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> dstTmpBuff = dstTensor.template ReinterpretCast<int16_t>();
            LocalTensor<int16_t> tmpBuf = sharedTmpBuffer.template ReinterpretCast<int16_t>();
            BinaryReduceByFirstAxis<int16_t, isReuseSource, And<int16_t, false>>(
                dstTmpBuff, srcTmpBuff, tmpBuf, first, last, padLast);
        }
    } else {
        LocalTensor<T> tmpTensor = sharedTmpBuffer.ReinterpretCast<T>();
        if constexpr (IsSameType<pattern, Pattern::Reduce::AR>::value) {
            BlockReduceByLastAxis<T, isReuseSource, ApiMode::API_MODE_ALL, Min<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        } else {
            BinaryReduceByFirstAxis<T, isReuseSource, Min<T, false>>(
                dstTensor, srcTensor, tmpTensor, first, last, padLast);
        }
    }
    SetMaskNorm();
    ResetMask();
}
}
}
# 25 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h" 2






namespace AscendC {
#pragma begin_pipe(V)
# 47 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMeanImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 72 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMean(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMean<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMax(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMaxImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 124 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMax(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMax<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 151 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMin(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceMinImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 176 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceMin(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceMin<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 204 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceSum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const LocalTensor<uint8_t> &sharedTmpBuffer,
                                  const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceSumImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 229 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceSum(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
                                  const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceSum<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 256 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceProd(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceProdImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 280 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceProd(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceProd<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 307 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAny(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceAnyImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 331 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAny(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceAny<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 358 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAll(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t srcShape[], bool srcInnerPad)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    Internal::ReduceAllImpl<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}
# 382 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/reduce/reduce.h"
template <class T, class pattern, bool isReuseSource = false>
[aicore] inline void ReduceAll(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    const uint32_t srcShape[], bool srcInnerPad)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    ReduceAll<T, pattern, isReuseSource>(dstTensor, srcTensor, sharedTmpBuffer, srcShape, srcInnerPad);
}

#pragma end_pipe
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata_common.h" 1
# 13 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata_common.h"
namespace AscendC {
template <typename T, typename U>
struct TransDataParams {
    T srcLayout;
    U dstLayout;
};



struct TransDataConfig {
    DataFormat srcFormat;
    DataFormat dstFormat;
};

}
# 14 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata.h" 2

# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/transdata/transdata_impl.h" 1
# 15 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/transdata/transdata_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 16 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/../../../impl/adv_api/detail/transpose/transdata/transdata_impl.h" 2




namespace AscendC {
namespace Internal {

namespace {
constexpr int32_t n0 = 16;
constexpr int32_t c0 = 16;
constexpr int32_t hw0 = 16;
constexpr int32_t ncdhwDims = 5;
constexpr int32_t fractalZ3DDims = 7;
constexpr int32_t ndc1hwc0Dims = 6;
}

struct TransDataTmpParams {
    int32_t n;
    int32_t c;
    int32_t d;
    int32_t h;
    int32_t w;
    int32_t n1;
    int32_t c1;
    int32_t padHw;
};

constexpr int32_t DEFAULT_TRANSDATA_5HD_LIST = 16;

template <typename T>
[aicore] inline void DC1Hwn1n0c0ToC1DHwn1n0c0HWAlign(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const TransDataTmpParams& params)
{

    int32_t d = params.d;
    int32_t h = params.h;
    int32_t w = params.w;
    int32_t n1 = params.n1;
    int32_t c1 = params.c1;
    int32_t padHw = params.padHw;

    uint32_t dim0 = d;
    uint32_t dim1 = c1;
    uint32_t lastDim = h * w * n1 * n0 * c0;


    int32_t n1n0c0DimElems = n1 * n0 * c0;
    int32_t hwAlignElems = padHw * n1n0c0DimElems;
    int32_t hwPadElems = (padHw - h * w) * n1n0c0DimElems;

    uint16_t blockCount = dim1;
    uint16_t blockLen = lastDim * sizeof(T) / ONE_BLK_SIZE;
    uint16_t srcGap = 0;
    uint16_t dstGap = ((dim0 - 1) * hwAlignElems + hwPadElems) * sizeof(T) / ONE_BLK_SIZE;

    uint32_t dstSize = c1 * d * padHw * n1 * n0 * c0;
    Duplicate<T>(dst, static_cast<T>(0), dstSize);
    PipeBarrier<PIPE_V>();

    DataCopyParams dataCopyParams = { blockCount, blockLen, srcGap, dstGap };
    for (uint32_t d0 = 0; d0 < dim0; d0++) {
        DataCopy(dst[d0 * hwAlignElems], src[d0 * dim1 * lastDim], dataCopyParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void C1Dhwn1n0c0ToC1C0Dhwn1n0(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const TransDataTmpParams& params)
{

    int32_t d = params.d;
    int32_t n1 = params.n1;
    int32_t c1 = params.c1;
    int32_t padHw = params.padHw;

    TransDataTo5HDParams transDataParams;
    transDataParams.dstHighHalf = false;
    transDataParams.srcHighHalf = false;
    transDataParams.repeatTimes = d * padHw * n1;
    if (transDataParams.repeatTimes == 1) {
        transDataParams.srcRepStride = 0;
        transDataParams.dstRepStride = 0;
    } else {
        transDataParams.srcRepStride = DEFAULT_TRANSDATA_5HD_LIST * c0 * sizeof(T) / ONE_BLK_SIZE;
        transDataParams.dstRepStride = n0 * sizeof(T) / ONE_BLK_SIZE;
    }

    uint64_t srcOffsetArr[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t dstOffsetArr[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t srcAddr = (uint64_t)src.GetPhyAddr();
    uint64_t dstAddr = (uint64_t)dst.GetPhyAddr();
    for (uint32_t j = 0; j < c1; j++) {
        uint32_t outOffset = j * d * padHw * n1 * n0 * c0;
        for (uint8_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            srcOffsetArr[i] = (uint64_t)(srcAddr + (outOffset + i * n0) * sizeof(T));
            dstOffsetArr[i] = (uint64_t)(dstAddr + (outOffset + i * d * padHw * n1 * n0) * sizeof(T));
        }
        TransDataTo5HD<T>(dstOffsetArr, srcOffsetArr, transDataParams);
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void C1c0dhwN1n0ToNcdhw(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<T>& tmp, const TransDataTmpParams& params)
{

    int32_t d = params.d;
    int32_t n1 = params.n1;
    int32_t padHw = params.padHw;
    int32_t currN = params.n;
    int32_t c = params.c;

    TransDataTo5HDParams transDataParams;
    transDataParams.dstHighHalf = false;
    transDataParams.srcHighHalf = false;
    transDataParams.repeatTimes = c * d * padHw / n0;
    if (transDataParams.repeatTimes == 1) {
        transDataParams.srcRepStride = 0;
        transDataParams.dstRepStride = 0;
    } else {
        transDataParams.srcRepStride = DEFAULT_TRANSDATA_5HD_LIST * n1 * n0 * sizeof(T) / ONE_BLK_SIZE;
        transDataParams.dstRepStride = c0 * sizeof(T) / ONE_BLK_SIZE;
    }

    uint64_t srcOffsetArr[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t dstOffsetArr[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t srcAddr = (uint64_t)src.GetPhyAddr();
    uint64_t dstAddr = (uint64_t)dst.GetPhyAddr();
    uint64_t tmpAddr = (uint64_t)tmp.GetPhyAddr();
    for (uint32_t j = 0; j < n1; j++) {
        if (n0 - currN > 0) {
            for (uint8_t i = 0; i < currN; i++) {
                dstOffsetArr[i] = (uint64_t)(dstAddr + (j * d * c * padHw * n0 + i * c * d * padHw) * sizeof(T));
            }
            for (uint8_t i = currN; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
                dstOffsetArr[i] = (uint64_t)(tmpAddr + i * ONE_BLK_SIZE * sizeof(T));
            }
        } else {
            for (uint8_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
                dstOffsetArr[i] = (uint64_t)(dstAddr + (j * d * c * padHw * n0 + i * c * d * padHw) * sizeof(T));
            }
        }
        for (uint8_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            srcOffsetArr[i] = (uint64_t)(srcAddr + (j * n0 + i * n0 * n1) * sizeof(T));
        }
        TransDataTo5HD<T>(dstOffsetArr, srcOffsetArr, transDataParams);
        currN -= n0;
    }
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void N1n0C1c0DHWToNCDHW(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const TransDataTmpParams& params)
{

    int32_t n = params.n;
    int32_t c = params.c;
    int32_t d = params.d;
    int32_t c1 = params.c1;
    int32_t padHw = params.padHw;

    uint16_t blockCount = n;
    uint16_t blockLen = (c * (d * padHw)) * sizeof(T) / ONE_BLK_SIZE;
    uint16_t srcGap = ((c1 * c0 - c) * (d * padHw)) * sizeof(T) /ONE_BLK_SIZE;
    uint16_t dstGap = 0;
    DataCopyParams dataCopyParams = { blockCount, blockLen, srcGap, dstGap };
    DataCopy(dst, src, dataCopyParams);
    PipeBarrier<PIPE_V>();
}

template <typename T>
[aicore] inline void TransDataFractalToNcdhw(const LocalTensor<T>& dst, const LocalTensor<T>& src,
    const LocalTensor<uint8_t>& tmpBuffer, const TransDataTmpParams& params)
{
    int32_t d = params.d;
    int32_t n1 = params.n1;
    int32_t c1 = params.c1;
    int32_t padHw = params.padHw;
    int32_t n = params.n;
    int32_t c = params.c;

    LocalTensor<half> tmp = tmpBuffer.template ReinterpretCast<half>();
    LocalTensor<half> srcTmp = src.template ReinterpretCast<half>();
    if (c == c1 * c0 && n == n1 * n0) {
        LocalTensor<half> dstTmp = dst.template ReinterpretCast<half>();

        DC1Hwn1n0c0ToC1DHwn1n0c0HWAlign<half>(dstTmp, srcTmp, params);

        C1Dhwn1n0c0ToC1C0Dhwn1n0<half>(tmp, dstTmp, params);

        C1c0dhwN1n0ToNcdhw<half>(dstTmp, tmp, tmp, params);
    } else {
        LocalTensor<half> transDataTmp = tmp[n1 * n0 * c1 * c0 * d * padHw];
        LocalTensor<half> dstTmp = dst.template ReinterpretCast<half>();

        DC1Hwn1n0c0ToC1DHwn1n0c0HWAlign<half>(tmp, srcTmp, params);

        C1Dhwn1n0c0ToC1C0Dhwn1n0<half>(transDataTmp, tmp, params);

        C1c0dhwN1n0ToNcdhw<half>(dstTmp, transDataTmp, tmp, params);
    }
}


template <typename T>
[aicore] inline void TransDataImplNcdhwToFractal(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<uint8_t>& tmpBuffer,
    const TransDataTmpParams& param)
{
    constexpr int32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    const int32_t n = param.n, c = param.c, d = param.d, h = param.h, w = param.w;
    constexpr int32_t c0 = 16;
    constexpr int32_t n0 = 16;
    const int32_t c1 = DivCeil(c, c0);
    const int32_t n1 = DivCeil(n, n0);
    int32_t padHw = AlignUp(h * w, elePerBlk);
    int32_t currAxis = c * d * padHw;
    int32_t tmpDupSize = currAxis;
    if (d * h * w * n1 * n0 > tmpDupSize) {
        tmpDupSize = d * h * w * n1 * n0;
    }
    Duplicate<T>(tmpBuffer.ReinterpretCast<T>(), static_cast<T>(0), tmpDupSize);
    PipeBarrier<PIPE_V>();
    auto tmpDstTensor = tmpBuffer[tmpDupSize * sizeof(T)].ReinterpretCast<T>();
    uint64_t dstLocalList[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t srcLocalList[DEFAULT_TRANSDATA_5HD_LIST];

    uint64_t dstTensorAddr = (uint64_t)dst.GetPhyAddr();
    uint64_t srcTensorAddr = (uint64_t)src.GetPhyAddr();
    uint64_t tmpDstTensorAddr = (uint64_t)tmpDstTensor.GetPhyAddr();
    uint64_t tmpBufferAddr = (uint64_t)tmpBuffer.GetPhyAddr();


    TransDataTo5HDParams transDataParams;
    transDataParams.dstHighHalf = false;
    transDataParams.srcHighHalf = false;
    transDataParams.repeatTimes = currAxis / elePerBlk;

    transDataParams.dstRepStride = transDataParams.repeatTimes == 1 ? 0 : n1 * n0;
    transDataParams.srcRepStride = transDataParams.repeatTimes == 1 ? 0 : 1;

    bool isPadded = padHw != h * w;

    auto tmpIfPadAddr = isPadded ? tmpDstTensorAddr : dstTensorAddr;
    for (int j = 0; j < n1; j++) {
        uint64_t currDstAddr = tmpIfPadAddr + j * n0 * sizeof(T);
        uint64_t currSrcAddr = srcTensorAddr + j * currAxis * n0 * sizeof(T);

        int remain = j == n1 - 1 ? n - j * n0 : n0;
        for (int32_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            dstLocalList[i] = currDstAddr + (i * n1 * n0) * sizeof(T);
        }
        for (int32_t i = 0; i < remain; i++) {
            srcLocalList[i] = currSrcAddr + i * currAxis * sizeof(T);
        }
        for (int32_t i = remain; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            srcLocalList[i] = tmpBufferAddr;
        }
        TransDataTo5HD<half>(dstLocalList, srcLocalList, transDataParams);
    }
    PipeBarrier<PIPE_V>();

    DataCopyParams copyParams;
    if (isPadded) {
        currAxis = h * w * n1 * n0;
        copyParams.blockCount = c * d;
        copyParams.blockLen = currAxis / elePerBlk;

        copyParams.srcStride = (padHw - h * w) * n1 * n0 / elePerBlk;
        copyParams.dstStride = 0;
        DataCopy(dst, tmpDstTensor, copyParams);
        PipeBarrier<PIPE_V>();
    }


    currAxis = d * h * w * n1 * n0;
    transDataParams.repeatTimes = currAxis / elePerBlk;
    transDataParams.dstRepStride = transDataParams.repeatTimes == 1 ? 0 : c0;
    transDataParams.srcRepStride = transDataParams.repeatTimes == 1 ? 0 : 1;
    for (int32_t j = 0; j < c1; j++) {
        uint64_t currDstAddr = tmpDstTensorAddr + j * currAxis * c0 * sizeof(T);
        uint64_t currSrcAddr = dstTensorAddr + j * currAxis * c0 * sizeof(T);
        int remain = j == c1 - 1 ? c - j * c0 : c0;
        for (int32_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            dstLocalList[i] = currDstAddr + i * c0 * sizeof(T);
        }
        for (int32_t i = 0; i < remain; i++) {
            srcLocalList[i] = currSrcAddr + i * currAxis * sizeof(T);
        }
        for (int32_t i = remain; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
            srcLocalList[i] = tmpBufferAddr;
        }
        TransDataTo5HD<half>(dstLocalList, srcLocalList, transDataParams);
    }
    PipeBarrier<PIPE_V>();

    currAxis = c0 * h * w * n1 * n0;
    copyParams.blockCount = d;
    copyParams.blockLen = currAxis / elePerBlk;

    copyParams.srcStride = 0;
    copyParams.dstStride = (c1 - 1) * currAxis / elePerBlk;
    for (int32_t i = 0; i < c1; i++) {
        DataCopy(dst[i * currAxis], tmpDstTensor[i * d * currAxis], copyParams);
    }
    PipeBarrier<PIPE_V>();
}


template <typename T>
[aicore] inline void TransDataImplNcdhwTo6Hd(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<uint8_t>& tmpBuffer,
    const TransDataTmpParams& param)
{
    constexpr int32_t c0 = 16;
    constexpr int32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    const int32_t n = param.n, c = param.c, d = param.d, h = param.h, w = param.w;
    const int32_t c1 = DivCeil(c, c0);
    const int32_t padHw = AlignUp(h * w, elePerBlk);
    int32_t currAxis = d * padHw;

    int32_t axisHwd = h * w * d;
    int32_t axisHwc0 = h * w * c0;
    int32_t axisC1hwc0 = axisHwc0 * c1;
    int32_t axisC1hwdc0 = axisC1hwc0 * d;
    int32_t axisPadHwd = padHw * d;
    int32_t axisPadHwc0 = padHw * c0;
    int32_t axisPadHwdc0 = padHw * c0 * d;
    Duplicate<T>(tmpBuffer.ReinterpretCast<T>(), static_cast<T>(0), axisPadHwd);
    PipeBarrier<PIPE_V>();


    auto tmpDstTensor = tmpBuffer[axisPadHwd * sizeof(T)].ReinterpretCast<T>();

    uint64_t dstTensorAddr = (uint64_t)dst.GetPhyAddr();
    uint64_t srcTensorAddr = (uint64_t)src.GetPhyAddr();
    uint64_t tmpDstTensorAddr = (uint64_t)tmpDstTensor.GetPhyAddr();
    uint64_t tmpBufferAddr = (uint64_t)tmpBuffer.GetPhyAddr();
    uint64_t dstLocalList[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t srcLocalList[DEFAULT_TRANSDATA_5HD_LIST];
    TransDataTo5HDParams transDataParams;
    transDataParams.dstHighHalf = false;
    transDataParams.srcHighHalf = false;
    transDataParams.repeatTimes = axisPadHwd / elePerBlk;
    transDataParams.dstRepStride = transDataParams.repeatTimes == 1 ? 0 : c0;
    transDataParams.srcRepStride = transDataParams.repeatTimes == 1 ? 0 : 1;

    DataCopyParams copyParams;
    copyParams.blockCount = d;
    copyParams.blockLen = axisHwc0 / elePerBlk;
    copyParams.srcStride = (padHw - h * w) * c0 / elePerBlk;
    copyParams.dstStride = (c1 - 1) * axisHwc0 / elePerBlk;

    for (int32_t k = 0; k < n; k++) {
        int32_t currSrcStart = k * axisPadHwd * c;
        int32_t currDstStart = k * axisC1hwdc0;


        for (int32_t j = 0; j < c1; j++) {
            uint64_t currDstAddr = tmpDstTensorAddr + j * axisPadHwdc0 * sizeof(T);
            uint64_t currSrcAddr = srcTensorAddr + (currSrcStart + j * axisPadHwdc0) * sizeof(T);
            int remain = j == c1 - 1 ? c - j * c0 : c0;
            for (int32_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
                dstLocalList[i] = currDstAddr + i * c0 * sizeof(T);
            }
            for (int32_t i = 0; i < remain; i++) {
                srcLocalList[i] = currSrcAddr + i * axisPadHwd * sizeof(T);
            }
            for (int32_t i = remain; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
                srcLocalList[i] = tmpBufferAddr;
            }
            TransDataTo5HD<half>(dstLocalList, srcLocalList, transDataParams);
        }
        PipeBarrier<PIPE_V>();

        for (int32_t i = 0; i < c1; i++) {
            DataCopy(dst[currDstStart + i * axisHwc0], tmpDstTensor[i * axisPadHwdc0], copyParams);
        }
        PipeBarrier<PIPE_V>();
    }
}


template <typename T>
[aicore] inline void TransDataImpl6HdToNcdhw(const LocalTensor<T>& dst, const LocalTensor<T>& src, const LocalTensor<uint8_t>& tmpBuffer,
    const TransDataTmpParams& param)
{
    const int32_t n = param.n, c = param.c, d = param.d, h = param.h, w = param.w;
    constexpr int32_t c0 = 16;
    constexpr int32_t elePerBlk = ONE_BLK_SIZE / sizeof(T);
    const int32_t c1 = DivCeil(c, c0);
    const int32_t padHw = AlignUp(h * w, elePerBlk);
    constexpr int32_t reservedDummy = 512;
    auto tmpDstTensor = tmpBuffer[reservedDummy].template ReinterpretCast<T>();
    uint64_t dstLocalList[DEFAULT_TRANSDATA_5HD_LIST];
    uint64_t srcLocalList[DEFAULT_TRANSDATA_5HD_LIST];

    uint64_t dstTensorAddr = (uint64_t)dst.GetPhyAddr();
    uint64_t tmpDstTensorAddr = (uint64_t)tmpDstTensor.GetPhyAddr();
    uint64_t tmpBufferAddr = (uint64_t)tmpBuffer.GetPhyAddr();

    int32_t axisHwd = h * w * d;
    int32_t axisHwc0 = h * w * c0;
    int32_t axisC1hwc0 = axisHwc0 * c1;
    int32_t axisC1hwdc0 = axisC1hwc0 * d;
    int32_t axisPadHwd = padHw * d;
    int32_t axisPadHwc0 = padHw * c0;
    int32_t axisPadHwdc0 = padHw * c0 * d;
    TransDataTo5HDParams transDataParams;
    transDataParams.dstHighHalf = false;
    transDataParams.srcHighHalf = false;
    transDataParams.repeatTimes = padHw * d / elePerBlk;
    transDataParams.srcRepStride = transDataParams.repeatTimes == 1 ? 0 : c0;
    transDataParams.dstRepStride = transDataParams.repeatTimes == 1 ? 0 : 1;

    DataCopyParams copyParams;
    copyParams.blockCount = c1;
    copyParams.blockLen = h * w * c0 / elePerBlk;
    copyParams.srcStride = 0;
    copyParams.dstStride = (d * padHw - h * w) * c0 / elePerBlk;

    for (int32_t k = 0; k < n; k++) {

        int32_t currSrcStart = k * axisC1hwdc0;
        int32_t currDstStart = k * axisPadHwd * c;
        for (int32_t i = 0; i < d; i++) {
            DataCopy(tmpDstTensor[i * axisPadHwc0], src[currSrcStart + i * axisC1hwc0], copyParams);
        }
        PipeBarrier<PIPE_V>();


        for (int32_t j = 0; j < c1; j++) {
            int32_t remain = j == c1 - 1 ? c - j * c0 : c0;
            uint64_t currDstAddr = dstTensorAddr + (currDstStart + j * axisPadHwdc0) * sizeof(T);
            uint64_t currSrcAddr = tmpDstTensorAddr + j * axisPadHwdc0 * sizeof(T);
            for (int32_t i = 0; i < remain; i++) {
                dstLocalList[i] = currDstAddr + i * axisPadHwd * sizeof(T);
            }
            for (int32_t i = remain; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {

                dstLocalList[i] = tmpBufferAddr + i * ONE_BLK_SIZE;
            }
            for (int32_t i = 0; i < DEFAULT_TRANSDATA_5HD_LIST; i++) {
                srcLocalList[i] = currSrcAddr + i * c0 * sizeof(T);
            }
            TransDataTo5HD<half>(dstLocalList, srcLocalList, transDataParams);
        }
        PipeBarrier<PIPE_V>();
    }
}

template <typename T, typename U, typename S>
[aicore] inline void TransDataCheck(const TransDataParams<U, S>& params)
{
    static_assert(SupportType<T, half, bfloat16_t, uint16_t, int16_t>(),
        "Currents only supports half/bfloat16_t/uint16_t/int16_t types.");
    static_assert(is_layout_v<U>, "srcLayout must be a layout");
    static_assert(is_layout_v<S>, "dstLayout must be a layout");
    using SrcShapeTuple = Std::remove_cvref_t<decltype(params.srcLayout.GetShape())>;
    using DstShapeTuple = Std::remove_cvref_t<decltype(params.dstLayout.GetShape())>;
    static_assert(Std::is_tuple_v<SrcShapeTuple>, "srcLayout.GetShape() must be a shape.");
    static_assert(Std::is_tuple_v<DstShapeTuple>, "dstLayout.GetShape() must be a shape.");
}

template <const TransDataConfig& config, typename T, typename U, typename S>
[aicore] inline void TransDataImpl(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const TransDataParams<U, S>& params)
{
    TransDataCheck<T, U, S>(params);
    auto srcShape = params.srcLayout.GetShape();
    auto dstShape = params.dstLayout.GetShape();
    constexpr uint32_t srcShapeSize = static_cast<uint32_t>(Std::tuple_size<decltype(srcShape)>::value);
    constexpr uint32_t dstShapeSize = static_cast<uint32_t>(Std::tuple_size<decltype(dstShape)>::value);
                                                                                                           ;
    using srcType = decltype(srcShape);
    using dstType = decltype(dstShape);
    using ncdhwType = Std::conditional_t<config.srcFormat == DataFormat::NCDHW, srcType, dstType>;
    ncdhwType ncdhwShape;
    if constexpr (config.srcFormat == DataFormat::NCDHW) {
        ncdhwShape = params.srcLayout.GetShape();
    } else {
        ncdhwShape = params.dstLayout.GetShape();
    }
    int32_t n = Std::get<0>(ncdhwShape);
    int32_t c = Std::get<1>(ncdhwShape);
    int32_t d = Std::get<2>(ncdhwShape);
    int32_t h = Std::get<3>(ncdhwShape);
    int32_t w = Std::get<4>(ncdhwShape);
    int32_t n1 = (n + n0 - 1) / n0;
    int32_t c1 = (c + c0 - 1) / c0;
    int32_t hw1 = (h * w + hw0 - 1) / hw0;
    int32_t padHw = hw1 * hw0;
    TransDataTmpParams tmpParams = { n, c, d, h, w, n1, c1, padHw };
    if constexpr (config.srcFormat == DataFormat::NCDHW && config.dstFormat == DataFormat::FRACTAL_Z_3D) {
        static_assert(srcShapeSize == ncdhwDims, "srcLayout's shape dims must be equal to 5!");
        static_assert(dstShapeSize == fractalZ3DDims, "dstLayout's shape dims must be equal to 7!");
        TransDataImplNcdhwToFractal(dstTensor, srcTensor, sharedTmpBuffer, tmpParams);
    } else if constexpr (config.srcFormat == DataFormat::FRACTAL_Z_3D && config.dstFormat == DataFormat::NCDHW) {
        static_assert(srcShapeSize == fractalZ3DDims, "srcLayout's shape dims must be equal to 7!");
        static_assert(dstShapeSize == ncdhwDims, "dstLayout's shape dims must be equal to 5!");
        TransDataFractalToNcdhw<T>(dstTensor, srcTensor, sharedTmpBuffer, tmpParams);
    } else if constexpr (config.srcFormat == DataFormat::NCDHW && config.dstFormat == DataFormat::NDC1HWC0) {
        static_assert(srcShapeSize == ncdhwDims, "srcLayout's shape dims must be equal to 5!");
        static_assert(dstShapeSize == ndc1hwc0Dims, "dstLayout's shape dims must be equal to 6!");
        TransDataImplNcdhwTo6Hd(dstTensor, srcTensor, sharedTmpBuffer, tmpParams);
    } else if constexpr (config.srcFormat == DataFormat::NDC1HWC0 && config.dstFormat == DataFormat::NCDHW) {
        static_assert(srcShapeSize == ndc1hwc0Dims, "srcLayout's shape dims must be equal to 6!");
        static_assert(dstShapeSize == ncdhwDims, "dstLayout's shape dims must be equal to 5!");
        TransDataImpl6HdToNcdhw(dstTensor, srcTensor, sharedTmpBuffer, tmpParams);
    }
}

}
}
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/transpose/transdata.h" 2





namespace AscendC {

template <const TransDataConfig& config, typename T, typename U, typename S>
[aicore] inline void TransData(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const TransDataParams<U, S>& params)
{
    Internal::TransDataImpl<config, T, U, S>(dstTensor, srcTensor, sharedTmpBuffer, params);
}

template <const TransDataConfig& config, typename T, typename U, typename S>
[aicore] inline void TransData(const LocalTensor<T>& dstTensor, const LocalTensor<T>& srcTensor,
    const TransDataParams<U, S>& params)
{

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> tmp;
    const bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(tmp);
                                                                                 ;

    TransData<config, T, U, S>(dstTensor, srcTensor, tmp, params);
}
}
# 99 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cumsum/cumsum_common_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cumsum/cumsum_common_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cumsum/cumsum_common_impl.h" 2



# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/math/cumsum_utils.h" 1
# 18 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/math/cumsum_utils.h"
namespace AscendC {
enum class CumSumAlgorithm {
    CUMSUM_ALGORITHM_LINEBYLINE = 0,
    CUMSUM_ALGORITHM_SKLANSKY = 1
};

struct CumSumConfig {
    bool isLastAxis{true};
    bool isReuseSource{false};
    bool outputLastRow{false};
    CumSumAlgorithm algorithm{CumSumAlgorithm::CUMSUM_ALGORITHM_LINEBYLINE};
};

struct CumSumInfo {
    uint32_t outter{0};
    uint32_t inner{0};
};

};
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/cumsum/cumsum_common_impl.h" 2

namespace AscendC {

[aicore] inline TransDataTo5HDParams ExtractTransDataParam(uint8_t repeatTimes, uint32_t inner, uint16_t alignOutter,
    uint32_t oneBlockElementNum, uint16_t dstRepStride, uint32_t srcRepStride)
{
    repeatTimes = inner / oneBlockElementNum;
    if (repeatTimes > 1) {


        return TransDataTo5HDParams(false, false, repeatTimes, alignOutter, 1);
    } else {
        return TransDataTo5HDParams(false, false, repeatTimes, dstRepStride, srcRepStride);
    }
}

template <typename T>
[aicore] inline void CumSumLastDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<T> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(T);
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;


    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = 1;
            srcRepStride = cumSumInfo.inner;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {
        TransDataTo5HDParams params = ExtractTransDataParam(repeatTimes, cumSumInfo.inner, alignOutter,
            oneBlockElementNum, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] = (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                    n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) * cumSumInfo.inner)].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    LocalTensor<float> floatTempBuffer = tempBuffer[alignOutter * cumSumInfo.inner].template ReinterpretCast<float>();
    Cast<float, T, false>(floatTempBuffer, tempBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
        1, {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();

    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;
    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(floatTempBuffer[row * alignOutter], floatTempBuffer[(row - 1) * alignOutter],
            floatTempBuffer[row * alignOutter], MASK_PLACEHOLDER, 1, binaryParams);
        PipeBarrier<PIPE_V>();
    }

    SetVectorMask<T, MaskMode::COUNTER>(alignOutter * cumSumInfo.inner);
    Cast<T, float, false>(tempBuffer, floatTempBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
        {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / oneBlockElementNum;
        if (repeatTimes > 1) {
            dstRepStride = cumSumInfo.inner;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[i * NCHW_CONV_ADDR_LIST_SIZE + n * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {
            srcRepStride = 1;
            dstRepStride = alignOutter;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + alignOutter * n].GetPhyAddr();
                transDataTo5HDDstLocalList[n] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
            }
            TransDataTo5HD<T>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<T>(0, cumSumInfo.outter * cumSumInfo.inner);
    Adds<T, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <>
[aicore] inline void CumSumLastDim(const LocalTensor<float> &dstTensor, const LocalTensor<float> &srcTensor,
    LocalTensor<float> tempBuffer, const CumSumInfo &cumSumInfo)
{
    constexpr uint32_t oneBlockElementNum = ONE_BLK_SIZE / sizeof(float);
    uint8_t repeatTimes = 1;
    uint16_t dstRepStride = 0;
    uint16_t srcRepStride = 0;
    uint16_t alignOutter =
        (cumSumInfo.outter + NCHW_CONV_ADDR_LIST_SIZE - 1) / NCHW_CONV_ADDR_LIST_SIZE * NCHW_CONV_ADDR_LIST_SIZE;
    uint64_t transDataTo5HDDstLocalList[NCHW_CONV_ADDR_LIST_SIZE];
    uint64_t transDataTo5HDSrcLocalList[NCHW_CONV_ADDR_LIST_SIZE];


    if (cumSumInfo.outter == alignOutter && alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = 2;
            srcRepStride = cumSumInfo.inner * 2;
        }
        TransDataTo5HDParams params(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)srcTensor[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[(i * oneBlockElementNum + n) * alignOutter + oneBlockElementNum].GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    } else {
        TransDataTo5HDParams params = ExtractTransDataParam(repeatTimes, cumSumInfo.inner, alignOutter,
            oneBlockElementNum, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE; n++) {
                transDataTo5HDSrcLocalList[n] = (uint64_t)srcTensor[((i * NCHW_CONV_ADDR_LIST_SIZE +
                    n % (cumSumInfo.outter - i * NCHW_CONV_ADDR_LIST_SIZE)) * cumSumInfo.inner)].GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, params);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, alignOutter);
    const BinaryRepeatParams binaryParams;
    uint32_t addOffset = alignOutter;
    for (uint32_t row = 1; row < cumSumInfo.inner; ++row) {
        Add<float, false>(tempBuffer[addOffset],
            tempBuffer[addOffset - alignOutter],
            tempBuffer[addOffset],
            MASK_PLACEHOLDER,
            1,
            binaryParams);
        addOffset += alignOutter;
        PipeBarrier<PIPE_V>();
    }
    SetMaskNorm();
    ResetMask();

    auto tempBuffer2 = tempBuffer[alignOutter * cumSumInfo.inner];
    if (alignOutter > cumSumInfo.inner) {
        repeatTimes = alignOutter / NCHW_CONV_ADDR_LIST_SIZE;
        if (repeatTimes > 1) {


            dstRepStride = cumSumInfo.inner * 2;
            srcRepStride = 2;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, dstRepStride, srcRepStride);
        for (int32_t i = 0; i < cumSumInfo.inner / oneBlockElementNum; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * oneBlockElementNum * alignOutter + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + n * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[i * oneBlockElementNum + (n + oneBlockElementNum) * cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }

    } else {
        repeatTimes = cumSumInfo.inner / oneBlockElementNum;
        if (repeatTimes > 1) {


            dstRepStride = alignOutter;
            srcRepStride = 1;
        } else {
            dstRepStride = 0;
            srcRepStride = 0;
        }
        TransDataTo5HDParams paramsBack(false, false, repeatTimes, srcRepStride, dstRepStride);
        for (int32_t i = 0; i < alignOutter / NCHW_CONV_ADDR_LIST_SIZE; i++) {
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDSrcLocalList[n] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter].GetPhyAddr();
                transDataTo5HDSrcLocalList[n + NCHW_CONV_ADDR_LIST_SIZE / 2] =
                    (uint64_t)tempBuffer[i * NCHW_CONV_ADDR_LIST_SIZE + n * alignOutter + oneBlockElementNum]
                        .GetPhyAddr();
            }
            for (int32_t n = 0; n < NCHW_CONV_ADDR_LIST_SIZE / 2; n++) {
                transDataTo5HDDstLocalList[n * 2] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + n) * cumSumInfo.inner].GetPhyAddr();
                transDataTo5HDDstLocalList[n * 2 + 1] =
                    (uint64_t)tempBuffer2[(i * NCHW_CONV_ADDR_LIST_SIZE + (n + NCHW_CONV_ADDR_LIST_SIZE / 2)) *
                                          cumSumInfo.inner]
                        .GetPhyAddr();
            }
            TransDataTo5HD<float>(transDataTo5HDDstLocalList, transDataTo5HDSrcLocalList, paramsBack);
        }
    }
    PipeBarrier<PIPE_V>();
    SetMaskCount();
    SetVectorMask<float>(0, cumSumInfo.outter * cumSumInfo.inner);
    Adds<float, false>(
        dstTensor, tempBuffer2, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
    PipeBarrier<PIPE_V>();
    SetMaskNorm();
    ResetMask();
}

template <typename T>
[aicore] inline void CumSumFirstDim(const LocalTensor<T> &dstTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr (sizeof(T) == sizeof(half)) {
        const uint32_t minTmpBufferSize = cumSumInfo.outter * cumSumInfo.inner * sizeof(float);
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();






        SetMaskCount();
        SetVectorMask<float, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        LocalTensor<float> tmpBuffer = sharedTmpBuffer.ReinterpretCast<float>();
        Cast<float, T, false>(tmpBuffer, srcTensor, RoundMode::CAST_NONE, MASK_PLACEHOLDER, 1,
            {1, 1, DEFAULT_REPEAT_STRIDE, HALF_DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

        SetVectorMask<T>(0, cumSumInfo.inner);
        const BinaryRepeatParams binaryParams;
        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<float, false>(tmpBuffer[row * cumSumInfo.inner], tmpBuffer[(row - 1) * cumSumInfo.inner],
                tmpBuffer[row * cumSumInfo.inner], MASK_PLACEHOLDER, 1, binaryParams);
            PipeBarrier<PIPE_V>();
        }

        SetVectorMask<T, MaskMode::COUNTER>(cumSumInfo.outter * cumSumInfo.inner);
        Cast<T, float, false>(dstTensor, tmpBuffer, RoundMode::CAST_NONE, MASK_PLACEHOLDER,
            1, {1, 1, HALF_DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();

    } else {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(
            dstTensor, srcTensor, 0, MASK_PLACEHOLDER, 1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        const BinaryRepeatParams binaryParams;
        for (uint32_t row = 1; row < cumSumInfo.outter; ++row) {
            Add<T, false>(dstTensor[row * cumSumInfo.inner],
                dstTensor[(row - 1) * cumSumInfo.inner],
                srcTensor[row * cumSumInfo.inner],
                MASK_PLACEHOLDER,
                1,
                binaryParams);
            PipeBarrier<PIPE_V>();
        }
        SetMaskNorm();
        ResetMask();
    }
}

template <typename T, const CumSumConfig &config>
[aicore] inline void CumSumImpl(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor,
    const LocalTensor<T> &srcTensor, LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                     ;

    if constexpr (config.isLastAxis) {
        uint32_t minCastTempBufferSize = 0;
        if constexpr (sizeof(T) == sizeof(half)) {
            minCastTempBufferSize = cumSumInfo.inner * NCHW_CONV_ADDR_LIST_SIZE * sizeof(half);
        }

        const uint32_t minTmpBufferSize = minCastTempBufferSize +
                                          NCHW_CONV_ADDR_LIST_SIZE * cumSumInfo.inner * sizeof(T) * 2;
        const uint32_t tmpBufferSize = sharedTmpBuffer.GetSize();







        const uint32_t oneRepeateSize = tmpBufferSize / minTmpBufferSize * NCHW_CONV_ADDR_LIST_SIZE;
        const uint32_t rangeM = cumSumInfo.outter / oneRepeateSize;
        const uint32_t tailM = cumSumInfo.outter - oneRepeateSize * rangeM;
        uint32_t dstLocalOffset = 0;
        uint32_t srcLocalOffset = 0;
        LocalTensor<T> tmpBuffer = sharedTmpBuffer.ReinterpretCast<T>();
        for (uint32_t i = 0; i < rangeM; i++) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {oneRepeateSize, cumSumInfo.inner});
            dstLocalOffset += cumSumInfo.inner * oneRepeateSize;
            srcLocalOffset += cumSumInfo.inner * oneRepeateSize;
        }

        if (tailM != 0) {
            CumSumLastDim<T>(
                dstTensor[dstLocalOffset], srcTensor[srcLocalOffset], tmpBuffer, {tailM, cumSumInfo.inner});
        }
    } else {
        CumSumFirstDim<T>(dstTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
    }

    if constexpr (config.outputLastRow) {
        SetMaskCount();
        SetVectorMask<T>(0, cumSumInfo.inner);
        Adds<T, false>(lastRowTensor, dstTensor[(cumSumInfo.outter - 1) * cumSumInfo.inner], 0, MASK_PLACEHOLDER,
            1, {1, 1, DEFAULT_REPEAT_STRIDE, DEFAULT_REPEAT_STRIDE});
        PipeBarrier<PIPE_V>();
        SetMaskNorm();
        ResetMask();
    }
}
}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h" 2






namespace AscendC {
#pragma begin_pipe(V)

constexpr CumSumConfig defaultCumSumConfig = {true, false, true};
# 46 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] inline void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    LocalTensor<uint8_t> &sharedTmpBuffer, const CumSumInfo &cumSumInfo)
{
    CumSumImpl<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
}
# 65 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/cumsum.h"
template <typename T, const CumSumConfig &config = defaultCumSumConfig>
[aicore] inline void CumSum(LocalTensor<T> &dstTensor, LocalTensor<T> &lastRowTensor, const LocalTensor<T> &srcTensor,
    const CumSumInfo &cumSumInfo)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    CumSum<T, config>(dstTensor, lastRowTensor, srcTensor, sharedTmpBuffer, cumSumInfo);
}

#pragma end_pipe
}
# 100 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/fmod/fmod_common_impl.h" 1
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/../../../impl/adv_api/detail/math/fmod/fmod_common_impl.h"
namespace AscendC {
namespace {
constexpr uint32_t SRC0_IDX = 1;
constexpr uint32_t SRC1_IDX = 2;
constexpr uint32_t TRUNC_IDX = 3;
}

[aicore] inline void FmodCompute(const LocalTensor<float> &dstTensor, const LocalTensor<float> &src0Tensor,
    const LocalTensor<float> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t stackSize,
    const uint32_t calCount)
{
    PipeBarrier<PIPE_V>();

    Div(dstTensor, src0Tensor, src1Tensor, calCount);
    PipeBarrier<PIPE_V>();

    Trunc(dstTensor, dstTensor, sharedTmpBuffer, calCount);
    PipeBarrier<PIPE_V>();

    Mul(dstTensor, dstTensor, src1Tensor, calCount);
    PipeBarrier<PIPE_V>();

    Sub(dstTensor, src0Tensor, dstTensor, calCount);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void FmodCompute(const LocalTensor<half> &dstTensor, const LocalTensor<half> &src0Tensor,
    const LocalTensor<half> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t stackSize,
    const uint32_t calCount)
{


    LocalTensor<float> floatTmpTensor = sharedTmpBuffer.ReinterpretCast<float>();
    LocalTensor<float> tmpSrc0 = floatTmpTensor[SRC0_IDX * stackSize];
    LocalTensor<float> tmpSrc1 = floatTmpTensor[SRC1_IDX * stackSize];

    PipeBarrier<PIPE_V>();

    Cast<float, half>(tmpSrc0, src0Tensor, RoundMode::CAST_NONE, calCount);

    Cast<float, half>(tmpSrc1, src1Tensor, RoundMode::CAST_NONE, calCount);
    PipeBarrier<PIPE_V>();

    FmodCompute(floatTmpTensor, tmpSrc0, tmpSrc1, sharedTmpBuffer[TRUNC_IDX * stackSize * sizeof(float)], stackSize, calCount);

    Cast<half, float>(dstTensor, floatTmpTensor, RoundMode::CAST_NONE, calCount);
    PipeBarrier<PIPE_V>();
}

template <typename T, bool isReuseSource = false>
[aicore] inline void FmodImpl(const LocalTensor<T> &dstTensor, const LocalTensor<T> &src0Tensor,
    const LocalTensor<T> &src1Tensor, const LocalTensor<uint8_t> &sharedTmpBuffer, const uint32_t calCount)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                                                      ;

    if constexpr (sizeof(T) == sizeof(float)) {
        FmodCompute(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize(), calCount);
        return;
    }

    constexpr uint32_t maxLiveNodeCount = 8;
    uint32_t bufferSize = sharedTmpBuffer.GetSize();
    uint32_t stackSize =
        bufferSize / sizeof(T) / maxLiveNodeCount / ONE_BLK_SIZE * ONE_BLK_SIZE;
    CheckTmpBufferSize(stackSize, 0, bufferSize);
    stackSize = stackSize > src0Tensor.GetSize() ? src0Tensor.GetSize() : stackSize;

    const uint32_t round = calCount / stackSize;
    const uint32_t tail = calCount % stackSize;

    for (uint32_t i = 0; i < round; ++i) {
        FmodCompute(dstTensor[i * stackSize], src0Tensor[i * stackSize], src1Tensor[i * stackSize], sharedTmpBuffer,
            stackSize, stackSize);
    }
    if (tail > 0) {
        FmodCompute(dstTensor[round * stackSize], src0Tensor[round * stackSize], src1Tensor[round * stackSize],
            sharedTmpBuffer, stackSize, tail);
    }
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 41 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer, const uint32_t calCount)
{
    FmodImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 58 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const uint32_t calCount)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FmodImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, calCount);
}
# 82 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor, const LocalTensor<uint8_t>& sharedTmpBuffer)
{
    FmodImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
# 98 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/math/fmod.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void Fmod(const LocalTensor<T>& dstTensor, const LocalTensor<T>& src0Tensor,
    const LocalTensor<T>& src1Tensor)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ret = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;
    FmodImpl<T, isReuseSource>(dstTensor, src0Tensor, src1Tensor, sharedTmpBuffer, src0Tensor.GetSize());
}
#pragma end_pipe
}
# 101 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/groupnorm.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/groupnorm.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/groupnorm/groupnorm_common_impl.h" 1
# 24 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/../../../impl/adv_api/detail/normalization/groupnorm/groupnorm_common_impl.h"
namespace AscendC {
    namespace {
        constexpr uint32_t GROUPNORM_MASK_MAX_VAL = 64;
        constexpr uint32_t GROUPNORM_MASK_SMALLEST_VAL = 8;
        constexpr uint32_t GROUPNORM_MASK_STEP_VAL = 8;
        constexpr uint32_t GROUPNORM_ONE_BLK_SIZE = 8;
    }

template <typename T> struct GroupNormParams
{
    [aicore] GroupNormParams(){};
    LocalTensor<T> tempTensorA;
    LocalTensor<T> tempTensorB;
    LocalTensor<T> tempTensorC;
    LocalTensor<T> meanTmpTensor;
    LocalTensor<T> varianceTmpTensor;
};

[aicore] inline uint32_t GetGroupNormWholeReduceMask1(const GroupNormTiling& tiling)
{
    uint32_t mask1{0};
    if (tiling.dhwAlignSize > GROUPNORM_MASK_MAX_VAL) {
        mask1 = GROUPNORM_MASK_MAX_VAL;
        while (mask1 != 0 && tiling.dhwAlignSize % mask1 != 0) {
            mask1 -= GROUPNORM_MASK_STEP_VAL;
        }
        return mask1;
    }
    return tiling.dhwAlignSize;
}

[aicore] inline void GetGroupNormOutputMean(const LocalTensor<float>& x_in,
    const LocalTensor<float>& tmp, const LocalTensor<float>& mean,
    const GroupNormTiling& tiling, const int32_t loopCount, const int32_t mvOffset)
{
    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        ReduceSum<float>(mean[i + loopCount * tiling.bsCurLength + mvOffset],
            x_in[buffIndex], tmp[buffIndex], tiling.dhwAlignSize);
        PipeBarrier<PIPE_V>();
    }

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskOffset = static_cast<uint64_t>(1) << (i + loopCount * tiling.bsCurLength + mvOffset);
        uint64_t mask[1] = {maskOffset};
        Muls(mean, mean, tiling.factor, mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
    }


    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);
}

[aicore] inline void GetGroupNormOutputVar(const LocalTensor<float>& x_in,
    const LocalTensor<float>& tmp1, const LocalTensor<float>& tmp2,
    const LocalTensor<float>& mean, const LocalTensor<float>& var, const GroupNormTiling& tiling,
    const int32_t loopCount, const int32_t mvOffset)
{
    for (uint32_t i = 0; i < tiling.d * tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.hwAlignSize;
        Adds(tmp1[buffIndex], x_in[buffIndex],
            -1.0f * mean.GetValue((i + loopCount * tiling.d * tiling.bsCurLength + mvOffset * tiling.d) / tiling.d), tiling.hw);
        PipeBarrier<PIPE_V>();
    }

    Mul(tmp2, tmp1, tmp1, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        ReduceSum<float>(var[i + loopCount * tiling.bsCurLength + mvOffset],
            tmp2[buffIndex], tmp2[buffIndex], tiling.dhwAlignSize);
        PipeBarrier<PIPE_V>();
    }

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskOffset = static_cast<uint64_t>(1) << (i + loopCount * tiling.bsCurLength + mvOffset);
        uint64_t mask[1] = {maskOffset};
        Muls(var, var, tiling.factor, mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void GetGroupNormOutputPre(const LocalTensor<float>& inout,
    const LocalTensor<float>& tmp, const LocalTensor<float>& tempOnes, const LocalTensor<float>& variance,
    const GroupNormTiling& tiling, const float epsilon, const int32_t loopCount, const int32_t mvOffset)
{
    for (uint32_t i = 0; i < tiling.bsCurLength; i++) {
        uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
        uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
        uint32_t loopIndex = maskMod * GROUPNORM_MASK_MAX_VAL;
        maskMvOffset = maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL;
        uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset);
        uint64_t mask[1] = {maskOffset};

        Adds<float, true>(tmp, variance[loopIndex], epsilon, mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();

        Duplicate<float, true>(tempOnes, 1.0f, mask, 1, 1, 1);
        PipeBarrier<PIPE_V>();

        Sqrt<float, true>(tmp, tmp, mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();

        Div<float, true>(tmp, tempOnes, tmp, mask, 1,
            {1, 1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();
    }

    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);


    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Muls<float, true>(inout[buffIndex], inout[buffIndex],
            tmp.GetValue((i + loopCount * tiling.bsCurLength + mvOffset) % GROUPNORM_MASK_MAX_VAL), tiling.dhwAlignSize);
        PipeBarrier<PIPE_V>();
    }


    auto eventIdSToV = GetTPipePtr()->FetchEventID(HardEvent::S_V);
    SetFlag<HardEvent::V_S>(eventIdSToV);
    WaitFlag<HardEvent::V_S>(eventIdSToV);

    PipeBarrier<PIPE_V>();
}

[aicore] inline void GetGroupNormOutput(const LocalTensor<float>& inout,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const GroupNormTiling& tiling, const int32_t loopCount, const int32_t mvOffset)
{
    size_t channelIndex = loopCount * tiling.bsCurLength * tiling.d + mvOffset * tiling.d;
    for (uint32_t channel_offset = 0; channel_offset < tiling.bsCurLength * tiling.d; ++channel_offset) {
        Muls(inout[channel_offset * tiling.hwAlignSize], inout[channel_offset * tiling.hwAlignSize],
        gamma.GetValue(channelIndex % tiling.c), tiling.hw);
        channelIndex += 1;
        PipeBarrier<PIPE_V>();
    }

    channelIndex = loopCount * tiling.bsCurLength * tiling.d + mvOffset * tiling.d;
    for (uint32_t channel_offset = 0; channel_offset < tiling.bsCurLength * tiling.d; ++channel_offset) {
        Adds(inout[channel_offset * tiling.hwAlignSize], inout[channel_offset * tiling.hwAlignSize],
        beta.GetValue(channelIndex % tiling.c), tiling.hw);
        channelIndex += 1;
        PipeBarrier<PIPE_V>();
    }
}

[aicore] inline void GroupNormExe(const LocalTensor<half>& inputX,
    const LocalTensor<half>& gamma, const LocalTensor<half>& beta,
    const LocalTensor<half>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const half epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount,
    const int32_t mvOffset)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;
    Duplicate(tempTensorA, 0.0f, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();
    Cast<float, half>(tempTensorB, inputX, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutputMean(tempTensorB, tempTensorC, outputMean, tiling, loopCount, mvOffset);

    GetGroupNormOutputVar(tempTensorB, tempTensorB, tempTensorC, outputMean, outputVariance, tiling, loopCount, mvOffset);

    GetGroupNormOutputPre(tempTensorB, tempTensorA, tempTensorC, outputVariance, tiling,
                            static_cast<float>(epsilon), loopCount, mvOffset);

    Cast<float, half>(tempTensorA, gamma, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();
    Cast<float, half>(tempTensorC, beta, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutput(tempTensorB, tempTensorA, tempTensorC, tiling, loopCount, mvOffset);

    Cast<half, float>(output, tempTensorB, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
}


[aicore] inline void GroupNormExe(const LocalTensor<float>& inputX,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const LocalTensor<float>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const float epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount,
    const int32_t mvOffset)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    GetGroupNormOutputMean(inputX, output, outputMean, tiling, loopCount, mvOffset);

    Duplicate(output, 0.0f, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutputVar(inputX, output, tempTensorC, outputMean, outputVariance, tiling, loopCount, mvOffset);

    GetGroupNormOutputPre(output, tempTensorA, tempTensorB, outputVariance, tiling, epsilon, loopCount, mvOffset);

    GetGroupNormOutput(output, gamma, beta, tiling, loopCount, mvOffset);
}

[aicore] inline void GroupNormExeSmallShape(const LocalTensor<half>& inputX,
    const LocalTensor<half>& gamma, const LocalTensor<half>& beta,
    const LocalTensor<half>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const half epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount,
    const int32_t mvOffset)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;
    Duplicate(tempTensorA, 0.0f, tiling.inputRoundSize * tiling.numberOfTmpBuf);
    PipeBarrier<PIPE_V>();

    Cast<float, half>(tempTensorB, inputX, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    uint32_t mask1 = GetGroupNormWholeReduceMask1(tiling);
                                                                                 ;

    uint32_t repeat1 = tiling.dhwAlignSize / mask1 * tiling.meanVarRoundSize;
    uint32_t mask2 = tiling.dhwAlignSize / mask1 * GROUPNORM_MASK_SMALLEST_VAL;
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorC, tempTensorB, mask1, repeat1,
                                GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputMean[loopCount * tiling.bsCurLength + mvOffset], tempTensorC, mask2, tiling.bsCurLength,
                                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
        uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
        uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL);
        uint64_t mask[1] = {maskOffset};
        Muls(outputMean[GROUPNORM_MASK_MAX_VAL * maskMod], outputMean[GROUPNORM_MASK_MAX_VAL * maskMod], tiling.factor,
            mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();
    }

    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Adds(tempTensorB[buffIndex], tempTensorB[buffIndex], -1.0f * outputMean.GetValue(i + loopCount * tiling.bsCurLength + mvOffset),
            tiling.hw, tiling.d, {1, 1, static_cast<uint8_t>(tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE),
            static_cast<uint8_t>(tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE)});
        PipeBarrier<PIPE_V>();
    }

    Mul(tempTensorC, tempTensorB, tempTensorB, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorA, tempTensorC, mask1, repeat1,
                                GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputVariance[loopCount * tiling.bsCurLength + mvOffset], tempTensorA, mask2, tiling.bsCurLength,
                                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
        uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
        uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL);
        uint64_t mask[1] = {maskOffset};

        Muls(outputVariance[GROUPNORM_MASK_MAX_VAL * maskMod], outputVariance[GROUPNORM_MASK_MAX_VAL * maskMod], tiling.factor,
            mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();
    }

    GetGroupNormOutputPre(tempTensorB, tempTensorA, tempTensorC, outputVariance, tiling, static_cast<float>(epsilon), loopCount, mvOffset);

    Cast<float, half>(tempTensorA, gamma, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();
    Cast<float, half>(tempTensorC, beta, RoundMode::CAST_NONE, tiling.c);
    PipeBarrier<PIPE_V>();

    GetGroupNormOutput(tempTensorB, tempTensorA, tempTensorC, tiling, loopCount, mvOffset);

    Cast<half, float>(output, tempTensorB, RoundMode::CAST_NONE, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
}

[aicore] inline void GroupNormExeSmallShape(const LocalTensor<float>& inputX,
    const LocalTensor<float>& gamma, const LocalTensor<float>& beta,
    const LocalTensor<float>& output, const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const float epsilon, const GroupNormTiling& tiling, const GroupNormParams<float>& params, const int32_t loopCount,
    const int32_t mvOffset)
{
    LocalTensor<float> tempTensorA = params.tempTensorA;
    LocalTensor<float> tempTensorB = params.tempTensorB;
    LocalTensor<float> tempTensorC = params.tempTensorC;

    if (mvOffset) {
        for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
            uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
            uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
            uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL);
            uint64_t mask[1] = {maskOffset};

            Duplicate(output[GROUPNORM_MASK_MAX_VAL * maskMod], 0.0f, mask, 1, 1, GROUPNORM_MASK_SMALLEST_VAL);
            PipeBarrier<PIPE_V>();
        }
    } else {
        Duplicate(output, 0.0f, tiling.inputRoundSize);
        PipeBarrier<PIPE_V>();
    }

    Duplicate(tempTensorC, 0.0f, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();
    uint32_t mask1 = GetGroupNormWholeReduceMask1(tiling);
                                                                                 ;

    uint32_t repeat1 = tiling.dhwAlignSize / mask1 * tiling.meanVarRoundSize;
    uint32_t mask2 = tiling.dhwAlignSize / mask1 * GROUPNORM_MASK_SMALLEST_VAL;
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorC, inputX, mask1, repeat1,
                                GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputMean[loopCount * tiling.bsCurLength + mvOffset], tempTensorC, mask2, tiling.bsCurLength,
                                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
        uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
        uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL);
        uint64_t mask[1] = {maskOffset};
        Muls(outputMean[GROUPNORM_MASK_MAX_VAL * maskMod], outputMean[GROUPNORM_MASK_MAX_VAL * maskMod], tiling.factor,
            mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
    }

    auto eventIdVToS = GetTPipePtr()->FetchEventID(HardEvent::V_S);
    SetFlag<HardEvent::V_S>(eventIdVToS);
    WaitFlag<HardEvent::V_S>(eventIdVToS);

    auto repeatStride = tiling.hwAlignSize / GROUPNORM_ONE_BLK_SIZE;
    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint32_t buffIndex = i * tiling.dhwAlignSize;
        Adds(output[buffIndex], inputX[buffIndex], -1.0f * outputMean.GetValue(i + loopCount * tiling.bsCurLength + mvOffset), tiling.hw, tiling.d,
        {1, 1, static_cast<uint8_t>(repeatStride), static_cast<uint8_t>(repeatStride)});

        PipeBarrier<PIPE_V>();
    }

    Mul(tempTensorC, output, output, tiling.bshCurLength);
    PipeBarrier<PIPE_V>();

    Duplicate(tempTensorA, 0.0f, tiling.inputRoundSize);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(tempTensorA, tempTensorC, mask1, repeat1,
                                GROUPNORM_MASK_SMALLEST_VAL, DEFAULT_BLK_STRIDE, mask1 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    WholeReduceSum<float, true>(outputVariance[loopCount * tiling.bsCurLength + mvOffset], tempTensorA, mask2, tiling.bsCurLength,
                                DEFAULT_BLK_STRIDE, DEFAULT_BLK_STRIDE, mask2 / GROUPNORM_MASK_SMALLEST_VAL);
    PipeBarrier<PIPE_V>();

    for (uint32_t i = 0; i < tiling.bsCurLength; ++i) {
        uint64_t maskMvOffset = i + loopCount * tiling.bsCurLength + mvOffset;
        uint32_t maskMod = maskMvOffset / GROUPNORM_MASK_MAX_VAL;
        uint64_t maskOffset = static_cast<uint64_t>(1) << (maskMvOffset - maskMod * GROUPNORM_MASK_MAX_VAL);
        uint64_t mask[1] = {maskOffset};
        Muls(outputVariance[GROUPNORM_MASK_MAX_VAL * maskMod], outputVariance[GROUPNORM_MASK_MAX_VAL * maskMod], tiling.factor,
            mask, 1, {1, 1, GROUPNORM_MASK_SMALLEST_VAL, GROUPNORM_MASK_SMALLEST_VAL});
        PipeBarrier<PIPE_V>();
    }

    GetGroupNormOutputPre(output, tempTensorA, tempTensorB, outputVariance, tiling, epsilon, loopCount, mvOffset);

    GetGroupNormOutput(output, gamma, beta, tiling, loopCount, mvOffset);
}

template <bool isReuseSource = false>
[aicore] inline void GetGroupNormNDTensorInfo(const LocalTensor<half>& inputX,
    const LocalTensor<half>& outputMean, const LocalTensor<half>& outputVariance,
    const LocalTensor<float>& stackBuffer, const GroupNormTiling& tiling, GroupNormParams<float>& params)
{
    params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
    params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
    params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];
    params.meanTmpTensor = stackBuffer[tiling.meanTmpTensorPos];
    params.varianceTmpTensor = stackBuffer[tiling.varianceTmpTensorPos];




      ;



      ;
}

template <bool isReuseSource = false>
[aicore] inline void GetGroupNormNDTensorInfo(const LocalTensor<float>& inputX,
    const LocalTensor<float>& outputMean, const LocalTensor<float>& outputVariance,
    const LocalTensor<float>& stackBuffer, const GroupNormTiling& tiling, GroupNormParams<float>& params)
{
    params.meanTmpTensor = outputMean;
    params.varianceTmpTensor = outputVariance;

    if constexpr (isReuseSource) {
        params.tempTensorA = inputX;
        params.tempTensorB = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.secondTmpStartPos];




          ;
    } else {
        params.tempTensorA = stackBuffer[tiling.firstTmpStartPos];
        params.tempTensorB = stackBuffer[tiling.secondTmpStartPos];
        params.tempTensorC = stackBuffer[tiling.thirdTmpStartPos];




          ;
    }




      ;
}

[aicore] inline void GetOutputMeanVariance(const LocalTensor<half>& outputMean,
    const LocalTensor<half>& outputVariance, const GroupNormTiling& tiling, const GroupNormParams<float>& params)
{
    Cast<half, float>(outputMean, params.meanTmpTensor, RoundMode::CAST_NONE, tiling.n * tiling.g);
    Cast<half, float>(outputVariance, params.varianceTmpTensor, RoundMode::CAST_NONE, tiling.n * tiling.g);
}

template <typename T>
[aicore] inline void GroupNormNDCommon(const LocalTensor<T>& inputX,
    const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const LocalTensor<T>& output, const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const T epsilon, GroupNormTiling& tiling, const GroupNormParams<float>& params)
{
    uint32_t inputOffset = 0;
    uint32_t mvOffset = 0;

    if (tiling.smallShape) {
        for (uint32_t index = 0; index < tiling.loopRound; index++) {
            GroupNormExeSmallShape(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor,
            params.varianceTmpTensor, epsilon, tiling, params, index, mvOffset);

            inputOffset += tiling.inputRoundSize;
        }
    } else {
        for (uint32_t index = 0; index < tiling.loopRound; index++) {
            GroupNormExe(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor,
            params.varianceTmpTensor, epsilon, tiling, params, index, mvOffset);

            inputOffset += tiling.inputRoundSize;
        }
    }

    if (tiling.inputTailSize > 0) {
        tiling.bshCurLength = tiling.inputTailSize;
        tiling.bsCurLength = tiling.meanVarTailSize;

        inputOffset = tiling.inputTailPos;
        mvOffset = tiling.meanVarTailPos;

        if (tiling.smallShape) {
            GroupNormExeSmallShape(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor,
            params.varianceTmpTensor, epsilon, tiling, params, 0, mvOffset);
        } else {
            GroupNormExe(inputX[inputOffset], gamma, beta, output[inputOffset],
            params.meanTmpTensor,
            params.varianceTmpTensor, epsilon, tiling, params, 0, mvOffset);
        }


        tiling.bshCurLength = tiling.inputRoundSize;
        tiling.bsCurLength = tiling.meanVarRoundSize;
    }

    if constexpr (sizeof(T) == sizeof(half)) {
        GetOutputMeanVariance(outputMean, outputVariance, tiling, params);
    }
}

template <typename T, bool isReuseSource = false>
[aicore] inline void GroupNormImpl(const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const LocalTensor<T>& inputX, const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, GroupNormTiling& tiling)
{

                         ;
                                                                                                         ;

    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
    LocalTensor<float> stackBuffer = sharedTmpBuffer.ReinterpretCast<float>();
                                                                                                               ;

    GroupNormParams<float> params;
    GetGroupNormNDTensorInfo<isReuseSource>(inputX, outputMean, outputVariance, stackBuffer, tiling, params);

    GroupNormNDCommon<T>(inputX, gamma, beta, output, outputMean, outputVariance, epsilon, tiling, params);
}

template <typename T, bool isReuseSource = false>
[aicore] inline void GroupNormImpl(const LocalTensor<T>& output,
    const LocalTensor<T>& outputMean, const LocalTensor<T>& outputVariance,
    const LocalTensor<T>& inputX, const LocalTensor<T>& gamma, const LocalTensor<T>& beta,
    const T epsilon, GroupNormTiling& tiling)
{
    LocalTensor<uint8_t> sharedTmpBuffer;
    bool ans = PopStackBuffer<uint8_t, TPosition::LCM>(sharedTmpBuffer);
                                                                                 ;

    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon, tiling);
}

}
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/groupnorm.h" 2


namespace AscendC {
#pragma begin_pipe(V)
# 40 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/groupnorm.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void GroupNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const LocalTensor<uint8_t>& sharedTmpBuffer, const T epsilon, GroupNormTiling& tiling)
{
    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, sharedTmpBuffer, epsilon,
        tiling);
}
# 63 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/normalization/groupnorm.h"
template <typename T, bool isReuseSource = false>
[aicore] inline void GroupNorm(const LocalTensor<T>& output, const LocalTensor<T>& outputMean,
    const LocalTensor<T>& outputVariance, const LocalTensor<T>& inputX, const LocalTensor<T>& gamma,
    const LocalTensor<T>& beta, const T epsilon, GroupNormTiling& tiling)
{
    GroupNormImpl<T, isReuseSource>(output, outputMean, outputVariance, inputX, gamma, beta, epsilon, tiling);
}
#pragma end_pipe
}
# 102 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/init_global_memory.h" 1
# 21 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/init_global_memory.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/../../../impl/adv_api/detail/utils/init_global_memory/init_global_memory_v220_impl.h" 1
# 19 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/../../../impl/adv_api/detail/utils/init_global_memory/init_global_memory_v220_impl.h"
# 1 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 1
# 20 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/../../../impl/adv_api/detail/utils/init_global_memory/init_global_memory_v220_impl.h" 2


namespace AscendC {
template <typename T>
[aicore] inline void InitGlobalMemoryImpl(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    if constexpr(g_coreType == AscendC::AIC) {
        return;
    }
                                                                                   ;
    LocalTensor<T> popBuffer;
    constexpr uint32_t MAX_REPEAT_LEN = 256;
    bool ret = PopStackBuffer<T, TPosition::LCM>(popBuffer);
                                                                                                     ;
    constexpr uint32_t maxBurstSize = (MAX_REPEAT_TIMES * MAX_REPEAT_LEN) / sizeof(T);
    const uint32_t popSize = popBuffer.GetSize() >= maxBurstSize ? maxBurstSize : popBuffer.GetSize();
    const uint32_t round = size / popSize;
    const uint32_t tail = size % popSize;
    const uint32_t roundSize = round != 0 ? popSize : 0;
    Duplicate<T>(popBuffer, value, popSize);
    event_t eventIDVToMTE3 = static_cast<event_t>(GetTPipePtr()->FetchEventID(HardEvent::V_MTE3));
    SetFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    WaitFlag<HardEvent::V_MTE3>(eventIDVToMTE3);
    struct DataCopyExtParams repeatParams;
    repeatParams.blockCount = 1;
    uint32_t comOffset = 0;

    repeatParams.blockLen = static_cast<uint32_t>(roundSize * sizeof(T));
    for (uint32_t index = 0; index < round; ++index) {
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
        comOffset += roundSize;
    }

    repeatParams.blockLen = static_cast<uint32_t>(tail * sizeof(T));
    if (tail != 0) {
        comOffset = round * roundSize;
        DataCopyPad(gmWorkspaceAddr[comOffset], popBuffer, repeatParams);
    }
    PipeBarrier<PIPE_MTE3>();
}

template <typename T>
[aicore] inline __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void InitGlobalMemory(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    InitGlobalMemoryImpl<T>(gmWorkspaceAddr, size, value);
}
}
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/init_global_memory.h" 2




namespace AscendC {
# 45 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/init_global_memory.h"
template <typename T>
[aicore] inline __attribute__((in_pipe("V")))
    __attribute__((out_pipe("MTE3"))) void Fill(GlobalTensor<T> &gmWorkspaceAddr, const uint64_t size, const T value)
{
    InitGlobalMemoryImpl<T>(gmWorkspaceAddr, size, value);
}
# 70 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/adv_api/utils/init_global_memory.h"
}
# 103 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/tikcpp/tikcfw/include/adv_api/kernel_api.h" 2
# 52 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/basic_api/kernel_operator_intf.h" 2
# 22 "/home/lnick/Ascend/cann-8.5.0/x86_64-linux/asc/include/kernel_operator.h" 2
# 11 "/home/lnick/GitHub/Ascend/AscendC/lesson_01/0_introduction/0_helloworld/hello_world.cpp" 2

extern "C" __attribute__((cce_kernel)) [aicore] void hello_world()
{
    AscendC::do { auto __enable_feature_for_compile_printf = 1; auto __enable_feature_for_compile_printfBufSize = 1048576; } while (0);
}

void hello_world_do(uint32_t blockDim, void *stream)
{
    hello_world<<<blockDim, nullptr, stream>>>();
}
